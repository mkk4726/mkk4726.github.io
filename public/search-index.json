[
  {
    "id": "Causal Inference/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects",
    "title": "[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects",
    "date": "2025-07-15",
    "excerpt": "논문 리뷰",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/1712.04912)"
  },
  {
    "id": "Career/2025-07-15-toefl-preparation-guide",
    "title": "TOEFL 100점 돌파를 위한 실전 준비 가이드",
    "date": "2025-07-15",
    "excerpt": "OMSCS 지원을 위한 체계적인 TOEFL 학습 전략",
    "category": "Career",
    "tags": [
      "TOEFL",
      "영어",
      "대학원",
      "OMSCS"
    ],
    "content": "\n# 배경\n\nOMSCS 지원을 위해서는 TOEFL iBT 100점 이상이 필요하다. 단순히 영어 실력만으로는 부족하고, TOEFL이라는 시험의 특성과 전략을 이해해야 목표 점수를 달성할 수 있다. 실제 경험을 바탕으로 효과적인 준비 방법을 정리해본다.\n\n# TOEFL iBT 개요\n\n## 📊 시험 구성\n- **Reading** (54-72분): 3-4개 지문, 각 10문제 → 30점\n- **Listening** (41-57분): 강의 4-6개, 대화 2-3개 → 30점  \n- **Speaking** (17분): 4개 Task → 30점\n- **Writing** (50분): 2개 Task → 30점\n- **총점**: 120점 (목표: 100점 이상)\n\n## 🎯 목표 점수 배분\n100점을 달성하기 위한 현실적인 점수 배분:\n- **Reading**: 26-28점 (상대적으로 높은 점수 확보)\n- **Listening**: 25-27점 (꾸준한 연습으로 안정화)\n- **Speaking**: 23-25점 (한국인에게 가장 어려운 영역)\n- **Writing**: 24-26점 (템플릿 활용으로 안정적 점수)\n\n# 영역별 준비 전략\n\n## 📖 **Reading (26-28점 목표)**\n\n### 핵심 전략\n1. **시간 관리가 핵심**: 지문당 18-20분 배분\n2. **스키밍 + 스캐닝**: 전체 구조 파악 후 세부 내용 찾기\n3. **어휘력 강화**: 학술적 어휘 집중 학습\n\n### 학습 방법\n- **교재**: ETS 공식 문제집, Barron's TOEFL\n- **일일 학습량**: 지문 2-3개 (약 1시간)\n- **어휘**: 매일 학술 어휘 50개씩 암기\n- **실전 연습**: 시간 제한 두고 풀기\n\n### 문제 유형별 공략법\n- **어휘 문제**: 문맥으로 추론, 접두사/어근 활용\n- **세부 내용**: 키워드로 해당 부분 빠르게 찾기\n- **추론 문제**: 명시되지 않은 내용 추론\n- **요약 문제**: 주요 포인트 3개 선택\n\n## 🎧 **Listenin"
  },
  {
    "id": "Causal Inference/review-Towards-R-learner-with-Continuous-Treatments",
    "title": "[Paper Review] Towards R-learner with Continuous Treatments",
    "date": "2025-07-11",
    "excerpt": "연속형 처치를 위한 R-learner를 어떻게 구현할 수 있는지에 대한 논의",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/2208.00872)\n\n# 논문의 배경\n---\n\n[Quasi-Oracle Estimation of Heterogeneous Treatment Effects](https://arxiv.org/pdf/1712.04912) 에서 개인화 처치 효과를 추정하는 방법을 제안했습니다.\n\n<small> * 개인화 처치효과 : 어떤 처치를 했을 때 개인별로 어떤 효과가 있을지 추정한 것</small>\n\n개인화 처치효과를 추정하는 건 인과추론의 가장 핵심적인 문제이며, 이는 다양한 분야에서 통찰을 제공합니다.\n예를 들어 정밀의학에서는 환자별 처치 효과를 추정하여 처치 선택을 결정하고, 교육에서는 학생별 처치 효과를 추정하여 교육 방법을 결정하고, 온라인 마케팅에서는 사용자별 처치 효과를 추정하여 맞춤형 광고를 제공하고, 오프라인 정책 평가에서는 지역별 처치 효과를 추정하여 정책을 결정할 수 있습니다.\n\n> 즉, 개인화 처치효과를 알게 되면 어떤 선택에 대한 근거를 제공할 수 있습니다.\n\n기존 논문에서는 이진 처치의 개인화 처치효과를 추정하는 방법을 이야기했고, 이 논문에서는 이를 확장해서 연속형 처치에 대해서도 이를 적용하기 위한 방법론을 이 논문에서 이야기하고 있습니다.\n\n[[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects](/posts/Causal%20Inference/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects) <- 이 논문에 대한 리뷰는 여기서 확인할 수 있습니다.\n\n> 기존의 방법을 확장할 때 발생하는 문제와 이를 해결한 방법론에 대한 이야기에 집중해서 이 논문을 이해했습니다.\n\n# 논문 내용 정리\n---\n\n## Abstract\n\n> However, extending the R-learner framework from binary to continuo"
  },
  {
    "id": "Data Science/2025-01-10-jupyter-notebook-test",
    "title": "주피터 노트북 완전 테스트 - 셀에서 메타데이터 설정",
    "date": "2025-07-10",
    "excerpt": "첫 번째 셀에서 메타데이터를 설정하는 방법을 보여주는 예제입니다.",
    "category": "Data Science",
    "tags": [
      "jupyter",
      "python",
      "데이터분석",
      "pandas",
      "numpy",
      "matplotlib"
    ],
    "content": "---\ntitle: \"주피터 노트북 완전 테스트 - 셀에서 메타데이터 설정\"\ncategory: \"Data Science\"\ntags: [\"jupyter\", \"python\", \"데이터분석\", \"pandas\", \"numpy\", \"matplotlib\"]\nexcerpt: \"첫 번째 셀에서 메타데이터를 설정하는 방법을 보여주는 예제입니다.\"\n--- # 주피터 노트북 완전 테스트\n\n이것은 블로그에서 주피터 노트북이 완전히 어떻게 표시되는지 테스트하는 예제입니다.\n\n## 메타데이터 설정 방법\n\n**첫 번째 셀에 YAML 형식으로 메타데이터를 설정할 수 있습니다:**\n\n```yaml\n---    \ntitle: \"포스트 제목\"  \ncategory: \"카테고리명\"   \ntags: [\"태그1\", \"태그2\", \"태그3\"]   \nexcerpt: \"포스트 요약\"   \n---  \n```    \n\n## 데이터 분석 예제\n\nPython을 사용한 완전한 데이터 분석을 해보겠습니다.\n\n**주요 내용:**\n- 데이터 생성 및 로드\n- 기본 통계 분석\n- 데이터 시각화 import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"패키지를 성공적으로 import했습니다!\")\n # 샘플 데이터 생성\nnp.random.seed(42)\ndata = {\n    \"x\": np.random.randn(100),\n    \"y\": np.random.randn(100) * 2 + 5\n}\ndf = pd.DataFrame(data)\nprint(\"샘플 데이터를 생성했습니다.\")\ndf.head()\n # 기본 통계량 확인\nprint(\"기본 통계량:\")\ndf.describe()\n # 결론\n\n이 예제에서는 다음을 확인했습니다:\n\n1. **데이터 생성**: NumPy를 사용하여 랜덤 데이터를 생성했습니다\n2. **기본 분석**: Pandas를 사용하여 기본 통계량을 확인했습니다\n3. **코드 실행**: 주피터 노트북의 코드 셀과 출력이 잘 표시되는지 "
  },
  {
    "id": "Causal Inference/why-we-need-causal-inference",
    "title": "인과추론이 필요한 이유",
    "date": "2025-07-10",
    "excerpt": "기존 연구의 한계점에 대해 이야기하고, 인과추론은 이를 어떻게 해결할 수 있는지에 대한 이야기",
    "category": "Causal Inference",
    "tags": [
      "ICL-Lenze-sizing"
    ],
    "content": "\n\n# 배경\n---\n렌즈 삽입술에 사용하는 렌즈 크기를 추천하는 연구들은 주로 예측 모델을 고도화하는 방향으로 이루어졌습니다.\n\n\n\n"
  },
  {
    "id": "Causal Inference/review-HETEROGENEOUS-TREATMENT-EFFECTS-ESTIMATION-WHEN-MACHINE-LEARNING-MEETS-MULTIPLE-TREATMENT-REGIME",
    "title": "HETEROGENEOUS TREATMENT EFFECTS ESTIMATION: WHEN MACHINE LEARNING MEETS MULTIPLE TREATMENT REGIME",
    "date": "2025-07-10",
    "excerpt": "HETEROGENEOUS TREATMENT EFFECTS ESTIMATION: WHEN MACHINE LEARNING MEETS MULTIPLE TREATMENT REGIME",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n\n[paper link](https://arxiv.org/pdf/2205.14714v1)"
  },
  {
    "id": "Causal Inference/review-Causal-Effect-Inference-for-Structured-Treatments",
    "title": "Causal Effect Inference for Structured Treatments",
    "date": "2025-07-10",
    "excerpt": "Causal Effect Inference for Structured Treatments",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n\n\n[paper link](https://arxiv.org/pdf/2106.01939)"
  },
  {
    "id": "Causal Inference/residualization-fwl-theorem-test",
    "title": "FWL에 기반한 잔차화 방법론 구현 노트북",
    "date": "2025-07-10",
    "excerpt": "잔차화를 하기 위해 nusiance function을 만들고 결과를 분석하는 노트북",
    "category": "Causal Inference",
    "tags": [
      "residualization",
      "FWL"
    ],
    "content": "---\ntitle: \"FWL에 기반한 잔차화 방법론 구현 노트북\"\ncategory: \"Causal Inference\"\ntags: [\"residualization\", \"FWL\"]\nexcerpt: \"잔차화를 하기 위해 nusiance function을 만들고 결과를 분석하는 노트북\"\n--- # Load Data # from src.dataloader.vault.main import VaultDatasetLoader\n# from src.dataloader.vault.utils import split_df_od_os\n# from src.utils.db import SOFTCRM_DBINFO, get_conn\n\n# import warnings\n# import pandas as pd\n# import numpy as np\n\n# warnings.filterwarnings(\"ignore\")\n\n# # pandas display 옵션 설정 - 모든 열 보이기\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', None)\n\n# loader = VaultDatasetLoader()\n# feature_df, y_df = loader.run(start_date=\"2022-01-01\", end_date=\"2024-12-31\", piol_data_path=\"./data/PIOL렌즈주문서.xlsx\", refresh=False, db=\"crm_emr\")\n\n\n# # y_df의 oper_date와 feature_df의 date를 기준으로 병합\n# merged_df = pd.merge(y_df, feature_df,\n#                     left_on=['cust_num', 'oper_date'],\n#                     right_on=['cust_num', 'date'],\n#           "
  },
  {
    "id": "Career/대학원에대한고민",
    "title": "대학원을 진학해야할까? 하면 어디로?",
    "date": "2025-07-10",
    "excerpt": "나의 다음 선택지는 어디로?",
    "category": "Career",
    "tags": [
      "대학원"
    ],
    "content": "\n\n\n# 커리어 선배형에게 물어본 내용들\n\n\nㅇㅇ 내가 자주보는 괜찮은 ds관련 공고사이트줄테니까, 일단 있는 공고들 쫘-악 훑어봐봐. 그리고 공통적인 키워드들이 있을테니까 그거위주로 생각해보든가. (ex. LLM, RAG, agent, ML serving, triton 등)\n\n- zighang\n- offercent\n- bzpp\n\n"
  },
  {
    "id": "Career/고려대야간대학원",
    "title": "고려대 야간대학원 관련 정리",
    "date": "2025-07-10",
    "excerpt": "기회비용과 기대되는 가치",
    "category": "Career",
    "tags": [
      "대학원"
    ],
    "content": "\n# 배경\n\n제가 원하는 커리어의 방향을 생각해볼 때 대학원에 대한 선택을 늘 고민하게 됩니다.\nAI/ML 리서처나 관련 연구를 하는 일들을 하기 위해서는 최소 석사 이상의 학력을 요구하는 경우가 많기 때문입니다.\n\n# 늦었나?\n\n지금 나이가 만으로 26정도, 내년 전기에 시작해도 29~30에 끝나니까 그렇게 늦은 시기도 아니라고 생각이 듭니다.\n\n그리고 나이가 더 많았더라도 이걸 통해 얻을 수 있는게 분명하다면, 기회비용보다 더 크다고 판단된다면 늦은 시기란 없지 않을까 싶습니다.\n\n\n# 기회비용 (시간과 가격)\n\n학비가 대력 4천정도. 일단 돈이 정말 많이 듭니다.\n아직 3천만원도 모아본 적 없는 제가 이렇게 큰 비용을 감당할 수 있을까? 라는 생각도 듭니다.\n(학자금 대출 받고 조금씩 갚으면 언젠간 갚겠죠...?)\n\n\n그리고 시간도 많이 필요합니다. 퇴근 후에는 온전히 시간을 다 쏟아야 하고 좋은 결과를 위해서는 주말에도 이 부분만 보고 있지 않을까 싶습니다.\n2년 반정도의 시간도 고려해야 합니다.\n\n# 기대되는 가치\n\n제가 원하는 커리어의 방향에 도움이 됩니다.\n\n- 학력\n석사라는 학력은 사실상 필수적이지 않을까 싶습니다.\n사이드 프로젝트를 하면서 만나는 다른 분들을 봐도 다 석사 이상의 학력을 가지고 있습니다.\n\n- 논문\n그리고 좋은 논문을 작성할 수 있어야만 단순 학위에 그치지 않고 진짜 도움이 될 것 같습니다.\n\n# 지원시기와 준비할 것들\n\n올해 후기 지원은 놓쳤고 내년 전기에 지원하지 않을까 싶습니다.\n다른 블로그들을 살펴보니 준비해야하는 건 기본지식(통계, 선형대수 등등)과 연구계획서 정도입니다.\n\n## 기본지식\n\n통계, 선형대수와 같은 질문들을 면접때 받는다고 합니다.\n경쟁률이 약 6:1정도 된다고 하니, 이런 질문에 대한 대답을 미리 철저하게 준비할 필요가 있어보입니다.\n\n\n\n## 연구계획서\n\n이 부분에 대한 고민이 많이 필요합니다.\n> 그래서 어떤 연구를 하고 싶은거지? \n\n이 부분이 많이 비어있어서 꾸준히 채워나가보려고 합니다.\n\n\n# "
  },
  {
    "id": "Career/25-07-10-visuworks-thoughts-of-my-career",
    "title": "나의 현재 커리어 상황에 대한 생각",
    "date": "2025-07-10",
    "excerpt": "나의 현재 커리어에 대한 고찰",
    "category": "Career",
    "tags": [
      "내 커리어는 어디로 가야하나"
    ],
    "content": "\n# 현재 나의 상황\n\n요즘 정말 커리어에 대한 걱정과 고민이 많습니다... ㅎㅎ\n지금 다니고 있는 회사의 도메인에 묶여있진 않을지. 여기서 이룬 성과들이 나의 커리어에 도움이 될지.\n\n사실 생각만하면 정리가 안되는 부분들이 많기 때문에 글로 적어보면서 고민들에 대한 나름의 대답들을 적어보려고 합니다.\n\n저는 산업공학을 전공하고, 6개월정도 부트캠프에서 컴퓨터 비전 부분을 공부하고, 의료 도메인에서 일하고 있습니다.\n지금 다니고 있는 회사(visuworks)는 시력교정병원 (비앤빛 안과)를 주된 고객사로 삼고 있고 이 병원에서 만들어졌다고도 볼 수 있습니다.\n\n여기서 지금까지 한 프로젝트는\n- OCR pipeline 개발\n- 고객 상담용 챗봇 개발\n- 렌즈 사이징 추천 서비스 개발\n\n정도가 있습니다.\n\n# 문제 상황들\n\n제가 생각하는 문제들을 정리해보고 어떻게 해결해볼 수 있을지에 대한 생각들을 정리해보려고 합니다.\n\n## 연봉이 이렇게 적나?\n\n되게 단순하게 일을 일단 시작하는 마음으로 이 회사에 고민없이 입사했는데, 연봉이 생각보다 현저히 적어서 조금 놀랐습니다.\n더 정확히는 연봉이 낮지만 일단 경력을 쌓자는 마음으로 입사했습니다.\n\n8개월정도 일하고 나름 2개의 프로젝트를 성공적으로 끝낸 시점에서, 연봉협상을 했지만 큰 폭으로 오르지는 못했습니다.\n\n> 연봉 테이블이 좋은 회사로 빠르게 옮겨야겠다.\n\n하지만 쉽지 않네요... ㅠ\n지금 4~5개월정도 이직을 준비하고 있는데 대부분의 기업에서 떨어지고 있습니다.\n그냥 이정도 가치를 가진 능력인가?라는 의구심이 들지만 보완해야할 점들을 보완하면서 이직을 준비 중입니다."
  },
  {
    "id": "Career/25-07-10-lunit-cancer-screening",
    "title": "(Seoul) Research Engineer · AutoML ",
    "date": "2025-07-10",
    "excerpt": "루닛 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "루닛 채용공고"
    ],
    "content": "\n# 채용공고\n\n<details>\n<summary>📋 <strong>채용공고 상세정보</strong></summary>\n\n[링크](https://www.linkedin.com/jobs/search/?currentJobId=4261590818&keywords=Lunit%20Cancer%20Screening&origin=JOB_COLLECTION_PAGE_KEYWORD_AUTOCOMPLETE&refresh=true)\n\n- Lunit, a portmanteau of ‘Learning unit,' is a medical AI software company devoted to providing AI-powered total cancer care. Our AI solutions help discover cancer and predict cancer treatment outcomes, achieving timely and individually tailored cancer treatment.\n\n🗨️ About The Team\n\n- AutoML team at Lunit automates AI product development processes to streamline time-consuming tasks and advance cutting-edge AutoML research. By combining engineering expertise with state-of-the-art deep learning techniques, our team plays an important role in accelerating product development for cancer detection and treatment. Join us in our mission to Conquer Cancer Through AI where your contributions will directly influence AI innovations that improve patient outcomes wo"
  },
  {
    "id": "Career/25-07-10-hyundai-autoever",
    "title": "현대 오토에버 MLOps / AI 검색 엔지니어",
    "date": "2025-07-10",
    "excerpt": "현대 오토에버 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "현대 오토에버 채용공고"
    ],
    "content": "\n\n# 채용공고\n\n<details>\n<summary>📋 <strong>채용공고 상세정보</strong></summary>\n\n[Tech] Machine Learning Engineer - MLOps / AI 검색 엔지니어\n- 📃 누구나 ​마음 ​속에 ​이력서 한 ​장은 있으니까, \n- ⚡ 1분 ​컷 ​지원으로 현대오토에버로의 ​여정을 시작하세요. (이력서 ​자율양식)\n- ✅ MLOps ​/ ​AI 검색 엔지니어\n\n🚀 ​합류하실 ​팀을 ​소개해요\n- 언어AI기술팀\n\n💻 합류하시면 함께 ​할 ​업무예요\n- AI 대화형 서비스를 ​위한 ​데이터베이스 ​설계 및 개발\n- AI ​검색 엔진 ​개발 ​및 운영\n- MLOps ​구축 및 ​운영\n\n🔍 ​이런 분과 함께 ​하고 싶어요\n\n- 검색 ​/ 챗봇 관련 모듈 및 서비스 개발 경험 \n- Docker / Kubernetes 활용 개발 및 배포 경험 \n- Python / JAVA 개발 경력 3년 이상 혹은 그에 준하는 실력\n\n🔍 이런 분이라면 더욱 좋아요\n\n- 정보검색/SW공학 관련 석사 이상\n- ELK(Elasticsearch, Kibana, Kubernetes, Kafka) 구성 설계 및 운영 경험자\n- Neo4j / Redis 기반 DB 설계 및 구축 경험자\n- 벡터 DB (Milvus / qdrant / faiss 등) 경험자 \n- ES 플러그인 개발 경험자\n\n⌛ 이렇게 합류해요\n- 서류 접수 → 서류 검토 → 직무역량테스트(코딩 또는 과제테스트) 및 인성검사 → 1차면접 → 2차면접 → 처우협의 및 채용검진 → 최종 합격🎉\n\n📍 만나게 될 근무지는 여기예요\n- 서울 강남\n\n📌 참고해 주세요\n- 채용 시 마감되는 상시 채용 공고로 운영되며, 채용 절차와 일정은 변동될 수 있어요.\n- 사회적 배려 대상자(보훈 취업지원대상자, 장애인)는 관계 법령과 내규에 따라 우대해요.\n- 모집 분야 및 담당 업무에 따라 영어 구술평가, 레퍼런스 체크, 또는 기타 전형이 실시될 수 있어요.\n- 지원자의 경험과 역량을 고려"
  },
  {
    "id": "English Study/07-09-격려하는표현",
    "title": "격려하는 표현",
    "date": "2025-07-09",
    "excerpt": "You did a great job on, I'm happy to see that",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "\n# 오늘 배운거\n\n- You did a great job on the speaking practice.\n\n- I'm happy to see that you're improving.\n- I'm happy to see that you've overcome your fear of English.\n\n- I can tell that she's sad.\n- I could tell that she broke up the her boyfriend.\n- I can tell that you put a lof of thought into this.\n\n\n# 간단한 작문\n\nToday I wanna complement myself.\n\nYou did a great job on the solving problem.\nWorking on company 1년 조금넘게, I solved a lot of problems.\nOn the OCR project, I solve communicate problem and make greak OCR pipeline.\nOn the chatbot project, I made chatbot which 만족하다 고객사`s needs.\n\nAnd then, I'm happy to see that you`ve overcome of 비교하면서 뒤쳐진다고 느끼는 fear.\nWhen I was 21-25, I was just frozen because of fear that 난 뒤쳐졌고 인생은 망했다.\n\nI can tell that your life is great. I 일인분의 삶은 살아간다. And normal life is always wonderful.\n\n# claud-4-sonnet 피드백\n\n> gemini-2-pro보다 나은 듯\n\n## 수정된 버전:\n\nToday I want to compliment myself.\n\nYou did a great job at problem-solving.\nWorking at a company for a little over a ye"
  },
  {
    "id": "Causal Inference/what-is-statistical-bias",
    "title": "통계적인 편향과 이를 제거하기 위한 방법론",
    "date": "2025-07-09",
    "excerpt": "실제 데이터에서 통계적 편향을 확인하고 이를 잔차화를 통해 제거하는 과정",
    "category": "Causal Inference",
    "tags": [
      "편향",
      "통계적",
      "bias"
    ],
    "content": "\n# 배경\n---\n\n인과추론의 기본적인 개념은 통계적 편향 (Statistical Bias)을 제거하는 것입니다.\n인과추론을 더 잘 이해하고 효과를 확인하기 위해서는, 이 통계적 편향을 이해하고 확인하는 과정이 필요하다고 생각합니다.\n\n# 정의\n---\n\n> 통계적 편향(Statistical Bias)은 통계 분석 과정에서 발생하는 체계적인 오류로, 측정이나 추정 과정에서 실제 모집단의 모수(parameter)와 표본 통계량 사이에 일관된 차이가 발생하는 현상을 말합니다.\n\n- 체계적(systematic) 오류란?\n    - 측정 방법이나 분석 방법에 문제가 있어 발생하는 오류\n    - 무작위가 아닌, 일정한 패턴이나 방향성을 가짐\n- 체계적 오류 (Systematic Error) vs 무작위 오류 (Random Error)\n    - 체계적 오류 (Systematic Error):\n        항상 같은 방향으로 발생 (예: 항상 실제값보다 높게 측정)\n        - 측정 횟수를 늘려도 줄어들지 않음\n        - 측정 도구나 방법의 결함에서 비롯됨\n    - 무작위 오류 (Random Error):\n        - 방향이 불규칙함 (때로는 높게, 때로는 낮게)\n        - 측정 횟수를 늘리면 평균적으로 상쇄됨\n        - 우연한 변동에서 비롯됨\n\n\n제가 이해한 개념은, 랜덤 노이즈가 아닌 구조상의 문제가 있어 결과를 확인하는데 오해를 만드는 것이라고 생각합니다.\n\n예를 들어 보면:\n\n- 구조상 문제: 온라인 설문조사로만 의견을 수집 → 디지털 기기 사용이 어려운 고령층 의견 누락되고 젊은 층의 의견만 반영됨\n- 결과의 오해: \"젊은 층의 의견이 전체 의견\"이라고 잘못 해석할 수 있습니다.\n\n\n## 통계적 편향의 종류와 설명\n\n### 1. 선택 편향 (Selection Bias)\n> 연구 대상을 선택하는 과정에서 발생하는 편향으로, 표본이 모집단을 제대로 대표하지 못할 때 발생합니다.\n\n**예시:**\n- 병원 데이터만으로 질병 연구를 할"
  },
  {
    "id": "Reflections/2025-07-08-optimization-pitfall",
    "title": "최적화의 함정에 대하여",
    "date": "2025-07-08",
    "excerpt": "패배주의에서 벗어나자",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "\n# 배경\n\n저는 꽤나 오랜시간 일종의 패배주의에서 벗어나지 못하고 고통스러워하며 시간을 보냈습니다.\n저보다 훨씬 뛰어난 사람들을 보며 나는 저런 길을 걸어오지 않았으니까 안될 것 같다는 생각들에 사로잡혀 있었습니다.\n\n이러한 사고방식을 어떻게 정의할 수 있을지 몰랐는데, \"뉴욕털게\"님의 영상들을 보면서 \"최적화의 함정\"에 빠졌다고 정의할 수 있었습니다.\n\n# 최적화의 함정이란?\n\n인생에는 어떤 최적화된 길이 있고, 이 길을 따라가야하며, 따라가지 못한 나는 패배재자라고 생각하는 것입니다.\n그래서 최적화된 길을 찾으려고 하면서 완벽주의에 빠지게 되는 것입니다.\n\n완벽한 계획에 대해 고민하면서 시간을 보내고, 막상 앞으로 한발자국도 나아가지 못하는 상황에 빠지게 됩니다.\n\n[뉴욕털게님 영상 링크](https://www.youtube.com/watch?v=aB58_Z7ShT4)\n\n>계획이 원대해지면 내 하루는 비참해진다.\n\n가장 인상깊은 말이자, 제 과거 삶을 관통하는 말입니다.\n\n# 뉴욕털게님의 사고방식\n\n뉴욕털게님이 말씀하시는 부분들을 제 삶에 정말 많은 도움이 되고 있습니다.\n여러가지 개념들을 말씀해주시는데, 공통적인 개념은 \"수용의 자세\"가 아닐까 싶습니다.\n이는 제가 좋아하는 \"법륜스님\"의 말씀과 비슷한 점이 많습니다.\n\n과거, 어쩌면 지금도 제가 고통받는 이유를 생각해보면 제가 원하는 이상적인 상을 그려놓고 이것과 비교하면서 제 하루하루를 비참하게 생각했기 때문이 아닐까 싶습니다.\n\n역설적으로 제 삶을 부정하고 원대한 계획을 세울수록 삶은 나아지지 않는 것 같습니다.\n그저 비참한 하루를 보내며 시간을 보내기만 할 뿐입니다.\n\n그래서 이 수용의 자세는 삶을 살아가는데 굉장히 중요한 자세가 아닐까 싶습니다.\n\n지금 내 모습이 내가 상상했던 모습은 아니지만, 충분히 훌륭하다.\n1인분의 삶을 온전히 살아나가고 있고 조금씩 나아가고 있다.\n이렇게 지금 삶을 온전히 수용할 때, 비로소 발전할 수 있는 내가 된다고 생각합니다.\n\n# 나의 삶에 적용해보기\n\n원하는 회"
  },
  {
    "id": "Reflections/2025-07-08-how-to-write-resume",
    "title": "이력서는 어떻게 적어야할까?",
    "date": "2025-07-08",
    "excerpt": "이력서를 적으며 했던 나의 고민들",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "\n# 나의 배경\n\n일한지 1년이 조금 넘는 시점에서 이직을 준비하고 있습니다.\n이력서를 준비하며 나를 어떻게 표현하면 좋을지에 대한 고민들을 하고 있습니다.\n\n모르는 내용을 공부하고 좋은 코드를 작성하고 좋은 모델을 만드는 일들은 어렵지만 그 길이 나름 명확합니다.\n이런 과정을 적어도 5년이상 거쳐왔으니 자신이 있습니다.\n\n하지만 나를 표현하는 부분들은 정말 어렵게 느껴집니다.\n나는 진짜 경쟁력이 있는지에 대한 의구심부터, 내가 했던 일들을 어디서부터 어디까지 설명해야하는지 등등\n\n이런 과정에서 했던 고민들을 하나씩 정리해봅니다.\n\n\n\n# 내 이력서에 대한 피드백\n\n## 사이드프로젝트를 같이하는 개발자분\n> 임팩트가 없다. 이력서를 더 짧게 쓰고 포트폴리오를 풍부하게 가져가는게 좋겠다.   \n\n이 피드백에 동의하는 부분이 많았습니다.\n내가 한 프로젝트의 결과를 수치적으로 표현하는데에만 집중했지, 진짜 어떤 문제를 풀어서 어떤 영향을 줬는지에 대한 내용이 부족한 것 같다고 느꼈습니다.\n특히 포트폴리오를 따로 준비하지 않았던 부분은 꼭 수정이 핗요하다고 느껴집니다.\n\n"
  },
  {
    "id": "Recommendation/2025-07-08-thoughts-on-review-quality",
    "title": "리뷰 노이즈에 대한 고민",
    "date": "2025-07-08",
    "excerpt": "카카오맵 리뷰 데이터의 노이즈들에 대하여",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템",
      "추천시나리오"
    ],
    "content": "\n\n# 배경\n\n데이터과학자로 1년 조금 넘게 일하면서 가장 크기 느끼는 부분은, **데이터의 중요성**입니다.\n모델은 데이터 안에서 패턴을 찾을 뿐, 그 안에 패턴이 없거나 노이즈가 크면 모델은 제 역할을 못하게 된다고 생각합니다.\n\n그래서 데이터를 뜯어보고 그 안의 노이즈를 살펴보는 일은 굉장히 중요한 일입니다.\n\n> 리뷰 데이터에는 어떤 노이즈가 있는지 살펴보고 이를 정리했습니다\n\n*<small>같이 사이즈 프로젝트를 하는 '신보현'님의 분석 결과를 참고해 정리헀습니다.</small>*\n\n# 문제 상황들\n\n여러 노이즈들이 존재하는데 그 중 가장 심각하다고 느끼는 부분들에 대해 정리했습니다.\n\n## 1. 장소(음식점)과 상관없는 이유인 부정적/긍정적인 리뷰들\n\n가장 눈에 띈 부분은 계엄과 그 후의 시위들과 관련된 리뷰들입니다.\n계엄을 찬성하거나 반대할 경우 관련된 사람들이 '댓글 테러'를 하는 경우를 발견했습니다.\n\n그 외에도 제가 다니고 있는 헬스장에 여자 트레이너가 뚱뚱하다는 이유로 부정적인 리뷰를 남기는 경우도 있었습니다.\n\n\n## 2. 마케팅 목적으로 작성된 리뷰들\n\n마케팅을 위해 의도적으로 작성된 리뷰들입니다.\n예전에 알바했던 음식점에서도 이런 마케팅을 진행했었습니다.\n\nAI로 작성해서 문체에 티가 나는 경우라면 다르게 접근할 수도 있겠지만, \n사람이 작성한 듯한 댓글이 많아서 이를 어떻게 거를 수 있는지도 고민할 필요가 있습니다.\n\n\n# 그래서?\n\n이런 부분을 상쇄할만큼 리뷰가 많으면 이게 상쇄가 되는지, 혹은 모델로 이를 극복할 수 있을지.\n데이터를 필터링할 수 있는 부분들을 고민해야할지에 대한 논의가 추가로 필요한 상황입니다.\n\n이 부분들에 대한 내용도 추가로 정리할 예정입니다."
  },
  {
    "id": "Recommendation/2025-07-08-thoughts-on-deciding-scenario",
    "title": "시나리오를 정하기 위한 고민",
    "date": "2025-07-08",
    "excerpt": "추천시스템을 구현하는 사이드 프로젝트를 진행하면서 추천 시나리오는 어떻게 정하면 좋을지에 대한 논의를 정리했습니다.",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템",
      "추천시나리오"
    ],
    "content": "\n# 배경\n\n사이드 프로젝트를 진행하며 추천 서비스를 구현해보고 있습니다.\n구현하면서 필요한 여러 고민 중 하나는 어떤 상황에서 어떻게 추천을 해줄 것인지에 대한 고민입니다.\n\n이를 추천 시나리오라고 정의하고 이를 구체화하는 과정에 있습니다.\n\n# 콜드 유저에 대해 어떻게 추천할 것인가\n\n서비스를 새롭게 만들게 된다면 이 서비스를 사용하는 유저는 콜드 유저일 것입니다.\n이에 대해 어떻게 대처할 수 있을지에 대한 논의가 주된 논의였습니다.\n\n## 1. 인기도 기반 추천 + 필터링\n\n가장 대표적으로 사용되는 방법입니다.\n여기에 추가로 날씨라던지, 위치라던지, 특정 필터링을 붙이는 방식입니다.\n\n## 2. 유저 프로필 완성 (온보딩 기반 추천)\n\n유저에게 정보를 완성하도록 요청하고, 이를 바탕으로 추천을 하는 방식입니다.\n대표적으로 왓챠나 넷플릭스의 경우, 처음에 본인이 좋아하는 영화를 선택하도록 구성하고 그 후 이를 바탕으로 추천을 해줍니다.\n\n이런식으로 유저 정보를 받을 수 있는 서비스에 대한 논의를 했습니다."
  },
  {
    "id": "Recommendation/2025-07-08-cold-start-solution",
    "title": "Cold Start 해결 방법에 대한 고민",
    "date": "2025-07-08",
    "excerpt": "사이드프로젝트에서 진행한 cold start 해결방안과 데이터의 한계점 극복 고민",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템"
    ],
    "content": "\n# 배경\n\n모두의 연구소에서 \"쩝쩝LAB\"이라는 이름으로 진행한 사이드 프로젝트에 대한 정리입니다.\n맛집 추천 시스템을 구현하고 있습니다.\n\n여러 과제 중 Cold Start를 어떻게 해결할지에 대해 논의한 내용들을 정리해봤습니다.\n\n# Cold Start Problem이란?\n\n> 사용자나 아이템에 대한 정보가 없거나 희소한 문제\n\n이 중에서 유저에 대한 cold start 문제를 어떻게 풀지에 대해 고민하고 있습니다.\n\n\n*<small>같이 사이즈 프로젝트를 하는 '이윤선'님의 분석 결과를 참고해 정리헀습니다.</small>*\n\n\n# 기본적인 추천의 컨셉\n\n> Popularity Model에서 Context를 반영하여, 유저가 만족할만한 음식을 추천해주고 싶다.   \n\n콜드 유저에게 인기도 기반 추천을 내주는 것처럼, 계절과 날씨를 고려해 추천을 내주면 좋을 것 같다는 아이디어입니다.\n\n(예시)\n| 상황 | 유저의 생각 | 추천 가능 음식 |\n| --- | --- | --- |\n| 맑고 청명한 날 | “밖에 나가서 먹고 싶어” | 샌드위치, 김밥, 분식 |\n| 흐리고 습한 날 | “뭔가 시원한 게 땡긴다” | 냉면, 물회, 아이스커피 |\n| 비 올 것 같은 날 | “집에 일찍 가고 싶어” | 국물 요리, 칼국수, 해장국 |\n| 겨울철 | “대게가 제철이네?” | 대게찜, 어탕국수, 전골류 |\n\n계졀 날씨 데이터를 모델에 포함시켜야 하는 이유로 3가지를 제시했습니다.\n\n\n- 왜 계절 날씨 데이터를 모델에 포함해야 할까?\n    - 개인화 추천 강화\n        → 동일한 유저도 날씨에 따라 선택이 달라짐\n        → ‘유저 + 날씨’ 조합 기반의 더 똑똑한 추천 가능\n        \n    - 모델의 정밀도 향상\n        → 기존 모델에 컨텍스트 데이터를 추가함으로써 예측 정확도 향상\n    \n    - Cold Start 상황에서도 강력한 보완\n        → 유저 정보가 없을 때도, **그날의 날씨 + 인기 메뉴**로 합리적인 추천 가능\n\n"
  },
  {
    "id": "English Study/07-08-칭찬하는표현",
    "title": "영어공부 정리",
    "date": "2025-07-08",
    "excerpt": "I was really impressed, It couldn`t be any better, I couldn`t have done it without you",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "\n# 오늘 배운거\n\n- I was really impressed\n- I was really impresed with your exhibit\n\n- It couldn`t be any better\n\n- I couldn`t have done it without you\n- I couldn`t have graduate without you\n\n# 이거 사용해서 간단한 작문\n\nToday, I attend side-project 모임.\nWe talked about recommmend system.\nThe topic was, how to delete data noise, how to check '가정', distribute tasks for project.\n\nAfter that 동료 ,who 프로젝트를 같이하는 사람, give me advice.\nBecause nowadays I fall 서류탈락, so I ask them how can I improve my 이력서.\n\nI was really impressed with their advice.\nThey 고민하다 for me. How to improve my resume.\n\nIt couldn`t be any better. I was so 감동받다.\n\nI always appreciate this project.\nThey gave me so many 도움.\nI couldn`t have be better person without them.\n\n# 문장수정 (gemini-2.5-pro)\n\nToday, I attended a side-project meeting.\nWe talked about recommendation systems.\nThe main topics were how to remove noisy data, how to test our assumptions, and how to distribute tasks for the project.\n\nAfter the meeting, a teammate who is working on th"
  },
  {
    "id": "Codility Study/공부방향",
    "title": "코딩테스트 공부 방향",
    "date": "2025-07-08",
    "excerpt": "코딩테스트 공부 방향에 대한 정리",
    "category": "Codility Study",
    "tags": [
      "고민정리"
    ],
    "content": "\n\n\n# 배경\n\n잠깜 공부하다가 내려놓고 다시 공부를 시작하려고 합니다.\n주니어, 혹은 시니어더라도 코딩테스트는 선택이 아닌 필수가 아닐까 싶습니다. (적어도 10년 이하까지는?)\n그래서 꾸준히 공부해보려고 하는데 어떤 방향으로 잡을지에 대한 정리를 해보려고 합니다.\n\n또 사이드 프로젝트를 같이 하는 '신보현'님의 경험을 들어보니, 라이브코테는 필수적으로 보고 난이도가 꽤 있어서 꾸준한 준비가 필요하다고 합니다.\n지금부터라도 꾸준히 공부해보려고 합니다.\n\n# 코테 공부하기 좋은 곳 (GPT 정리)\n> 와 엄청 정리 잘해주네요...\n\n코딩 테스트(코테) 공부에 도움이 되는 좋은 사이트들을 아래에 용도별로 정리해드릴게요. 각 사이트마다 특성이 다르니, 본인의 학습 목적(기초 다지기, 문제 풀이, 실전 대비 등)에 맞춰 선택하시면 됩니다.\n\n---\n\n## ✅ **1. 문제 풀이 중심**\n\n### **백준 온라인 저지 (BOJ)**\n\n* **URL**: [https://www.acmicpc.net/](https://www.acmicpc.net/)\n* **특징**:\n\n  * 국내 코테 준비자들이 가장 많이 사용하는 사이트.\n  * 다양한 알고리즘 분류, 난이도별 문제 제공.\n  * **단계별로 풀어보기**, **문제집 기능**으로 체계적인 학습 가능.\n  * C++, Python, Java 등 다양한 언어 지원.\n\n### **프로그래머스 (Programmers)**\n\n* **URL**: [https://programmers.co.kr/learn/challenges](https://programmers.co.kr/learn/challenges)\n* **특징**:\n\n  * 실제 기업 코딩테스트 문제 수록 (카카오, 네이버 등).\n  * 실전 감각을 기르기에 적합.\n  * **레벨별로 분류**되어 있어서 입문자부터 고급자까지 활용 가능.\n  * Python, JavaScript 등 실무에 많이 쓰는 언어에 최적화.\n\n### **LeetCode**\n\n* **URL**: [ht"
  },
  {
    "id": "Reflections/2025-07-03-importance-of-numbers",
    "title": "수치의 중요성",
    "date": "2025-07-03",
    "excerpt": "수치로 표현하는게 왜 중요한지에 대한 생각 정리",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "\n"
  },
  {
    "id": "Causal Inference/2025-07-02-what-is-fwl",
    "title": "FWL(Frisch-Waugh-Lovell) 정리란?",
    "date": "2025-07-02",
    "excerpt": "통계적 편향을 제거하기 위한 방법론",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "\n## 참고자료\n\n1. 실무로 통하는 인과추론 with python\n2. [네이버블로그_ (선형모형) 04 Frisch-Waugh-Lovell 정리](https://blog.naver.com/dillid94/222482368731), 수식 풀이가 잘되어 있음\n\n# 배경 설명\n\n---\n\n인과추론이란 \"원인 -> 결과\"의 패턴인 \"인과관계\"를 찾는데에 목적이 있습니다.\n이 패턴을 찾는건 어려운 일인데, 그 이유 중 하나는 원인과 결과에 모두 영향을 주는 교란변수 때문입니다.\n교란변수(confounding variable)란 종속변수와 독립변수에 모두 영향을 줘 잘못된 인과관계를 찾도록 만드는 변수를 말합니다.\n\n즉 교란변수가 잘못된 패턴을 찾도록 유도하기 때문입니다.\n\n![confounding variable 예시그림](/post/confounding_bias_예시그림.png)\n\n가장 대표적인 예시로 \"아이스크림 판매량\"과 \"상어 어택횟수\"를 생각해볼 수 있습니다.\n\"기온\"이라는 변수는 \"아이스크림 판매량\"과 \"상어의 공격횟수\"에 모두 영향을 줍니다.\n\n그래서 \"기온\"이 올라가서 \"아이스크림 판매량\"이 증가하고 \"상어의 공격횟수\"가 증가한 것인데, \"아이스크림 판매량\"이 증가해서 \"상어의 공격횟수\"가 증가했다는 생각을 하게 됩니다.\n\n즉 \"교란 변수\"는 변수간의 관계를 잘못 해석할 가능성을 만들게 됩니다.\n따라서 변수간의 관계를 볼 땐 교란변수를 제거해주는게 굉장히 중요합니다.\n그렇다면 다음과 같은 질문을 해볼 수 있습니다.\n\n> 교란변수를 없애야 한다는 건 이해했어, 그러면 이걸 어떻게 없앨 건데?\n\n가장 좋은 건 교란변수를 파악하고 실험 설계를 통해 없애는 것입니다.\n위의 예시에서는 \"기온\"이라는 교란편수를 파악하고, 이를 통제한 후에 두 변수 간의 관계를 살펴볼 수 있습니다.\n\n하지만 교란변수를 파악하더라도 이를 통제할 수 없을 때가 있습니다.\n예를 들어 수술방법(독립변수)과 수술 후 결과(종속변수)의 관계에서는 나이, 생활습관 등의 교란변수가 있습니다.\n\n"
  },
  {
    "id": "Causal Inference/correlation-vs-causation",
    "title": "상관관계와 인과관계의 이해",
    "date": "2025-06-28",
    "excerpt": "상관관계와 인과관계의 차이점을 이해하고, 예측 모델의 한계와 인과추론의 중요성에 대한 이야기",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "\n상관관계와 인과관계를 이해하고 구분하는 건 결과를 해석할 때 중요한 점입니다.\n\n\n상관관계란, 두 변수 사이에 일정한 패턴이나 동반 변동이 존재함을 의미하며, 한 변수가 커질 때 다른 변수가 일정하게 커지거나(양의 상관), 작아지는(음의 상관) 경향을 보이는 통계적 관계를 말합니다. 인과관계란, 한 변수가 변할 때 다른 변수가 그 변화에 의해 직접적·체계적으로 영향을 받아 변화하는 관계를 의미합니다\n\n\n보통 두 변수간의 관계를 살펴볼 때 상관관계를 살펴보곤 합니다. 하지만 상관관계를 인과관계로 해석하지 않도록 주의해야 합니다.\n\n\n예를 들어 아이스크림 판매량과 일일 기온 사이에는 높은 기온일수록 판매량이 증가하는 강한 양의 상관관계가 관찰되지만, 이는 기온이 아이스크림 판매량을 직접 “원인”한다고 단정할 수 없으며, 사람들이 더운 날씨에 밖에 나와 판매량이 늘어나는 등 다양한 제3의 요인이 동시에 작용했을 가능성을 배제할 수 없습니다.\n\n\n예측 모델은 주어진 데이터에서 변수 간의 동시 변동 패턴, 즉 상관관계를 학습해 결과를 예측합니다.\n이 모델의 예측결과는 인과관계를 보장하지 못합니다.\n\n\n\n|  | 가격 | 매출 |\n| --- | --- | --- |\n| 1 | 100 | 1000 |\n| 2 | 150 | 900 |\n| 3 | 600 | 10000 |\n| 4 | 700 | 12000 |\n\n\n\n예를 들어 숙박 가격과 매출이 주어진 데이터에서 가격이 높을수록 매출이 높아 보인다면, 모델은 “가격이 오르면 매출이 오른다”고 예측할 것입니다. \n하지만 이 패턴 뒤에는 “성수기/비수기 여부”라는 숨겨진 외부 요인이 있을 수 있습니다. \n\n\n가격이 비수기에 낮아지고 성수기에 올라가는 동시에 매출도 함께 변했다면, 실제로는 계절성이 매출을 결정짓는 진짜 원인인데, 단순 예측 모델은 이를 구분하지 못합니다. \n따라서 상관관계에 기반한 예측 결과를 인과관계로 오해하면 “매출을 올리려면 가격을 올려야 한다”는 잘못된 결론에 이를 수 있고, 실제 효과가 있는 개입(예: 비수"
  },
  {
    "id": "Causal Inference/what-is-causal-inference",
    "title": "인과추론이란?",
    "date": "2025-06-20",
    "excerpt": "인과추론의 개념에 대해 소개하는 이야기",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "\n“인과추론” (Causal Inference)은 최근에 의료 분야, 마케팅 분야 등 선택에 대한 분석이 필요한 곳에 자주 사용되는 개념입니다. \n\n인과추론은 다음과 같은 질문들에 답을 합니다. \n“마케팅 비용을 늘렸더니 매출이 증가했네, 마케팅 비용을 늘려서 그런건가?” . “환자에게 A라는 약물을 처방했더니 상태가 괜찮아졌어. A 덕분인가?”\n\n이러한 질문들은 전통적으로 대조군과 실험군을 통해 증명되어왔습니다.\n실험하려는 조건 외에 다른 조건은 모두 통제한 후에 실험결과를 비교합니다.\n\n예를 들어 새로 개발한 비료의 효과를 검증하려고 합니다. \n같은 품종의 옥수수 묘목 100포기를 두 그룹으로 나누어, 실험군에는 새 비료를 표준량만큼 투입하고 대조군에는 기존 비료(혹은 비료를 전혀 주지 않음)를 동일한 방식으로 처리합니다. \n이때 물주기, 일조량, 토양 성분, 온도 등 나머지 재배 조건은 두 그룹에서 완전히 동일하게 유지합니다. \n일정 기간 후 두 그룹의 생장 속도, 수확량, 작물 건강 상태를 비교하면, 오로지 “비료 종류”의 차이만이 결과에 영향을 미쳤다고 판단할 수 있게 됩니다.\n\n이러한 접근법은 가장 합리적이지만, 현실 문제에 적용하기엔 어려움이 있습니다.\n마블의 멀티버스 세계관이 아니라면, 2025년 7월 1일에 마케팅 비용을 100만원과 1000만원을 모두 사용할 수 없습니다.\n한 명의 환자에게 렌즈 사이즈를 12.1를 삽입한 후 결과를 확인하고, 렌즈를 뺀 후 12.6를 삽입해 결과를 확인할 수 없습니다.\n즉, 대조군과 실험군을 설정하는데 어려움이 있습니다.\n\n인과추론은 이를 통계적으로 보완하여 결과를 추정하는 방식입니다.\n\n예를 들면, 온라인 쇼핑몰에서 A라는 광고 캠페인이 판매량에 미친 영향을 알고 싶을 때, 실제로는 동일한 고객에게 광고를 보여주지 않은 상태와 보여준 상태를 모두 경험시킬 수 없으므로 인과추론 기법을 활용합니다. \n이때 고객의 연령, 성별, 과거 구매 이력 등 다양한 **고객 특성**을 보고, 광고를 본 그룹과 보지 않은 그"
  },
  {
    "id": "Causal Inference/review-Multi-Study-R-Learner-for-Estimating-Heterogeneous-Treatment-Effects-Across-Studies-Using-Statistical-Machine-Learning",
    "title": "Multi-Study R-Learner for Estimating Heterogeneous Treatment Effects Across Studies Using Statistical Machine Learning",
    "date": "2025-01-16",
    "excerpt": "Multi-Study R-Learner 논문에 대한 리뷰 및 분석",
    "category": "Causal Inference",
    "tags": [
      "R-Learner",
      "Heterogeneous Treatment Effects",
      "Multi-Study",
      "Statistical Machine Learning",
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/2306.01086)"
  },
  {
    "id": "Career/OMSCS관련",
    "title": "OMSCS(Georgia Tech 온라인 CS 석사) 지원 준비 계획",
    "date": "2025-01-15",
    "excerpt": "세계 최고 가성비 CS 석사 프로그램 분석과 지원 전략",
    "category": "Career",
    "tags": [
      "대학원",
      "OMSCS",
      "Georgia Tech",
      "컴퓨터사이언스"
    ],
    "content": "\n# 배경\n\n데이터 사이언스 분야에서 더 깊이 있는 커리어를 쌓기 위해 석사 과정을 고려하던 중, OMSCS(Georgia Tech Online Master of Science in Computer Science)라는 혁신적인 프로그램을 발견했다. 세계 톱급 CS 프로그램을 온라인으로, 그것도 극도로 저렴한 비용으로 이수할 수 있다는 점이 매력적이다.\n\n# OMSCS란?\n\n## 📊 기본 정보\n- **정식명칭**: Georgia Institute of Technology Online Master of Science in Computer Science\n- **개설연도**: 2014년 (Udacity, AT&T와 협력)\n- **총 비용**: 약 $7,000-$8,500 (한화 900만-1,100만원)\n- **학위**: 캠퍼스와 동일한 Georgia Tech MS in Computer Science (온라인 표기 없음)\n- **기간**: 평균 2-3년 (파트타임으로 진행 가능)\n- **과정 구성**: 10개 과정 (30학점)\n- **합격률**: 약 70%\n\n## ✅ 주요 장점\n\n### 1. **압도적인 가성비**\n- 전체 프로그램 비용이 $7,000-$8,500\n- 타 명문대 온캠퍼스 프로그램 대비 1/5~1/10 수준\n- 회사 학비 지원으로 완전 커버 가능\n\n### 2. **명문대 브랜드 + 동등한 학위**\n- Georgia Tech는 CS 분야 세계 8위 (US News 2024)\n- 졸업장에 \"온라인\" 표기 없음\n- 실리콘밸리에서 인정받는 브랜드\n\n### 3. **유연성**\n- 풀타임 직장과 병행 가능\n- 자신의 페이스로 진도 조절\n- 전 세계 어디서나 수강 가능\n\n### 4. **실무 중심 커리큘럼**\n- Machine Learning, AI, Computer Vision 등\n- 현업 엔지니어들이 직접 활용할 수 있는 스킬\n- 프로젝트 기반 학습\n\n## ⚠️ 단점 및 고려사항\n\n### 1. **높은 자기관리 요구**\n- 온라인 특성상 강한 자기 동기부여 필요"
  },
  {
    "id": "Recommendation/2024-07-05-recommendation-system-interest",
    "title": "추천시스템에 관심있는 이유",
    "date": "2024-07-04 23:30:00 +0900",
    "excerpt": "",
    "category": "Recommendation",
    "tags": [
      "Recommendation"
    ],
    "content": "\n잠깐 추천시스템을 공부했었는데, 다시 공부를 시작하면서 프로젝트를 하나씩 쌓아가려고 한다.  \n그 전에 내가 왜 관심이 있는지, 또 어떤 프로젝트를 해보고 싶은지 정리해보려 한다.   \n\n\n## 추천시스템이란?\n\n추천시스템은 검색과 비슷한 목적을 가지고 있다.   \n> 많은 정보 속에서 필요한 정보를 필터링하는 것\n\n\"진짜 많은 정보들, 컨텐츠들이 있는데 유저에게 어떤 것을 보여줘야할까?\"에 대한 답을 하는 것이다.  \n\n## 왜 중요할까?\n\n추천시스템이 필요한 곳은 대표적으로 OTT 회사들이 떠오른다.   \n\n소비자는 언제 구독을 그만둘까?  \n> 당연하게도 더 이상 볼게 없다고 느껴질때 그만둘 것이다.  \n\n전체 컨텐츠에 비해 소비한 컨텐츠는 소수일텐데 왜 볼게 없다고 느껴질까?  \n> 뭘 봐야할지 모르겠어서, 어떤게 내 재밌을지 몰라서.  \n   \n그래서 소비자에게 취향에 맞는 컨텐츠를 꾸준히 노출해줘야하고,   \n그래야 소비자가 떠나지 않도록 만들 수 있다.   \n\n## 어려운 점\n\n내가 생각했을 때 추천시스템을 구성하는데 가장 어려운 것은 소비자의 평가 데이터가 적기 때문인 것 같다.   \n어떤 사람이 평가한게 적으면 당연히 취향을 분석하기 어렵고, 그래서 추천해주기도 어렵다.  \n\n평가 데이터가 많은 사람에게 잘 추천해주는 것도 굉장히 중요하지만,   \n평가 데이터가 부족한 사람에게 어떻게 추천해줄지에 대한 문제를 해결하는게 가장 중요한 것 같다.   \n\n## 해보고 싶은 프로젝트 \n\n요즘 해보려는 프로젝트는 LLM을 이용해서 소비자의 취향을 알아내는 것이다.   \n구체적으로는 Multi-Agent-System으로 추천시스템을 구축해보려고 하고,    \n맨 앞단에 취향을 분석하는 Agent를 구축해 사용해보려고 한다.   \n\n먼저 영화로 시작해서 책과 같은 분야로도 확장해보려고 한다.  \n\n취향을 분석한다는게 굉장히 애매한 부분인 것 같아서 고민이 많이 필요할 것 같다.  \n이게 가능해진다면 많은 산업에서 정말 매력적으로 느끼는 기술이 아닐까?  "
  },
  {
    "id": "Reflections/2024-07-04-interest-concerns",
    "title": "관심사에 대한 고민",
    "date": "2024-07-04 19:49:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "Interest"
    ],
    "content": "\n\n\n## 현재 나의 상황\n\n이제 데이터과학자로 일한지 3개월정도 되었다.  \n아이펠이라는 모두의 연구소에서 운영하는 부트캠프를 진행했었는데, 이때 기업연계프로젝트로 진행했던 기업에 취업을 했다.  \n분야는 안과의료분야이며, 비앤빛 밝은세상 안과로부터 만들어진 스타트업이다.  \n\n이때 진행한 프로젝트는 당뇨병성 망막병증의 병변들을 탐지해내는 모델을 만드는 것이었다.  \n이에 대한 링크는 다음과 같다.  \n- [발표자료](https://github.com/mkk4726/DR-GeuAl/blob/main/%EC%B5%9C%EC%A2%85%EB%B0%9C%ED%91%9C.pdf)\n- [발표링크](https://www.youtube.com/watch?v=ox_jmqZ1V64)\n\n결과적으로는 의료 데이터를 가지고 Segmentation 작업을 수행하고 발전시켜볼 수 있어서 좋았다.  \n또한 이게 어떻게 쓰일 수 있을지에 대한 고민도 해볼 수 있어서 좋았다.  \n\n그렇게 인공지능을 통해 건강에 기여할 수 있겠다는 꿈을 가지고 입사를 하게 되었다.  \n대부분의 스토리가 그러하듯, 나 역시 기대와 많이 다른 회사생활을 하게 되었다.  \n\n## 안과 분야에서의 인공지능 개발현황\n\n의료 분야 중에서 특히 안과에 대해서만 알고, 그래서 안과에 대해서만 한정해서 이야기하려고 한다.  \n\n### 비쥬웍스\n\n내가 일하고 있는 비쥬웍스라는 기업은 내가 느끼기에 가장 성취가 적다.  \n의료인증을 받은 모델도 없고, 받으려면 아직 멀었다.  \n\n자세하게 이야기할 수는 없지만, 회사의 방향도 당장의 수익을 만드는 것에 집중하고 있다.  \n따라서 내가 원했던 부분과는 많이 다르게 일을 하고 있다.  \n\n다른 회사로 이직하면 되는 걸까?   \n선두기업들은 어떻게 하고 있을까?    \n\n### 뷰노\n\n뷰노에서는 안저사진을 통한 질병 분류 모델을 출시하고, 결과에 CAM도 같이 보여준다.   \n(CAM을 통해 모델이 어느 부분을 집중적으로 보고 있는지 '추정'할 수 있음)   \n병원에 있는 의사분께"
  },
  {
    "id": "Tableau/2024-07-01-tableau-python-cloud-upload",
    "title": "태블러 python으로 데이터 cloud 업로드",
    "date": "2024-07-01 12:00:00 +0900",
    "excerpt": "",
    "category": "Tableau",
    "tags": [
      "Tableau",
      "Python"
    ],
    "content": "\n태블러 prep을 이용해 데이터 파이프라인을 구축해 태블러 클라우드에 데이터 웨어하우스를 구축할 수도 있지만,   \n버그도 많고 제한사항이 많다고 느껴져 파이썬에서 처리한 후 업로드하는 것을 선택했습니다.   \n  \n업로드 하기 위해서는 2가지 과정이 필요합니다.   \n1. csv to hyper\n2. hypter to cloud\n\n순서대로 코드를 공유하면서 간단히 설명하겠습니다.   \n\n## 1. csv to hyper\n\n태블러 클라우드에는 hyper 데이터타입이 들어가야해서 정제된 csv 파일을 hyper로 바꿔줘야 합니다.   \ncsv 파일을 바꾸는 것만 다루지만 다른 파일들도 쉽게 응용할 수 있을 것이라 생각합니다.   \n\n3개의 인자를 받는데 csv 경로, hyper 저장할 경로, csv 파일 타입 정의한 객체 경로.   \ncsv로 저장하는 과정에서 맘대로 데이터타입이 바뀌는 문제가 있어서 타입을 정의한 객체도 따로 저장한 후에 불러올 때 참고하도록 했습니다.   \n\nhyper 타입 데이터를 방식은 빈 테이블을 하나 만들고 채워나가는 것입니다.   \n따라서 어떤 테이블을 만들 것인지를 정의해줘야합니다.  \n이를 위해 columns 리스트를 정의해줍니다.   \n\n그 다음으로는 어떤 값을 추가할지 정의해줘야합니다.  \n이게 row 리스트를 정의하는 이유입니다.  \n여기서 주의깊게 봐야할 것은 null값을 처리하는 방식인데 pandas dataframe의 NaN값을 None으로 바꿔서 넣어줘야합니다.   \n그렇지 않으면 데이터타입이 맞지 않다는 오류가 발생합니다.  \n\n```python\ndef csv_to_hyper(csv_file_path, hyper_file_path, dtype_dict_path):\n    # CSV 파일 읽기\n    dtype_dict = load_data(dtype_dict_path)\n    df = pd.read_csv(csv_file_path, dtype=dtype_dict)\n\n    # 테이블 정의 해주기\n    column"
  },
  {
    "id": "Reflections/2024-04-10-job-hunting-important-things",
    "title": "취준할 때 중요한 것",
    "date": "2024-04-10 12:00:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "취준"
    ],
    "content": "\n   \n저는 현재 의료인공지능 회사에서 데이터과학자로 일하고 있습니다.   \n제가 데이터과학자를 준비하면서 가장 중요하게 생각했고, 도움이 됐던 생각을 공유해보려 합니다.    \n\n   \n> 회사와 비슷한 고민을 하고 내가 가진 자원으로 해결해보는 것   \n \n\n## 1. 데이터과학자\n \n데이터과학자란 문제를 정의하고 이걸 데이터로 해결해나가는 사람을 뜻합니다.   \n\n이는 크게 2가지 과정으로 나눠볼 수 있습니다.    \n\n1. 문제를 정의   \n2. 내가 가진 자원과 기술로 해결    \n   \n프로젝트를 하고 이를 포트폴리오를 만들 때는 이 2가지 과정이 꼭 잘 담겨있어야 합니다.    \n> 어떻게 문제를 정의했으며, 이를 해결하기 위한 과정에서 어떤 고민들을 헀는지.    \n\n  \n## 2. 회사\n\n회사는 문제를 정의하고 그걸 해결해나가며 수익을 내는 집단입니다.    \n회사에서 일하는 데이터과학자들은 본인이 정의하거나 운영진에서 정의한 문제를 회사의 자원과 본인의 기술들로 해결해나갑니다.    \n   \n> 데이터과학자를 준비하는 사람과 회사에서 데이터과학자로 일하는 사람의 과정은 완전히 동일하다고 할 수 있습니다.  \n\n## 3. 중요한 부분  \n  \n결국 중요한 것은 나의 과정들과 회사의 과정이 얼마나 겹치는지 입니다.    \n이 교집합이 클수록 본인의 프로젝트 혹은 포트폴리오는 매력적으로 보이게 됩니다.    \n   \n즉, 내가 생각했을 때 이런 문제가 있고 또는 중요하다고 생각한다.    \n그래서 나는 이런 문제를 이렇게 풀어봤다.   \n푸는 과정에서는 이런 어려움이 있었고 이렇게 해결해나갔다.    \n\n이게 포트폴리오에 잘 담겨있어야하며, 취업을 준비할 때 가장 중요한 점이라고 생각합니다.     \n  \n    \n실제로 저도 제가 정의하고 해결했던 문제가 현재 제가 회사에서 해결하고 있는 문제입니다.    \n면접에서도 이걸 가장 좋게 봐주셨고 취업에 가장 도움이 많이 됐다고 생각합니다.   \n   \n만약 제가 다른 분야로 이직을 준비한다면, 위"
  },
  {
    "id": "Reflections/2024-03-12-aifell-review",
    "title": "아이펠 후기",
    "date": "2024-03-12",
    "excerpt": "6개월 동안의 아이펠 후기",
    "category": "Reflections",
    "tags": [
      "아이펠",
      "이런저런 생각들"
    ],
    "content": "\n저는 23.9 ~ 24.2 동안 아이펠 6기 리서치 과정을 수료했고, 회고 겸 후기를 적어보려 합니다.   \n\n![졸업증](/post/아이펠_졸업장.jpeg)\n\n## 1. 신청했을 때의 나의 상황\n\n일단 제가 어떤 상태에서 아이펠을 들었는지 설명드리겠습니다.   \n  \n일단 저는 산업공학을 전공했고, 통계학을 부전공했습니다.  \n학부생때 ML에 관심이 많아서 혼자서 공부했었고, 관련 프로젝트도 진행했습니다.   \nDL 부분은 잘 몰랐는데, 시작하기 1달전에 \"밑바닥부터 시작하는 딥러닝\" 2회독정도 했습니다.   \n  \n제 배경지식은 통계학 + 컴퓨터공학 + DL 조금. 정도로 정리할 수 있겠습니다.   \n\n \n## 2. 내가 생각하는 난이도\n \n처음 1~2달 정도는 그 전에 공부했던 것들이라 복습하는 느낌으로 공부했고, 나머지는 개념정도만 아는 상태에서 공부를 했습니다. 그래서 따라가는데 어려움은 없었습니다.   \n\n다만, 양이 워낙 많아서 평균적으로 추가공부를 3~4시간, 적어도 1시간씩은 했던 것 같습니다.    \n\n배경지식이 전혀 없는 분이 리서치 과정을 수료하려면 적어도 매일 6시간씩은 추가공부해야할 것 같습니다.   \n\n \n\n## 3. 공부방식   \n \n선생님이 있는게 아닌, 공부자료가 주어지고 같이 공부하는 사람들과 함께 알아가는 방식입니다.   \n공부의 방향을 \"퍼실\" 분들이 잡아주고, 모르는 부분을 해소해주는 방식으로 도움을 주십니다.   \n\n   \n이게 보통 스터디 같은 경우 나보다 더 잘아는 사람과 해야지 얻는게 많을 것이라 생각하는데, 경험해보니 반대였던 것 같습니다.     \n\n> 왜 선생님이 가장 많이 배운다고 하잖아요, 저는 배경지식이 있어 \"선생님\" 역할을 맡을 때가 많았는데 정말 많이 배웠습니다.\n\n어느정도 알던 개념들이 설명을 하면서 완성되곤 했습니다.   \n특히 아에 모르던 개념들도 어떻게 해야 빠르게 습득하고 하나의 지식으로 만들 수 있는지 많이 배웠습니다.    \n  \n또 토의, 토론을 정말 많이 했는데, 이를 통해 제 지"
  },
  {
    "id": "Tutorial/nextjs-blog-setup",
    "title": "Building a Blog with Next.js: A Complete Guide",
    "date": "2024-01-20",
    "excerpt": "Learn how to build a modern, fast, and SEO-friendly blog using Next.js, TypeScript, and Tailwind CSS.",
    "category": "Tutorial",
    "tags": [
      "nextjs",
      "typescript",
      "tailwind",
      "tutorial",
      "blog"
    ],
    "content": "\n# Building a Blog with Next.js: A Complete Guide\n\nIn this post, I'll walk you through how I built this blog using Next.js, TypeScript, and Tailwind CSS. It's a modern, fast, and SEO-friendly solution that's perfect for personal blogs.\n\n## Why Next.js for Blogging?\n\nNext.js is an excellent choice for building blogs because it offers:\n\n- **Static Site Generation (SSG)**: Perfect for blogs with pre-rendered pages\n- **Server-Side Rendering (SSR)**: Great for dynamic content\n- **Built-in Routing**: File-based routing makes it simple\n- **TypeScript Support**: Type safety out of the box\n- **Excellent Performance**: Optimized by default\n\n## Project Structure\n\nHere's how I organized the project:\n\n```\nsrc/\n├── app/                 # Next.js 13+ app directory\n│   ├── page.tsx        # Home page\n│   ├── posts/          # Blog posts routes\n│   └── about/          # About page\n├── components/         # Reusable components\n├── lib/               # Utility functions\n└── content/           # Markdown "
  },
  {
    "id": "template",
    "title": "여기에 포스트 제목을 입력하세요",
    "date": "2024-01-01",
    "excerpt": "포스트에 대한 간단한 설명을 여기에 작성하세요 (선택사항)",
    "category": "카테고리명",
    "tags": [
      "태그1",
      "태그2",
      "태그3"
    ],
    "content": "\n# 포스트 제목\n\n포스트의 내용을 여기에 작성하세요.\n\n## 소제목\n\n### 더 작은 소제목\n\n일반 텍스트 내용입니다.\n\n> 인용구는 이렇게 작성할 수 있습니다.\n\n**굵은 글씨**와 *기울임 글씨*를 사용할 수 있습니다.\n\n- 목록 아이템 1\n- 목록 아이템 2\n- 목록 아이템 3\n\n1. 번호 목록 1\n2. 번호 목록 2\n3. 번호 목록 3\n\n```python\n# 코드 블록 예시\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n![이미지 설명](/이미지파일명.png)\n\n---\n\n추가 내용이 있다면 여기에 작성하세요. "
  },
  {
    "id": "Causal Inference/review-Towards-Optimal Doubly-Robust-Estimation-of-Heterogeneous-Causal-Effects",
    "title": "Towards Optimal Doubly-Robust Estimation of Heterogeneous Causal Effects",
    "date": "2024-01-01",
    "excerpt": "Doubly-Robust Estimation of Heterogeneous Causal Effects",
    "category": "Causal Inference",
    "tags": [
      "paper review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/2004.14497)\n\n\n\n\n"
  }
]