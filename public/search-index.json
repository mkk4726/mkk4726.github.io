[
  {
    "id": "English Study/SPEAK/07-23-뭐뭐하긴해도",
    "title": "뭐뭐하긴 해도",
    "date": "2025-07-23",
    "excerpt": "뭐뭐하긴 해도 쓰는 표현들",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "\n# 배운 것들\n\nAlthough, it`s a bit pricey, the bar has an amazing view at night.\n\nAlthough there is a long wait, it has the best bubble tea in town.\n\n\nEven though, I`m not as big fan of the movie, I still enjoyed it.\n\nI like chocolate, even though I don`t like sweet.\n\nI love burgers. It`s not healthy though.\n\nThe coat is too expensive. but it`s pretty though.\n\nFair enough : 인정\n\n# 간단한 작문\n\nAlthough seoul is confused, full of people, it`s a great place to live.\nMany 편의시설 in here.\n\nEven though I`m not a big fan of anime, I love 귀멸의 칼날.\n\nI really love burgers. It`s not healthy thought.\n\n\n# 더 자연스러운 작문 & 피드백\n\n1. Although Seoul is confusing and crowded, it's a great place to live.  \n   → 피드백: \"confused\" 대신 \"confusing and crowded\"로 자연스럽게 수정. \"Many 편의시설 in here.\"는 \"There are many convenient facilities here.\"로 바꾸면 좋아요.\n\n2. Even though I'm not a big fan of anime, I love Demon Slayer.  \n   → 피드백: \"귀멸의 칼날\"은 영어 제목 \"Demon Slayer\"로 쓰면 자연스러워요.\n\n3. I really love burgers. It's not healthy though.  \n   → 피드백: "
  },
  {
    "id": "Data Science/law-of-total-expectaion",
    "title": "Law of total expectation",
    "date": "2025-07-23",
    "excerpt": "전체기대법칙에 대해 정리",
    "category": "Data Science",
    "tags": [
      "math"
    ],
    "content": "\n### 전체기대법칙, 진짜 생활 버전\n\n> **핵심 아이디어**\n> “전체 평균 = ‘각 상황별 평균’들을 **확률로 가중한 평균**”\n\n---\n\n#### 1. 마트 계산대 이야기로 직관 잡기\n\n* 마트에는 \\*\\*빠른 계산대(셀프)\\*\\*와 **일반 계산대(직원)** 두 줄이 있다.\n* 셀프 계산대를 선택할 확률 70 %, 일반 계산대는 30 %.\n* 셀프에서 걸리는 평균 시간은 2분, 일반 계산대는 6분.\n* “전체 줄 서기 평균 시간”은?\n\n  * **조건부 평균**: 셀프 = 2분, 일반 = 6분\n  * **가중 평균**: $2 \\times 0.7 + 6 \\times 0.3 = 3.2$ 분\n  * 3.2 분이 **Law of Total Expectation** 결과다.  ([Wikipedia][1])\n\n> 한 줄 요약 ― **“상황별 평균을 구한 뒤, 각 상황이 일어날 확률로 다시 한 번 평균을 내면 전체 평균이 된다.”**\n\n---\n\n#### 2. 공식을 뜯어보면\n\n$$\n\\mathbb{E}[X] = \\mathbb{E}\\big[\\;\\mathbb{E}[X \\mid Y]\\;\\big]\n$$\n\n* $X$: 알고 싶은 대상(줄 서는 시간).\n* $Y$: 상황을 나누는 열쇠(선택한 계산대).\n* $\\mathbb{E}[X\\mid Y]$: **“상황 Y 가 주어졌을 때 평균”** — 셀프면 2분, 일반이면 6분.\n* $\\mathbb{E}[\\cdot]$: 그 값들을 **다시 평균** — 확률 0.7, 0.3으로 가중. ([Wikipedia][1])\n\n---\n\n#### 3. ‘3단 요리법’으로 기억하기\n\n| 단계        | 해야 할 일                     | 결과        |\n| --------- | -------------------------- | --------- |\n| ① 상황 나누기  | 문제를 쉽게 쪼갤 변수·사건 $Y$ 고르기    | ‘셀프 / 일반’ |\n| ② 상황별 기대값 | $\\mathbb{E}[X\\mid Y=y]$ 계산 |"
  },
  {
    "id": "Data Science/2025-01-10-jupyter-notebook-test",
    "title": "주피터 노트북 완전 테스트 - 셀에서 메타데이터 설정",
    "date": "2025-07-23",
    "excerpt": "첫 번째 셀에서 메타데이터를 설정하는 방법을 보여주는 예제입니다.",
    "category": "Data Science",
    "tags": [
      "jupyter",
      "python",
      "데이터분석",
      "pandas",
      "numpy",
      "matplotlib"
    ],
    "content": "---\ntitle: \"주피터 노트북 완전 테스트 - 셀에서 메타데이터 설정\"\ncategory: \"Data Science\"\ntags: [\"jupyter\", \"python\", \"데이터분석\", \"pandas\", \"numpy\", \"matplotlib\"]\nexcerpt: \"첫 번째 셀에서 메타데이터를 설정하는 방법을 보여주는 예제입니다.\"\n--- # 주피터 노트북 완전 테스트\n\n이것은 블로그에서 주피터 노트북이 완전히 어떻게 표시되는지 테스트하는 예제입니다.\n\n## 메타데이터 설정 방법\n\n**첫 번째 셀에 YAML 형식으로 메타데이터를 설정할 수 있습니다:**\n\n```yaml\n---    \ntitle: \"포스트 제목\"  \ncategory: \"카테고리명\"   \ntags: [\"태그1\", \"태그2\", \"태그3\"]   \nexcerpt: \"포스트 요약\"   \n---  \n```    \n\n## 데이터 분석 예제\n\nPython을 사용한 완전한 데이터 분석을 해보겠습니다.\n\n**주요 내용:**\n- 데이터 생성 및 로드\n- 기본 통계 분석\n- 데이터 시각화 import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nprint(\"패키지를 성공적으로 import했습니다!\")\n # 샘플 데이터 생성\nnp.random.seed(42)\ndata = {\n    \"x\": np.random.randn(100),\n    \"y\": np.random.randn(100) * 2 + 5\n}\ndf = pd.DataFrame(data)\nprint(\"샘플 데이터를 생성했습니다.\")\ndf.head()\n # 기본 통계량 확인\nprint(\"기본 통계량:\")\ndf.describe()\n # 결론\n\n이 예제에서는 다음을 확인했습니다:\n\n1. **데이터 생성**: NumPy를 사용하여 랜덤 데이터를 생성했습니다\n2. **기본 분석**: Pandas를 사용하여 기본 통계량을 확인했습니다\n3. **코드 실행**: 주피터 노트북의 코드 셀과 출력이 잘 표시되는지 "
  },
  {
    "id": "Causal Inference/what-is-r-learner",
    "title": "R-learner란?",
    "date": "2025-07-23",
    "excerpt": "R-learner의 개념에 대해 정리",
    "category": "Causal Inference",
    "tags": [
      "R-learner",
      "인과추론"
    ],
    "content": "\nR-learner는 CATE estimation에 사용되는 meta-learner 중 하나입니다.\n이에 대해 하나씩 정리헀습니다.\n\n# 1. CATE estimation이란?\n---\n\n## 1.1 ATE란?\n\n> ATE(Average Treatment Effect)는 전체 집단에서 처치(예: 신약, 정책 등)가 미치는 평균적인 인과 효과를 의미합니다.\n\n$$\n\\text{ATE} = \\mathbb{E}[Y^{(t)} - Y^{(0)}]\n$$\n여기서 $$Y^{(t)}$$은 처치를 받았을 때의 잠재적 결과 (Potential Outcome)이고, $$Y^{(0)}$$은 처치를 받지 않았을 때의 잠재적 결과입니다.\n\n즉 처치를 받았을 때의 평균적인 인과효과를 의미합니다.\n\n\n## 1.2 CATE란?\n\n> CATE(Conditional Average Treatment Effect)는 특정 조건에서 처치가 미치는 평균적인 인과 효과를 의미합니다.\n\n$$\n\\text{CATE} = \\tau(x) = \\mathbb{E}[Y^{(t)} - Y^{(0)} | X = x]\n$$\n여기서 $$X$$는 조건을 의미하고, $$Y^{(t)}$$는 처치를 받았을 때의 잠재적 결과, $$Y^{(0)}$$는 처치를 받지 않았을 때의 잠재적 결과를 의미합니다.\n\nATE를 통해 처치의 평균적인 인과효과(처치가 결과에 미치는 영향)을 추정했습니다.\n다만 개개인별로 인과효과가 다르다는 것은 직관적인데요. 이를 위해 추정하는 값이 CATE 입니다.\n\n> Estimating heteronenous treatment effects is fundamental in causal inference and provides insights into various fields.\n>\n> -> Towards R-learner with Continuous Treatment\n\n개인별 처치효과를 추정하는 일은 다양한 분야에서 핵심적인 역할을 합니다.\n학생별 교육효과를 추정해 개인화된 교육 프로그램을 실행하거나, 환자별 처치효"
  },
  {
    "id": "Causal Inference/residualization-fwl-theorem-test",
    "title": "FWL에 기반한 잔차화 방법론 구현 노트북",
    "date": "2025-07-23",
    "excerpt": "잔차화를 하기 위해 nusiance function을 만들고 결과를 분석하는 노트북",
    "category": "Causal Inference",
    "tags": [
      "residualization",
      "FWL"
    ],
    "content": "---\ntitle: \"FWL에 기반한 잔차화 방법론 구현 노트북\"\ncategory: \"Causal Inference\"\ntags: [\"residualization\", \"FWL\"]\nexcerpt: \"잔차화를 하기 위해 nusiance function을 만들고 결과를 분석하는 노트북\"\n--- # Load Data # from src.dataloader.vault.main import VaultDatasetLoader\n# from src.dataloader.vault.utils import split_df_od_os\n# from src.utils.db import SOFTCRM_DBINFO, get_conn\n\n# import warnings\n# import pandas as pd\n# import numpy as np\n\n# warnings.filterwarnings(\"ignore\")\n\n# # pandas display 옵션 설정 - 모든 열 보이기\n# pd.set_option('display.max_columns', None)\n# pd.set_option('display.width', None)\n# pd.set_option('display.max_colwidth', None)\n\n# loader = VaultDatasetLoader()\n# feature_df, y_df = loader.run(start_date=\"2022-01-01\", end_date=\"2024-12-31\", piol_data_path=\"./data/PIOL렌즈주문서.xlsx\", refresh=False, db=\"crm_emr\")\n\n\n# # y_df의 oper_date와 feature_df의 date를 기준으로 병합\n# merged_df = pd.merge(y_df, feature_df,\n#                     left_on=['cust_num', 'oper_date'],\n#                     right_on=['cust_num', 'date'],\n#           "
  },
  {
    "id": "Causal Inference/how-can-evaluate-casual-models",
    "title": "인과모델 평가 방법",
    "date": "2025-07-23",
    "excerpt": "인과모델 평가 방법에 대해 정리",
    "category": "Causal Inference",
    "tags": [
      "인과추론",
      "metrics"
    ],
    "content": "\n"
  },
  {
    "id": "English Study/SPEAK/07-22-선택의상황",
    "title": "선택의 상황",
    "date": "2025-07-22",
    "excerpt": "선택의 상황에서 쓰는 표현들",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "\n# 배운 것들\n\n## 1. best-thing\n~의 가장 좋은 점은 ~이다.\n\nThe best thing about iPhone is its camera.\n\nbest-thing -> 한번에 일기\n\naBOUT \n\n## 2. when it comes to\n~에 대해서는, ~가 가장 좋을 것이다.\n\nWhen it comes to online shopping, Amazon might be the best option.\n\n# 간단한 작문\n\nWhen it comes to smart-watch, Galaxy Fit is the best option.\nThe best thing about Galaxy Fit is price.\nIt is ver cheap, but have good design and good quality.\n\n---\n\n## 더 자연스러운 버전 & 피드백\n\n### 자연스러운 버전\n\nWhen it comes to smartwatches, the Galaxy Fit is the best option.  \nThe best thing about the Galaxy Fit is its price.  \nIt is very cheap, but it also has a good design and high quality.\n\n### 피드백\n\n1. **When it comes to smart-watch, Galaxy Fit is the best option.**  \n   **수정:** When it comes to smartwatches, the Galaxy Fit is the best option.  \n   **피드백:** \"smart-watch\"는 보통 복수형 \"smartwatches\"로 쓰는 것이 자연스럽고, \"the\"를 붙여 특정 제품을 지칭하는 것이 더 명확합니다.\n\n2. **The best thing about Galaxy Fit is price.**  \n   **수정:** The best thing about the Galaxy Fit is its price.  "
  },
  {
    "id": "Data Science/parametric-vs-non-parametric",
    "title": "Parametric vs Non-Parametric, GBDT는 왜 non-parametric일까?",
    "date": "2025-07-22",
    "excerpt": "Parametric과 Non-Parametric에 대한 간단한 설명",
    "category": "Data Science",
    "tags": [
      "parametric",
      "non-parametric",
      "machine-learning",
      "statistics"
    ],
    "content": "\n\n# Parametric이란?\n\n| 영어                        | 한국어         | 설명                               |\n| ------------------------- | ----------- | -------------------------------- |\n| **parametric model**      | **모수적 모델**  | 고정된 수의 파라미터(모수)를 갖는 모델           |\n| **parametric assumption** | **모수적 가정**  | 데이터가 특정한 분포 형태(예: 정규분포)를 따른다는 가정 |\n| **non-parametric model**  | **비모수적 모델** | 파라미터 수나 함수 형태를 가정하지 않는 유연한 모델    |\n\n데이터에서 패턴을 근사해내는게 Machine Learning의 핵심이라고 할 수 있습니다.\n이때 데이터가 특정한 함수 형태를 따른다고 가정하거나, 모델의 구조(예: 선형 회귀처럼)가 고정되어 있는 경우를 모수적(parametric) 모델이라고 합니다.\n\n# Parametric vs Non-Parametric\n\n| 구분                  | Parametric     | Non-parametric           |\n| ------------------- | -------------- | ------------------------ |\n| **모델 구조**           | 사전에 고정됨        | 유연하고 데이터 기반              |\n| **모수(parameter) 수** | 고정             | 데이터가 많아지면 증가함            |\n| **복잡도 제어**          | 파라미터 조정        | 모델 자체의 구조 변화로 조정         |\n| **예시**              | 선형 회귀, 로지스틱 회귀 | GBDT, KNN, 랜덤포레스트, 커널 회귀 |\n\n반대"
  },
  {
    "id": "Data Science/bias-variance",
    "title": "Bias-Variance Tradeoff",
    "date": "2025-07-22",
    "excerpt": "Bias-Variance Tradeoff에 대한 간단한 설명",
    "category": "Data Science",
    "tags": [
      "bias-variance",
      "machine-learning",
      "statistics"
    ],
    "content": "\n# Bias-Variance Tradeoff\n\n머신러닝에서 모델의 성능을 평가할 때 가장 중요한 개념 중 하나가 **Bias-Variance Tradeoff**입니다. 이는 모델의 복잡도와 일반화 성능 사이의 균형을 설명하는 핵심 개념입니다.\n\n## 1. 기본 개념\n\n### Bias (편향)\n> 학습된 모델의 예측값 평균과 실제 값 간의 차이\n\n- **정의**: 모델이 실제 데이터 생성 과정을 얼마나 잘 근사하는지를 나타내는 지표\n- **높은 Bias**: 모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못함\n- **낮은 Bias**: 모델이 복잡해서 데이터의 패턴을 잘 학습할 수 있음\n\n### Variance (분산)\n> 예측값이 얼마나 흩어졌는지를 나타내는 지표\n\n- **정의**: 모델이 훈련 데이터의 작은 변화에 얼마나 민감한지를 나타내는 지표\n- **높은 Variance**: 모델이 훈련 데이터에 과도하게 적합되어 새로운 데이터에 대해 성능이 떨어짐\n- **낮은 Variance**: 모델이 훈련 데이터의 노이즈에 덜 민감함\n\n## 2. 수학적 표현\n\n### 예측 오차의 분해\n\n$$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$\n\n여기서:\n- $y$: 실제 값\n- $\\hat{f}(x)$: 모델의 예측값\n- $\\text{Bias}^2 = E[(\\hat{f}(x) - f(x))^2]$: 편향의 제곱\n- $\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$: 분산\n- $\\text{Irreducible Error}$: 데이터 자체의 노이즈로 인한 오차\n\n### 편향과 분산의 계산\n\n편향과 분산은 다음과 같이 계산됩니다:\n\n$$\\text{Bias} = E[\\hat{f}(x)] - f(x)$$\n\n$$\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$\n\n## 3. Bias"
  },
  {
    "id": "Causal Inference/Paper Review/Double-Debiased-Machine-Learning-for-Treatment-and-Structural-Parameters",
    "title": "[Paper Review] Double Debiased Machine Learning for Treatment and Structural Parameters",
    "date": "2025-07-22",
    "excerpt": "Double Debiased Machine Learning for Treatment and Structural Parameters",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/1608.00060)\n\n"
  },
  {
    "id": "English Study/SPEAK/07-21-고민할때",
    "title": "고민하는 표현들",
    "date": "2025-07-21",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "\nOn one hand, the Note has a really big screen.\n\nOn one hand, I haven`t had Korean food in a while\n\nOn one hand, taking a cab would be really convenient\n\nOn the other hand, the new iPhone has a really good camera.\n\nOn the other hand, I`m craving fried chicken.\n\nOn the other hand, I can walk there in 15 minute.\n\nWhether I buy the Note or the new iPhone, I should buy a phone tommorow.\n\n"
  },
  {
    "id": "English Study/SPEAK/07-20-조언하는표현Review",
    "title": "Review day",
    "date": "2025-07-20",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "Review day"
    ],
    "content": "\n# 오늘 배운거\n\nTry to ~~\n\n- Try to cut down on drinking.\n- Try not to skip breakfast.\n\nMake sure to ~\n- \n- Make sure not to eat too much junk foods.\n\nI`m worried about ~\n- I`m worried about my love life.\n- I`m little worried about my bills.\n\nBe on the lookout for \n- Be on the lookout for pickpockets.\n\nBe careful with ~\n- Be careful with your phone.\n- Be careful not to burn yourself.\n- Be careful not to catch a cold.\n- Be careful not to cut yourself.\n\nBy the way\n- By the way, I was the blind date?\n\nWell the things is\n- Well the thing is I`m not really attracted to her.\n\n# 간단한 작문 (오늘 배운거 사용도 해보면서 작문하는 연습)\n\nTime is so fast. 정신을 차려보면 시간은 한참 지나있다.\nAcheving something like, get a good english, playing guitar well, 좋은 회사에 취직하는 것 ,... are needs many time in the days.\nIf you cant focus on today looking farway and beding nervous, I never acheive somethings that needs days.\n\nYou should know that today is so precious. You can do everthing, but you couldn`t do nothing if you don't move.\nSo, make sure to"
  },
  {
    "id": "Causal Inference/Paper Review/review-Towards-Optimal Doubly-Robust-Estimation-of-Heterogeneous-Causal-Effects",
    "title": "[Paper Review] Towards Optimal Doubly-Robust Estimation of Heterogeneous Causal Effects",
    "date": "2025-07-20",
    "excerpt": "Doubly-Robust Estimation of Heterogeneous Causal Effects 논문 리뷰",
    "category": "Causal Inference",
    "tags": [
      "paper review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/2004.14497)\n\n# Abstact\n\n\n\n\n\n"
  },
  {
    "id": "Book/만들어진 신 (The God Delusion)/대단히종교적인불신자",
    "title": "대단히 종교적인 불신자",
    "date": "2025-07-20",
    "excerpt": "나는 인격신을 상상하려는 시도는 하지 않는다. 신은 우리의 불충분한 감각으로 세계의 구조를 이해할 수 있도록 함으로써 경외심을 품게 하는 정도면 충분하다. <알베르트 아인슈타인>",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "만들어진 신"
    ],
    "content": "\n# 과학자들이 말하는 신은 뭘까?\n\n> 나는 인격신을 상상하려는 시도는 하지 않는다. 신은 우리의 불충분한 감각으로 세계의 구조를 이해할 수 있도록 함으로써 경외심을 품게 하는 정도면 충분하다. \n> \n> <알베르트 아인슈타인>\n\n과학자들이 말하는 신은 무엇일까?\n같은 신이라는 단어로 표현되고 있지만 이를 구분해서 이해할 필요가 있을 것 같다.\n아이슈타인은 신을 말하며 우주의 합리성과 질서에 대한 경외심을 들어내고 있다.\n예수나 부처와 같은 신을 의미하는게 아닌.\n\n> 훨씬 더 불행한 혼란은 아인슈타인식의 종교와 초자연적인 종교를 구분하지 못함으로써 빚어진다. 알베르트 아인슈타인(Albert Einstein)은 종종 신이라는 명칭을 사용함으로써(그런 무신론자 과학자가 그만은 아니다), 그런 유명한 과학자가 자신들의 편이기를 너무나 바라는 초자연주의자들의 오해를 자초하곤 했다.\n>\n> <만들어진 신>, 리처드 도킨스 - 밀리의 서재\n\n과학자들이 신을 이야기한다고 이들이 신을 믿는다고 생각하면 안될 듯 하다.\n\n> 우리 시대의 위대한 과학자들이 종교적인 말을 하는 듯이 보여도 그들의 신념을 더 깊이 파고 들어가면 대개 그렇지 않다는 사실이 드러난다.\n> \n> <만들어진 신>, 리처드 도킨스 - 밀리의 서재\n\n\n# 다른 사람들은 몰라도 과학자들 중 신을 믿는 사람들은 왜 있는걸까?\n\n고등학교때 친구를 따라 교회를 1~2년도 다녔던 적이 있다.\n그때도 신이라는 존재를 어떻게 믿을 수 있는지에 대해 의문이 들어 주위에 물어봤을 때, 믿음을 믿는다는 이야기를 들었다.\n즉 어떤 근거가 있어서 믿는다기 보다는 믿음 자체를 믿는다는 것이다.\n그래서 믿을 수 없다는 사람에게는 믿음이 부족하다, 성경을 더 공부하다보면 믿게 된다는 이야기를 한다.\n\n나는 이런 믿음이라는게 굉장히 위험하다고 생각한다.\n넷플릭스 시리즈의 \"나는 신이다\"에서 소개되는 JMS 같은 사이비 종교들의 논리와 교회가 다를게 뭔가?\n믿음을 우선시 하는 삶의 태도가 나는 도저히 납득이 안간다.\n\n과학을 하는 사"
  },
  {
    "id": "Current Affairs/중국문화대혁명과홍위병",
    "title": "중국 문화대혁명과 홍위병",
    "date": "2025-07-17",
    "excerpt": "문화대혁명과 홍위병에 대한 내용들",
    "category": "Current Affairs",
    "tags": [
      "중국",
      "문화대혁명",
      "홍위병"
    ],
    "content": "\n# 참고 자료\n\n- [리뷰엉이 - 삼체에서 가장 중요한 사건 문화대혁명](https://www.youtube.com/watch?v=4MwDN41Lja0&t=1349s)\n\n# 문화대혁명이란?\n\n**문화대혁명**(文化大革命, Cultural Revolution)은 정식 명칭이 **\"무산계급 문화대혁명\"**(無產階級文化大革命, Great Proletarian Cultural Revolution)으로, 1966년부터 1976년까지 10년간 중국에서 일어난 사회정치적 운동이다.\n\n## 발생 배경\n\n### 대약진운동의 실패 (1958-1962)\n- 마오쩌둥이 주도한 경제개발 정책인 대약진운동이 참혹하게 실패\n- 2천만 명 이상이 기근으로 사망\n- 마오쩌둥의 권력과 영향력이 크게 약화됨\n\n### 실용주의 노선의 등장\n- 류샤오치(劉少奇), 저우언라이(周恩來), 덩샤오핑(鄧小平) 등이 실용적 경제 정책 도입\n- 개인 농지 허용 등 시장경제 요소 도입으로 경제 회복 (1962-1965)\n- 마오쩌둥은 이러한 정책이 공산주의 원칙에 위배된다고 판단\n\n## 문화대혁명의 목표\n\n마오쩌둥이 문화대혁명을 일으킨 주요 목적:\n\n1. **권력 회복**: 대약진운동 실패로 잃은 정치적 영향력 되찾기\n2. **정적 제거**: 류샤오치, 덩샤오핑 등 실용주의 노선 지도자들 축출\n3. **혁명 정신 부활**: 관료화되고 있는 중국 공산당과 사회에 혁명 정신 주입\n4. **\"4구\"(四舊) 타파**: \n   - 구사상(舊思想) - 옛 사상\n   - 구문화(舊文化) - 옛 문화\n   - 구풍속(舊風俗) - 옛 풍습\n   - 구습관(舊習慣) - 옛 습관\n\n## 문화대혁명의 전개 과정\n\n### 1단계: 발발과 혼란 (1966-1968)\n\n#### 1966년 5월: 시작\n- **5.16 통지**: 중국 공산당 중앙위원회가 문화대혁명 시작을 공식 선언\n- 부르주아 분자들이 당, 정부, 군대, 문화계에 침투했다고 주장\n\n#### 홍위병(紅衛兵) 조직\n- 주로 학생들로 구성된 준군사조직\n- **마오쩌둥어록**"
  },
  {
    "id": "Causal Inference/what-is-ate",
    "title": "What is ATE?",
    "date": "2025-07-17",
    "excerpt": "What is ATE?",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "\n# ATE란?\n---\n\nAverage Treatment Effect (ATE)는 모든 사람들이 받은 처치의 평균 효과를 말한다.\n\n즉, 처치받은 결과와 받지 않은 결과의 차이의 평균이라고 이해할 수 있다.\n\n# ATE 추정\n---\n\n$$\n\\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) \\tag{1}\n$$\n\n\n정의에 따라 ATE는 (1)처럼 정의된다.\n하지만 실제로는 모든 사람들이 처치를 받지 않기 때문에 (1)을 직접 계산할 수 없다.\n\n"
  },
  {
    "id": "Causal Inference/double-robustness-explained",
    "title": "Double Robustness: 인과추론의 핵심 개념 완전 정복",
    "date": "2025-07-17",
    "excerpt": "Double Robustness의 이론적 기반부터 실용적 응용까지 완전 해부",
    "category": "Causal Inference",
    "tags": [
      "Double Robustness",
      "Causal Inference",
      "Theory",
      "R-learner"
    ],
    "content": "\n# Double Robustness: 인과추론의 핵심 개념 완전 정복\n\n**Double Robustness**(이중 강건성)는 현대 인과추론의 핵심 개념 중 하나로, R-learner와 같은 최신 방법론의 이론적 기반이 됩니다. 이 포스트에서는 Double Robustness의 모든 측면을 자세히 살펴보겠습니다.\n\n# 1. Double Robustness란 무엇인가?\n\n## 1.1 직관적 이해\n\n**Double Robustness**는 **\"두 가지 모델 중 하나만 정확해도 편향 없는 추정이 가능한 성질\"**입니다.\n\n### **간단한 비유**\n- **자물쇠 두 개**: 문을 열려면 둘 중 하나만 열면 됨\n- **백업 시스템**: 주 시스템이 실패해도 보조 시스템이 작동\n- **이중 보험**: 하나가 실패해도 다른 하나가 보장\n\n## 1.2 인과추론에서의 의미\n\n인과추론에서 우리가 추정해야 하는 두 가지 핵심 요소:\n\n1. **Outcome Model** (결과 모델): $\\mu(x, w) = \\mathbb{E}[Y | X = x, W = w]$\n2. **Propensity Score Model** (처치 확률 모델): $e(x) = \\text{Pr}(W = 1 | X = x)$\n\n**Double Robustness**: 이 둘 중 **하나만 정확해도** 처치효과를 편향 없이 추정할 수 있음!\n\n# 2. 수학적 이론: 핵심 메커니즘\n\n## 2.1 Potential Outcomes Framework\n\n**기본 설정**:\n- **잠재결과**: $Y_i(0), Y_i(1)$ (처치를 받지 않았을 때/받았을 때의 결과)\n- **관찰결과**: $Y_i = Y_i(W_i)$ \n- **개별 처치효과**: $\\tau_i = Y_i(1) - Y_i(0)$\n- **평균 처치효과**: $\\tau = \\mathbb{E}[Y(1) - Y(0)]$\n\n**근본적 문제**: 같은 개인에 대해 $Y_i(0)$과 $Y_i(1)$을 동시에 관찰할 수 없음!\n\n## 2.2 Unconfounde"
  },
  {
    "id": "Causal Inference/Paper Review/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects",
    "title": "[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects",
    "date": "2025-07-17",
    "excerpt": "R-leaner 방법 소개와 이게 가지는 quasi-oracle property에 대해 설명",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/1712.04912)\n\n\n# 논문 리뷰\n\n# Abstract\n\n> Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation.\n\n개인화된 처치효과를 유연하게 추정하는 것은 많은 분야에서 통계적 문제의 핵심입니다.\n\n개인화된 처치효과를 알고 있으면 개인화된 약처방이나 교육정책 결정, 자원 분배 등 많은 문제에서 좋은 결정을 내릴 수 있습니다.\n하지만 이를 추정하는 일은 꽤나 복잡한 일이라서 논문에서는 이를 유연하게 추정할 수 있는 방법을 제시하고 있습니다.\n\n이 논문에서는 marginal effects와 treatment propensity라는 두 가지 nuisance component를 추정하여 개인화된 처치효과를 추정하는 방법을 제시합니다.\n\n> we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. \n\n이러한 방법은 quasi-oracle property를 가지고 있다고 합니다.\n\n<small> *quasi-oracle property : marginal effects와 treatment propensity의 추정이 정확하지 않더라도, 마치 이 두 nuisance component를 미리 알고 있는 것과 같은 오차 경계를 달성할 수 있는 성질<"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/PropensityScoreNotebook",
    "title": "Propensity Score Notebook",
    "date": "2025-07-17",
    "excerpt": "Propensity Score Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "---\ntitle: \"Propensity Score Notebook\"\ndate: \"2025-07-17\"\nexcerpt: \"Propensity Score Notebook\"\ncategory: \"Causal Inference\"\ntags: [\"Causal Inference\"]\n--- [Propensity Score](https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html) # The Psychology of Growth 치료(treatment)와 결과(outcome) 변수 외에도, 이 연구에서는 다음과 같은 다른 특성들도 기록했습니다:\n\n- **schoolid**: 학생의 학교 식별자;\n- **success_expect**: 미래 성공에 대한 자가 보고 기대치, 사전 성취도의 대용 지표로, 무작위 배정 전에 측정됨;\n\n- **gender**: 학생이 식별한 성별의 범주형 변수;\n- **frst_in_family**: 학생의 1세대 대학생 지위에 대한 범주형 변수, 즉 가족 중 대학에 진학한 첫 번째 사람;\n- **school_urbanicity**: 학교의 도시화 정도에 대한 학교 수준의 범주형 변수, 즉 농촌, 교외 등;\n\n- **school_mindset**: 학생들의 고정된 사고방식의 학교 수준 평균, 무작위 배정 전에 보고됨, 표준화됨;\n\n- **school_achievement**: 학교 성취 수준, 이전 4개 학년 학생들의 시험 점수와 대학 준비도를 측정한 것으로, 표준화됨;\n\n- **school_ethnic_minority**: 학교 인종/민족 소수자 구성, 즉 흑인, 라티노, 또는 아메리카 원주민인 학생 비율, 표준화됨;\n\n- **school_poverty**: 학교 빈곤 집중도, 즉 연방 빈곤선 이하 소득을 가진 가정 출신 학생들의 비율, 표준화됨;\n\n- **school_size**: 학교의 모든 4개 학년 학생들의 총 수, 표준화됨. import pandas"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/PropensityScore",
    "title": "Propensity Score",
    "date": "2025-07-17",
    "excerpt": "Propensity Score",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "\n[Propensity Score](https://matheusfacure.github.io/python-causality-handbook/11-Propensity-Score.html)\n\n\n이 책의 저자가 참 글을 잘쓴다.\n되게 매력적인 주제로 이 내용을 풀어간다.\n\nGrowth Mindset vs Fixed Mindset\n\n심리학자들은 마인드셋이 인생에 큰 영향을 미친다. 내 능력이 고정되어있지 않고 충분히 변화할 수 있다고 믿는 사람들은, 내 능력이 고정되어있다고 믿는 사람보다 더 성공한다.\n나는 이 부분에 크게 공감하고, 성장형 마인드셋을 가지려고 한다.\n다만 인과추론을 다루는 입장에서는 이에 다음과 같은 의문을 던져볼 수 있다.\n\n> Is it that a growth mindset causes people to achieve more? Or is simply the case that people who achieve more are prone to develop a growth mindset as a result of their success? \n\n진짜 성장형 사고방식이 사람들을 더 성공하게 했나? 아니면 성공한 사람들이 그런 마인드셋을 가지고 있을 확률이 높은건가?\n되게 재밌는 질문이다.\n\n이 부분을 위한 실험이 진행된게 있구나.\n\n> To settle things, researchers designed the [The National Study of Learning Mindsets](https://studentexperiencenetwork.org/national-mindset-study/). It is a randomised study conducted in U.S. public high schools which aims at finding the impact of a growth mindset.\n\n> The real data on this study is not publicly available in order to preserve studen"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimationNotebook",
    "title": "Doubly Robust Estimation Notebook",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "---\ntitle: \"Doubly Robust Estimation Notebook\"\ndate: \"2025-07-17\"\nexcerpt: \"Doubly Robust Estimation Notebook\"\ncategory: \"Causal Inference\"\ntags: [\"Causal Inference\"]\n--- [Doubly Robust Estimation](https://matheusfacure.github.io/python-causality-handbook/12-Doubly-Robust-Estimation.html) import warnings\nwarnings.filterwarnings('ignore')\n\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import style\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\n\n%matplotlib inline\n\nstyle.use(\"fivethirtyeight\")\npd.set_option(\"display.max_columns\", None) # 학생들의 마인드셋을 향상시키기 위한 세미나를 진행했는데, 이게 어느정도 영향이 있었는지\n# 가상의 데이터\n\n# 세미나 참석 여부 -> intervention\n# 얼마나 성공했는지 혹은 성공할 확률 -> success_expect\n\ndata = pd.read_csv(\"./data/learning_mindset.csv\")\ndata.sample(5, random_state=5) # 성공한 정도 (success expect)가 높을수록 세미나 참석 여부(intervention)가 높은지\n\ndata.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) 학생의 요인에 따라 참석 여부가 결정되고, "
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimation",
    "title": "Doubly Robust Estimation",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "\n[Doubly Robust Estimation](https://matheusfacure.github.io/python-causality-handbook/12-Doubly-Robust-Estimation.html)\n\n"
  },
  {
    "id": "English Study/SPEAK/07-16-그나저나소개팅어땠어",
    "title": "그나저나 소개팅 어땠어?",
    "date": "2025-07-16",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "고민 및 조언하는 표햔"
    ],
    "content": "\n\n# 배운 것\n\nHow was it?\n\n그나저나, 그런데\nBy the way, how was the blind date?\n\nBTW (By The Way)\n친구 사이에서 가볍게 쓰는 말\n\nBy the way, how was the dinner yesterday?\n\nBlind date 소개팅 \n\nWell, the thing is he is a vegeterian.\n글쎄, 문제는\n\nout of budget\n\nto make it somewhere : 어디든 가다\n\n\n\n# 간단한 작문. Simple writing or essay\n\nDaily talk to myself as usual.\n\nBy the way, How was your day?\nDid you do your best?\n\nWell, the thing is condition.\nToday, I was so tired. So I can fully concentrate.\nSo I think I should have a sleep early.\n\n## 수정된 버전 (Improved Version)\n\nJust having my usual daily conversation with myself.\n\nBy the way, how was your day?\nDid you do your best?\n\nWell, the thing is my condition wasn't great.\nToday, I was so tired that I couldn't fully concentrate.\nSo I think I should go to sleep early tonight.\n\n## 피드백 (Feedback)\n\n**주요 수정 사항:**\n\n1. **\"Daily talk to myself\"** → **\"Having my usual daily conversation with myself\"**\n   - 더 완전한 문장 구조로 수정\n\n2. **\"Well, the thing is condition\"** → **\"Well, the thing is my con"
  },
  {
    "id": "Causal Inference/tikhonov-regularization-and-convex-optimization",
    "title": "Tikhonov Regularization과 Convex Optimization: 연속형 처치 R-learner의 이론적 배경",
    "date": "2025-07-16",
    "excerpt": "Ill-posed problem을 해결하는 Tikhonov regularization과 convex optimization의 원리",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Optimization",
      "Regularization"
    ],
    "content": "\n# 개요\n---\n\n연속형 처치에서 R-learner를 적용할 때 발생하는 **non-identification 문제**를 해결하기 위해 **Tikhonov regularization**과 **convex optimization** 이론이 핵심적인 역할을 합니다. 이 글에서는 이러한 수학적 도구들의 원리와 응용을 자세히 설명합니다.\n\n# 1. Ill-posed Problem과 Non-identification\n---\n\n## 1.1 Well-posed vs Ill-posed Problem\n\n**Well-posed problem (잘 정의된 문제)** 의 3가지 조건:\n1. **존재성 (Existence)**: 해가 존재한다\n2. **유일성 (Uniqueness)**: 해가 유일하다  \n3. **안정성 (Stability)**: 입력의 작은 변화가 해의 작은 변화를 가져온다\n\n**Ill-posed problem**은 이 중 하나 이상의 조건을 만족하지 않는 문제입니다.\n\n## 1.2 연속형 처치에서의 Non-identification\n\n연속형 처치 R-learner에서는 **유일성 조건**이 위반됩니다:\n\n$$L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right]$$\n\n이 손실함수를 최소화하는 해집합은:\n$$S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\}$$\n\n> **문제**: 무한히 많은 해가 존재하여 유일한 CATE 함수 $\\tau(x, t)$를 식별할 수 없음\n\n# 2. Tikhonov Regularization\n---\n\n## 2.1 기본 원리\n\n**Tikhonov regularization**은 ill-posed problem을 well-posed problem으로 변환하는 방법입니다.\n\n**원래 문제**:\n$$\\min_{h} L(h)$$\n\n**정규화된 문제"
  },
  {
    "id": "Book/만들어진 신 (The God Delusion)/책을읽기시작하면서",
    "title": "책을 읽기 시작하면서",
    "date": "2025-07-16",
    "excerpt": "책을 읽기 전에 어떤 생각을 가지고 있는지 정리",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "만들어진 신"
    ],
    "content": "\n나는 확실한 무신론자다.\n부모님의 영향도 있겠지만, 친구따라 교회에 다녔던 경험, 다양한 매체에서 접한 현상들 등등이 지금 나의 생각을 만들었다.\n신이 있는게 많이 되나?\n\n나의 사고방식은 직업적인 영향 (개발자 or 데이터과학자) 때문인지 몰라도 근거에 지배적이다.\n근거가 명확하다면 지금도 당장 바뀔 수 있고, 근거가 없다면 백날 설득을 하려해도 전혀 소용이 없을 것이다.\n\n내가 이 책을 읽게 된 이유는 신이 없는 이유를 알고 싶다기 보다는, 사람들이 왜 신을 믿는지가 더 궁금한게 크다. \n어쨌든 책에서는 사람들이 신을 믿는 이유를 언급하고 이를 반박할테니까.\n\n이 책의 영어 제목은 되게 파격적이다. 헛웃음이 나올정도로.\nDelusion, 망상이라는 단어를 쓰다니...\n\n> 누군가 망상에 시달리면 정신 이상이라고 한다.\n> 다수가 망상에 시달리면 종교라고 한다.\n> -> 로버트 퍼시그\n\n사실 나도 비슷한 생각이긴 하다. 내가 생각하는 신은 공통적으로 그리고 있는 허상이라고 생각한다.\n예전에 모시던 호랑이 신과 예수는 사실 같다고 보여진다. 사람들의 편의를 위해 어떤 허상의 개념을 만들고, 이를 함께 믿는 것. \n\n주위에 종교를 가진 친구들이 많은데 그들을 무시하는 생각은 진짜 전혀없다.\n그냥 나의 생각이 이럴 뿐.\n또 허상이면 어떻나 삶에 도움이 되면 그게 좋은거지 라는 생각도 한다.\n\n어쨌든 책을 읽어나가면서 나의 생각은 어떻게 바뀌고, 또 어떤 관점이 열릴지 기대가 된다. \n쭉 읽어봐야겠다."
  },
  {
    "id": "Recommendation/user-based-collaborative-filtering",
    "title": "User-Based Collaborative Filtering",
    "date": "2025-07-15",
    "excerpt": "User-Based Collaborative Filtering 알고리즘에 대한 설명",
    "category": "Recommendation",
    "tags": [
      "추천시스템"
    ],
    "content": "\n\n# User-Based Collaborative Filtering (사용자 기반 협업 필터링)\n\n## 개요\n\nUser-Based Collaborative Filtering은 추천 시스템에서 가장 오래되고 직관적인 이웃(neighborhood) 기반 방법입니다. 핵심 아이디어는 **\"비슷한 취향을 가진 사용자들이 비슷한 아이템을 좋아할 것이다\"** 라는 가정에 기반합니다.\n\n## 기본 개념\n\n- **사용자-아이템 평점 행렬 $R$**: 행은 사용자, 열은 아이템\n- **원소 $r_{u,i}$**: 사용자 $u$가 아이템 $i$에 매긴 평점 (또는 암시적 행동)\n- **목표**: 사용자 $u$에게 아이템 $i$를 추천할지 결정\n\n## 알고리즘 절차\n\n### 1. 유사도(Similarity) 계산\n\n사용자 간 유사도를 계산하는 방법들:\n\n#### Cosine Similarity\n$$\\text{sim}(u,v) = \\frac{\\mathbf{r}_u \\cdot \\mathbf{r}_v}{\\|\\mathbf{r}_u\\| \\times \\|\\mathbf{r}_v\\|}$$\n\n#### Pearson Correlation\n$$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)(r_{v,i} - \\bar{r}_v)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_v)^2}}$$\n\n여기서 $I_{uv}$는 두 사용자가 모두 평가한 아이템 집합\n\n#### Adjusted Cosine Similarity\n$$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)(r_{v,i} - \\bar{r}_i)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)^2} \\times \\sqrt{\\sum_{"
  },
  {
    "id": "English Study/TOEFL/DefinitionOfArt",
    "title": "Definition of Art",
    "date": "2025-07-15",
    "excerpt": "TOEFL Reading 문제 공부 - 예술의 정의",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원",
      "OMSCS"
    ],
    "content": "\n# Definition of Art\n예술의 정의\n\n## Paragraph 1\n\n<button class=\"toggle-button translations\" data-type=\"paragraph1\">🔍 AI 해석 가리기</button>\n\n<div class=\"paragraph1-content\">\n\n> How art is defined has varied throughout history, leading to a wide range of sometimes-contradictory meanings. \n\n<span class=\"translation\">예술이 어떻게 정의되는지는 역사를 통틀어 변화해왔으며, 때로는 상반되는 의미들의 광범위한 스펙트럼을 만들어냈다.</span>\n\n> In the ancient world, the term was used in a broad sense that was not strictly related to visual arts. \n\n<span class=\"translation\">고대 세계에서 이 용어는 시각 예술에 엄격하게 국한되지 않는 광범위한 의미로 사용되었다.</span>\n\n> The application of the word \"art\" exclusively to works in the form of paintings, sculptures, and prints came about only in the Renaissance, and later, in the eighteenth century, the use of the term expanded to include music and poetry. \n\n<span class=\"translation\">\"예술\"이라는 단어를 회화, 조각, 판화 형태의 작품들에만 독점적으로 적용하는 것은 르네상스 시대에 비로소 나타났으며, 이후 18세기에는 이 용어의 사용이 음악과 시를 포함하도록 확장되었다.</span>\n\n> Subsequently, in modern times, a new meaning c"
  },
  {
    "id": "English Study/SPEAK/07-15-정신똑바로차리고있어",
    "title": "정신 똑바로 차리고 있어",
    "date": "2025-07-15",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "고민 및 조언하는 표햔"
    ],
    "content": "\n\n# 배운 것들\n\nBe on the lookout : 정신 똑바로 차려\n\n- Be on the lookout for pickpockets\n- Be on the lookout for voicefishing\n\nBe careful with \n\n- Be careful with your bag on the subway.\n\nin + 자동차 (타서 앉으면 끝)\non + 비행기, 배, 지하철\n\npassport\n\ncatch a cold\n\n- Be careful not to catch a cold\n- Be careful not to burn yourself\n- Be careful not to hurt yourself\n\nDuring this outbreak?\n\noutbreak: 질병 발생기간\n\nscammers : 사기꾼\n\nMake sure not to go\n\n# 간단한 작문\n\nAs usaul, I talk to myself.\nHey MK, be on the lookout for 나태함.\nBe careful with your habit that do work later (미루는 습관)\n\nNice work today, just keep this pace.\nBut don`t forget happy is around you. Feel happy with 사소한 thing around you, and victories you made in today.\n\n# 수정된 작문 (Improved Version)\n\nAs usual, time for my daily self-talk.\nHey MK, don't get lazy on me!\nStop putting things off - let's break this procrastination habit once and for all.\n\n*procrastination: 꾸물거리다\n예문:\n\"Stop procrastinating and start studying!\" (미루지 말고 공부 시작해!)\n\"Procrastination is the thief of "
  },
  {
    "id": "Causal Inference/tilde-tau-explained",
    "title": "Why Do We Introduce the Centered Function tilde_tau(x,t)?",
    "date": "2025-07-15",
    "excerpt": "The role of the intermediate, mean-zero CATE in the continuous-treatment R-learner.",
    "category": "Causal Inference",
    "tags": [
      "Concept",
      "R-learner",
      "Continuous Treatment"
    ],
    "content": "\n# 1. The problem in one sentence\n\nR-loss를 그대로 최소화하면\n\n$$\nh(x,t) = \\tau(x,t) + s(x)\n$$\n\n형태로 **x 에만 의존하는 덧붙임 함수** $s(x)$ 가 있어도 손실값이 변하지 않는다. 그래서 최소값이 **무한히 많아져** 해가 유일하지 않다.\n\n---\n\n# 2. Where does s(x) come from?\n\n* R-loss에 $\\mathbb{E}[ h(X,T) \\mid X ]$ 항이 들어가므로 $h(x,t) + s(x)$ 를 넣어도 $s(x)$ 가 자연스럽게 상쇄된다.\n* 그 결과 손실값이 동일하니 $s(x)$ 를 얼마든지 붙일 수 있다.\n* 이것이 **non-identification(식별 불능)** 문제의 근원이다.\n\n---\n\n# 3. Idea: “center” the function\n\n중간 함수 $\\tilde{\\tau}(x,t)$ 를 다음처럼 정의한다.\n\n$$\n\\tilde{\\tau}(x,t) = \\tau(x,t) - \\mathbb{E}[ \\tau(X,T) \\mid X = x ]\n$$\n\n즉, **각 x 마다 t 전역 평균이 0** 이 되도록 중심화한다.\n\n* mean-zero 성질 때문에 $h(x,t)$ 와 $h(x,t)+s(x)$ 의 **차이점이 R-loss 안에서는 지워지지 않는다**. 실제로 $h(X,T)+s(X) - \\mathbb{E}[h(X,T)+s(X)\\mid X] = h(X,T)-\\mathbb{E}[h(X,T)\\mid X]$ 이므로, **R-loss 값은 그대로**다.\n* 따라서 중심화만으로는 식별 문제가 완전히 사라지지 않는다. **결정적인 열쇠는 다음 단계의 L2 정규화**다.\n  - L2 패널티 $\\rho\\,\\|h\\|_2^2$ 가 추가되면, $s(x)$ 를 붙이는 순간 노름이 커져서 목적함수가 증가한다.\n  - 최적화는 $s(x)=0$ 을 선택해 유일해를 만든다.\n* 즉 $\\tilde{\\tau}$ 는 \"mean-zero\" 조건을 만족하는 후보 중 하나이며, **식별을 완성"
  },
  {
    "id": "English Study/TOEFL/준비전략서치",
    "title": "어떻게 공부해야할지 서치하고 정리",
    "date": "2025-07-14",
    "excerpt": "OMSCS 지원을 위한 토플 공부",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원",
      "OMSCS"
    ],
    "content": ""
  },
  {
    "id": "English Study/TOEFL/Fact and Negative Fact",
    "title": "Fact와 Negative Fact 차이",
    "date": "2025-07-14",
    "excerpt": "Fact와 Negative Fact 유형 풀이법",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원",
      "OMSCS"
    ],
    "content": "\n\n"
  },
  {
    "id": "English Study/TOEFL/BirdDialects",
    "title": "Bird Dialects",
    "date": "2025-07-14",
    "excerpt": "Diagnostic Test 문제 공부",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원",
      "OMSCS"
    ],
    "content": "\n# Bird Dialects\n새 다이러렉츠?\n새의 방언\n\n## Paragraph 1\n\n<button class=\"toggle-button translations\" data-type=\"paragraph1\">🔍 AI 해석 가리기</button>\n\n<div class=\"paragraph1-content\">\n\n> Birds learn acoustic information from their parents and environment from an early age. \n\n<span class=\"translation\">새들은 어린 나이부터 부모와 환경으로부터 음향 정보를 배운다.</span>\n\n> The most complex form of avian vocalization is the birdsong, an often melodic series of communicative sounds produced primarily during breeding. \n\n<span class=\"translation\">조류의 발성 중 가장 복잡한 형태는 새의 노래로, 번식기에 주로 발생하는 종종 선율적인 의사소통 소리의 연속이다.</span>\n\n> In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects. \n\n<span class=\"translation\">개체군이 지리적 경계를 가진 경우, 노래에서 차이가 나타날 수 있으며, 이러한 지리적 한계가 뚜렷하면, 한 개체군 내에서 공유되는 노래 유형을 방언이라고 부른다.</span>\n\n> Ornithologists are keenly interested in how these dialects form and to what extent "
  },
  {
    "id": "English Study/SPEAK/07-14-걱정하는표현",
    "title": "걱정하는 표현",
    "date": "2025-07-14",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "\n# 배운 것들\n\nworried\nlove life : 연애 사업\n\n- I'm worried about my love life.\n\nworry about : (항상) ~를 걱정해\n\n- I worry about my future.\n\n'm worried about : (요즘) 걱정 돼\n\n- I'm worried about my love life.\n- I'm worried about my job interview.\n- I'm worried about the traffic. 차가 막힐까바ㅗ\n- I'm worried about my presentation tommorow.\n\n조금 걱정된다\n- I'm a little worried about\n엄청 걱정된다\n- I'm really worried about\n\n신경쓰인다\n- I'm concerned about the new flu virus.\n- I'm concerned about the news lately.\n\n\ntravel ban\n\n- What are some things that are on your mind?\n\nas well , 나도\n\n\n# 간단한 작문\n\nNowadays, I'm worried about my future of career.\n내가 시장 가치가 있을지? 다른 회사에 갈 수 있을지?에 대해 고민을 하고 있어.\nI'm preparing the job transition, but if fails in the resume screening.\nSo I can believe my self. 앞으로 더 나아질지. \nMy 연봉 can be better than now?\n\nI have a some concerns, but what I have to do or I can to is just focus on the today, just step ahead.\nSo 호흡을 가다듬고, 현재에 집중하려고 노력한다.\n\n## 수정된 영어 글\n\nNowadays, I'm worried about my career path. I keep won"
  },
  {
    "id": "Data Science/dependency-injection-pattern",
    "title": "의존성 주입 패턴으로 코드 품질 향상하기",
    "date": "2025-07-14",
    "excerpt": "의존성 주입 패턴을 통해 코드의 결합도를 낮추고 테스트 용이성을 높이는 방법을 실제 예제와 함께 설명합니다.",
    "category": "Data Science",
    "tags": [
      "python",
      "design-pattern",
      "dependency-injection",
      "software-architecture",
      "clean-code",
      "testing"
    ],
    "content": "\n# 배경\n\n최근에 대화형 AI 챗봇 프로젝트를 개발하면서 겪었던 코드 구조의 문제점들과, 이를 의존성 주입 패턴으로 개선한 경험을 정리해보았습니다.\n\n## 프로젝트 상황\n\n- **FastAPI 기반 웹 애플리케이션**\n- **OpenAI GPT 모델을 활용한 대화형 검색 시스템**\n- **ChromaDB를 사용한 벡터 검색**\n- **사용자별 대화 세션 관리**\n\n## 겪었던 문제점들\n\n### 1. 강한 결합 (Tight Coupling)\n클래스들이 서로 너무 밀접하게 연결되어 있어서, 하나를 수정하면 다른 것들도 함께 수정해야 하는 상황이 발생했습니다.\n\n### 2. 하드코딩된 설정\n모델명, 데이터베이스 경로, API 키 등이 코드에 직접 작성되어 있어서 환경별로 설정을 바꾸기 어려웠습니다.\n\n### 3. 테스트 어려움\n각 컴포넌트를 독립적으로 테스트하기 어려워서, 전체 시스템을 실행해야만 테스트할 수 있었습니다.\n\n# 관련 개념들 정리\n\n## 강한 결합 (Tight Coupling) - 쉽게 이해하기\n\n### 🏠 집 짓기 비유\n\n**강한 결합의 예시:**\n```python\n# 문제가 있는 코드 - 강한 결합\nclass 집:\n    def __init__(self):\n        # 집이 직접 벽돌공장을 만들어서 벽돌을 가져옴\n        self.벽돌공장 = 벽돌공장()\n        self.벽돌 = self.벽돌공장.벽돌만들기()\n        \n        # 집이 직접 목수공장을 만들어서 문을 가져옴\n        self.목수공장 = 목수공장()\n        self.문 = self.목수공장.문만들기()\n```\n\n**문제점:**\n- 집이 벽돌공장과 목수공장을 직접 알고 있어야 함\n- 다른 재료(콘크리트, 알루미늄)로 바꾸려면 집 클래스를 수정해야 함\n- 테스트할 때 진짜 공장 대신 가짜 공장을 사용하기 어려움\n\n**해결책 - 느슨한 결합:**\n```python\n# 개선된 코드 - 느슨한 결합\nclass 집:\n    def __init__"
  },
  {
    "id": "Causal Inference/Industry Application/why-we-need-causal-inference",
    "title": "인과 추론 도입배경",
    "date": "2025-07-14",
    "excerpt": "인과추론을 왜 사용해야하는지 설명하고 이를 도입하기 위해 설득하는 글",
    "category": "Causal Inference",
    "tags": [
      "ICL-Lenze-sizing"
    ],
    "content": "\n# 0. Abstract - 배경, 요약\n---\n\n환자에게 렌즈 삽입술을 위한 최적의 렌즈 사이즈를 추천하는, 렌즈 사이징 문제는 예측 모델을 고도화하는 방향으로 연구가 진행되어왔다. \n환자의 눈의 조건과 삽입한 렌즈 사이즈를 입력 받아 수술결과(vaulting, 렌즈 후면과 수정체 전면까지의 거리)를 예측하는 모델을 개발하고, 이 모델의 예측값을 통해 최적의 렌즈 사이즈를 제안하는 방식이다. 하지만 기존 예측 모델은 한계점이 존재한다.\n이번 보고서를 통해 기존 예측 모델의 한계점을 분석하고 이를 해결하기 위한 방법을 제안한다.\n\n기존 예측 모델의 한계점을 요약하면 렌즈 사이즈가 결과에 영향을 주는 인과적인 영향도를 추정하지 못하며, 단순히 예측 평가지표만을 고려하고 있어 추천의 신뢰도를 보장하지 못한다는 것이다.\n이러한 한계점을 극복하기 위해 인과추론 방법론을 도입할 것을 제안한다.\n이때 기대되는 효과는 다음과 같다. 개인화된 처치 효과를 추정할 수 있고 이를 통해 개인화된 최적의 렌즈 사이즈를 제안할 수 있다. 그리고 측정되지 않은 데이터에 대한 평가를 고려하여 추천의 신뢰도를 보장할 수 있다.\n\n\n# 1. Introduction - 기존 연구의 한계점 분석\n---\n\n렌즈 삽입술을 위한 최적의 렌즈 사이즈를 찾는 일은 의사의 높은 수준의 경험과 노하우에 의존하는 일이다. 이를 도와주기 위해 최적의 렌즈 사이즈를 추천하는 모델이 개발되어왔다.\n기존의 최적의 사이즈를 찾기 위한 렌즈 사이징 문제는 예측 모델을 고도화하는 방향으로 연구가 진행되어왔다.\n환자의 눈의 조건과 삽입한 렌즈 사이즈를 입력 받아 수술결과(vaulting, 렌즈 후면과 수정체 전면까지의 거리)를 예측하는 모델의 성능을 향상시키는 방향을 진행되었다.\n대표적으로 MAE와 같이 실제 값과 예측값의 차이를 평가하는 예측평가지표를 통해 모델의 성능을 평가하고 이를 개선하는 방향으로 연구가 진행되었다.\n*MAE: Mean Absolute Error, 실제 값과 예측값의 차이의 절대값의 평균*\n\n하지"
  },
  {
    "id": "Fitness/Workout Log/250713",
    "title": "간단한 달리기 + 산책",
    "date": "2025-07-13",
    "excerpt": "운동일지",
    "category": "Workout Log",
    "tags": [],
    "content": "\n확실히 런닝 자주 안하니까 진짜 안뛰어진다. 꾸준히 뛰어보자."
  },
  {
    "id": "English Study/TOEFL/토플준비",
    "title": "TOEFL 100점 이상 목표로 공부",
    "date": "2025-07-13",
    "excerpt": "OMSCS 지원을 위한 토플 공부",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원",
      "OMSCS"
    ],
    "content": "\n# 배경\n\nOMSCS를 지원하기 위해서는 토플 100점 이상이 필요한데요, 이를 위해서 토플 공부를 시작했습니다.\n공부하면서 어떻게 공부해야할지 정리해나가고 있습니다.\n\n목표 기간은 3~4개월 정도입니다. 올해 11월부터는 OSMC에 서류를 넣으려고 합니다.\n\n# 학습 계획 세워보기\n\n\n\n# 공부 전략\n\n- [유튜브 링크 - 🍀토플 독학을 마음먹으신 분들을 위한 첫 번째 가이드 (시험 이해, 공부 방법, 두려움 극복, 참고 링크)](https://www.youtube.com/watch?v=CiHUwx701D4)\n\n1. 명확한 목표 기간을 잡고 집중해서 공부하기\n2. 시험구성에 대해 정확히 이해하고 시작하기\n\n시험 구성\n- Reading\n2개 지문 x 10문제. 지문하나당 약 700단어. \n총 35분정도.\n\n- Listening\n2개 대화 (각 3분) x 5문제\n3개 강의 (각 3-5분) x 6문제\n총 36분 정도.\n\n- Speaking\n(1) 독립형 (15초 준비, 45초 답변)\n(2), (3) 통합형 - 읽+듣+말 (30초 준비, 60초 답변)\n(4) 통합형 - 듣+말 (20초 준비, 60초 답변)\n총 16분 정도.\n\n- Writing\n(1) 통합형 - 읽+듣+쓰 (20분)\n(2) 토론형 - 읽+쓰 (10분)\n총 30분정도.\n\n## 참고 사이트\n\n🔗참고 사이트 링크\n- ETS TOEFL 공식 웹사이트 시험 구성 및 문제 유형 안내 https://www.ets.org/toefl/test-takers...\n- ETS TOEFL TestReady 웹사이트 (모의고사, 액티비티) https://testready.ets.org/test-prep-l...\n- 테스트글라이더 웹사이트 https://www.testglider.com/ko/toefl\n\n\n\n\n# 참고자료\n\n- [[토플공부/토플 공부법] 초보자를 위한 토플 독학 가이드 : A to Z](https://beyondourlimit.tistory.com/68) : 블로그, 기본적인 개념 알기 좋다\n\n\n\n"
  },
  {
    "id": "English Study/SPEAK/07-13-조언하는 표현",
    "title": "조언하는 표현",
    "date": "2025-07-13",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "\n\n# 배운 것들\n\n- Try to drink more water\n- Drink more water (X, 명령하는 느낌이 남)\n\n발음 팁! t -> d\n\n- Try to get more sleep\n- Try to cut down on smoking\n\ncut down on : ~을 줄이다\n\n- Try not to skip breakfast.\n\n\n- Make sure to + \n- Make sure to take probiotics every day.\n- Make sure to take Vitamin C.\n- Make sure not to eat too much junk food.\n\nwipe down : 말끔히 닦다\ncome to work\nstock up : 대량으로 사놓다.\n\n\n\n# 간단한 작문\n\nI read a book , atomic habits that tell me how to make habit.\n\nThe book says that try to split the things to very small things.\nBecause it make me to do more.\n\nSo the lesson in the book is that make sure to make things, clear, attractive, easy.\n\n# 피드백. AI 작문\n\n# 피드백:\n1. \"I read a book , atomic habits\" → 쉼표 앞 공백 제거, 책 제목은 따옴표 사용\n2. \"that tell me\" → 단수 주어이므로 \"tells me\"\n3. \"how to make habit\" → \"how to build habits\" (복수형, 더 자연스러운 표현)\n4. \"says that try to\" → \"says to try to\" 또는 \"says that I should try to\"\n5. \"split the things to very small things\" → \"break things down into smaller pieces\"\n6. \"Because it"
  },
  {
    "id": "Book/아주 작은 습관의 힘 (Atomic Habits)/골디락스의 법칙",
    "title": "골디락스의 법칙",
    "date": "2025-07-13",
    "excerpt": "지금보다 조금 더 어려운 일에 도전하자",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "\n'아주 작은 습관의 힘'에서는 꾸준함을 어떻게 이어갈 것인지, 그리고 이에 도움을 주는 도구들을 소개하고 있습니다.\n이 중에서 '골디 락스의 법칙'에 대해 이야기한 부분을 정리해봅니다.\n\n<figure>\n  <img src=\"/post/골디락스의법칙/골디락스의법칙_그래프_이미지.jpg\" alt=\"골디락스의 법칙 그래프\" />\n  <figcaption>그림1.골디락스의 법칙: 너무 쉬우면 지루하고, 너무 어려우면 포기하게 된다. 적당히 도전적인 수준이 가장 효과적이다.</figcaption>\n</figure>\n\n골디락스의 법칙(Goldilocks Rule)의 법칙이란 일의 난이도가 너무 쉬우면 지루하고, 너무 어려우면 포기하게 되기 때문에, 조금 어려운 정도의 난이도가 일에 몰입하는데 효과적이라는 이야기입니다.\n\n\n> 골디락스의 법칙이란 인간은 자신이 할 수 있는 적합한 일을 할 때 동기가 극대화되는 경험을 한다는 것이다.  \n> 지나치게 어려워서도 안되며 지나치게 쉬워서도 안된다. 딱 들어맞아야 한다.\n> \n> — *아주 작은 습관의 힘*\n\n어떤 일을 꾸준히 하기 위해서는 이 법칙을 이해하고 활용할 수 있습니다.\n\n특히 어떤 일을 처음 시작하는거라면, 그 자체로 심리적 부담감이 크고 난이도가 어렵다고 느껴지기 때문에 정말 쉬운 일로 작게 쪼개는게 효과적일 것입니다.\n\n그 후에 어느정도 반복하면서 습관이 되었다면 조금씩 난이도를 올려가야 지루함을 느끼지 않고 꾸준히 할 수 있을 것입니다.\n\n이 골디락스의 법칙은 뇌의 긴장도를 어떻게 유지할 것인지와도 연결됩니다.\n\n## 예르크스-도드슨 법칙과의 연결\n\n골디락스의 법칙은 심리학의 **\"예르크스-도드슨 법칙(Yerkes-Dodson Law)\"**과 밀접한 관련이 있습니다.\n\n<figure>\n  <img src=\"/post/골디락스의법칙/여키스도슨법칙_그래프.png\" alt=\"예르크스-도드슨 법칙 그래프\" />\n  <figcaption>그림2. 예르크스-도드슨 법칙: 긴장도(각성 수준)와 성과의 관계. 적절한 긴장도"
  },
  {
    "id": "Guitar/Practice Log/250713",
    "title": "델리스파이스 고백",
    "date": "2025-07-12",
    "excerpt": "기타연습일지",
    "category": "Practice Log",
    "tags": [],
    "content": "\n\n- 크로매틱\n80bpm 16bit. 처음엔 잘 안된다가 나중에 손풀리니까 잘 됨\n\n- 스케일\n0~4. 이제 다 외울 때가 됐는데.. ㅎㅎ\n\n- 곡 연습\n델리스파이스 고백. 막 빡세게는 안하고 그냥 한번씩 쳐보는 정도?"
  },
  {
    "id": "Guitar/Practice Log/250712",
    "title": "어떻게",
    "date": "2025-07-12",
    "excerpt": "기타연습일지",
    "category": "Practice Log",
    "tags": [],
    "content": "\n\n# 연습 체크\n\n7월 12. 토\n- [ ] 크로메틱\n\n\n\n\n"
  },
  {
    "id": "Book/아주 작은 습관의 힘 (Atomic Habits)/습관추적",
    "title": "습관추적에 대하여",
    "date": "2025-07-12",
    "excerpt": "습관추적은 어떤 습관을 만들기 위한 훌륭한 도구다",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "\n'아주 작은 습관의 힘'에서 이야기하는 컨셉은 아주 명확합니다.\n어떤 일을 꾸준히 하기 위해서는 '명확해야 하며', '하기 쉬워야 하고', '만족감을 줘야 한다'.\n\n그리고 이렇게 만들기 위한 좋은 방법 중 하나로 습관 추적을 설명하고 있습니다.\n\n습관을 추적한다는 건 내가 어떤 일을 꾸준히 하고 있는지와 어떤 일을 해야하는지를 적고 확인해나가는 것입니다.\n내가 어떤 일을 해야하는지 적어서 해야할 일을 명확히 만들고, 했을 때는 이를 기록하면서 성취감을 느낄 수 있습니다.\n\n> 꾸준히 하기 위해서는 성취감을 받아야 한다. 이를 위해 습관추적은 좋은 도구다.\n\n특히 저에게는 매일 해나가면서 꾸준히 쌓이는 느낌이 강력한 동기부여로 작용합니다.\n\n개발을 하는 입장에서 가장 대표적인게 깃헙의 잔디밭이라고 생각합니다.\n\n<figure>\n  <img src=\"/post/습관추적/깃헙잔디밭.png\" alt=\"깃헙 잔디밭\">\n  <figcaption>깃헙 잔디밭</figcaption>\n</figure>\n\n내가 꾸준히 커밋(코드 작업)을 하고 있는게 한눈에 보이고, 이게 이 일을 더 꾸준히 하는데 동기부여를 해줍니다.\n\n> 꾸준히 쌓여가는 느낌은 굉장한 동기부여를 제공한다.\n\n또한 이렇게 쌓여가는 느낌을 받기 위해 블로그도 작성하기도 합니다.\n\n공부한 것들을 글로 작성하는 연습을 하기도 하고, 또 이렇게 공부한 것들이 하나씩 쌓여가는게 눈에 보이기 때문에 성취감을 줍니다.\n그래서 그냥 공부하는 것보다 저는 블로그에 글로 남기면서 하나씩 쌓여가는 걸 눈으로 볼 때 더 큰 성취감과 동기부여를 받습니다."
  },
  {
    "id": "English Study/SPEAK/07-11-긍정적인피드백주기",
    "title": "긍정적인 피드백 주는 표현들 복습",
    "date": "2025-07-11",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "\n# 리뷰 데이\n\n- I like the way \n\n- I appreciate it\n\n- I know that it`s hard\n\n- I want you to know that you call me any time.\n\n- This too shall pass.\n\n- I was really impressed with.\n\n- It couldnt be any better\n\n- I couldn`t have done it without you.\n\n- You did a great job on work\n\n- I`m happy to see that\n\n- I can tell that\n\n- I believe in you\n\n- You`ve made it this far.\n\n- You`re almost there\n\n"
  },
  {
    "id": "Causal Inference/ill-conditioned-matrix-theory",
    "title": "Ill-conditioned 행렬과 고유값: 연속형 처치 R-learner의 이론적 배경",
    "date": "2025-07-11",
    "excerpt": "연속형 처치에서 발생하는 non-identification 문제와 ill-conditioned 행렬, 고유값의 관계를 자세히 설명",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Linear Algebra",
      "Matrix Analysis"
    ],
    "content": "\n# Ill-conditioned 행렬과 고유값: 연속형 처치 R-learner의 이론적 배경\n\n## 개요\n\n연속형 처치 R-learner에서 발생하는 **non-identification 문제**는 수학적으로 **ill-conditioned 행렬** 문제로 나타납니다. 이 글에서는 이 개념들을 단계별로 자세히 설명합니다.\n\n## 1. 고유값(Eigenvalue)이란?\n\n### 1.1 기본 개념\n\n**고유값**은 행렬의 중요한 특성을 나타내는 스칼라 값입니다.\n\n**수학적 정의**:\n행렬 A에 대해, 0이 아닌 벡터 v와 스칼라 λ가 다음을 만족할 때:\n\n$$Av = \\lambda v$$\n\nλ를 A의 **고유값**, v를 **고유벡터**라고 합니다.\n\n### 1.2 직관적 이해\n\n고유값은 행렬이 벡터를 **어떤 방향으로 얼마나 늘리거나 줄이는지**를 나타냅니다.\n\n**예시**:\n\n$$\nA = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\n$$\n\n고유값: $\\lambda_1 = 2$, $\\lambda_2 = 3$  \n고유벡터: $v_1 = [1, 0]$, $v_2 = [0, 1]$\n\n- λ₁ = 2: x축 방향으로 2배 늘림\n- λ₂ = 3: y축 방향으로 3배 늘림\n\n### 1.3 고유값의 의미\n\n1. **λ > 1**: 해당 방향으로 확대\n2. **0 < λ < 1**: 해당 방향으로 축소\n3. **λ = 0**: 해당 방향으로 완전히 압축 (정보 손실)\n4. **λ < 0**: 해당 방향으로 반전\n\n## 2. Ill-conditioned 행렬이란?\n\n### 2.1 조건수(Condition Number)\n\n**조건수**는 행렬이 얼마나 \"불안정한지\"를 측정하는 지표입니다.\n\n**정의**:\n\n$$\n\\kappa(A) = \\|A\\| \\times \\|A^{-1}\\|\n$$\n\n여기서 $\\|A\\|$는 행렬 A의 노름(norm)입니다.\n\n### 2.2 Ill-conditioned의 의미\n\n**Well-conditioned**: 조건수가 작"
  },
  {
    "id": "Causal Inference/Paper Review/review-Towards-R-learner-with-Continuous-Treatments",
    "title": "[Paper Review] Towards R-learner with Continuous Treatments",
    "date": "2025-07-11",
    "excerpt": "연속형 처치를 위한 R-learner를 어떻게 구현할 수 있는지에 대한 논의",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/2208.00872)\n\n# 논문의 배경\n---\n\n[Quasi-Oracle Estimation of Heterogeneous Treatment Effects](https://arxiv.org/pdf/1712.04912) 에서 개인화 처치 효과를 추정하는 방법을 제안했습니다.\n\n<small> * 개인화 처치효과 : 어떤 처치를 했을 때 개인별로 어떤 효과가 있을지 추정한 것</small>\n\n개인화 처치효과를 추정하는 건 인과추론의 가장 핵심적인 문제이며, 이는 다양한 분야에서 통찰을 제공합니다.\n예를 들어 정밀의학에서는 환자별 처치 효과를 추정하여 처치 선택을 결정하고, 교육에서는 학생별 처치 효과를 추정하여 교육 방법을 결정하고, 온라인 마케팅에서는 사용자별 처치 효과를 추정하여 맞춤형 광고를 제공하고, 오프라인 정책 평가에서는 지역별 처치 효과를 추정하여 정책을 결정할 수 있습니다.\n\n> 즉, 개인화 처치효과를 알게 되면 어떤 선택에 대한 근거를 제공할 수 있습니다.\n\n기존 논문에서는 이진 처치의 개인화 처치효과를 추정하는 방법을 이야기했고, 이 논문에서는 이를 확장해서 연속형 처치에 대해서도 이를 적용하기 위한 방법론을 이 논문에서 이야기하고 있습니다.\n\n[[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects](/posts/Causal%20Inference/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects) <- 이 논문에 대한 리뷰는 여기서 확인할 수 있습니다.\n\n> 기존의 방법을 확장할 때 발생하는 문제와 이를 해결한 방법론에 대한 이야기에 집중해서 이 논문을 이해했습니다.\n\n간단히 요약하면 다음과 같습니다\n1. 이진 처치에서는 처치효과를 추정하는 방법이 있었는데, 이를 연속형 처치로 확장하면 문제가 발생한다. (유일해를 가지지 않는 문제)\n2. "
  },
  {
    "id": "Book/아주 작은 습관의 힘 (Atomic Habits)/만족스럽게만들어라",
    "title": "만족스럽게 만들어라",
    "date": "2025-07-11",
    "excerpt": "지속하기 위해서는 즉각적인 보상이, 그만두기 위해서는 즉각적인 페널티가 필요하다",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "\n# 어떤 일을 지속하기 위해 중요한 건 뭘까?\n\n많은 사람들이 원하는 결과를 얻기 위해서는 '꾸준함'이 가장 중요하다고 말합니다.\n이 꾸준함은 어떻게 만들 수 있을까요?\n\n강력한 의지와 다짐이 필요한 걸까요?\n\n'아주 작은 습관의 힘'에서는 원하는 습관을 만들기 위한 구체적인 방법들을 제시하고 있습니다.\n그 중 하나는 해야하는 일을 아주 작게 쪼개라는 것입니다. [쉬워야 달라진다](/posts/Book/쉬워야달라진다)\n\n이에 이어서 저자는 어떤 일을 꾸준히 하기 위해서는 **만족스럽게 만들어라** 라고 말하고 있습니다.\n\n# 우리의 뇌를 이해해보자\n\n우리의 뇌는 과거에 맞춰져 있습니다. \n뇌과학에서 밝혀진 바에 따르면, 인간의 뇌는 수십만 년 동안 수렵채집 시대의 환경에 최적화되어 진화했습니다. \n우리가 현대 문명에 살기 시작한 것은 불과 몇 천 년에 불과하기 때문입니다.\n\n이러한 진화적 배경 때문에 우리의 뇌는 장기적인 목표보다는 **즉각적인 보상**에 훨씬 더 강하게 반응하도록 설계되어 있습니다. \n원시 시대에는 지금 당장 얻을 수 있는 음식, 안전, 번식 기회가 생존에 직결되었기 때문입니다. \n\n따라서 우리가 장기적인 목표를 달성하려면 이러한 뇌의 특성을 이해하고 활용해야 합니다.\n\n> 우리의 뇌는 당장 즐거운면 하고 당장 고통스러우면 안한다.\n\n우리의 의지력은 한계가 있기 때문에, 이를 이해하고 활용해서 꾸준히 할 수 있는 환경 혹은 시스템을 만들어야 합니다.\n\n> 우리는 본능의 씨앗들에 반기를 들지 못하며 그것들과 함께 나아가야 한다. 가장 최선의 방법은 장기적으로 보상을 주는 습관에는 즉시적인 기쁨의 조각들을 덧붙이고, 그렇지 않은 습관에는 고통의 조각들을 덧붙이는 것이다. \n>\n> - 《아주 작은 습관의 힘》\n\n\n# 원하는 습관을 만들려면 즐겁게 만들자\n\n> 습관을 계속 유지하기 위해서는 성공했다는 느낌을 필수적으로 받아야 한다. 비록 아주 사소한 방식일지라도 말이다. 성공했다고 느끼는 것은 습관이 성과를 냈고, 그 일이 노력할 만한 가치가 있다"
  },
  {
    "id": "English Study/SPEAK/07-10-격려와신뢰한다는표현",
    "title": "격려와 신뢰한다는 표현",
    "date": "2025-07-10",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "\n# 오늘 배운 것들 정리\n\n- I believe in you.\n- I believe in your ability to come out ahead.\n- I believe in your ability to stay focused.\n- I believe in your ability to persevere.\n\n- You've made it this far.\n- You're almost there.\n\n\n# 간단한 작문\n\nAs I did yesterday (어제처럼), I wanna talk to myself. \nBecause I need some 격려.\n\nI believe in you MK.\nI believe in your ability to come out ahead.\n지금은 힘들더라도, you will made it.\n\nYou`ve made it this far.\n\nJust focus on today, one step ahead. You`re almost there.\n\n# 피드백\n\n## 잘한 점들 ✨\n- **학습한 표현 활용**: 오늘 배운 \"I believe in you\" 시리즈를 실제 작문에 바로 적용한 점이 훌륭합니다\n- **감정 표현**: 자신에게 격려가 필요하다는 솔직한 마음을 영어로 표현한 것이 자연스럽습니다\n- **개인적 메시지**: MK라는 이름을 넣어서 더 개인적이고 진심어린 느낌을 만들어냈습니다\n\n## 개선할 부분 🔧\n1. **문법 오류**: \n   - `you will made it` → `you will make it` (동사원형 사용)\n   - `You`ve` → `You've` (올바른 아포스트로피 사용)\n\n2. **어조 통일**: \n   - `wanna` → `want to` (좀 더 격식있는 표현)\n   - 한영 혼용을 좀 더 자연스럽게\n\n3. **표현 개선**:\n   - `one step ahead` → `one step at a time` (더 일반적인 표현)\n\n## 수정 제안 📝\n\n**원문 유지하되 이렇게 쓸 수도 있어요:**"
  },
  {
    "id": "Causal Inference/Paper Review/review-HETEROGENEOUS-TREATMENT-EFFECTS-ESTIMATION-WHEN-MACHINE-LEARNING-MEETS-MULTIPLE-TREATMENT-REGIME",
    "title": "[Paper Review] Heterogeneous Treatment Effects Estimation: When Machine Learning Meets Multiple Treatment Regime",
    "date": "2025-07-10",
    "excerpt": "HETEROGENEOUS TREATMENT EFFECTS ESTIMATION: WHEN MACHINE LEARNING MEETS MULTIPLE TREATMENT REGIME",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n\n[paper link](https://arxiv.org/pdf/2205.14714v1)"
  },
  {
    "id": "Causal Inference/Paper Review/review-Causal-Effect-Inference-for-Structured-Treatments",
    "title": "[Paper Review] Causal Effect Inference for Structured Treatments",
    "date": "2025-07-10",
    "excerpt": "Causal Effect Inference for Structured Treatments",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "\n\n\n[paper link](https://arxiv.org/pdf/2106.01939)"
  },
  {
    "id": "Career/Job Transition/25-07-10-lunit-cancer-screening",
    "title": "(Seoul) Research Engineer · AutoML ",
    "date": "2025-07-10",
    "excerpt": "루닛 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "루닛 채용공고"
    ],
    "content": "\n# 채용공고\n\n<details>\n<summary>📋 <strong>채용공고 상세정보</strong></summary>\n\n[링크](https://www.linkedin.com/jobs/search/?currentJobId=4261590818&keywords=Lunit%20Cancer%20Screening&origin=JOB_COLLECTION_PAGE_KEYWORD_AUTOCOMPLETE&refresh=true)\n\n- Lunit, a portmanteau of ‘Learning unit,' is a medical AI software company devoted to providing AI-powered total cancer care. Our AI solutions help discover cancer and predict cancer treatment outcomes, achieving timely and individually tailored cancer treatment.\n\n🗨️ About The Team\n\n- AutoML team at Lunit automates AI product development processes to streamline time-consuming tasks and advance cutting-edge AutoML research. By combining engineering expertise with state-of-the-art deep learning techniques, our team plays an important role in accelerating product development for cancer detection and treatment. Join us in our mission to Conquer Cancer Through AI where your contributions will directly influence AI innovations that improve patient outcomes wo"
  },
  {
    "id": "Career/Job Transition/25-07-10-hyundai-autoever",
    "title": "현대 오토에버 MLOps / AI 검색 엔지니어",
    "date": "2025-07-10",
    "excerpt": "현대 오토에버 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "현대 오토에버 채용공고"
    ],
    "content": "\n\n# 채용공고\n\n<details>\n<summary>📋 <strong>채용공고 상세정보</strong></summary>\n\n[Tech] Machine Learning Engineer - MLOps / AI 검색 엔지니어\n- 📃 누구나 ​마음 ​속에 ​이력서 한 ​장은 있으니까, \n- ⚡ 1분 ​컷 ​지원으로 현대오토에버로의 ​여정을 시작하세요. (이력서 ​자율양식)\n- ✅ MLOps ​/ ​AI 검색 엔지니어\n\n🚀 ​합류하실 ​팀을 ​소개해요\n- 언어AI기술팀\n\n💻 합류하시면 함께 ​할 ​업무예요\n- AI 대화형 서비스를 ​위한 ​데이터베이스 ​설계 및 개발\n- AI ​검색 엔진 ​개발 ​및 운영\n- MLOps ​구축 및 ​운영\n\n🔍 ​이런 분과 함께 ​하고 싶어요\n\n- 검색 ​/ 챗봇 관련 모듈 및 서비스 개발 경험 \n- Docker / Kubernetes 활용 개발 및 배포 경험 \n- Python / JAVA 개발 경력 3년 이상 혹은 그에 준하는 실력\n\n🔍 이런 분이라면 더욱 좋아요\n\n- 정보검색/SW공학 관련 석사 이상\n- ELK(Elasticsearch, Kibana, Kubernetes, Kafka) 구성 설계 및 운영 경험자\n- Neo4j / Redis 기반 DB 설계 및 구축 경험자\n- 벡터 DB (Milvus / qdrant / faiss 등) 경험자 \n- ES 플러그인 개발 경험자\n\n⌛ 이렇게 합류해요\n- 서류 접수 → 서류 검토 → 직무역량테스트(코딩 또는 과제테스트) 및 인성검사 → 1차면접 → 2차면접 → 처우협의 및 채용검진 → 최종 합격🎉\n\n📍 만나게 될 근무지는 여기예요\n- 서울 강남\n\n📌 참고해 주세요\n- 채용 시 마감되는 상시 채용 공고로 운영되며, 채용 절차와 일정은 변동될 수 있어요.\n- 사회적 배려 대상자(보훈 취업지원대상자, 장애인)는 관계 법령과 내규에 따라 우대해요.\n- 모집 분야 및 담당 업무에 따라 영어 구술평가, 레퍼런스 체크, 또는 기타 전형이 실시될 수 있어요.\n- 지원자의 경험과 역량을 고려"
  },
  {
    "id": "Career/Graduate School/대학원에대한고민",
    "title": "대학원을 진학해야할까? 하면 어디로?",
    "date": "2025-07-10",
    "excerpt": "나의 다음 선택지는 어디로?",
    "category": "Career",
    "tags": [
      "대학원"
    ],
    "content": "\n\n\n# 커리어 선배형에게 물어본 내용들\n\n\nㅇㅇ 내가 자주보는 괜찮은 ds관련 공고사이트줄테니까, 일단 있는 공고들 쫘-악 훑어봐봐. 그리고 공통적인 키워드들이 있을테니까 그거위주로 생각해보든가. (ex. LLM, RAG, agent, ML serving, triton 등)\n\n- zighang\n- offercent\n- bzpp\n\n"
  },
  {
    "id": "Career/Graduate School/고려대야간대학원",
    "title": "고려대 야간대학원 관련 정리",
    "date": "2025-07-10",
    "excerpt": "기회비용과 기대되는 가치",
    "category": "Career",
    "tags": [
      "대학원"
    ],
    "content": "\n# 배경\n\n제가 원하는 커리어의 방향을 생각해볼 때 대학원에 대한 선택을 늘 고민하게 됩니다.\nAI/ML 리서처나 관련 연구를 하는 일들을 하기 위해서는 최소 석사 이상의 학력을 요구하는 경우가 많기 때문입니다.\n\n# 늦었나?\n\n지금 나이가 만으로 26정도, 내년 전기에 시작해도 29~30에 끝나니까 그렇게 늦은 시기도 아니라고 생각이 듭니다.\n\n그리고 나이가 더 많았더라도 이걸 통해 얻을 수 있는게 분명하다면, 기회비용보다 더 크다고 판단된다면 늦은 시기란 없지 않을까 싶습니다.\n\n\n# 기회비용 (시간과 가격)\n\n학비가 대력 4천정도. 일단 돈이 정말 많이 듭니다.\n아직 3천만원도 모아본 적 없는 제가 이렇게 큰 비용을 감당할 수 있을까? 라는 생각도 듭니다.\n(학자금 대출 받고 조금씩 갚으면 언젠간 갚겠죠...?)\n\n\n그리고 시간도 많이 필요합니다. 퇴근 후에는 온전히 시간을 다 쏟아야 하고 좋은 결과를 위해서는 주말에도 이 부분만 보고 있지 않을까 싶습니다.\n2년 반정도의 시간도 고려해야 합니다.\n\n# 기대되는 가치\n\n제가 원하는 커리어의 방향에 도움이 됩니다.\n\n- 학력\n석사라는 학력은 사실상 필수적이지 않을까 싶습니다.\n사이드 프로젝트를 하면서 만나는 다른 분들을 봐도 다 석사 이상의 학력을 가지고 있습니다.\n\n- 논문\n그리고 좋은 논문을 작성할 수 있어야만 단순 학위에 그치지 않고 진짜 도움이 될 것 같습니다.\n\n# 지원시기와 준비할 것들\n\n올해 후기 지원은 놓쳤고 내년 전기에 지원하지 않을까 싶습니다.\n다른 블로그들을 살펴보니 준비해야하는 건 기본지식(통계, 선형대수 등등)과 연구계획서 정도입니다.\n\n## 기본지식\n\n통계, 선형대수와 같은 질문들을 면접때 받는다고 합니다.\n경쟁률이 약 6:1정도 된다고 하니, 이런 질문에 대한 대답을 미리 철저하게 준비할 필요가 있어보입니다.\n\n\n\n## 연구계획서\n\n이 부분에 대한 고민이 많이 필요합니다.\n> 그래서 어떤 연구를 하고 싶은거지? \n\n이 부분이 많이 비어있어서 꾸준히 채워나가보려고 합니다.\n\n\n# "
  },
  {
    "id": "Career/25-07-10-visuworks-thoughts-of-my-career",
    "title": "나의 현재 커리어 상황에 대한 생각",
    "date": "2025-07-10",
    "excerpt": "나의 현재 커리어에 대한 고찰",
    "category": "Career",
    "tags": [
      "내 커리어는 어디로 가야하나"
    ],
    "content": "\n# 현재 나의 상황\n\n요즘 정말 커리어에 대한 걱정과 고민이 많습니다... ㅎㅎ\n지금 다니고 있는 회사의 도메인에 묶여있진 않을지. 여기서 이룬 성과들이 나의 커리어에 도움이 될지.\n\n사실 생각만하면 정리가 안되는 부분들이 많기 때문에 글로 적어보면서 고민들에 대한 나름의 대답들을 적어보려고 합니다.\n\n저는 산업공학을 전공하고, 6개월정도 부트캠프에서 컴퓨터 비전 부분을 공부하고, 의료 도메인에서 일하고 있습니다.\n지금 다니고 있는 회사(visuworks)는 시력교정병원 (비앤빛 안과)를 주된 고객사로 삼고 있고 이 병원에서 만들어졌다고도 볼 수 있습니다.\n\n여기서 지금까지 한 프로젝트는\n- OCR pipeline 개발\n- 고객 상담용 챗봇 개발\n- 렌즈 사이징 추천 서비스 개발\n\n정도가 있습니다.\n\n# 문제 상황들\n\n제가 생각하는 문제들을 정리해보고 어떻게 해결해볼 수 있을지에 대한 생각들을 정리해보려고 합니다.\n\n## 연봉이 이렇게 적나?\n\n되게 단순하게 일을 일단 시작하는 마음으로 이 회사에 고민없이 입사했는데, 연봉이 생각보다 현저히 적어서 조금 놀랐습니다.\n더 정확히는 연봉이 낮지만 일단 경력을 쌓자는 마음으로 입사했습니다.\n\n8개월정도 일하고 나름 2개의 프로젝트를 성공적으로 끝낸 시점에서, 연봉협상을 했지만 큰 폭으로 오르지는 못했습니다.\n\n> 연봉 테이블이 좋은 회사로 빠르게 옮겨야겠다.\n\n하지만 쉽지 않네요... ㅠ\n지금 4~5개월정도 이직을 준비하고 있는데 대부분의 기업에서 떨어지고 있습니다.\n그냥 이정도 가치를 가진 능력인가?라는 의구심이 들지만 보완해야할 점들을 보완하면서 이직을 준비 중입니다."
  },
  {
    "id": "Book/아주 작은 습관의 힘 (Atomic Habits)/쉬워야달라진다",
    "title": "쉬워야 달라진다",
    "date": "2025-07-10",
    "excerpt": "어떻게 좋은 습관을 만들고, 나쁜 습관은 버릴 것인가",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "\n# 습관의 중요성\n\n인생을 살아가면서 여러 중요한 요소들이 있겠지만, 가장 중요한 건 습관인 것 같습니다.\n듀크 대학의 Wendy Wood 교수가 수행한 연구에 따르면, 우리 행동의 약 45%가 습관에 의한 것이라고 합니다.\n\n제가 가장 좋아하는 문구도 다음과 같습니다.\n> Habit is second nature\n\n이렇게 중요한 습관을 어떻게 관리할 수 있을까요?\n좋은 습관을 많이 만들고, 나쁜 습관을 버려 좋은 시스템을 만들어 나간다면 제가 원하는 삶에 가까워질 것이라 생각합니다.\n\n# 쉬워야 달라진다\n\n'아주 작은 습관의 힘'이라는 책에서는 2분 법칙을 제시합니다.\n해야하는 일을 아주 작게 쪼개라며 2분안에 끝낼 수 있는 일로 나누라고 말합니다.\n\n1~2달정도 이 부분을 삶에 적용해봤는데 정말 효과가 많았습니다.\n가장 큰 이유는 심리적 부담감을 줄여주기 때문이라고 생각합니다.\n\n보통 일을 미루는 이유를 생각해보면 심리적 부담감 때문입니다.\n특히 저는 일을 시작하기에 부담스러워서 시작조차 못하고 미루기만 했던 경험이 많습니다.\n\n그런데 해야할 일을 2분안에 끝낼 수 있는 작은 일들로 쪼개니까 이런 부담감이 거의 없어졌고,\n작은 일들을 성취하면서 그 성취감에 점점 더 많은 일들을 할 수 있게 됐습니다.\n\n> 아주 작은 일들로 쪼개자. 2분안에 끝낼 수 있는 일들로.\n\n이건 제 삶의 하나의 원칙처럼 되었습니다.\n아 미루지 말자 라고 생각하는게 아니라, 왜 미루는지 이해하고, 작은 일들로 쪼개기 시작했습니다.\n\n그리고 이렇게 작은 성취들이 모여 노력하지 않아도 자동으로 실행이 되는 습관이 되어갑니다.\n\n# 어떻게 나쁜 습관을 없앨까\n\n좋은 습관을 만드는 것과 반대로 하기 어렵게 만들면 됩니다.\n사람의 의지력은 한계가 있기 때문에 환경이 가장 중요하다고 생각합니다.\n\nRoy Baumeister의 연구에 의하면 의지력은 제한된 자원이라고 합니다.\n의지력은 근육처럼 작동하며, 사용하면 피로해지고 휴식이 필요해지게 됩니다.\n\n> 의지력은 한계가 있다. 환경이 중요함!"
  },
  {
    "id": "English Study/SPEAK/07-09-격려하는표현",
    "title": "격려하는 표현",
    "date": "2025-07-09",
    "excerpt": "You did a great job on, I'm happy to see that",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "\n# 오늘 배운거\n\n- You did a great job on the speaking practice.\n\n- I'm happy to see that you're improving.\n- I'm happy to see that you've overcome your fear of English.\n\n- I can tell that she's sad.\n- I could tell that she broke up the her boyfriend.\n- I can tell that you put a lof of thought into this.\n\n\n# 간단한 작문\n\nToday I wanna complement myself.\n\nYou did a great job on the solving problem.\nWorking on company 1년 조금넘게, I solved a lot of problems.\nOn the OCR project, I solve communicate problem and make greak OCR pipeline.\nOn the chatbot project, I made chatbot which 만족하다 고객사`s needs.\n\nAnd then, I'm happy to see that you`ve overcome of 비교하면서 뒤쳐진다고 느끼는 fear.\nWhen I was 21-25, I was just frozen because of fear that 난 뒤쳐졌고 인생은 망했다.\n\nI can tell that your life is great. I 일인분의 삶은 살아간다. And normal life is always wonderful.\n\n# claud-4-sonnet 피드백\n\n> gemini-2-pro보다 나은 듯\n\n## 수정된 버전:\n\nToday I want to compliment myself.\n\nYou did a great job at problem-solving.\nWorking at a company for a little over a ye"
  },
  {
    "id": "Causal Inference/Industry Application/what-is-statistical-bias",
    "title": "통계적인 편향과 이를 제거하기 위한 방법론",
    "date": "2025-07-09",
    "excerpt": "실제 데이터에서 통계적 편향을 확인하고 이를 잔차화를 통해 제거하는 과정",
    "category": "Causal Inference",
    "tags": [
      "편향",
      "통계적",
      "bias"
    ],
    "content": "\n# 배경\n---\n\n인과추론의 기본적인 개념은 통계적 편향 (Statistical Bias)을 제거하는 것입니다.\n인과추론을 더 잘 이해하고 효과를 확인하기 위해서는, 이 통계적 편향을 이해하고 확인하는 과정이 필요하다고 생각합니다.\n\n# 정의\n---\n\n> 통계적 편향(Statistical Bias)은 통계 분석 과정에서 발생하는 체계적인 오류로, 측정이나 추정 과정에서 실제 모집단의 모수(parameter)와 표본 통계량 사이에 일관된 차이가 발생하는 현상을 말합니다.\n\n- 체계적(systematic) 오류란?\n    - 측정 방법이나 분석 방법에 문제가 있어 발생하는 오류\n    - 무작위가 아닌, 일정한 패턴이나 방향성을 가짐\n- 체계적 오류 (Systematic Error) vs 무작위 오류 (Random Error)\n    - 체계적 오류 (Systematic Error):\n        항상 같은 방향으로 발생 (예: 항상 실제값보다 높게 측정)\n        - 측정 횟수를 늘려도 줄어들지 않음\n        - 측정 도구나 방법의 결함에서 비롯됨\n    - 무작위 오류 (Random Error):\n        - 방향이 불규칙함 (때로는 높게, 때로는 낮게)\n        - 측정 횟수를 늘리면 평균적으로 상쇄됨\n        - 우연한 변동에서 비롯됨\n\n\n제가 이해한 개념은, 랜덤 노이즈가 아닌 구조상의 문제가 있어 결과를 확인하는데 오해를 만드는 것이라고 생각합니다.\n\n예를 들어 보면:\n\n- 구조상 문제: 온라인 설문조사로만 의견을 수집 → 디지털 기기 사용이 어려운 고령층 의견 누락되고 젊은 층의 의견만 반영됨\n- 결과의 오해: \"젊은 층의 의견이 전체 의견\"이라고 잘못 해석할 수 있습니다.\n\n\n## 통계적 편향의 종류와 설명\n\n### 1. 선택 편향 (Selection Bias)\n> 연구 대상을 선택하는 과정에서 발생하는 편향으로, 표본이 모집단을 제대로 대표하지 못할 때 발생합니다.\n\n**예시:**\n- 병원 데이터만으로 질병 연구를 할"
  },
  {
    "id": "Reflections/2025-07-08-optimization-pitfall",
    "title": "최적화의 함정에 대하여",
    "date": "2025-07-08",
    "excerpt": "패배주의에서 벗어나자",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "\n# 배경\n\n저는 꽤나 오랜시간 일종의 패배주의에서 벗어나지 못하고 고통스러워하며 시간을 보냈습니다.\n저보다 훨씬 뛰어난 사람들을 보며 나는 저런 길을 걸어오지 않았으니까 안될 것 같다는 생각들에 사로잡혀 있었습니다.\n\n이러한 사고방식을 어떻게 정의할 수 있을지 몰랐는데, \"뉴욕털게\"님의 영상들을 보면서 \"최적화의 함정\"에 빠졌다고 정의할 수 있었습니다.\n\n# 최적화의 함정이란?\n\n인생에는 어떤 최적화된 길이 있고, 이 길을 따라가야하며, 따라가지 못한 나는 패배재자라고 생각하는 것입니다.\n그래서 최적화된 길을 찾으려고 하면서 완벽주의에 빠지게 되는 것입니다.\n\n완벽한 계획에 대해 고민하면서 시간을 보내고, 막상 앞으로 한발자국도 나아가지 못하는 상황에 빠지게 됩니다.\n\n[뉴욕털게님 영상 링크](https://www.youtube.com/watch?v=aB58_Z7ShT4)\n\n>계획이 원대해지면 내 하루는 비참해진다.\n\n가장 인상깊은 말이자, 제 과거 삶을 관통하는 말입니다.\n\n# 뉴욕털게님의 사고방식\n\n뉴욕털게님이 말씀하시는 부분들을 제 삶에 정말 많은 도움이 되고 있습니다.\n여러가지 개념들을 말씀해주시는데, 공통적인 개념은 \"수용의 자세\"가 아닐까 싶습니다.\n이는 제가 좋아하는 \"법륜스님\"의 말씀과 비슷한 점이 많습니다.\n\n과거, 어쩌면 지금도 제가 고통받는 이유를 생각해보면 제가 원하는 이상적인 상을 그려놓고 이것과 비교하면서 제 하루하루를 비참하게 생각했기 때문이 아닐까 싶습니다.\n\n역설적으로 제 삶을 부정하고 원대한 계획을 세울수록 삶은 나아지지 않는 것 같습니다.\n그저 비참한 하루를 보내며 시간을 보내기만 할 뿐입니다.\n\n그래서 이 수용의 자세는 삶을 살아가는데 굉장히 중요한 자세가 아닐까 싶습니다.\n\n지금 내 모습이 내가 상상했던 모습은 아니지만, 충분히 훌륭하다.\n1인분의 삶을 온전히 살아나가고 있고 조금씩 나아가고 있다.\n이렇게 지금 삶을 온전히 수용할 때, 비로소 발전할 수 있는 내가 된다고 생각합니다.\n\n# 나의 삶에 적용해보기\n\n원하는 회"
  },
  {
    "id": "Reflections/2025-07-08-how-to-write-resume",
    "title": "이력서는 어떻게 적어야할까?",
    "date": "2025-07-08",
    "excerpt": "이력서를 적으며 했던 나의 고민들",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "\n# 나의 배경\n\n일한지 1년이 조금 넘는 시점에서 이직을 준비하고 있습니다.\n이력서를 준비하며 나를 어떻게 표현하면 좋을지에 대한 고민들을 하고 있습니다.\n\n모르는 내용을 공부하고 좋은 코드를 작성하고 좋은 모델을 만드는 일들은 어렵지만 그 길이 나름 명확합니다.\n이런 과정을 적어도 5년이상 거쳐왔으니 자신이 있습니다.\n\n하지만 나를 표현하는 부분들은 정말 어렵게 느껴집니다.\n나는 진짜 경쟁력이 있는지에 대한 의구심부터, 내가 했던 일들을 어디서부터 어디까지 설명해야하는지 등등\n\n이런 과정에서 했던 고민들을 하나씩 정리해봅니다.\n\n\n\n# 내 이력서에 대한 피드백\n\n## 사이드프로젝트를 같이하는 개발자분\n> 임팩트가 없다. 이력서를 더 짧게 쓰고 포트폴리오를 풍부하게 가져가는게 좋겠다.   \n\n이 피드백에 동의하는 부분이 많았습니다.\n내가 한 프로젝트의 결과를 수치적으로 표현하는데에만 집중했지, 진짜 어떤 문제를 풀어서 어떤 영향을 줬는지에 대한 내용이 부족한 것 같다고 느꼈습니다.\n특히 포트폴리오를 따로 준비하지 않았던 부분은 꼭 수정이 핗요하다고 느껴집니다.\n\n"
  },
  {
    "id": "Recommendation/2025-07-08-thoughts-on-review-quality",
    "title": "리뷰 노이즈에 대한 고민",
    "date": "2025-07-08",
    "excerpt": "카카오맵 리뷰 데이터의 노이즈들에 대하여",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템",
      "추천시나리오"
    ],
    "content": "\n\n# 배경\n\n데이터과학자로 1년 조금 넘게 일하면서 가장 크기 느끼는 부분은, **데이터의 중요성**입니다.\n모델은 데이터 안에서 패턴을 찾을 뿐, 그 안에 패턴이 없거나 노이즈가 크면 모델은 제 역할을 못하게 된다고 생각합니다.\n\n그래서 데이터를 뜯어보고 그 안의 노이즈를 살펴보는 일은 굉장히 중요한 일입니다.\n\n> 리뷰 데이터에는 어떤 노이즈가 있는지 살펴보고 이를 정리했습니다\n\n*<small>같이 사이즈 프로젝트를 하는 '신보현'님의 분석 결과를 참고해 정리헀습니다.</small>*\n\n# 문제 상황들\n\n여러 노이즈들이 존재하는데 그 중 가장 심각하다고 느끼는 부분들에 대해 정리했습니다.\n\n## 1. 장소(음식점)과 상관없는 이유인 부정적/긍정적인 리뷰들\n\n가장 눈에 띈 부분은 계엄과 그 후의 시위들과 관련된 리뷰들입니다.\n계엄을 찬성하거나 반대할 경우 관련된 사람들이 '댓글 테러'를 하는 경우를 발견했습니다.\n\n그 외에도 제가 다니고 있는 헬스장에 여자 트레이너가 뚱뚱하다는 이유로 부정적인 리뷰를 남기는 경우도 있었습니다.\n\n\n## 2. 마케팅 목적으로 작성된 리뷰들\n\n마케팅을 위해 의도적으로 작성된 리뷰들입니다.\n예전에 알바했던 음식점에서도 이런 마케팅을 진행했었습니다.\n\nAI로 작성해서 문체에 티가 나는 경우라면 다르게 접근할 수도 있겠지만, \n사람이 작성한 듯한 댓글이 많아서 이를 어떻게 거를 수 있는지도 고민할 필요가 있습니다.\n\n\n# 그래서?\n\n이런 부분을 상쇄할만큼 리뷰가 많으면 이게 상쇄가 되는지, 혹은 모델로 이를 극복할 수 있을지.\n데이터를 필터링할 수 있는 부분들을 고민해야할지에 대한 논의가 추가로 필요한 상황입니다.\n\n이 부분들에 대한 내용도 추가로 정리할 예정입니다."
  },
  {
    "id": "Recommendation/2025-07-08-thoughts-on-deciding-scenario",
    "title": "시나리오를 정하기 위한 고민",
    "date": "2025-07-08",
    "excerpt": "추천시스템을 구현하는 사이드 프로젝트를 진행하면서 추천 시나리오는 어떻게 정하면 좋을지에 대한 논의를 정리했습니다.",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템",
      "추천시나리오"
    ],
    "content": "\n# 배경\n\n사이드 프로젝트를 진행하며 추천 서비스를 구현해보고 있습니다.\n구현하면서 필요한 여러 고민 중 하나는 어떤 상황에서 어떻게 추천을 해줄 것인지에 대한 고민입니다.\n\n이를 추천 시나리오라고 정의하고 이를 구체화하는 과정에 있습니다.\n\n# 콜드 유저에 대해 어떻게 추천할 것인가\n\n서비스를 새롭게 만들게 된다면 이 서비스를 사용하는 유저는 콜드 유저일 것입니다.\n이에 대해 어떻게 대처할 수 있을지에 대한 논의가 주된 논의였습니다.\n\n## 1. 인기도 기반 추천 + 필터링\n\n가장 대표적으로 사용되는 방법입니다.\n여기에 추가로 날씨라던지, 위치라던지, 특정 필터링을 붙이는 방식입니다.\n\n## 2. 유저 프로필 완성 (온보딩 기반 추천)\n\n유저에게 정보를 완성하도록 요청하고, 이를 바탕으로 추천을 하는 방식입니다.\n대표적으로 왓챠나 넷플릭스의 경우, 처음에 본인이 좋아하는 영화를 선택하도록 구성하고 그 후 이를 바탕으로 추천을 해줍니다.\n\n이런식으로 유저 정보를 받을 수 있는 서비스에 대한 논의를 했습니다."
  },
  {
    "id": "Recommendation/2025-07-08-cold-start-solution",
    "title": "Cold Start 해결 방법에 대한 고민",
    "date": "2025-07-08",
    "excerpt": "사이드프로젝트에서 진행한 cold start 해결방안과 데이터의 한계점 극복 고민",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템"
    ],
    "content": "\n# 배경\n\n모두의 연구소에서 \"쩝쩝LAB\"이라는 이름으로 진행한 사이드 프로젝트에 대한 정리입니다.\n맛집 추천 시스템을 구현하고 있습니다.\n\n여러 과제 중 Cold Start를 어떻게 해결할지에 대해 논의한 내용들을 정리해봤습니다.\n\n# Cold Start Problem이란?\n\n> 사용자나 아이템에 대한 정보가 없거나 희소한 문제\n\n이 중에서 유저에 대한 cold start 문제를 어떻게 풀지에 대해 고민하고 있습니다.\n\n\n*<small>같이 사이즈 프로젝트를 하는 '이윤선'님의 분석 결과를 참고해 정리헀습니다.</small>*\n\n\n# 기본적인 추천의 컨셉\n\n> Popularity Model에서 Context를 반영하여, 유저가 만족할만한 음식을 추천해주고 싶다.   \n\n콜드 유저에게 인기도 기반 추천을 내주는 것처럼, 계절과 날씨를 고려해 추천을 내주면 좋을 것 같다는 아이디어입니다.\n\n(예시)\n| 상황 | 유저의 생각 | 추천 가능 음식 |\n| --- | --- | --- |\n| 맑고 청명한 날 | “밖에 나가서 먹고 싶어” | 샌드위치, 김밥, 분식 |\n| 흐리고 습한 날 | “뭔가 시원한 게 땡긴다” | 냉면, 물회, 아이스커피 |\n| 비 올 것 같은 날 | “집에 일찍 가고 싶어” | 국물 요리, 칼국수, 해장국 |\n| 겨울철 | “대게가 제철이네?” | 대게찜, 어탕국수, 전골류 |\n\n계졀 날씨 데이터를 모델에 포함시켜야 하는 이유로 3가지를 제시했습니다.\n\n\n- 왜 계절 날씨 데이터를 모델에 포함해야 할까?\n    - 개인화 추천 강화\n        → 동일한 유저도 날씨에 따라 선택이 달라짐\n        → ‘유저 + 날씨’ 조합 기반의 더 똑똑한 추천 가능\n        \n    - 모델의 정밀도 향상\n        → 기존 모델에 컨텍스트 데이터를 추가함으로써 예측 정확도 향상\n    \n    - Cold Start 상황에서도 강력한 보완\n        → 유저 정보가 없을 때도, **그날의 날씨 + 인기 메뉴**로 합리적인 추천 가능\n\n"
  },
  {
    "id": "English Study/SPEAK/07-08-칭찬하는표현",
    "title": "칭찬하는 표현",
    "date": "2025-07-08",
    "excerpt": "I was really impressed, It couldn`t be any better, I couldn`t have done it without you",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "\n# 오늘 배운거\n\n- I was really impressed\n- I was really impresed with your exhibit\n\n- It couldn`t be any better\n\n- I couldn`t have done it without you\n- I couldn`t have graduate without you\n\n# 이거 사용해서 간단한 작문\n\nToday, I attend side-project 모임.\nWe talked about recommmend system.\nThe topic was, how to delete data noise, how to check '가정', distribute tasks for project.\n\nAfter that 동료 ,who 프로젝트를 같이하는 사람, give me advice.\nBecause nowadays I fall 서류탈락, so I ask them how can I improve my 이력서.\n\nI was really impressed with their advice.\nThey 고민하다 for me. How to improve my resume.\n\nIt couldn`t be any better. I was so 감동받다.\n\nI always appreciate this project.\nThey gave me so many 도움.\nI couldn`t have be better person without them.\n\n# 문장수정 (gemini-2.5-pro)\n\nToday, I attended a side-project meeting.\nWe talked about recommendation systems.\nThe main topics were how to remove noisy data, how to test our assumptions, and how to distribute tasks for the project.\n\nAfter the meeting, a teammate who is working on th"
  },
  {
    "id": "Codility Study/공부방향",
    "title": "코딩테스트 공부 방향",
    "date": "2025-07-08",
    "excerpt": "코딩테스트 공부 방향에 대한 정리",
    "category": "Codility Study",
    "tags": [
      "고민정리"
    ],
    "content": "\n\n\n# 배경\n\n잠깜 공부하다가 내려놓고 다시 공부를 시작하려고 합니다.\n주니어, 혹은 시니어더라도 코딩테스트는 선택이 아닌 필수가 아닐까 싶습니다. (적어도 10년 이하까지는?)\n그래서 꾸준히 공부해보려고 하는데 어떤 방향으로 잡을지에 대한 정리를 해보려고 합니다.\n\n또 사이드 프로젝트를 같이 하는 '신보현'님의 경험을 들어보니, 라이브코테는 필수적으로 보고 난이도가 꽤 있어서 꾸준한 준비가 필요하다고 합니다.\n지금부터라도 꾸준히 공부해보려고 합니다.\n\n# 코테 공부하기 좋은 곳 (GPT 정리)\n> 와 엄청 정리 잘해주네요...\n\n코딩 테스트(코테) 공부에 도움이 되는 좋은 사이트들을 아래에 용도별로 정리해드릴게요. 각 사이트마다 특성이 다르니, 본인의 학습 목적(기초 다지기, 문제 풀이, 실전 대비 등)에 맞춰 선택하시면 됩니다.\n\n---\n\n## ✅ **1. 문제 풀이 중심**\n\n### **백준 온라인 저지 (BOJ)**\n\n* **URL**: [https://www.acmicpc.net/](https://www.acmicpc.net/)\n* **특징**:\n\n  * 국내 코테 준비자들이 가장 많이 사용하는 사이트.\n  * 다양한 알고리즘 분류, 난이도별 문제 제공.\n  * **단계별로 풀어보기**, **문제집 기능**으로 체계적인 학습 가능.\n  * C++, Python, Java 등 다양한 언어 지원.\n\n### **프로그래머스 (Programmers)**\n\n* **URL**: [https://programmers.co.kr/learn/challenges](https://programmers.co.kr/learn/challenges)\n* **특징**:\n\n  * 실제 기업 코딩테스트 문제 수록 (카카오, 네이버 등).\n  * 실전 감각을 기르기에 적합.\n  * **레벨별로 분류**되어 있어서 입문자부터 고급자까지 활용 가능.\n  * Python, JavaScript 등 실무에 많이 쓰는 언어에 최적화.\n\n### **LeetCode**\n\n* **URL**: [ht"
  },
  {
    "id": "Reflections/2025-07-03-importance-of-numbers",
    "title": "수치의 중요성",
    "date": "2025-07-03",
    "excerpt": "수치로 표현하는게 왜 중요한지에 대한 생각 정리",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "\n"
  },
  {
    "id": "Causal Inference/what-is-fwl",
    "title": "FWL(Frisch-Waugh-Lovell) 정리란?",
    "date": "2025-07-02",
    "excerpt": "통계적 편향을 제거하기 위한 방법론",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "\n## 참고자료\n\n1. 실무로 통하는 인과추론 with python\n2. [네이버블로그_ (선형모형) 04 Frisch-Waugh-Lovell 정리](https://blog.naver.com/dillid94/222482368731), 수식 풀이가 잘되어 있음\n\n# 배경 설명\n\n---\n\n인과추론이란 \"원인 -> 결과\"의 패턴인 \"인과관계\"를 찾는데에 목적이 있습니다.\n이 패턴을 찾는건 어려운 일인데, 그 이유 중 하나는 원인과 결과에 모두 영향을 주는 교란변수 때문입니다.\n교란변수(confounding variable)란 종속변수와 독립변수에 모두 영향을 줘 잘못된 인과관계를 찾도록 만드는 변수를 말합니다.\n\n즉 교란변수가 잘못된 패턴을 찾도록 유도하기 때문입니다.\n\n![confounding variable 예시그림](/post/confounding_bias_예시그림.png)\n\n가장 대표적인 예시로 \"아이스크림 판매량\"과 \"상어 어택횟수\"를 생각해볼 수 있습니다.\n\"기온\"이라는 변수는 \"아이스크림 판매량\"과 \"상어의 공격횟수\"에 모두 영향을 줍니다.\n\n그래서 \"기온\"이 올라가서 \"아이스크림 판매량\"이 증가하고 \"상어의 공격횟수\"가 증가한 것인데, \"아이스크림 판매량\"이 증가해서 \"상어의 공격횟수\"가 증가했다는 생각을 하게 됩니다.\n\n즉 \"교란 변수\"는 변수간의 관계를 잘못 해석할 가능성을 만들게 됩니다.\n따라서 변수간의 관계를 볼 땐 교란변수를 제거해주는게 굉장히 중요합니다.\n그렇다면 다음과 같은 질문을 해볼 수 있습니다.\n\n> 교란변수를 없애야 한다는 건 이해했어, 그러면 이걸 어떻게 없앨 건데?\n\n가장 좋은 건 교란변수를 파악하고 실험 설계를 통해 없애는 것입니다.\n위의 예시에서는 \"기온\"이라는 교란편수를 파악하고, 이를 통제한 후에 두 변수 간의 관계를 살펴볼 수 있습니다.\n\n하지만 교란변수를 파악하더라도 이를 통제할 수 없을 때가 있습니다.\n예를 들어 수술방법(독립변수)과 수술 후 결과(종속변수)의 관계에서는 나이, 생활습관 등의 교란변수가 있습니다.\n\n"
  },
  {
    "id": "Causal Inference/correlation-vs-causation",
    "title": "상관관계와 인과관계의 이해",
    "date": "2025-06-28",
    "excerpt": "상관관계와 인과관계의 차이점을 이해하고, 예측 모델의 한계와 인과추론의 중요성에 대한 이야기",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "\n> 상관관계와 인과관계를 이해하고 구분하는 건 결과를 해석할 때 중요한 점입니다.\n\n상관관계란, 두 변수 사이에 일정한 패턴이나 동반 변동이 존재함을 의미하며, 한 변수가 커질 때 다른 변수가 일정하게 커지거나(양의 상관), 작아지는(음의 상관) 경향을 보이는 통계적 관계를 말합니다. 인과관계란, 한 변수가 변할 때 다른 변수가 그 변화에 의해 직접적·체계적으로 영향을 받아 변화하는 관계를 의미합니다\n\n보통 두 변수간의 관계를 살펴볼 때 상관관계를 살펴보곤 합니다. 하지만 상관관계를 인과관계로 해석하지 않도록 주의해야 합니다.\n\n예를 들어 아이스크림 판매량과 일일 기온 사이에는 높은 기온일수록 판매량이 증가하는 강한 양의 상관관계가 관찰되지만, 이는 기온이 아이스크림 판매량을 직접 “원인”한다고 단정할 수 없으며, 사람들이 더운 날씨에 밖에 나와 판매량이 늘어나는 등 다양한 제3의 요인이 동시에 작용했을 가능성을 배제할 수 없습니다.\n\n> 예측 모델은 주어진 데이터에서 변수 간의 동시 변동 패턴, 즉 상관관계를 학습해 결과를 예측합니다.\n\n이 모델의 예측결과는 인과관계를 보장하지 못합니다.\n\n|  | 가격 | 매출 |\n| --- | --- | --- |\n| 1 | 100 | 1000 |\n| 2 | 150 | 900 |\n| 3 | 600 | 10000 |\n| 4 | 700 | 12000 |\n\n예를 들어 숙박 가격과 매출이 주어진 데이터에서 가격이 높을수록 매출이 높아 보인다면, 모델은 “가격이 오르면 매출이 오른다”고 예측할 것입니다. \n하지만 이 패턴 뒤에는 “성수기/비수기 여부”라는 숨겨진 외부 요인이 있을 수 있습니다. \n\n가격이 비수기에 낮아지고 성수기에 올라가는 동시에 매출도 함께 변했다면, 실제로는 계절성이 매출을 결정짓는 진짜 원인인데, 단순 예측 모델은 이를 구분하지 못합니다. \n따라서 상관관계에 기반한 예측 결과를 인과관계로 오해하면 “매출을 올리려면 가격을 올려야 한다”는 잘못된 결론에 이를 수 있고, 실제 효과가 있는 개입(예: 비수기 프로"
  },
  {
    "id": "Causal Inference/what-is-causal-inference",
    "title": "인과추론이란?",
    "date": "2025-06-20",
    "excerpt": "인과추론의 개념에 대해 소개하는 이야기",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "\n“인과추론” (Causal Inference)은 최근에 의료 분야, 마케팅 분야 등 선택에 대한 분석이 필요한 곳에 자주 사용되는 개념입니다. \n\n인과추론은 다음과 같은 질문들에 답을 합니다. \n“마케팅 비용을 늘렸더니 매출이 증가했네, 마케팅 비용을 늘려서 그런건가?” . “환자에게 A라는 약물을 처방했더니 상태가 괜찮아졌어. A 덕분인가?”\n\n이러한 질문들은 전통적으로 대조군과 실험군을 통해 증명되어왔습니다.\n실험하려는 조건 외에 다른 조건은 모두 통제한 후에 실험결과를 비교합니다.\n\n예를 들어 새로 개발한 비료의 효과를 검증하려고 합니다. \n같은 품종의 옥수수 묘목 100포기를 두 그룹으로 나누어, 실험군에는 새 비료를 표준량만큼 투입하고 대조군에는 기존 비료(혹은 비료를 전혀 주지 않음)를 동일한 방식으로 처리합니다. \n이때 물주기, 일조량, 토양 성분, 온도 등 나머지 재배 조건은 두 그룹에서 완전히 동일하게 유지합니다. \n일정 기간 후 두 그룹의 생장 속도, 수확량, 작물 건강 상태를 비교하면, 오로지 “비료 종류”의 차이만이 결과에 영향을 미쳤다고 판단할 수 있게 됩니다.\n\n이러한 접근법은 가장 합리적이지만, 현실 문제에 적용하기엔 어려움이 있습니다.\n마블의 멀티버스 세계관이 아니라면, 2025년 7월 1일에 마케팅 비용을 100만원과 1000만원을 모두 사용할 수 없습니다.\n한 명의 환자에게 렌즈 사이즈를 12.1를 삽입한 후 결과를 확인하고, 렌즈를 뺀 후 12.6를 삽입해 결과를 확인할 수 없습니다.\n즉, 대조군과 실험군을 설정하는데 어려움이 있습니다.\n\n인과추론은 이를 통계적으로 보완하여 결과를 추정하는 방식입니다.\n\n예를 들면, 온라인 쇼핑몰에서 A라는 광고 캠페인이 판매량에 미친 영향을 알고 싶을 때, 실제로는 동일한 고객에게 광고를 보여주지 않은 상태와 보여준 상태를 모두 경험시킬 수 없으므로 인과추론 기법을 활용합니다. \n이때 고객의 연령, 성별, 과거 구매 이력 등 다양한 **고객 특성**을 보고, 광고를 본 그룹과 보지 않은 그"
  },
  {
    "id": "Causal Inference/Paper Review/review-Multi-Study-R-Learner-for-Estimating-Heterogeneous-Treatment-Effects-Across-Studies-Using-Statistical-Machine-Learning",
    "title": "[Paper Review] Multi-Study R-Learner for Estimating Heterogeneous Treatment Effects Across Studies Using Statistical Machine Learning",
    "date": "2025-01-16",
    "excerpt": "Multi-Study R-Learner 논문에 대한 리뷰 및 분석",
    "category": "Causal Inference",
    "tags": [
      "R-Learner",
      "Heterogeneous Treatment Effects",
      "Multi-Study",
      "Statistical Machine Learning",
      "Paper Review"
    ],
    "content": "\n[paper link](https://arxiv.org/pdf/2306.01086)"
  },
  {
    "id": "Career/Graduate School/OMSCS관련",
    "title": "OMSCS(Georgia Tech 온라인 CS 석사) 지원 준비 계획",
    "date": "2025-01-15",
    "excerpt": "세계 최고 가성비 CS 석사 프로그램 분석과 지원 전략",
    "category": "Career",
    "tags": [
      "대학원",
      "OMSCS",
      "Georgia Tech",
      "컴퓨터사이언스"
    ],
    "content": "\n# 배경\n\n데이터 사이언스 분야에서 더 깊이 있는 커리어를 쌓기 위해 석사 과정을 고려하던 중, OMSCS(Georgia Tech Online Master of Science in Computer Science)라는 혁신적인 프로그램을 발견했다. 세계 톱급 CS 프로그램을 온라인으로, 그것도 극도로 저렴한 비용으로 이수할 수 있다는 점이 매력적이다.\n\n# OMSCS란?\n\n## 📊 기본 정보\n- **정식명칭**: Georgia Institute of Technology Online Master of Science in Computer Science\n- **개설연도**: 2014년 (Udacity, AT&T와 협력)\n- **총 비용**: 약 $7,000-$8,500 (한화 900만-1,100만원)\n- **학위**: 캠퍼스와 동일한 Georgia Tech MS in Computer Science (온라인 표기 없음)\n- **기간**: 평균 2-3년 (파트타임으로 진행 가능)\n- **과정 구성**: 10개 과정 (30학점)\n- **합격률**: 약 70%\n\n## ✅ 주요 장점\n\n### 1. **압도적인 가성비**\n- 전체 프로그램 비용이 $7,000-$8,500 (한화 900만-1,100만원)\n- 타 명문대 온캠퍼스 프로그램 대비 1/5~1/10 수준\n\n### 2. **명문대 브랜드 + 동등한 학위**\n- Georgia Tech는 CS 분야 세계 8위 (US News 2024)\n- 졸업장에 \"온라인\" 표기 없음\n- 실리콘밸리에서 인정받는 브랜드\n\n다른 블로그 글들이나 링크드인 봤을 때 job interview 기회도 확실히 더 얻을 수 있다.\n해외 이직도 고려하고 있으니까 지금 상황에서 할 수 있는 가장 좋은 선택이 아닐까.\n\n### 3. **유연성**\n- 풀타임 직장과 병행 가능\n- 자신의 페이스로 진도 조절\n- 전 세계 어디서나 수강 가능\n\n보통 1과목당 주당 20~40시간정도 필요하다고 함.\n\n\n### 4. **실무 중심 커리큘럼**\n- Machine Lear"
  },
  {
    "id": "Recommendation/2024-07-05-recommendation-system-interest",
    "title": "추천시스템에 관심있는 이유",
    "date": "2024-07-04 23:30:00 +0900",
    "excerpt": "",
    "category": "Recommendation",
    "tags": [
      "Recommendation"
    ],
    "content": "\n잠깐 추천시스템을 공부했었는데, 다시 공부를 시작하면서 프로젝트를 하나씩 쌓아가려고 한다.  \n그 전에 내가 왜 관심이 있는지, 또 어떤 프로젝트를 해보고 싶은지 정리해보려 한다.   \n\n\n## 추천시스템이란?\n\n추천시스템은 검색과 비슷한 목적을 가지고 있다.   \n> 많은 정보 속에서 필요한 정보를 필터링하는 것\n\n\"진짜 많은 정보들, 컨텐츠들이 있는데 유저에게 어떤 것을 보여줘야할까?\"에 대한 답을 하는 것이다.  \n\n## 왜 중요할까?\n\n추천시스템이 필요한 곳은 대표적으로 OTT 회사들이 떠오른다.   \n\n소비자는 언제 구독을 그만둘까?  \n> 당연하게도 더 이상 볼게 없다고 느껴질때 그만둘 것이다.  \n\n전체 컨텐츠에 비해 소비한 컨텐츠는 소수일텐데 왜 볼게 없다고 느껴질까?  \n> 뭘 봐야할지 모르겠어서, 어떤게 내 재밌을지 몰라서.  \n   \n그래서 소비자에게 취향에 맞는 컨텐츠를 꾸준히 노출해줘야하고,   \n그래야 소비자가 떠나지 않도록 만들 수 있다.   \n\n## 어려운 점\n\n내가 생각했을 때 추천시스템을 구성하는데 가장 어려운 것은 소비자의 평가 데이터가 적기 때문인 것 같다.   \n어떤 사람이 평가한게 적으면 당연히 취향을 분석하기 어렵고, 그래서 추천해주기도 어렵다.  \n\n평가 데이터가 많은 사람에게 잘 추천해주는 것도 굉장히 중요하지만,   \n평가 데이터가 부족한 사람에게 어떻게 추천해줄지에 대한 문제를 해결하는게 가장 중요한 것 같다.   \n\n## 해보고 싶은 프로젝트 \n\n요즘 해보려는 프로젝트는 LLM을 이용해서 소비자의 취향을 알아내는 것이다.   \n구체적으로는 Multi-Agent-System으로 추천시스템을 구축해보려고 하고,    \n맨 앞단에 취향을 분석하는 Agent를 구축해 사용해보려고 한다.   \n\n먼저 영화로 시작해서 책과 같은 분야로도 확장해보려고 한다.  \n\n취향을 분석한다는게 굉장히 애매한 부분인 것 같아서 고민이 많이 필요할 것 같다.  \n이게 가능해진다면 많은 산업에서 정말 매력적으로 느끼는 기술이 아닐까?  "
  },
  {
    "id": "Reflections/2024-07-04-interest-concerns",
    "title": "관심사에 대한 고민",
    "date": "2024-07-04 19:49:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "Interest"
    ],
    "content": "\n\n\n## 현재 나의 상황\n\n이제 데이터과학자로 일한지 3개월정도 되었다.  \n아이펠이라는 모두의 연구소에서 운영하는 부트캠프를 진행했었는데, 이때 기업연계프로젝트로 진행했던 기업에 취업을 했다.  \n분야는 안과의료분야이며, 비앤빛 밝은세상 안과로부터 만들어진 스타트업이다.  \n\n이때 진행한 프로젝트는 당뇨병성 망막병증의 병변들을 탐지해내는 모델을 만드는 것이었다.  \n이에 대한 링크는 다음과 같다.  \n- [발표자료](https://github.com/mkk4726/DR-GeuAl/blob/main/%EC%B5%9C%EC%A2%85%EB%B0%9C%ED%91%9C.pdf)\n- [발표링크](https://www.youtube.com/watch?v=ox_jmqZ1V64)\n\n결과적으로는 의료 데이터를 가지고 Segmentation 작업을 수행하고 발전시켜볼 수 있어서 좋았다.  \n또한 이게 어떻게 쓰일 수 있을지에 대한 고민도 해볼 수 있어서 좋았다.  \n\n그렇게 인공지능을 통해 건강에 기여할 수 있겠다는 꿈을 가지고 입사를 하게 되었다.  \n대부분의 스토리가 그러하듯, 나 역시 기대와 많이 다른 회사생활을 하게 되었다.  \n\n## 안과 분야에서의 인공지능 개발현황\n\n의료 분야 중에서 특히 안과에 대해서만 알고, 그래서 안과에 대해서만 한정해서 이야기하려고 한다.  \n\n### 비쥬웍스\n\n내가 일하고 있는 비쥬웍스라는 기업은 내가 느끼기에 가장 성취가 적다.  \n의료인증을 받은 모델도 없고, 받으려면 아직 멀었다.  \n\n자세하게 이야기할 수는 없지만, 회사의 방향도 당장의 수익을 만드는 것에 집중하고 있다.  \n따라서 내가 원했던 부분과는 많이 다르게 일을 하고 있다.  \n\n다른 회사로 이직하면 되는 걸까?   \n선두기업들은 어떻게 하고 있을까?    \n\n### 뷰노\n\n뷰노에서는 안저사진을 통한 질병 분류 모델을 출시하고, 결과에 CAM도 같이 보여준다.   \n(CAM을 통해 모델이 어느 부분을 집중적으로 보고 있는지 '추정'할 수 있음)   \n병원에 있는 의사분께"
  },
  {
    "id": "Data Science/Tableau/2024-07-01-tableau-python-cloud-upload",
    "title": "태블러 python으로 데이터 cloud 업로드",
    "date": "2024-07-01 12:00:00 +0900",
    "excerpt": "",
    "category": "Tableau",
    "tags": [
      "Tableau",
      "Python"
    ],
    "content": "\n태블러 prep을 이용해 데이터 파이프라인을 구축해 태블러 클라우드에 데이터 웨어하우스를 구축할 수도 있지만,   \n버그도 많고 제한사항이 많다고 느껴져 파이썬에서 처리한 후 업로드하는 것을 선택했습니다.   \n  \n업로드 하기 위해서는 2가지 과정이 필요합니다.   \n1. csv to hyper\n2. hypter to cloud\n\n순서대로 코드를 공유하면서 간단히 설명하겠습니다.   \n\n## 1. csv to hyper\n\n태블러 클라우드에는 hyper 데이터타입이 들어가야해서 정제된 csv 파일을 hyper로 바꿔줘야 합니다.   \ncsv 파일을 바꾸는 것만 다루지만 다른 파일들도 쉽게 응용할 수 있을 것이라 생각합니다.   \n\n3개의 인자를 받는데 csv 경로, hyper 저장할 경로, csv 파일 타입 정의한 객체 경로.   \ncsv로 저장하는 과정에서 맘대로 데이터타입이 바뀌는 문제가 있어서 타입을 정의한 객체도 따로 저장한 후에 불러올 때 참고하도록 했습니다.   \n\nhyper 타입 데이터를 방식은 빈 테이블을 하나 만들고 채워나가는 것입니다.   \n따라서 어떤 테이블을 만들 것인지를 정의해줘야합니다.  \n이를 위해 columns 리스트를 정의해줍니다.   \n\n그 다음으로는 어떤 값을 추가할지 정의해줘야합니다.  \n이게 row 리스트를 정의하는 이유입니다.  \n여기서 주의깊게 봐야할 것은 null값을 처리하는 방식인데 pandas dataframe의 NaN값을 None으로 바꿔서 넣어줘야합니다.   \n그렇지 않으면 데이터타입이 맞지 않다는 오류가 발생합니다.  \n\n```python\ndef csv_to_hyper(csv_file_path, hyper_file_path, dtype_dict_path):\n    # CSV 파일 읽기\n    dtype_dict = load_data(dtype_dict_path)\n    df = pd.read_csv(csv_file_path, dtype=dtype_dict)\n\n    # 테이블 정의 해주기\n    column"
  },
  {
    "id": "Reflections/2024-04-10-job-hunting-important-things",
    "title": "취준할 때 중요한 것",
    "date": "2024-04-10 12:00:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "취준"
    ],
    "content": "\n   \n저는 현재 의료인공지능 회사에서 데이터과학자로 일하고 있습니다.   \n제가 데이터과학자를 준비하면서 가장 중요하게 생각했고, 도움이 됐던 생각을 공유해보려 합니다.    \n\n   \n> 회사와 비슷한 고민을 하고 내가 가진 자원으로 해결해보는 것   \n \n\n## 1. 데이터과학자\n \n데이터과학자란 문제를 정의하고 이걸 데이터로 해결해나가는 사람을 뜻합니다.   \n\n이는 크게 2가지 과정으로 나눠볼 수 있습니다.    \n\n1. 문제를 정의   \n2. 내가 가진 자원과 기술로 해결    \n   \n프로젝트를 하고 이를 포트폴리오를 만들 때는 이 2가지 과정이 꼭 잘 담겨있어야 합니다.    \n> 어떻게 문제를 정의했으며, 이를 해결하기 위한 과정에서 어떤 고민들을 헀는지.    \n\n  \n## 2. 회사\n\n회사는 문제를 정의하고 그걸 해결해나가며 수익을 내는 집단입니다.    \n회사에서 일하는 데이터과학자들은 본인이 정의하거나 운영진에서 정의한 문제를 회사의 자원과 본인의 기술들로 해결해나갑니다.    \n   \n> 데이터과학자를 준비하는 사람과 회사에서 데이터과학자로 일하는 사람의 과정은 완전히 동일하다고 할 수 있습니다.  \n\n## 3. 중요한 부분  \n  \n결국 중요한 것은 나의 과정들과 회사의 과정이 얼마나 겹치는지 입니다.    \n이 교집합이 클수록 본인의 프로젝트 혹은 포트폴리오는 매력적으로 보이게 됩니다.    \n   \n즉, 내가 생각했을 때 이런 문제가 있고 또는 중요하다고 생각한다.    \n그래서 나는 이런 문제를 이렇게 풀어봤다.   \n푸는 과정에서는 이런 어려움이 있었고 이렇게 해결해나갔다.    \n\n이게 포트폴리오에 잘 담겨있어야하며, 취업을 준비할 때 가장 중요한 점이라고 생각합니다.     \n  \n    \n실제로 저도 제가 정의하고 해결했던 문제가 현재 제가 회사에서 해결하고 있는 문제입니다.    \n면접에서도 이걸 가장 좋게 봐주셨고 취업에 가장 도움이 많이 됐다고 생각합니다.   \n   \n만약 제가 다른 분야로 이직을 준비한다면, 위"
  },
  {
    "id": "Reflections/2024-03-12-aifell-review",
    "title": "아이펠 후기",
    "date": "2024-03-12",
    "excerpt": "6개월 동안의 아이펠 후기",
    "category": "Reflections",
    "tags": [
      "아이펠",
      "이런저런 생각들"
    ],
    "content": "\n저는 23.9 ~ 24.2 동안 아이펠 6기 리서치 과정을 수료했고, 회고 겸 후기를 적어보려 합니다.   \n\n![졸업증](/post/아이펠_졸업장.jpeg)\n\n## 1. 신청했을 때의 나의 상황\n\n일단 제가 어떤 상태에서 아이펠을 들었는지 설명드리겠습니다.   \n  \n일단 저는 산업공학을 전공했고, 통계학을 부전공했습니다.  \n학부생때 ML에 관심이 많아서 혼자서 공부했었고, 관련 프로젝트도 진행했습니다.   \nDL 부분은 잘 몰랐는데, 시작하기 1달전에 \"밑바닥부터 시작하는 딥러닝\" 2회독정도 했습니다.   \n  \n제 배경지식은 통계학 + 컴퓨터공학 + DL 조금. 정도로 정리할 수 있겠습니다.   \n\n \n## 2. 내가 생각하는 난이도\n \n처음 1~2달 정도는 그 전에 공부했던 것들이라 복습하는 느낌으로 공부했고, 나머지는 개념정도만 아는 상태에서 공부를 했습니다. 그래서 따라가는데 어려움은 없었습니다.   \n\n다만, 양이 워낙 많아서 평균적으로 추가공부를 3~4시간, 적어도 1시간씩은 했던 것 같습니다.    \n\n배경지식이 전혀 없는 분이 리서치 과정을 수료하려면 적어도 매일 6시간씩은 추가공부해야할 것 같습니다.   \n\n \n\n## 3. 공부방식   \n \n선생님이 있는게 아닌, 공부자료가 주어지고 같이 공부하는 사람들과 함께 알아가는 방식입니다.   \n공부의 방향을 \"퍼실\" 분들이 잡아주고, 모르는 부분을 해소해주는 방식으로 도움을 주십니다.   \n\n   \n이게 보통 스터디 같은 경우 나보다 더 잘아는 사람과 해야지 얻는게 많을 것이라 생각하는데, 경험해보니 반대였던 것 같습니다.     \n\n> 왜 선생님이 가장 많이 배운다고 하잖아요, 저는 배경지식이 있어 \"선생님\" 역할을 맡을 때가 많았는데 정말 많이 배웠습니다.\n\n어느정도 알던 개념들이 설명을 하면서 완성되곤 했습니다.   \n특히 아에 모르던 개념들도 어떻게 해야 빠르게 습득하고 하나의 지식으로 만들 수 있는지 많이 배웠습니다.    \n  \n또 토의, 토론을 정말 많이 했는데, 이를 통해 제 지"
  },
  {
    "id": "template",
    "title": "여기에 포스트 제목을 입력하세요",
    "date": "2024-01-01",
    "excerpt": "포스트에 대한 간단한 설명을 여기에 작성하세요 (선택사항)",
    "category": "카테고리명",
    "tags": [
      "태그1",
      "태그2",
      "태그3"
    ],
    "content": "\n# 포스트 제목\n\n포스트의 내용을 여기에 작성하세요.\n\n## 소제목\n\n### 더 작은 소제목\n\n일반 텍스트 내용입니다.\n\n> 인용구는 이렇게 작성할 수 있습니다.\n\n**굵은 글씨**와 *기울임 글씨*를 사용할 수 있습니다.\n\n- 목록 아이템 1\n- 목록 아이템 2\n- 목록 아이템 3\n\n1. 번호 목록 1\n2. 번호 목록 2\n3. 번호 목록 3\n\n```python\n# 코드 블록 예시\ndef hello_world():\n    print(\"Hello, World!\")\n```\n\n![이미지 설명](/이미지파일명.png)\n\n---\n\n추가 내용이 있다면 여기에 작성하세요. "
  }
]