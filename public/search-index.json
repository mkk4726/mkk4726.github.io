[
  {
    "id": "Data Science/Statistics/pearson-vs-spearman-coefficient",
    "title": "Pearson vs Spearman ì •ë¦¬",
    "date": "2025-08-19",
    "excerpt": "ìƒê´€ê´€ê³„ ì§€í‘œì— ëŒ€í•œ ë¹„êµ",
    "category": "Data Science",
    "tags": [
      "statistics",
      "correlation"
    ],
    "content": "ì°¸ê³  ê¸€ ë§í¬ë“œì¸ - Timer Bikmukhametov ë§í¬ë“œì¸ ê¸€ ì •ë¦¬ > Pearson vs Spearman Coefficient. ì–´ë–¤ ê²ƒì„ ì„ íƒí• ê¹Œ? <figure> <img src=\"/post/DataScience/linear_corr.gif\" alt=\"Pearson vs Spearman\" style=\"width: 50%; height: auto;\" /> <figcaption>Pearson vs Spearman</figcaption> </figure> Pearson coefficient Pearson coefficientëŠ” ë‘ ë³€ìˆ˜ì˜ covarianceë¥¼ ê° ë³€ìˆ˜ì˜ standard deviationì˜ ê³±ìœ¼ë¡œ ë‚˜ëˆˆ ê°’ì…ë‹ˆë‹¤. ì ˆëŒ“ê°’ì´ ì •í™•íˆ Â±1ì´ë©´ Xì™€ Y ì‚¬ì´ì˜ ê´€ê³„ë¥¼ linear equationìœ¼ë¡œ ì™„ë²½í•˜ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì¥ì : ê°„ë‹¨í•˜ë©°, normally distributed dataì— ì˜ ì‘ë™í•©ë‹ˆë‹¤ linear relationshipì„ ì˜ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ ë‹¨ì : non-linear relationshipì„ í¬ì°©í•˜ì§€ ëª»í•©ë‹ˆë‹¤ outlierì— ë¯¼ê°í•˜ì—¬ ê°’ì´ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤ Spearman coefficient monotonic relationshipì„ ì¸¡ì •í•©ë‹ˆë‹¤ (ë°ì´í„°ê°€ linearí•˜ì§€ ì•Šë”ë¼ë„ í•¨ê»˜ ì¦ê°€í•˜ê±°ë‚˜ ê°ì†Œí•˜ëŠ” ê´€ê³„). ì¤‘ë³µëœ data valueê°€ ì—†ë‹¤ë©´, +1 ë˜ëŠ” -1ì˜ ì™„ë²½í•œ Spearman correlationì´ ë°œìƒí•˜ëŠ” ê²½ìš°ëŠ” ê° ë³€ìˆ˜ê°€ ë‹¤ë¥¸ ë³€ìˆ˜ì˜ ì™„ë²½í•œ monotone functionì¼ ë•Œì…ë‹ˆë‹¤. ì¥ì : non-linear relationshipê³¼ outlierì— ì˜ ì‘ë™í•©ë‹ˆë‹¤ ë‹¨ì : absolute differenceë¥¼ ë¬´ì‹œí•˜ê³  rankë§Œ ì¤‘ìš”í•©ë‹ˆë‹¤ rankingì— ì˜ì¡´í•˜ë¯€ë¡œ outlierì— ëŒ€í•œ sensitivityê°€ ë‚®ìŠµë‹ˆë‹¤ ì„ íƒ ê°€ì´ë“œ ê¸°ë³¸ ì›ì¹™ ë°ì´í„°ê°€ linear trendë¥¼ ë”°ë¥¸ë‹¤ë©´, Pearsonì´ ì¢‹ì€ ì„ íƒì…ë‹ˆë‹¤ ë°ì´í„°ê°€ non-linear patternì´ë‚˜ outlierë¥¼ ê°€ì§„ë‹¤ë©´, Spearmanì´ ë” ì‹ ë¢°í•  ë§Œí•©ë‹ˆë‹¤ ì¶”ê°€ ê³ ë ¤ì‚¬í•­ ë°ì´í„° ë¶„í¬ í™•ì¸: ì •ê·œë¶„í¬: Pearsonì´ ë” ì í•© (parametric method) ë¹„ì •ê·œë¶„í¬: Spearmanì´ ë” ì•ˆì „ (non-parametric method) ìˆœì„œí˜• ë°ì´í„°: Spearmanë§Œ ì˜ë¯¸ê°€ ìˆìŒ ìƒ˜í”Œ í¬ê¸°: ì‘ì€ ìƒ˜í”Œ (n < 30): Spearmanì´ ë” robust í° ìƒ˜í”Œ: ë‘˜ ë‹¤ ì‚¬ìš© ê°€ëŠ¥í•˜ì§€ë§Œ ëª©ì ì— ë”°ë¼ ì„ íƒ í•´ì„ì˜ ëª©ì : ì„ í˜• ê´€ê³„ì˜ ê°•ë„ ì¸¡ì •ì´ ëª©ì : Pearson ë‹¨ì¡° ê´€ê³„ì˜ ë°©í–¥ì„± í™•ì¸ì´ ëª©ì : Spearman ìˆœìœ„ ê¸°ë°˜ ë¶„ì„: Spearman (ì˜ˆ: ì„¤ë¬¸ì¡°ì‚¬ ìˆœìœ„ ë°ì´í„°) ì‹¤ë¬´ íŒ: ë‘ ë°©ë²• ëª¨ë‘ ê³„ì‚°í•´ë³´ê³  ê²°ê³¼ê°€ í¬ê²Œ ë‹¤ë¥´ë©´ ë°ì´í„°ë¥¼ ì¬ê²€í†  Pearson â‰« Spearman: ê°•í•œ ì„ í˜• ê´€ê³„ ì¡´ì¬ Spearman â‰« Pearson: ë¹„ì„ í˜•ì´ì§€ë§Œ ë‹¨ì¡°ì  ê´€ê³„ ì¡´ì¬ ë‘˜ ë‹¤ ë‚®ìŒ: ê´€ê³„ê°€ ì•½í•˜ê±°ë‚˜ ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ ì£¼ì˜ì‚¬í•­: ìƒê´€ê´€ê³„ â‰  ì¸ê³¼ê´€ê³„ (ë‘˜ ë‹¤ ë™ì¼í•˜ê²Œ ì ìš©) êµ¬ê°„ë³„ë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ ë³´ì´ëŠ” ë°ì´í„°ëŠ” ì „ì²´ ìƒê´€ê³„ìˆ˜ë¡œ íŒë‹¨í•˜ì§€ ë§ ê²ƒ ì´ìƒì¹˜ ì œê±° ì „í›„ë¡œ ë¹„êµ ë¶„ì„ ê¶Œì¥ ìˆ˜ì‹ê³¼ ê³„ì‚° ë°©ì‹ Pearson ìƒê´€ê³„ìˆ˜ (r) > Pearson ìƒê´€ê³„ìˆ˜ëŠ” ë‘ ë³€ìˆ˜ ê°„ì˜ ì„ í˜• ê´€ê³„ì˜ ê°•ë„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ìˆ˜ì‹: $$ r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} $$ ë˜ëŠ” ê³µë¶„ì‚°ì„ ì‚¬ìš©í•˜ì—¬: $$ r = \\frac{Cov(X,Y)}{S_x \\cdot S_y} $$ ì—¬ê¸°ì„œ: $x_i, y_i$: ê°œë³„ ë°ì´í„° í¬ì¸íŠ¸ $\\bar{x}, \\bar{y}$: ê° ë³€ìˆ˜ì˜ í‰ê·  $Cov(X,Y)$: Xì™€ Yì˜ ê³µë¶„ì‚° $S_x, S_y$: ê° ë³€ìˆ˜ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚° ê³¼ì •: ê° ë³€ìˆ˜ì˜ í‰ê· ì„ ê³„ì‚° ê° ë°ì´í„° í¬ì¸íŠ¸ì—ì„œ í‰ê· ì„ ëº€ í¸ì°¨ë¥¼ ê³„ì‚° í¸ì°¨ì˜ ê³±ì˜ í•©ì„ êµ¬í•¨ (ê³µë¶„ì‚°ì˜ ë¶„ì) ê° ë³€ìˆ˜ì˜ í¸ì°¨ ì œê³±í•©ì„ êµ¬í•˜ê³  ì œê³±ê·¼ì„ ì·¨í•¨ ê³µë¶„ì‚°ì„ í‘œì¤€í¸ì°¨ì˜ ê³±ìœ¼ë¡œ ë‚˜ëˆ” Spearman ìƒê´€ê³„ìˆ˜ (Ï) > Spearman ìƒê´€ê³„ìˆ˜ëŠ” ë‘ ë³€ìˆ˜ì˜ ìˆœìœ„(rank) ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. ìˆ˜ì‹: $$ \\rho = 1 - \\frac{6\\sum_{i=1}^{n}d_i^2}{n(n^2-1)} $$ ì—¬ê¸°ì„œ: $d_i$: ê° ê´€ì¸¡ì¹˜ì˜ ë‘ ë³€ìˆ˜ ìˆœìœ„ ì°¨ì´ $n$: ê´€ì¸¡ì¹˜ì˜ ê°œìˆ˜ ë˜ëŠ” ìˆœìœ„ì— ëŒ€í•œ Pearson ìƒê´€ê³„ìˆ˜ë¡œ: $$ \\rho = \\frac{\\sum_{i=1}^{n}(R(x_i) - \\overline{R(x)})(R(y_i) - \\overline{R(y)})}{\\sqrt{\\sum_{i=1}^{n}(R(x_i) - \\overline{R(x)})^2}\\sqrt{\\sum_{i=1}^{n}(R(y_i) - \\overline{R(y)})^2}} $$ ì—¬ê¸°ì„œ: $R(x_i), R(y_i)$: ê° ë³€ìˆ˜ì˜ ìˆœìœ„ $\\overline{R(x)}, \\overline{R(y)}$: ìˆœìœ„ì˜ í‰ê·  ê³„ì‚° ê³¼ì •: ê° ë³€ìˆ˜ì˜ ë°ì´í„°ë¥¼ ìˆœìœ„ë¡œ ë³€í™˜ (ê°€ì¥ ì‘ì€ ê°’ì´ 1ìˆœìœ„) ë™ì¼í•œ ê°’ì´ ìˆëŠ” ê²½ìš° í‰ê·  ìˆœìœ„ë¥¼ ë¶€ì—¬ ê° ê´€ì¸¡ì¹˜ì— ëŒ€í•´ ë‘ ë³€ìˆ˜ì˜ ìˆœìœ„ ì°¨ì´ $d_i$ë¥¼ ê³„ì‚° $d_i^2$ì˜ í•©ì„ êµ¬í•¨ ê³µì‹ì— ëŒ€ì…í•˜ì—¬ ìƒê´€ê³„ìˆ˜ë¥¼ ê³„ì‚° ì˜ˆì‹œ: ë°ì´í„°: X = [1, 2, 3, 4, 5], Y = [2, 4, 1, 5, 3] ìˆœìœ„ ë³€í™˜: Xì˜ ìˆœìœ„: [1, 2, 3, 4, 5] Yì˜ ìˆœìœ„: [2, 4, 1, 5, 3] ìˆœìœ„ ì°¨ì´: d = [1-2, 2-4, 3-1, 4-5, 5-3] = [-1, -2, 2, -1, 2] $d^2$ = [1, 4, 4, 1, 4], í•©ê³„ = 14 $\\rho = 1 - \\frac{6 \\times 14}{5(25-1)} = 1 - \\frac{84}{120} = 1 - 0.7 = 0.3$",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ë¹„êµë°ë¹„ìœ /08-18-ê·¼ë ¥ìš´ë™ì€í• ìˆ˜ë¡í˜ì´ì„¸ì ¸",
    "title": "ê·¼ë ¥ ìš´ë™ì€ í• ìˆ˜ë¡ í˜ì´ ì„¸ì ¸",
    "date": "2025-08-18",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ë¹„êµ ë° ë¹„ìœ í•  ë•Œ ì“°ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "í•˜ë©´ í• ìˆ˜ë¡ ì¢‹ì•„ As you speak more, you get more confident. The more you speak, the more confident you get. The more you lift, the stronger you get. ë°°ìš´ ê²ƒë“¤ ì •ë¦¬ The more you run, the fitter you get. Lift is like a box of chocalates. ê°„ë‹¨í•œ ì‘ë¬¸ Today I learned expressions using \"the more ~, the more\" and \"~ is like ~\". These expression is used to metaphor ~~. I had band í•©ì£¼ class today. It was so much fun. í•©ì´ ë§ëŠ”ë‹¤ëŠ” ëŠë‚Œì€ êµ‰ì¥íˆ ê¸°ë¶„ì´ ì¢‹ë”ë¼. I learend guiatr a year from last year. I'm sure that the more I practice, the better I play. (ì¡°ê¸ˆì”© ê¸°íƒ€ë¥¼ ì˜ ì¹˜ê²Œ ë˜ëŠ” ê±¸ ëŠë‚„ ë•Œ êµ‰ì¥íˆ ê¸°ì˜ë‹¤.) When I feel I'm playing guitar little better than before, I feel so happy. Practing guitar is like a life. No matter what I do, I can be better if I practice consistently. Don't be hurry up, just take the rythm. ë” ìì—°ìŠ¤ëŸ¬ìš´ ë²„ì „ Today I learned expressions using \"the more ~, the more\" and \"~ is like ~\". These expressions are used for metaphors. I had a band practice class today. It was so much fun. The feeling of being in sync was amazing. I have been learning guitar for a year since last year. I'm sure that the more I practice, the better I will play. When I feel myself getting a little better at guitar than before, I feel so happy. Playing guitar is like life itself. No matter what I do, I can get better if I practice consistently. Don't rush, just keep the rhythm. í”¼ë“œë°± ğŸ‘ ì˜í•œ ì ë“¤: \"the more ~, the more\" êµ¬ì¡°ë¥¼ ì •í™•í•˜ê²Œ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤ ì „ì²´ì ì¸ ë©”ì‹œì§€ê°€ ëª…í™•í•˜ê²Œ ì „ë‹¬ë©ë‹ˆë‹¤ ì€ìœ (metaphor) í‘œí˜„ì„ ì˜ í™œìš©í–ˆìŠµë‹ˆë‹¤ ğŸ”§ ë¬¸ë²• ìˆ˜ì •ì‚¬í•­: \"These expression\" â†’ \"These expressions\" (ë³µìˆ˜í˜•) \"metaphor\" â†’ \"metaphors\" (ë™ì‚¬ ë’¤ì—ëŠ” ëª…ì‚¬ ë³µìˆ˜í˜•) \"I learend\" â†’ \"I have been learning\" (í˜„ì¬ì™„ë£Œì§„í–‰í˜•ì´ ë” ìì—°ìŠ¤ëŸ½) \"a year from last year\" â†’ \"for a year since last year\" (ê¸°ê°„ í‘œí˜„) \"Practing\" â†’ \"Playing\" (ì² ì + ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"be hurry up\" â†’ \"rush\" (ë” ê°„ë‹¨í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"rythm\" â†’ \"rhythm\" (ì² ì) ğŸ’¡ í‘œí˜„ ê°œì„ : \"í•©ì´ ë§ëŠ”ë‹¤\" â†’ \"being in sync\"ê°€ ë” ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤ \"life\" â†’ \"life itself\"ë¡œ ê°•ì¡° ê´„í˜¸ ì•ˆì˜ í•œêµ­ì–´ë¥¼ ì˜ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë²ˆì—­í–ˆìŠµë‹ˆë‹¤ ì „ë°˜ì ìœ¼ë¡œ ì¢‹ì€ ì‘ë¬¸ì´ì—ˆìŠµë‹ˆë‹¤! ê³„ì† ì—°ìŠµí•˜ì‹œë©´ ë”ìš± ìì—°ìŠ¤ëŸ¬ì›Œì§ˆ ê±°ì˜ˆìš”! ğŸ¸",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/residualization-fwl-theorem-test",
    "title": "FWLì— ê¸°ë°˜í•œ ì”ì°¨í™” ë°©ë²•ë¡  êµ¬í˜„ ë…¸íŠ¸ë¶",
    "date": "2025-07-10T00:00:00.000Z",
    "excerpt": "ì”ì°¨í™”ë¥¼ í•˜ê¸° ìœ„í•´ nusiance functionì„ ë§Œë“¤ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë…¸íŠ¸ë¶",
    "category": "Causal Inference",
    "tags": [
      "residualization",
      "FWL"
    ],
    "content": "--- title: \"FWLì— ê¸°ë°˜í•œ ì”ì°¨í™” ë°©ë²•ë¡  êµ¬í˜„ ë…¸íŠ¸ë¶\" date: 2025-07-10 category: \"Causal Inference\" tags: [\"residualization\", \"FWL\"] excerpt: \"ì”ì°¨í™”ë¥¼ í•˜ê¸° ìœ„í•´ nusiance functionì„ ë§Œë“¤ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë…¸íŠ¸ë¶\" --- # Load Data # from src.dataloader.vault.main import VaultDatasetLoader from src.dataloader.vault.utils import split_df_od_os from src.utils.db import SOFTCRM_DBINFO, get_conn import warnings import pandas as pd import numpy as np warnings.filterwarnings(\"ignore\") # pandas display ì˜µì…˜ ì„¤ì • - ëª¨ë“  ì—´ ë³´ì´ê¸° pd.set_option('display.max_columns', None) pd.set_option('display.width', None) pd.set_option('display.max_colwidth', None) loader = VaultDatasetLoader() feature_df, y_df = loader.run(start_date=\"2022-01-01\", end_date=\"2024-12-31\", piol_data_path=\"./data/PIOLë Œì¦ˆì£¼ë¬¸ì„œ.xlsx\", refresh=False, db=\"crm_emr\") # y_dfì˜ oper_dateì™€ feature_dfì˜ dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© merged_df = pd.merge(y_df, feature_df, left_on=['cust_num', 'oper_date'], right_on=['cust_num', 'date'], how='left') merged_df = split_df_od_os(merged_df) query = \"\"\"SELECT CUST_NUM, BIRTH_DAY FROM VIEW_CUSTOM_JUMIN\"\"\" with get_conn(SOFTCRM_DBINFO) as conn, conn.cursor() as cursor: cursor.execute(query) result = cursor.fetchall() age_df = pd.DataFrame([list(res) for res in result], columns=[\"cust_num\", \"birthday\"]) age_df = age_df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip() == \"\" else x) age_df[\"birthday\"] = pd.to_datetime(age_df[\"birthday\"], errors=\"coerce\", format=\"%Y-%m-%d\") age_df = age_df.dropna().reset_index(drop=True) age_df[\"cust_num\"] = age_df[\"cust_num\"].apply(lambda x: x.strip()) cust_age_dict = dict(zip(age_df[\"cust_num\"], age_df[\"birthday\"])) merged_df['birthday'] = merged_df['cust_num'].map(cust_age_dict) merged_df['age'] = merged_df.apply(lambda row: pd.to_datetime(row['oper_date']).year - row['birthday'].year if pd.notnull(row['birthday']) else np.nan, axis=1) # merged_df.to_csv(\"./merged_df_2022_01_01_2024_12_31_with_age.csv\", index=False) # Modules from src.preprocessing.constants import ALL_RANGES_NO_SIDE import pandas as pd import numpy as np def load_data() -> tuple[pd.DataFrame, list, list, list]: \"\"\" ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ë¶„ì„ì— í•„ìš”í•œ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. Returns: tuple[pd.DataFrame, list, list, list]: df_clean (pd.DataFrame): ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ features (list): íŠ¹ì„± ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] treatment (list): ì²˜ì¹˜ ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ [\"size\"] y (list): ê²°ê³¼ ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ [\"meas_vault\"] Note: NaN ê°’ì´ ìˆëŠ” í–‰ì€ ì œê±°ë©ë‹ˆë‹¤ sizeê°€ [12.1, 12.6, 13.2]ê°€ ì•„ë‹Œ ê²½ìš°ëŠ” ì œê±°ë©ë‹ˆë‹¤ \"\"\" df = pd.read_csv(\"./data/merged_df_2022_01_01_2024_12_31_with_age.csv\") df.head() features = [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] treatment = [\"size\"] y = [\"meas_vault\"] # df ì¤‘ features, treatment, y ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  ì¹¼ëŸ¼ ì œê±°, ì´ ì¹¼ëŸ¼ë“¤ì— ëŒ€í•œ ê°’ì´ NaNì¸ ê²ƒë„ ì œê±° selected_columns = features + treatment + y df_clean = df[selected_columns].copy() # df['size']ê°€ [12.1, 12.6, 13.2] ì´ ì•„ë‹Œ ê²½ìš° ì œê±° df_clean = df_clean[df_clean[\"size\"].isin([12.1, 12.6, 13.2])].reset_index(drop=True) # ì´ìƒì¹˜ ì œê±° ë¡œì§ # ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ ë²”ìœ„ ì ìš© for column, range_obj in ALL_RANGES_NO_SIDE.items(): if column in df_clean.columns: # ì»¬ëŸ¼ ê°’ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ np.nanìœ¼ë¡œ ë³€ê²½ df_clean[column] = df_clean[column].where((df_clean[column] >= range_obj.min) & (df_clean[column] <= range_obj.max), np.nan) # Cas_cctë¡œ vault ë²”ìœ„ ë§Œë“¤ê³ , ë„˜ì–´ê°€ë©´ ì—†ì• ì£¼ê¸° max_vault = ALL_RANGES_NO_SIDE[\"Cas_cct\"].max 1.5 df_clean = df_clean[df_clean[\"meas_vault\"] <= max_vault].reset_index(drop=True) # NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±° df_clean = df_clean.dropna() # ë‚˜ì´ê°€ 60ì´ìƒ ì œê±° df_clean = df_clean[df_clean[\"age\"] < 60].reset_index(drop=True) df_clean[y] = df_clean[y].abs() return df_clean, features, treatment, y # ì”ì°¨í™” ì§„í–‰í•˜ê¸° > R-learnerëŠ” FWL theoremì— ì´ë¡ ì  ë°°ê²½ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤. FWL ì •ë¦¬ëŠ” ë‹¤ì¤‘ íšŒê·€ë¶„ì„ì—ì„œ íŠ¹ì • ë³€ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¤€ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤: ì§ì ‘ íšŒê·€: ëª¨ë“  ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ì „ì²´ íšŒê·€ëª¨í˜• ì”ì°¨í™” í›„ íšŒê·€: ê´€ì‹¬ ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ê°ê° íšŒê·€í•˜ì—¬ ì”ì°¨ë¥¼ êµ¬í•œ í›„, ì´ ì”ì°¨ë“¤ ê°„ì˜ íšŒê·€ ìˆ˜í•™ì  í‘œí˜„: ëª¨í˜•: Y = XÎ² + ZÎ³ + Îµì—ì„œ Î²ë¥¼ êµ¬í•˜ê³  ì‹¶ë‹¤ë©´: ë°©ë²• 1: ì „ì²´ íšŒê·€ `` Î²Ì‚ = (X'M_Z X)^(-1) X'M_Z Y ` ë°©ë²• 2: ì”ì°¨í™” ` XÌƒ = M_Z X (Xë¥¼ Zì— íšŒê·€í•œ ì”ì°¨) á»¸ = M_Z Y (Yë¥¼ Zì— íšŒê·€í•œ ì”ì°¨) Î²Ì‚ = (XÌƒ'XÌƒ)^(-1) XÌƒ'á»¸ ` ì—¬ê¸°ì„œ M_Z = I - Z(Z'Z)^(-1)Z'ëŠ” Zì— ìˆ˜ì§ì¸ íˆ¬ì˜í–‰ë ¬ì…ë‹ˆë‹¤. ì”ì°¨í™”ê°€ ê°€ëŠ¥í•´ì§€ëŠ” ì´ìœ : ì§êµì„±: ì”ì°¨í™”ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ì˜í–¥ì„ ì œê±°í•˜ì—¬ ìˆœìˆ˜í•œ íš¨ê³¼ë§Œ ë¶„ë¦¬ ì°¨ì› ì¶•ì†Œ: ë³µì¡í•œ ë‹¤ë³€ëŸ‰ ë¬¸ì œë¥¼ ë‹¨ìˆœí•œ ì´ë³€ëŸ‰ ë¬¸ì œë¡œ ë³€í™˜ í•´ì„ ìš©ì´ì„±: ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ í†µì œí•œ í›„ì˜ ìˆœìˆ˜í•œ ê´€ê³„ íŒŒì•… > R-learnerëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‘ ë‹¨ê³„ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤: Propensity Score: Treatmentë¥¼ Featuresë¡œ ì˜ˆì¸¡ Outcome Function: Outcomeì„ Featuresë¡œ ì˜ˆì¸¡ Step 1: Propensity Score (Treatment ì”ì°¨í™”): `python X â†’ T íšŒê·€ (Treatmentë¥¼ Featuresë¡œ ì˜ˆì¸¡) model = LogisticRegression() model.fit(X_train, y_train) propensity_scores = model.predict_proba(X_val) ` Step 2: Outcome Function (Y ì”ì°¨í™”): `python X â†’ Y íšŒê·€ (Outcomeì„ Featuresë¡œ ì˜ˆì¸¡) Y_residual = Y - E[Y X] T_residual = T - E[T X] (propensity score í™œìš©) ` ì™œ ì”ì°¨í™”ê°€ \"ê°€ëŠ¥\"í•´ì§€ëŠ”ê°€? : A. ì¡°ê±´ë¶€ ë…ë¦½ì„± í™•ë³´ ` E[T_residual X] = 0 E[Y_residual X] = 0 ` Xì˜ ì˜í–¥ì„ ì œê±°í•¨ìœ¼ë¡œì¨ ìˆœìˆ˜í•œ Tâ†’Y íš¨ê³¼ë§Œ ë¶„ë¦¬ B. í¸í–¥ ì œê±° (Bias Elimination) Selection bias: ê´€ì¸¡ë˜ì§€ ì•Šì€ êµë€ë³€ìˆ˜ì˜ ì˜í–¥ Confounding: Xê°€ Tì™€ Y ëª¨ë‘ì— ì˜í–¥ì„ ì£¼ëŠ” ë¬¸ì œ C. ê³¼ì í•© ë°©ì§€ `python Cross-fitting ì‚¬ìš© kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) ` Out-of-sample ì˜ˆì¸¡ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€ ì‹¤ì œ ì¸ê³¼íš¨ê³¼ ì¶”ì • ìµœì¢…ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ íšŒê·€ë¥¼ ìˆ˜í–‰: ` Y_residual = Ï„ Ã— T_residual + noise ` ì—¬ê¸°ì„œ Ï„`ê°€ ìˆœìˆ˜í•œ treatment effectì…ë‹ˆë‹¤. ê²°ë¡ : FWL ì •ë¦¬ì—ì„œ ì”ì°¨í™”ê°€ ê°€ëŠ¥í•´ì§€ëŠ” ì´ìœ ëŠ”: ìˆ˜í•™ì : ì§êµ íˆ¬ì˜ì˜ ì„±ì§ˆë¡œ ì¸í•œ ê³„ìˆ˜ ë¶ˆë³€ì„± í†µê³„ì : êµë€ë³€ìˆ˜ í†µì œë¥¼ í†µí•œ í¸í–¥ ì œê±° ì‹¤ìš©ì : ë³µì¡í•œ ë‹¤ë³€ëŸ‰ ë¬¸ì œë¥¼ ë‹¨ìˆœí•œ ì´ë³€ëŸ‰ ë¬¸ì œë¡œ ë³€í™˜ > R-learner êµ¬í˜„ì€ ì´ ì´ë¡ ì  ê¸°ë°˜ì„ ì¸ê³¼ì¶”ë¡ ì— ì‹¤ì œ ì ìš©í•œ ì‚¬ë¡€ë¡œ, íŠ¹íˆ ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼(Heterogeneous Treatment Effects) ì¶”ì •ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. # 1. propensity score function > P(T X) Xì¼ ë•Œ Tì¼ í™•ë¥ ì„ êµ¬í•´ì„œ, ì´ë¥¼ í†µí•´ ì”ì°¨í™” ì§„í–‰ propensity scoreë¥¼ í†µì œí•˜ëŠ” ê²ƒê³¼ Të¥¼ í†µì œí•˜ëŠ” ê²ƒì€ ê°™ì€ ì˜ë¯¸ì´ê¸° ë•Œë¬¸ì— df, features, treatment, y = load_data() df[treatment] = df[treatment].astype(\"category\") (treatment,) = treatment print(treatment) df.head() ì¤‘ìš”í•œì ! > ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ cross-fitting (ë˜ëŠ” sample splitting)ì„ ì‚¬ìš©í•´ì„œ out-of-sample ì˜ˆì¸¡ê°’ì„ í™œìš© import numpy as np import pandas as pd from sklearn.model_selection import StratifiedKFold from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score from sklearn.preprocessing import StandardScaler RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) ì¹´í…Œê³ ë¦¬í˜• treatmentë¥¼ ìˆ«ìë¡œ ë³€í™˜ X_features = df[features] y_treatment_cat = df[treatment].cat.codes # ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ìë¡œ ë³€í™˜ print(f\"Treatment ì¹´í…Œê³ ë¦¬: {df[treatment].cat.categories}\") print(f\"Treatment ë¶„í¬:\\n{df[treatment].value_counts()}\") print(f\"ìˆ«ì ë³€í™˜ëœ ë¶„í¬:\\n{pd.Series(y_treatment_cat).value_counts()}\") ê° í´ë˜ìŠ¤ë³„ propensity score ì €ì¥ìš© ë°°ì—´ class_categories = df[treatment].cat.categories n_classes = len(class_categories) propensity_scores_all = np.zeros((len(df), n_classes)) StratifiedKFoldë¡œ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ ë§ì¶¤ kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) fold_performances = [] for fold, (train_idx, val_idx) in enumerate(kf.split(X_features, y_treatment_cat)): print(f\"\\n=== Fold {fold + 1} ===\") X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx] y_train, y_val = y_treatment_cat[train_idx], y_treatment_cat[val_idx] # ê° foldì˜ treatment ë¶„í¬ í™•ì¸ print(f\"Train ë¶„í¬: {pd.Series(y_train).value_counts().to_dict()}\") print(f\"Val ë¶„í¬: {pd.Series(y_val).value_counts().to_dict()}\") # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ìˆ˜ë ´ ì„±ëŠ¥ í–¥ìƒ) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_val_scaled = scaler.transform(X_val) # ëª¨ë¸ í›ˆë ¨ (ìˆ˜ë ´ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì„¤ì •) model = LogisticRegression( random_state=42, max_iter=1000, # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€ (ê¸°ë³¸ê°’: 100) solver=\"liblinear\", # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì í•©í•œ ì†”ë²„ ) model.fit(X_train_scaled, y_train) # í›ˆë ¨ ì„±ëŠ¥ í™•ì¸ train_pred = model.predict(X_train_scaled) train_accuracy = accuracy_score(y_train, train_pred) # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë“  í´ë˜ìŠ¤ë³„ propensity score ê³„ì‚° val_propensity = model.predict_proba(X_val_scaled) propensity_scores_all[val_idx] = val_propensity # ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥  ì €ì¥ # ê²€ì¦ ì„±ëŠ¥ í™•ì¸ val_pred = model.predict(X_val_scaled) val_accuracy = accuracy_score(y_val, val_pred) fold_performances.append( {\"fold\": fold + 1, \"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy, \"train_size\": len(X_train), \"val_size\": len(X_val)} ) print(f\"í›ˆë ¨ ì •í™•ë„: {train_accuracy:.3f}\") print(f\"ê²€ì¦ ì •í™•ë„: {val_accuracy:.3f}\") ì›ë³¸ dfì— ê° í´ë˜ìŠ¤ë³„ propensity_score ì»¬ëŸ¼ ì¶”ê°€ for i, category in enumerate(class_categories): df[f\"propensity_score_{category}\"] = propensity_scores_all[:, i] print(f\"\\n=== ì „ì²´ ê²°ê³¼ ===\") performance_df = pd.DataFrame(fold_performances) print(f\"í‰ê·  í›ˆë ¨ ì •í™•ë„: {performance_df['train_accuracy'].mean():.3f} Â± {performance_df['train_accuracy'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ ì •í™•ë„: {performance_df['val_accuracy'].mean():.3f} Â± {performance_df['val_accuracy'].std():.3f}\") print(f\"\\nì²˜ìŒ 10ê°œ ìƒ˜í”Œ:\") propensity_cols = [f\"propensity_score_{cat}\" for cat in class_categories] print(df[[\"meas_vault\", treatment] + propensity_cols].head(10)) print(f\"\\nìƒì„±ëœ propensity score ì»¬ëŸ¼ë“¤:\") for col in propensity_cols: print(f\"{col}: í‰ê·  {df[col].mean():.3f}\") df # 2. Outcome function >m(Y X) ì´ë¥¼ í†µí•´ Yì—ì„œ Xì˜ ì˜í–¥ì„ ì œê±°í•œ ì”ì°¨ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ. df, features, treatment, y = load_data() df[treatment] = df[treatment].astype(\"category\") (treatment,) = treatment print(treatment) df.head() ì¤‘ìš”í•œì ! > ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ cross-fitting (ë˜ëŠ” sample splitting)ì„ ì‚¬ìš©í•´ì„œ out-of-sample ì˜ˆì¸¡ê°’ì„ í™œìš© import numpy as np import pandas as pd import lightgbm as lgb from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error import matplotlib.pyplot as plt í•œê¸€ í°íŠ¸ ì„¤ì • (ë§¥ í™˜ê²½) import matplotlib.font_manager as fm plt.rcParams[\"font.family\"] = \"AppleGothic\" # ë§¥ìš© í•œê¸€ í°íŠ¸ plt.rcParams[\"axes.unicode_minus\"] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€ RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) Outcome function (featuresë¡œ y ì˜ˆì¸¡) print(\"=== Outcome Function í•™ìŠµ (features â†’ y) ===\") X_features = df[features] y_outcome = df[y[0]] # yëŠ” ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œ ì¶”ì¶œ print(f\"Features: {features}\") print(f\"Target: {y[0]}\") print(f\"ë°ì´í„° í¬ê¸°: X={X_features.shape}, y={y_outcome.shape}\") print(f\"Target í†µê³„:\") print(f\" í‰ê· : {y_outcome.mean():.2f}\") print(f\" í‘œì¤€í¸ì°¨: {y_outcome.std():.2f}\") print(f\" ë²”ìœ„: [{y_outcome.min():.2f}, {y_outcome.max():.2f}]\") Out-of-sample ì˜ˆì¸¡ê°’ ì €ì¥ìš© ë°°ì—´ outcome_predictions = np.zeros(len(df)) KFoldë¡œ ë¶„í•  (íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ StratifiedKFold ëŒ€ì‹  KFold ì‚¬ìš©) kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) fold_performances = [] LightGBM íŒŒë¼ë¯¸í„° ì„¤ì • (ê³¼ì í•© ë°©ì§€ì— ì¤‘ì ) lgb_params = { \"objective\": \"regression\", \"metric\": \"rmse\", \"boosting_type\": \"gbdt\", \"num_leaves\": 15, # ì‘ê²Œ ì„¤ì •í•˜ì—¬ ê³¼ì í•© ë°©ì§€ \"learning_rate\": 0.1, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 5, \"lambda_l1\": 0.1, # L1 ì •ê·œí™” \"lambda_l2\": 0.1, # L2 ì •ê·œí™” \"min_data_in_leaf\": 20, \"min_split_gain\": 0.1, \"max_depth\": 6, \"verbose\": -1, \"random_state\": RANDOM_SEED, } for fold, (train_idx, val_idx) in enumerate(kf.split(X_features, y_outcome)): print(f\"\\n=== Fold {fold + 1} ===\") X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx] y_train, y_val = y_outcome.iloc[train_idx], y_outcome.iloc[val_idx] print(f\"Train í¬ê¸°: {len(X_train)}, Val í¬ê¸°: {len(X_val)}\") print(f\"Train target í‰ê· : {y_train.mean():.2f}, Val target í‰ê· : {y_val.mean():.2f}\") # LightGBM ë°ì´í„°ì…‹ ìƒì„± train_data = lgb.Dataset(X_train, label=y_train) val_data = lgb.Dataset(X_val, label=y_val, reference=train_data) # ëª¨ë¸ í›ˆë ¨ model = lgb.train( lgb_params, train_data, valid_sets=[train_data, val_data], valid_names=[\"train\", \"eval\"], num_boost_round=500, callbacks=[ lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0), # ë¡œê·¸ ì¶œë ¥ ë¹„í™œì„±í™” ], ) # ì˜ˆì¸¡ train_pred = model.predict(X_train, num_iteration=model.best_iteration) val_pred = model.predict(X_val, num_iteration=model.best_iteration) # Out-of-sample ì˜ˆì¸¡ê°’ ì €ì¥ outcome_predictions[val_idx] = val_pred # ì„±ëŠ¥ ê³„ì‚° train_rmse = np.sqrt(mean_squared_error(y_train, train_pred)) val_rmse = np.sqrt(mean_squared_error(y_val, val_pred)) train_r2 = r2_score(y_train, train_pred) val_r2 = r2_score(y_val, val_pred) train_mae = mean_absolute_error(y_train, train_pred) val_mae = mean_absolute_error(y_val, val_pred) fold_performances.append( { \"fold\": fold + 1, \"train_rmse\": train_rmse, \"val_rmse\": val_rmse, \"train_r2\": train_r2, \"val_r2\": val_r2, \"train_mae\": train_mae, \"val_mae\": val_mae, \"best_iteration\": model.best_iteration, \"train_size\": len(X_train), \"val_size\": len(X_val), } ) print(f\"í›ˆë ¨ RMSE: {train_rmse:.3f}, ê²€ì¦ RMSE: {val_rmse:.3f}\") print(f\"í›ˆë ¨ RÂ²: {train_r2:.3f}, ê²€ì¦ RÂ²: {val_r2:.3f}\") print(f\"í›ˆë ¨ MAE: {train_mae:.3f}, ê²€ì¦ MAE: {val_mae:.3f}\") print(f\"ìµœì  ë°˜ë³µíšŸìˆ˜: {model.best_iteration}\") ì›ë³¸ dfì— outcome ì˜ˆì¸¡ê°’ ì¶”ê°€ df[\"outcome_prediction\"] = outcome_predictions print(f\"\\n=== ì „ì²´ Outcome Function ê²°ê³¼ ===\") performance_df = pd.DataFrame(fold_performances) print(f\"í‰ê·  í›ˆë ¨ RMSE: {performance_df['train_rmse'].mean():.3f} Â± {performance_df['train_rmse'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ RMSE: {performance_df['val_rmse'].mean():.3f} Â± {performance_df['val_rmse'].std():.3f}\") print(f\"í‰ê·  í›ˆë ¨ RÂ²: {performance_df['train_r2'].mean():.3f} Â± {performance_df['train_r2'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ RÂ²: {performance_df['val_r2'].mean():.3f} Â± {performance_df['val_r2'].std():.3f}\") print(f\"í‰ê·  í›ˆë ¨ MAE: {performance_df['train_mae'].mean():.3f} Â± {performance_df['train_mae'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ MAE: {performance_df['val_mae'].mean():.3f} Â± {performance_df['val_mae'].std():.3f}\") ê³¼ì í•© ì²´í¬ overfitting_score = performance_df[\"train_r2\"].mean() - performance_df[\"val_r2\"].mean() print(f\"\\nê³¼ì í•© ì§€í‘œ (Train RÂ² - Val RÂ²): {overfitting_score:.3f}\") if overfitting_score > 0.1: print(\"âš ï¸ ì‹¬ê°í•œ ê³¼ì í•©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ê°•í™”í•˜ì„¸ìš”.\") elif overfitting_score > 0.05: print(\"âš ï¸ ì•½ê°„ì˜ ê³¼ì í•©ì´ ìˆìŠµë‹ˆë‹¤.\") else: print(\"âœ… ê³¼ì í•©ì´ ì˜ ì œì–´ë˜ê³  ìˆìŠµë‹ˆë‹¤.\") print(f\"\\nì²˜ìŒ 10ê°œ ìƒ˜í”Œì˜ ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’:\") comparison_df = df[[y[0], \"outcome_prediction\"]].head(10) comparison_df[\"residual\"] = comparison_df[y[0]] - comparison_df[\"outcome_prediction\"] print(comparison_df) print(f\"\\nOutcome ì˜ˆì¸¡ê°’ í†µê³„:\") print(f\" í‰ê· : {df['outcome_prediction'].mean():.2f}\") print(f\" í‘œì¤€í¸ì°¨: {df['outcome_prediction'].std():.2f}\") print(f\" ë²”ìœ„: [{df['outcome_prediction'].min():.2f}, {df['outcome_prediction'].max():.2f}]\") ì”ì°¨ ê³„ì‚° (R-learnerì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´) df[\"outcome_residual\"] = df[y[0]] - df[\"outcome_prediction\"] print(f\"\\nOutcome ì”ì°¨ í†µê³„:\") print(f\" í‰ê· : {df['outcome_residual'].mean():.2f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") print(f\" í‘œì¤€í¸ì°¨: {df['outcome_residual'].std():.2f}\") ì‹œê°í™” plt.figure(figsize=(12, 4)) ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ plt.subplot(1, 3, 1) plt.scatter(df[y[0]], df[\"outcome_prediction\"], alpha=0.6, s=10) plt.plot([df[y[0]].min(), df[y[0]].max()], [df[y[0]].min(), df[y[0]].max()], \"r--\", lw=2) plt.xlabel(\"ì‹¤ì œê°’\") plt.ylabel(\"ì˜ˆì¸¡ê°’\") plt.title(\"ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’\") ì”ì°¨ ë¶„í¬ plt.subplot(1, 3, 2) plt.hist(df[\"outcome_residual\"], bins=30, alpha=0.7, edgecolor=\"black\") plt.xlabel(\"ì”ì°¨\") plt.ylabel(\"ë¹ˆë„\") plt.title(\"ì”ì°¨ ë¶„í¬\") plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) Foldë³„ ì„±ëŠ¥ plt.subplot(1, 3, 3) plt.plot(performance_df[\"fold\"], performance_df[\"train_r2\"], \"o-\", label=\"Train RÂ²\", alpha=0.7) plt.plot(performance_df[\"fold\"], performance_df[\"val_r2\"], \"s-\", label=\"Val RÂ²\", alpha=0.7) plt.xlabel(\"Fold\") plt.ylabel(\"RÂ² Score\") plt.title(\"Foldë³„ ì„±ëŠ¥\") plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” print(f\"\\n=== Foldë³„ ì„±ëŠ¥ ìš”ì•½ ===\") print(performance_df[[\"fold\", \"train_rmse\", \"val_rmse\", \"train_r2\", \"val_r2\", \"best_iteration\"]].round(3)) df.iloc[[4799, 4800]] import matplotlib.pyplot as plt import numpy as np í•œê¸€ í°íŠ¸ ì„¤ì • plt.rcParams[\"font.family\"] = \"AppleGothic\" plt.rcParams[\"axes.unicode_minus\"] = False Sizeë³„ ì”ì°¨ ë¶„í¬ ì‹œê°í™” fig, axes = plt.subplots(2, 3, figsize=(18, 10)) Sizeë³„ íˆìŠ¤í† ê·¸ë¨ (ê²¹ì¹˜ê²Œ) plt.subplot(2, 3, 1) for size in sorted(df[\"size\"].unique()): residuals = df[df[\"size\"] == size][\"outcome_residual\"] plt.hist(residuals, bins=30, alpha=0.7, label=f\"Size {size}\", density=True) plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.xlabel(\"Outcome ì”ì°¨\") plt.ylabel(\"ë°€ë„\") plt.title(\"Sizeë³„ ì”ì°¨ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)\") plt.legend() plt.grid(True, alpha=0.3) Sizeë³„ ë°•ìŠ¤í”Œë¡¯ plt.subplot(2, 3, 2) size_groups = [df[df[\"size\"] == size][\"outcome_residual\"] for size in sorted(df[\"size\"].unique())] size_labels = [f\"Size {size}\" for size in sorted(df[\"size\"].unique())] plt.boxplot(size_groups, labels=size_labels) plt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.ylabel(\"Outcome ì”ì°¨\") plt.title(\"Sizeë³„ ì”ì°¨ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)\") plt.grid(True, alpha=0.3) ë°”ì´ì˜¬ë¦° í”Œë¡¯ plt.subplot(2, 3, 3) parts = plt.violinplot(size_groups, positions=range(1, len(size_groups) + 1)) plt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.xticks(range(1, len(size_labels) + 1), size_labels) plt.ylabel(\"Outcome ì”ì°¨\") plt.title(\"Sizeë³„ ì”ì°¨ ë¶„í¬ (ë°”ì´ì˜¬ë¦° í”Œë¡¯)\") plt.grid(True, alpha=0.3) 4-6. Sizeë³„ ê°œë³„ íˆìŠ¤í† ê·¸ë¨ for i, size in enumerate(sorted(df[\"size\"].unique())): plt.subplot(2, 3, 4 + i) residuals = df[df[\"size\"] == size][\"outcome_residual\"] plt.hist(residuals, bins=30, alpha=0.7, edgecolor=\"black\") plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.axvline(residuals.mean(), color=\"blue\", linestyle=\"-\", alpha=0.7, label=f\"í‰ê· : {residuals.mean():.1f}\") plt.xlabel(\"Outcome ì”ì°¨\") plt.ylabel(\"ë¹ˆë„\") plt.title(f\"Size {size} ì”ì°¨ ë¶„í¬ (n={len(residuals)})\") plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() Sizeë³„ í†µê³„ ìš”ì•½ print(\"=== Sizeë³„ ì”ì°¨ í†µê³„ ===\") for size in sorted(df[\"size\"].unique()): residuals = df[df[\"size\"] == size][\"outcome_residual\"] print(f\"\\nSize {size} (n={len(residuals)}):\") print(f\" í‰ê· : {residuals.mean():8.2f}\") print(f\" í‘œì¤€í¸ì°¨: {residuals.std():8.2f}\") print(f\" ì¤‘ì•™ê°’: {residuals.median():8.2f}\") print(f\" ë²”ìœ„: [{residuals.min():6.1f}, {residuals.max():6.1f}]\") ì „ì²´ ì”ì°¨ í†µê³„ print(f\"\\nì „ì²´ (n={len(df)}):\") print(f\" í‰ê· : {df['outcome_residual'].mean():8.2f}\") print(f\" í‘œì¤€í¸ì°¨: {df['outcome_residual'].std():8.2f}\") ANOVA ê²€ì • (sizeë³„ ì”ì°¨ í‰ê·  ì°¨ì´ ê²€ì •) from scipy.stats import f_oneway size_12_1 = df[df[\"size\"] == 12.1][\"outcome_residual\"] size_12_6 = df[df[\"size\"] == 12.6][\"outcome_residual\"] size_13_2 = df[df[\"size\"] == 13.2][\"outcome_residual\"] f_stat, p_value = f_oneway(size_12_1, size_12_6, size_13_2) print(f\"\\n=== ANOVA ê²€ì • (sizeë³„ ì”ì°¨ í‰ê·  ì°¨ì´) ===\") print(f\"F-í†µê³„ëŸ‰: {f_stat:.4f}\") print(f\"p-value: {p_value:.6f}\") if p_value < 0.05: print(\"âŒ Sizeë³„ë¡œ ì”ì°¨ í‰ê· ì— ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŒ (ë¬¸ì œ)\") else: print(\"âœ… Sizeë³„ ì”ì°¨ í‰ê· ì— ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (ì¢‹ìŒ)\") > ì”ì°¨í™”ê°€ ì˜ ëëŠ”ì§€ëŠ” ì–´ë–»ê²Œ íŒë‹¨í•  ìˆ˜ ìˆëŠ”ê±°ì§€? ì£¼ìš” í‰ê°€ ê¸°ì¤€: ì¡°ê±´ë¶€ ë…ë¦½ì„± (ê°€ì¥ ì¤‘ìš”) E[ì”ì°¨ X] â‰ˆ 0 RÂ² < 0.01 ì´ë©´ ìš°ìˆ˜ RÂ² > 0.05 ì´ë©´ ë¬¸ì œ í¸í–¥ ì œê±° ì”ì°¨ í‰ê·  â‰ˆ 0 ì²´ê³„ì ì¸ ê³¼ëŒ€/ê³¼ì†Œì˜ˆì¸¡ ì—†ìŒ ì •ê·œì„± ì”ì°¨ê°€ ì •ê·œë¶„í¬ í˜•íƒœ ì™œë„, ì²¨ë„ê°€ 0ì— ê°€ê¹Œì›€ ì”ì°¨ ê°„ ë…ë¦½ì„± Treatment ì”ì°¨ì™€ Outcome ì”ì°¨ ê°„ ë‚®ì€ ìƒê´€ê´€ê³„ ìƒê´€ê³„ìˆ˜ < 0.1 ì´ë©´ ì´ìƒì  import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.font_manager as fm from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from scipy import stats í•œê¸€ í°íŠ¸ ì„¤ì • plt.rcParams[\"font.family\"] = \"AppleGothic\" plt.rcParams[\"axes.unicode_minus\"] = False def check_conditional_independence_comprehensive(df): \"\"\" Outcome ì”ì°¨ì˜ ì¡°ê±´ë¶€ ë…ë¦½ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í™•ì¸ \"\"\" features = [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] residual_col = \"outcome_residual\" print(\"=\" 60) print(\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ê²€ì‚¬: E[outcome_residual X] = 0\") print(\"=\" 60) X = df[features] residuals = df[residual_col] # ===== 1. ì „ì²´ RÂ² í™•ì¸ ===== print(\"\\n1. ì „ì²´ ì¡°ê±´ë¶€ ë…ë¦½ì„± ê²€ì‚¬\") print(\"-\" 40) model = LinearRegression() model.fit(X, residuals) predictions = model.predict(X) r2_total = r2_score(residuals, predictions) print(f\"Featuresë¡œ ì”ì°¨ ì˜ˆì¸¡ RÂ²: {r2_total:.6f}\") # RÂ² í‰ê°€ if r2_total < 0.001: print(\"ğŸ‰ ìš°ìˆ˜: ë§¤ìš° ê°•í•œ ì¡°ê±´ë¶€ ë…ë¦½ì„±\") grade = \"A+\" elif r2_total < 0.01: print(\"âœ… ìš°ìˆ˜: ê°•í•œ ì¡°ê±´ë¶€ ë…ë¦½ì„±\") grade = \"A\" elif r2_total < 0.05: print(\"âš ï¸ ì–‘í˜¸: ì•½ê°„ì˜ ì˜ì¡´ì„± ì¡´ì¬\") grade = \"B\" elif r2_total < 0.1: print(\"âš ï¸ ë³´í†µ: ì¤‘ê°„ ì •ë„ì˜ ì˜ì¡´ì„±\") grade = \"C\" else: print(\"âŒ ë¬¸ì œ: ê°•í•œ ì˜ì¡´ì„± - ëª¨ë¸ ê°œì„  í•„ìš”\") grade = \"D\" # ===== 2. ê° featureë³„ ìƒê´€ê´€ê³„ ===== print(f\"\\n2. ê° Featureë³„ ìƒê´€ê´€ê³„\") print(\"-\" 40) correlations = {} for feature in features: corr = np.corrcoef(df[feature], residuals)[0, 1] correlations[feature] = corr if abs(corr) < 0.05: status = \"âœ… ë§¤ìš° ë‚®ìŒ\" elif abs(corr) < 0.1: status = \"âš ï¸ ë‚®ìŒ\" elif abs(corr) < 0.2: status = \"âš ï¸ ì¤‘ê°„\" else: status = \"âŒ ë†’ìŒ\" print(f\"{feature:15}: {corr:8.4f} {status}\") # ===== 3. ì”ì°¨ì˜ ê¸°ë³¸ í†µê³„ ===== print(f\"\\n3. ì”ì°¨ ê¸°ë³¸ í†µê³„\") print(\"-\" 40) mean_residual = residuals.mean() std_residual = residuals.std() skew_residual = stats.skew(residuals) kurt_residual = stats.kurtosis(residuals) print(f\"í‰ê· : {mean_residual:8.4f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") print(f\"í‘œì¤€í¸ì°¨: {std_residual:8.4f}\") print(f\"ì™œë„: {skew_residual:8.4f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") print(f\"ì²¨ë„: {kurt_residual:8.4f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") # ===== 4. í†µê³„ì  ê²€ì • ===== print(f\"\\n4. í†µê³„ì  ê²€ì •\") print(\"-\" 40) # F-test for overall significance n = len(residuals) k = len(features) f_stat = (r2_total / k) / ((1 - r2_total) / (n - k - 1)) f_p_value = 1 - stats.f.cdf(f_stat, k, n - k - 1) print(f\"F-ê²€ì • (ì „ì²´ ëª¨ë¸ ìœ ì˜ì„±):\") print(f\" F-í†µê³„ëŸ‰: {f_stat:.4f}\") print(f\" p-value: {f_p_value:.6f}\") if f_p_value > 0.05: print(\" âœ… ì”ì°¨ì™€ features ê°„ ìœ ì˜í•œ ê´€ê³„ ì—†ìŒ (ì¢‹ìŒ)\") else: print(\" âŒ ì”ì°¨ì™€ features ê°„ ìœ ì˜í•œ ê´€ê³„ ì¡´ì¬ (ë¬¸ì œ)\") # Shapiro-Wilk test for normality (ìƒ˜í”Œì´ ë„ˆë¬´ í¬ë©´ ìƒëµ) if len(residuals) <= 5000: shapiro_stat, shapiro_p = stats.shapiro(residuals) print(f\"\\nShapiro-Wilk ì •ê·œì„± ê²€ì •:\") print(f\" í†µê³„ëŸ‰: {shapiro_stat:.4f}\") print(f\" p-value: {shapiro_p:.6f}\") if shapiro_p > 0.05: print(\" âœ… ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„\") else: print(\" âš ï¸ ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë²—ì–´ë‚¨\") # ===== 5. ì‹œê°í™” ===== print(f\"\\n5. ì‹œê°ì  ì§„ë‹¨\") print(\"-\" 40) fig, axes = plt.subplots(2, 3, figsize=(18, 12)) fig.suptitle(\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ì§„ë‹¨\", fontsize=16) # 5-1. ê° feature vs ì”ì°¨ for i, feature in enumerate(features): row = i // 2 col = i % 2 axes[row, col].scatter(df[feature], residuals, alpha=0.6, s=10) axes[row, col].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) # ì¶”ì„¸ì„  z = np.polyfit(df[feature], residuals, 1) p = np.poly1d(z) x_line = np.linspace(df[feature].min(), df[feature].max(), 100) axes[row, col].plot(x_line, p(x_line), \"r-\", alpha=0.8, linewidth=2) axes[row, col].set_xlabel(feature) axes[row, col].set_ylabel(\"Outcome ì”ì°¨\") axes[row, col].set_title(f\"{feature} vs ì”ì°¨ (ìƒê´€ê³„ìˆ˜: {correlations[feature]:.3f})\") axes[row, col].grid(True, alpha=0.3) # 5-2. ì˜ˆì¸¡ê°’ vs ì”ì°¨ axes[1, 0].scatter(df[\"outcome_prediction\"], residuals, alpha=0.6, s=10) axes[1, 0].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) z = np.polyfit(df[\"outcome_prediction\"], residuals, 1) p = np.poly1d(z) x_line = np.linspace(df[\"outcome_prediction\"].min(), df[\"outcome_prediction\"].max(), 100) axes[1, 0].plot(x_line, p(x_line), \"r-\", alpha=0.8, linewidth=2) axes[1, 0].set_xlabel(\"ì˜ˆì¸¡ê°’\") axes[1, 0].set_ylabel(\"ì”ì°¨\") axes[1, 0].set_title(\"ì˜ˆì¸¡ê°’ vs ì”ì°¨ (ë“±ë¶„ì‚°ì„± ê²€ì‚¬)\") axes[1, 0].grid(True, alpha=0.3) # 5-3. ì”ì°¨ ë¶„í¬ axes[1, 1].hist(residuals, bins=50, density=True, alpha=0.7, edgecolor=\"black\") # ì •ê·œë¶„í¬ ê³¡ì„  ì˜¤ë²„ë ˆì´ mu, sigma = residuals.mean(), residuals.std() x_norm = np.linspace(residuals.min(), residuals.max(), 100) axes[1, 1].plot(x_norm, stats.norm.pdf(x_norm, mu, sigma), \"r-\", lw=2, label=\"ì •ê·œë¶„í¬\") axes[1, 1].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) axes[1, 1].set_xlabel(\"ì”ì°¨\") axes[1, 1].set_ylabel(\"ë°€ë„\") axes[1, 1].set_title(\"ì”ì°¨ ë¶„í¬\") axes[1, 1].legend() axes[1, 1].grid(True, alpha=0.3) plt.tight_layout() plt.show() # ===== 6. Q-Q Plot ===== plt.figure(figsize=(8, 6)) stats.probplot(residuals, dist=\"norm\", plot=plt) plt.title(\"Q-Q Plot: ì”ì°¨ ì •ê·œì„± ê²€ì •\") plt.grid(True, alpha=0.3) plt.show() # ===== 7. ì¢…í•© í‰ê°€ ===== print(f\"\\n\" + \"=\" 60) print(\"ì¢…í•© í‰ê°€\") print(\"=\" 60) issues = [] if r2_total > 0.05: issues.append(\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ìœ„ë°˜\") if abs(mean_residual) > 0.1: issues.append(\"í¸í–¥ ì¡´ì¬\") if any(abs(corr) > 0.2 for corr in correlations.values()): issues.append(\"ê°•í•œ feature ì˜ì¡´ì„±\") if f_p_value < 0.05: issues.append(\"í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì˜ì¡´ì„±\") print(f\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ë“±ê¸‰: {grade}\") print(f\"ì „ì²´ RÂ²: {r2_total:.6f}\") print(f\"ë°œê²¬ëœ ë¬¸ì œ: {len(issues)}ê°œ\") if issues: print(\"\\në¬¸ì œì :\") for issue in issues: print(f\" âŒ {issue}\") print(f\"\\nê°œì„  ë°©ì•ˆ:\") print(f\" ğŸ”§ ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ (ë” ë§ì€ features, interaction terms)\") print(f\" ğŸ”§ ë¹„ì„ í˜• ë³€í™˜ ì ìš©\") print(f\" ğŸ”§ ì •ê·œí™” íŒŒë¼ë¯¸í„° ì¡°ì •\") print(f\" ğŸ”§ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì‹œë„ (XGBoost, Neural Network)\") else: print(\"\\nğŸ‰ ì¡°ê±´ë¶€ ë…ë¦½ì„±ì´ ì˜ ë§Œì¡±ë©ë‹ˆë‹¤!\") print(\"âœ… R-learnerì˜ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥\") return { \"r2_total\": r2_total, \"correlations\": correlations, \"mean_residual\": mean_residual, \"f_p_value\": f_p_value, \"grade\": grade, \"issues\": issues, } ì‹¤í–‰ results = check_conditional_independence_comprehensive(df) # 3. CATE ì¶”ì • ëª¨ë¸ > CATE(X, T) class MultiTreatmentRLearner: def __init__(self, model_y, model_t, model_final): self.model_y = model_y # Y ëª¨ë¸ self.model_t = model_t # Treatment propensity ëª¨ë¸ self.model_final = model_final # Final stage ëª¨ë¸ def fit(self, Y, T, X, W=None): # Step 1: Estimate E[Y X,W] if W is not None: XW = np.concatenate([X, W], axis=1) else: XW = X self.model_y.fit(XW, Y) Y_pred = self.model_y.predict(XW) Y_res = Y - Y_pred # Y residuals # Step 2: Multi-treatment propensity estimation # One-hot encode treatments T_onehot = pd.get_dummies(T, drop_first=True) T_pred = np.zeros_like(T_onehot) for k, col in enumerate(T_onehot.columns): # Each treatment vs rest self.model_t.fit(XW, T_onehot[col]) T_pred[:, k] = self.model_t.predict_proba(XW)[:, 1] T_res = T_onehot - T_pred # Treatment residuals # Step 3: Final stage - regress Y_res on T_res X # Create interaction terms features = [] for k in range(T_res.shape[1]): # Î¸_k(X) = X Î±_k (linear case) interaction = T_res[:, k : k + 1] X features.append(interaction) final_features = np.concatenate(features, axis=1) self.model_final.fit(final_features, Y_res) def predict(self, X, T0, T1): # Predict treatment effects between T0 and T1 # Î¸(X) (T1 - T0) pass from econml.dml import DML from econml.metalearners import XLearner, TLearner EconMLì˜ meta-learnersëŠ” multi-treatment ì§€ì› xl = XLearner(models=RandomForestRegressor()) xl.fit(Y, T, X=X) # T can be multi-valued from causalml.inference.meta import XLearner Multi-treatment support",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-manski-bounds",
    "title": "[Paper Review] Nonparametric Bounds on Treatment Effects",
    "date": "2025-08-20",
    "excerpt": "Manski boundsì— ëŒ€í•œ ì„¤ëª…",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "",
    "public": false
  },
  {
    "id": "Data Science/Causal Inference/when-assumptions-failed",
    "title": "ì–‘ìˆ˜ì„± ê°€ì • ë“± í•„ìš”í•œ ê°€ì •ì´ ê¹¨ì¡Œì„ ë•Œ ëŒ€ì²˜ë°©ì•ˆ",
    "date": "2025-08-19",
    "excerpt": "ì–‘ìˆ˜ì„± ê°€ì • ë“± í•„ìš”í•œ ê°€ì •ì´ ê¹¨ì¡Œì„ ë•Œ ëŒ€ì²˜ë°©ì•ˆ",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "GPT Pro ë‹µë³€ ì•„ì£¼ í•µì‹¬ë§Œ ë¨¼ì € ìš”ì•½í• ê²Œìš”. ì–‘ìˆ˜ì„±(positivity/overlap)ì´ ì•ˆ ë§ìœ¼ë©´ â€˜ì›ë˜ ATEë¥¼ ê·¸ëŒ€ë¡œâ€™ëŠ” ë¹„(é)ì‹ë³„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ (1) ë¬¸ì œì˜ ì„±ê²©ì„ ì§„ë‹¨í•˜ê³ , (2) í‘œë³¸/ëŒ€ìƒÂ·ê°€ì¤‘ì¹˜Â·ì¶”ì •ëŸ‰Â·ì§ˆë¬¸(estimand) ì¤‘ ì–´ë””ë¥¼ ë°”ê¿€ì§€ ê²°ì •í•˜ë©°, (3) ë°”ë€ ì (ëŒ€ìƒÂ·í‰ê°€ëŸ‰)ì„ íˆ¬ëª…í•˜ê²Œ ë³´ê³ í•˜ëŠ” ê²ƒì´ ì •ì„ì…ë‹ˆë‹¤. ì´ ì•„ë˜ì— ì´ë¡ ì  ë°°ê²½â†’ì§„ë‹¨â†’ëŒ€ì‘ì „ëµ(ì˜µì…˜ë³„ ì¥ë‹¨ì )â†’ì‹¤ë¬´ ë ˆì‹œí”¼ ìˆœìœ¼ë¡œ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. --- 1) ì´ë¡ ì  ë°°ê²½: ì™œ ì–‘ìˆ˜ì„±ì´ ì¤‘ìš”í•œê°€? (ì´ì‚° ì²˜ë¦¬ $A\\in\\{0,1\\}$) ì–‘ìˆ˜ì„±ì€ $0< P(A=a\\mid X=x)$ ê°€ í•´ë‹¹ ê³µë³€ëŸ‰ ì§€ì§€ì§‘í•© ì „ì²´ì—ì„œ ì„±ë¦½í•´ì•¼ í•œë‹¤ëŠ” ê°€ì •ì…ë‹ˆë‹¤. ê°•í•œ ì˜ë¯¸ë¡œëŠ” $\\exists\\ \\epsilon>0$ s.t. $\\epsilon\\le P(A=1\\mid X)\\le 1-\\epsilon$. ì´ ê°€ì •ì´ ê¹¨ì§€ë©´ ì–´ë–¤ $x$ì— ëŒ€í•´ì„œëŠ” $A=a$ê°€ â€œì ˆëŒ€â€ ê´€ì¸¡ë˜ì§€ ì•Šì•„ $\\mathbb{E}[Y\\mid A=a,X=x]$ ìì²´ê°€ ë°ì´í„°ë¡œ í™•ì¸ ë¶ˆê°€(ë¬´í•œ ì™¸ì‚½)ë¼ ATEì˜ ë¹„ëª¨ìˆ˜ì  ì‹ë³„ì´ ë¬´ë„ˆì§‘ë‹ˆë‹¤. HernÃ¡n & Robins êµê³¼ì„œì—ì„œë„ ì–‘ìˆ˜ì„±ì„ êµí™˜ê°€ëŠ¥ì„±(ë¬´êµë€)Â·ì¼ê´€ì„±ê³¼ í•¨ê»˜ í•µì‹¬ ì‹ë³„ê°€ì •ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤. ([Miguel HernÃ¡n][1], [Harvard SPH Content][2]) ì‹¤ë¬´ì—ì„œëŠ” ë‘ ë¶€ë¥˜ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤. êµ¬ì¡°ì  ìœ„ë°˜(structural): ê·œì •/ì˜ë£Œ ê¸ˆê¸° ë“±ìœ¼ë¡œ íŠ¹ì • $X=x$ì—ì„œ ì–´ë–¤ ì²˜ì¹˜ê°€ ì›ì²œ ë¶ˆê°€ëŠ¥. í™•ë¥ ì (ì‹¤ë¬´) ìœ„ë°˜(practical): ê°€ëŠ¥ì€ í•˜ì§€ë§Œ í‘œë³¸ì—ì„œ ê±°ì˜/ì „í˜€ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•„ ê°€ì¤‘ì¹˜ê°€ í­ì£¼. Petersen ë“±ì€ ë‘ ìƒí™©ì„ êµ¬ë¶„í•˜ê³ , ì „ìëŠ” â€œì§ˆë¬¸ì„ ë°”ê¿”ì•¼â€ í•˜ê³  í›„ìëŠ” â€œì¶”ì • ì ˆì°¨ë¥¼ ì•ˆì •í™”â€í•˜ëŠ” ì ‘ê·¼ì„ ì •ë¦¬í•©ë‹ˆë‹¤. ([biostats.bepress.com][3]) ì‹œê°„ê°€ë³€ ë…¸ì¶œì—ì„œëŠ” ê° ì‹œì ë§ˆë‹¤ ìˆœì°¨ ì–‘ìˆ˜ì„±ì´ í•„ìš”í•©ë‹ˆë‹¤. ìœ„ë°˜ë˜ë©´ MSM/IPW ê°™ì€ ë°©ë²•ì˜ ê°€ì¤‘ì¹˜ê°€ í­ì£¼í•´ ë¶„ì‚°Â·í¸í–¥ì´ ì»¤ì§‘ë‹ˆë‹¤. (ê°€ì¤‘ì¹˜ ì•ˆì •í™”ëŠ” ë¶„ì‚°ì„ ì¤„ì—¬ì£¼ì§€ë§Œ, ì‹ë³„ ìì²´ë¥¼ ë³µì›í•´ ì£¼ì§„ ì•ŠìŠµë‹ˆë‹¤.) ([Epidemiologic Research][4]) --- 2) ë¨¼ì € í•˜ëŠ” ì¼: ì§„ë‹¨ ì²´í¬ë¦¬ìŠ¤íŠ¸ ì¶”ì •ëœ PS ë¶„í¬ì˜ ê²¹ì¹¨(íˆìŠ¤í† ê·¸ë¨/ì»¤ë„ë°€ë„Â·Qâ€“Q/eCDF ê²¹ì¹¨): ê¼¬ë¦¬ì—ì„œ í•œìª½ ì§‘ë‹¨ì´ â€œì‚¬ë¼ì§€ë©´â€ ê²½ê³ . Imbens & Rubin êµê³¼ì„œì˜ â€œoverlap ì§„ë‹¨â€ ë…¼ì˜ì™€ ìœ ì‚¬. ([Cambridge University Press & Assessment][5]) ê·¹ë‹¨ ê°€ì¤‘ì¹˜/ìœ íš¨í‘œë³¸í¬ê¸°(ESS): $\\text{ESS}=(\\sum_i w_i)^2/\\sum_i w_i^2$ (ì§‘ë‹¨ë³„ ESSë„ í™•ì¸). ESSê°€ ê¸‰ê°í•˜ë©´ ì‹¤ë¬´ì  ì–‘ìˆ˜ì„± ë¬¸ì œê°€ ì˜ì‹¬ë©ë‹ˆë‹¤. ([R Project Search][6]) ë¶„ë¦¬(separation)/ì™„ì „ì˜ˆì¸¡: PS ë¡œì§€ìŠ¤í‹±ì—ì„œ ì™„ì „ë¶„ë¦¬ ì§•í›„(ê³„ìˆ˜ ë°œì‚°). ì—°ì†/ë‹¤ë‹¨ê³„ ë…¸ì¶œ: â€œì—°ì† ë…¸ì¶œìš© ì–‘ìˆ˜ì„± ì§„ë‹¨â€ ê°™ì€ ì „ìš© í…ŒìŠ¤íŠ¸/ì‹œê°í™”ë¥¼ ë³‘í–‰. ([Wiley Online Library][7]) ë¶€íŠ¸ìŠ¤íŠ¸ë© ê¸°ë°˜ ì·¨ì•½ì„± í‰ê°€: íŒŒë¼ë©”íŠ¸ë¦­ ë¶€íŠ¸ìŠ¤íŠ¸ë©ìœ¼ë¡œ ì–‘ìˆ˜ì„± ì·¨ì•½ êµ¬ê°„ì„ ì •ëŸ‰í™”(í­ì£¼ ê°€ì¤‘ì¹˜ê°€ ì¶”ì •ëŸ‰ì„ ì§€ë°°í•˜ëŠ”ì§€). ([biostats.bepress.com][3]) --- 3) ëŒ€ì‘ ì „ëµ ì§€ë„(ë¬´ì—‡ì„ ë°”ê¿€ ê²ƒì¸ê°€?) A. í‘œë³¸/ëŒ€ìƒ(population)ì„ ë°”ê¾¸ê¸° â€” ê²¹ì¹¨ì´ ìˆëŠ” ë¶€ë¶„ë§Œ ì¶”ì • ê³µí†µì§€ì§€(common support) ì œí•œ/íŠ¸ë¦¬ë°: PSê°€ $[\\alpha,1-\\alpha]$ì— ìˆëŠ” ë‹¨ìœ„ë§Œ ë¶„ì„. Crump et al.(2009)ëŠ” ë¶„ì‚°ì„ ìµœì†Œí™”í•˜ëŠ” ê·¼ì‚¬ ê·œì¹™ìœ¼ë¡œ $0.1\\sim0.9$ íŠ¸ë¦¬ë°ì„ ì œì•ˆ(ìƒí™© ì˜ì¡´ì ì´ë©° ë³´ê³ ì— íˆ¬ëª…ì„± í•„ìš”). ([EconPapers][8]) íŠ¸ë¦¬/ê·œì¹™ ê¸°ë°˜ ì§€ì§€ì˜ì—­ ì„ íƒ: Traskin & Small(2011)ì€ CARTë¡œ í•´ì„ ê°€ëŠ¥í•œ ê²¹ì¹¨ ì˜ì—­ì„ êµ¬ì„±. ([SpringerLink][9]) Coarsened Exact Matching (CEM): ì‚¬ì „ ì½”ì–´ìŠ¤ë‹í•œ ì¸µì—ì„œ ì²˜ì¹˜/ëŒ€ì¡°ê°€ ëª¨ë‘ ì¡´ì¬í•˜ëŠ” ì¸µë§Œ ìœ ì§€ â†’ ì–‘ìˆ˜ì„±ì„ â€œì„¤ê³„ë¡œâ€ ê°•í™”. ë‹¨, \\\\ëŒ€ìƒ(estimand)ì´ â€˜ë§¤ì¹­ í›„ í‘œë³¸â€™\\\\ìœ¼ë¡œ ë°”ë€ë‹ˆë‹¤. ([JSTOR][10]) > ì¥ì : ì™¸ì‚½ì„ í”¼í•˜ê³  ì•ˆì •ì . ë‹¨ì : \\\\ATEê°€ ì•„ë‹ˆë¼ â€˜ê²¹ì¹¨ ìˆëŠ” ë¶€ë¶„ì˜ íš¨ê³¼â€™\\\\ë¡œ ì§ˆë¬¸ì´ ë°”ë€ë‹ˆë‹¤. (ë°˜ë“œì‹œ ë³´ê³ ) --- B. ê°€ì¤‘ì¹˜ë¥¼ ë°”ê¾¸ê¸°(ì•ˆì •í™”Â·ìº˜ë¦¬ë¸Œë ˆì´ì…˜) â€” ì‹¤ë¬´ ìœ„ë°˜ ì§„ì •ì‹œí‚¤ê¸° Stabilized IPTW: ë¶„ì‚°ì„ ì¤„ì´ê³  ì†Œìˆ˜ ê´€ì¸¡ì¹˜ê°€ ì§€ë°°í•˜ëŠ” ê²ƒì„ ë§‰ìŠµë‹ˆë‹¤(ì‹ë³„ì„ ë³µì›í•˜ëŠ” ê±´ ì•„ë‹˜). ([Epidemiologic Research][4]) ê°€ì¤‘ì¹˜/PS ì ˆë‹¨(truncation): $[c,1-c]$ë¡œ ìë¥´ê±°ë‚˜ ê·¹ë‹¨ ê°€ì¤‘ì¹˜ë¥¼ ìœˆì €ë¼ì´ì¦ˆ. í¸í–¥â€“ë¶„ì‚° ì ˆì¶©ì´ë¯€ë¡œ cutoffë¥¼ ë°ì´í„° êµ¬ë™ì ìœ¼ë¡œ ê³ ë¥´ë ¤ëŠ” ì—°êµ¬ê°€ ìˆìŠµë‹ˆë‹¤(ì˜ˆ: AJE 2022). ([Oxford Academic][11]) Overlap Weights(OW): ì²˜ì¹˜êµ°ì€ $1-e(X)$, ëŒ€ì¡°êµ°ì€ $e(X)$ë¡œ ê°€ì¤‘ â†’ ê°€ì¤‘ì¹˜ê°€ ìœ ê³„ì´ê³  ê²¹ì¹¨ì´ í° êµ¬ê°„ì— ì´ˆì (ATO). ì´ë¡ ì ìœ¼ë¡œ ë¶„ì‚° ìµœì Â·ì‘ì€ í‘œë³¸ì—ì„œ í‰ê· ì˜ ì •í™•ê· í˜•ê¹Œì§€ ë³´ì¥. ì˜ë£Œ ì‘ìš© íŠœí† ë¦¬ì–¼ë„ ë‹¤ìˆ˜. ([Oxford Academic][12]) ê· í˜•ì§€í–¥ PS/ë³´ì •: CBPS(ê³µë³€ëŸ‰ ê· í˜•ì„ ì¼ìœ¼í‚¤ë„ë¡ PSë¥¼ ì¶”ì •)Â·Entropy Balancing(ëª¨ë©˜íŠ¸ ì œì•½ìœ¼ë¡œ ì§ì ‘ ê· í˜•). ê·¹ë‹¨ ê°€ì¤‘ì¹˜ë¥¼ ì–µì œí•˜ë©° ê· í˜•ì„ ë³´ì¥. ([imai.fas.harvard.edu][13], [Massachusetts Institute of Technology][14]) > ì¥ì : ì›í‘œë³¸ì„ í¬ê²Œ ë²„ë¦¬ì§€ ì•Šê³  ì•ˆì •í™”. ë‹¨ì : ATEâ†’ATO/ATT ë“± ëŒ€ìƒì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ(íŠ¹íˆ OW), ë˜ëŠ” ì ˆë‹¨ìœ¼ë¡œ ì‘ì€ í¸í–¥ì´ ë“¤ì–´ì˜´(ë°˜ë“œì‹œ ë¯¼ê°ë„ ë¶„ì„). ([Oxford Academic][11]) --- C. ì¶”ì •ëŸ‰ì„ ë°”ê¾¸ê¸°(ëª¨í˜•ê¸°ë°˜Â·DR) â€” ì™¸ì‚½ ë¯¼ê°ë„ë¥¼ ì¤„ì´ê¸° ì´ì¤‘ê°•ê±´(AIPW)Â·TMLE: ê²°ê³¼ëª¨í˜•ê³¼ PSëª¨í˜•ì„ í•¨ê»˜ ì‚¬ìš©í•´ í•œìª½ì´ ì•½ê°„ ë¹—ë‚˜ê°€ë„ ì¼ê´€ì„± ë³´ì¡´. \\\\ë‹¤ë§Œ ì‹ë³„ ê°€ì •(ì–‘ìˆ˜ì„± ìì²´)\\\\ì€ ì—¬ì „íˆ í•„ìš”. TMLE/CTMLEëŠ” ì‘ì€ $g(X)=P(A=1\\mid X)$ ì— ë¯¼ê°í•œ â€œclever covariateâ€ë¥¼ ì•ˆì •í™”í•˜ë ¤ëŠ” ì ˆë‹¨/í˜‘ì—…í˜• ë°©ë²•ì´ ì œì•ˆë˜ì–´ ì™”ìŠµë‹ˆë‹¤. ([CRAN][15], [arXiv][16], [Oxford Academic][11]) --- D. ì§ˆë¬¸(estimand)ì„ ë°”ê¾¸ê¸° â€” ì–‘ìˆ˜ì„± ê°€ì •ì„ ì•½í™”/íšŒí”¼ Incremental Propensity Score Interventions (IPSI): â€œëª¨ë“  ì‚¬ëŒì„ 1ë¡œ/0ìœ¼ë¡œâ€ê°€ ì•„ë‹ˆë¼ \\\\ì²˜ì¹˜ í™•ë¥ ì˜ log-oddsë¥¼ $\\delta$ë§Œí¼ í‚¤ìš°ë©´ ê²°ê³¼ê°€ ì–¼ë§ˆë‚˜ ë³€í•˜ë‚˜?\\\\ë¥¼ ë¬»ëŠ” ì ì§„ì  ê°œì… íš¨ê³¼. ì–‘ìˆ˜ì„± ê°€ì •ì´ í•„ìš” ì—†ìŠµë‹ˆë‹¤(ì²˜ì¹˜ í™•ë¥ ì„ â€˜ì¦ê°â€™ì‹œí‚¤ëŠ” ê°œì…). ë¹„ëª¨ìˆ˜ ì´ë¡ Â·ì¶”ì •Â·ì‹ ë¢°ëŒ€ì—­ì´ ì •ë¦¬ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ([arXiv][17], [Beyond the Average Treatment Effect][18]) Stochastic/Modified Treatment Policies (MTP/LMTP): â€œìì—°ì  ì²˜ì¹˜ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ $\\delta$ë§Œí¼ ëŠ˜ë¦¬ê¸°/ìƒí•œì„ ë‘ê¸°â€ ê°™ì€ í˜„ì‹¤ì  ì •ì±… ê°œì…ì„ ì •ì˜Â·ì¶”ì • â†’ ì‹œê°„ê°€ë³€ ë…¸ì¶œì—ì„œë„ ìˆœì°¨ ì–‘ìˆ˜ì„±ì˜ ë¶€ë‹´ì„ ëœì–´ì¤ë‹ˆë‹¤. ([IDEAS/RePEc][19], [Epidemiologic Research][20]) > ì¥ì : í˜„ì‹¤ì Â·í•´ì„ ê°€ëŠ¥, ì–‘ìˆ˜ì„± ë¶€ë‹´ ì™„í™”/íšŒí”¼. ë‹¨ì : ì§ˆë¬¸ì´ ATEì™€ ë‹¤ë¦„â€”ì •ì±…/ê³¼í•™ì  ëª©ì ê³¼ ì¼ì¹˜í•˜ëŠ”ì§€ í•©ì˜ê°€ í•„ìš”. --- E. ë¶€ë¶„ì‹ë³„/ë°”ìš´ë”© â€” êµ¬ì¡°ì  ìœ„ë°˜ì—ì„œ ì •ì§í•œ ë³´ê³  Manski bounds ë“±: â€œìµœì•…â€“ìµœì„ â€ ë²”ìœ„ë¥¼ ì œì‹œ(í•„ìš” ì‹œ IV/ë‹¨ì¡°ì„±ìœ¼ë¡œ ì¶•ì†Œ). êµ¬ì¡°ì  ìœ„ë°˜ì—ì„  ì´ëŸ° ë°”ìš´ë“œ ë³´ê³ ê°€ ì†”ì§í•œ ì„ íƒì…ë‹ˆë‹¤. ìµœê·¼ì—” ìœ„ë°˜ ê°•ë„ë¥¼ ë§¤ê°œë³€ìˆ˜í™”í•´ breakdown frontierê¹Œì§€ ì œì‹œí•˜ëŠ” í”„ë ˆì„ì›Œí¬ë„ ë‚˜ì˜µë‹ˆë‹¤. ([JSTOR][21], [arXiv][22]) --- 4) ì‹¤ë¬´ ë ˆì‹œí”¼(ì´ì§„ ì²˜ì¹˜, ë‹¨ì¼ ì‹œì  ì˜ˆ) ì§„ë‹¨: PS ê²¹ì¹¨Â·ESSÂ·ê·¹ë‹¨ê°€ì¤‘ì¹˜ ì²´í¬(ì§‘ë‹¨ë³„ ESS, max/min PS ë“±). ì—°ì†/ë‹¤ìˆ˜ì¤€ì´ë©´ ì „ìš© ì§„ë‹¨ë„ ë³‘í–‰. ([R Project Search][6], [Wiley Online Library][7]) êµ¬ì¡°ì  vs ì‹¤ë¬´ ìœ„ë°˜ êµ¬ë¶„ êµ¬ì¡°ì : ê·¸ $X$ ì˜ì—­ì˜ ATEëŠ” ë¹„ì‹ë³„. â†’ ëŒ€ìƒ ë³€ê²½ (ê²¹ì¹¨ ì˜ì—­ìœ¼ë¡œ ì œí•œ/OW) ë˜ëŠ” ì§ˆë¬¸ ë³€ê²½(IPSI/MTP), ë˜ëŠ” ë¶€ë¶„ì‹ë³„ ë°”ìš´ë“œ. ([biostats.bepress.com][3]) ì‹¤ë¬´ ìœ„ë°˜: ê·¹ë‹¨ê°€ì¤‘ì¹˜/ì € ESS. â†’ ê°€ì¤‘ì¹˜ ì•ˆì •í™”(Stabilized IPTW, ì ˆë‹¨)Â·ê· í˜•ì§€í–¥ ê°€ì¤‘(CBPS/EB)Â·OWÂ·DR/TMLEì„ ì¡°í•©. ([Epidemiologic Research][4], [Oxford Academic][11]) ëŒ€ìƒê³¼ ì¶”ì •ê¸°ì˜ ë§¤ì¹­ ATE ìœ ì§€ ê³ ì§‘: ìµœì†Œí•œ ëŒ€ìƒ ì œí•œ+DR(ì˜ˆ: $[0.05,0.95]$ íŠ¸ë¦¬ë° + AIPW/TMLE) & ì ˆë‹¨ ë¯¼ê°ë„ ê³ë“¤ì„. ([Oxford Academic][11]) ê²¹ì¹¨ì§‘ë‹¨ íš¨ê³¼(ATO) í—ˆìš©: Overlap Weights + AIPW/TMLE ê¶Œì¥(ê°€ì¤‘ì¹˜ ìœ ê³„Â·ê· í˜• ìš°ìˆ˜). ([Oxford Academic][12]) ì •ì±…/í˜„ì‹¤ ì§ˆë¬¸ì— ê´€ì‹¬: IPSI(ë‹¨ì¼/ì¢…ë‹¨) ë˜ëŠ” LMTP(ì—°ì†/ì‹œê°„ê°€ë³€)ë¡œ ì§ˆë¬¸ì„ ì¬ì •ì˜. ([arXiv][17], [Epidemiologic Research][20]) ë³´ê³ : (i) ì„ íƒí•œ ëŒ€ìƒ/estimandì™€ ì´ìœ , (ii) íŠ¸ë¦¬ë°/ì ˆë‹¨ ê·œì¹™, (iii) ê°€ì¤‘ì¹˜ ë¶„í¬Â·ESS, (iv) ê· í˜• ì§€í‘œ, (v) ë¯¼ê°ë„(ì»·ì˜¤í”„Â·ë°©ë²• ê°„ ë¹„êµ). --- 5) ì‹œê°„ê°€ë³€Â·ì—°ì† ë…¸ì¶œì˜ ì£¼ì˜ì  ìˆœì°¨ ì–‘ìˆ˜ì„±ì´ íŠ¹íˆ ì·¨ì•½. Stabilized IPWÂ·Super Learner ë“±ìœ¼ë¡œ í’ˆì§ˆì„ ë†’ì´ë˜, í­ì£¼í•˜ë©´ LMTP/í™•ë¥ ì  ê°œì…/IPSIë¡œ ë¬¸ë‹µì„ ë°”ê¾¸ëŠ” ê²ƒì´ í•©ë¦¬ì ì¼ ë•Œê°€ ë§ìŠµë‹ˆë‹¤. ([Epidemiologic Research][4], [tlverse][23], [arXiv][17]) ì—°ì† ë…¸ì¶œ: ì–‘ìˆ˜ì„± ì§„ë‹¨ ë„êµ¬ë¥¼ í™œìš©í•˜ê³ (ì§€ì—­ì  ë°€ë„ ê²°í•), shift/truncation ì •ì±… íš¨ê³¼ ê°™ì€ MTPë¥˜ë¥¼ ê³ ë ¤. ([Wiley Online Library][7], [IDEAS/RePEc][19]) --- 6) ì–´ë–¤ ì„ íƒì´ ì–¸ì œ ì¢‹ì€ê°€? (ìš”ì•½ í‘œ) ìƒí™© ê¶Œì¥ ì „ëµ í•µì‹¬ ì¥ì  ì£¼ì˜ì  ---------------------- ------------------------- --------------------------- ---------------------------------------------------- ê¼¬ë¦¬ì—ì„œ ê²¹ì¹¨ ê±°ì˜ ì—†ìŒ(ì‹¤ë¬´ ìœ„ë°˜) Overlap Weights + DR ê°€ì¤‘ì¹˜ ìœ ê³„, ê²¹ì¹¨ì§‘ë‹¨(ATO)ì— ìµœì  íš¨ìœ¨Â·ê· í˜• ATO(ì§ˆë¬¸)ë¡œ ë³€ê²½ë¨ ([Oxford Academic][12]) ATE ê³ ìˆ˜í•˜ë˜ ê·¹ë‹¨ê°€ì¤‘ì¹˜ ë¬¸ì œ íŠ¸ë¦¬ë°/ì ˆë‹¨ + TMLE/AIPW í¸í–¥â€“ë¶„ì‚° ì ˆì¶©ìœ¼ë¡œ ì•ˆì •í™” ì ˆë‹¨ì´ ì²´ê³„ì  í¸í–¥ ë„ì… ê°€ëŠ¥ â†’ ë¯¼ê°ë„ ë³´ê³  í•„ìˆ˜ ([Oxford Academic][11]) êµ¬ì¡°ì  ê¸ˆê¸°(ì–´ë–¤ $X$ì—ì„œ ì²˜ì¹˜ ë¶ˆê°€) ê²¹ì¹¨ì˜ì—­ í•œì • ë˜ëŠ” ë°”ìš´ë“œ ë³´ê³  ì™¸ì‚½ íšŒí”¼/ì •ì§í•œ ë¶ˆí™•ì‹¤ì„± ë³´ê³  ì›ë˜ ATEëŠ” ë¹„ì‹ë³„ ([biostats.bepress.com][3], [JSTOR][21]) ì •ì±…ì  â€œì–¼ë§ˆë‚˜ ë” ê¶Œì¥/ê°ì†Œâ€ê°€ ê¶ê¸ˆ IPSI ì–‘ìˆ˜ì„± ë¶ˆìš”, ì¶”ì„¸ê³¡ì„ ìœ¼ë¡œ í•´ì„ ìš©ì´ ATEì™€ ë‹¤ë¥¸ ì§ˆë¬¸ ([arXiv][17]) ì¢…ë‹¨Â·ì—°ì† ë…¸ì¶œì—ì„œ í­ì£¼ LMTP/í™•ë¥ ì  ê°œì… í˜„ì‹¤ì  ì •ì±… ì •ì˜, ìˆœì°¨ ì–‘ìˆ˜ì„± ë¶€ë‹´ ì™„í™” ê°œì… ì •ì˜Â·í•´ì„ì„ ëª…í™•íˆ ë¬¸ì„œí™” í•„ìš” ([Epidemiologic Research][20]) --- 7) ì°¸ê³ Â·ê·¼ê±°(ì¼ë¶€) ì§„ë‹¨/ëŒ€ì‘ ì¢…í•©: Petersen et al., Diagnosing and Responding to Violations in the Positivity Assumption (Berkeley Working Paper; êµ¬ì¡°ì /í™•ë¥ ì  ìœ„ë°˜, í‘œë³¸ ì œí•œÂ·ì§ˆë¬¸ ë³€ê²½ì˜ ì›ì¹™). ([biostats.bepress.com][3]) íŠ¸ë¦¬ë° ê·œì¹™: Crumpâ€“Hotzâ€“Imbensâ€“Mitnik (2009, Biometrika): $[0.1,0.9]$ ê·œì¹™ì˜ ê·¼ì‚¬ ìµœì ì„±. ([EconPapers][8]) Overlap Weights/ATO: Liâ€“Morganâ€“Zaslavsky (2018, JASA) ì´ë¡ Â·ìœ ê³„ì„±Â·ì •í™•ê· í˜•; ì˜ë£Œ ì‘ìš© ìš”ì•½(JAMA 2020). ([Oxford Academic][12]) Stabilized IPW: Cole & HernÃ¡n (2008, AJE). ([Epidemiologic Research][4]) ì ˆë‹¨ì˜ ë°ì´í„°êµ¬ë™ ì„ íƒ/CTMLE: Gruber et al. (2022, AJE); Positivityâ€‘Câ€‘TMLE (2017). ([Oxford Academic][11], [arXiv][16]) CBPS/Entropy Balancing: Imai & Ratkovic (2014, JRSSâ€‘B); Hainmueller (2012, Political Analysis). ([imai.fas.harvard.edu][13], [Massachusetts Institute of Technology][14]) IPSI: Kennedy (2019, JASA): ì–‘ìˆ˜ì„± ì—†ì´ ì ì§„ì  ê°œì… íš¨ê³¼. ([arXiv][17]) Stochastic/Modified Treatment Policies: DÃ­az & van der Laan (2012, Biometrics); Youngâ€“HernÃ¡nâ€“Robins (2014, Epidemiologic Methods) ë“±. ([IDEAS/RePEc][19], [De Gruyter Brill][24]) ì—°ì†ë…¸ì¶œ ì–‘ìˆ˜ì„± ì§„ë‹¨: â€œA Simple Diagnostic for the Positivity Assumption for Continuous Exposures.â€ ([Wiley Online Library][7]) ESS ì§€í‘œ: WeightIt ë¬¸ì„œ(ESS ê³µì‹). ([R Project Search][6]) --- 8) ì‹¤ì „ ì ìš©ì‹œ ì²´í¬ë¦¬ìŠ¤íŠ¸(ì§§ê²Œ) [ ] (í•„ìˆ˜) ê²¹ì¹¨/ESS/ê°€ì¤‘ì¹˜ ë¶„í¬ ë³´ê³  [ ] (í•„ìˆ˜) \\\\ì„ íƒí•œ estimand(ATE/ATT/ATO/IPSI/LMTP)\\\\ì™€ ëŒ€ìƒ(population) ëª…ì‹œ [ ] (ê¶Œì¥) íŠ¸ë¦¬ë°/ì ˆë‹¨ ê·œì¹™ê³¼ ë¯¼ê°ë„ ë¶„ì„(ì»·ì˜¤í”„Â·ë°©ë²• ê°„ ë¹„êµ) [ ] (ê¶Œì¥) ê· í˜• ì§€í‘œ(SMD ë“±)ì™€ ì¶”ì •ê¸° DR ì‚¬ìš© ì—¬ë¶€ [ ] (êµ¬ì¡°ì  ìœ„ë°˜ ì‹œ) ë¶€ë¶„ì‹ë³„ ë°”ìš´ë“œ ë˜ëŠ” ì§ˆë¬¸ ë³€ê²½ ì´ìœ  ì„¤ëª… --- ì›í•˜ì‹œë©´, ê°€ì§€ê³  ê³„ì‹  ë°ì´í„°/ì£¼ì œì— ë§ì¶° (i) ê²¹ì¹¨ ì§„ë‹¨ í”Œë¡¯ê³¼ ESS ê³„ì‚°, (ii) ATE vs ATO(OW), (iii) ì ˆë‹¨ ë¯¼ê°ë„, (iv) IPSI/LMTPë¡œ ì§ˆë¬¸ ì¬ì •ì˜ê¹Œì§€ í•œ ë²ˆì— ëŒì•„ê°€ëŠ” ë¶„ì„ ì›Œí¬í”Œë¡œë¥¼ ë§Œë“¤ì–´ ë“œë¦´ê²Œìš”. [1]: https://miguelhernan.org/whatifbook?utm_source=chatgpt.com \"Causal Inference: What If (the book) â€” Miguel HernÃ¡n\" [2]: https://content.sph.harvard.edu/wwwhsph/sites/1268/2018/12/hernanrobins_v1.10.37.pdf?utm_source=chatgpt.com \"Causal Inference\" [3]: https://biostats.bepress.com/ucbbiostat/paper269/ \" \\\"Diagnosing and Responding to Violations in the Positivity Assumption\\\" by Maya L. Petersen, Kristin Porter et al. \" [4]: https://epiresearch.org/wp-content/uploads/2014/07/Cole_AJE_2008_168_656.pdf?utm_source=chatgpt.com \"kwn164 656..664 - epiresearch.org\" [5]: https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/assessing-overlap-in-covariate-distributions/0186880D45FC793BE1D23E4016A3419F?utm_source=chatgpt.com \"Assessing Overlap in Covariate Distributions (Chapter 14) - Causal ...\" [6]: https://search.r-project.org/CRAN/refmans/WeightIt/html/ESS.html?utm_source=chatgpt.com \"R: Compute effective sample size of weighted sample\" [7]: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.70194?utm_source=chatgpt.com \"A Simple Diagnostic for the Positivity Assumption for Continuous Exposures\" [8]: https://econpapers.repec.org/RePEc%3Aoup%3Abiomet%3Av%3A96%3Ay%3A2009%3Ai%3A1%3Ap%3A187-199 \"EconPapers: Dealing with limited overlap in estimation of average treatment effects\" [9]: https://link.springer.com/content/pdf/10.1007/s12561-011-9036-3.pdf?utm_source=chatgpt.com \"Defining the Study Population for an Observational Study to Ensure ...\" [10]: https://www.jstor.org/stable/pdf/41403736.pdf?utm_source=chatgpt.com \"Causal Inference without Balance Checking: Coarsened Exact Matching on ...\" [11]: https://academic.oup.com/aje/article/191/9/1640/6580570?utm_source=chatgpt.com \"Data-Adaptive Selection of the Propensity Score Truncation Level for ...\" [12]: https://academic.oup.com/aje/article/188/1/250/5090958?utm_source=chatgpt.com \"Addressing Extreme Propensity Scores via the Overlap Weights\" [13]: https://imai.fas.harvard.edu/research/files/CBPS.pdf?utm_source=chatgpt.com \"Covariate balancing propensity score - Harvard University\" [14]: https://www.mit.edu/~jhainm/Paper/eb.pdf?utm_source=chatgpt.com \"Entropy Balancing for Causal Effects: A Multivariate Reweighting ... - MIT\" [15]: https://cran.r-project.org/web/packages/tmle/tmle.pdf?utm_source=chatgpt.com \"tmle: Targeted Maximum Likelihood Estimation\" [16]: https://arxiv.org/pdf/1707.05861?utm_source=chatgpt.com \"On Adaptive Propensity Score Truncation in Causal Inference\" [17]: https://arxiv.org/abs/1704.00211?utm_source=chatgpt.com \"Nonparametric causal effects based on incremental propensity score interventions\" [18]: https://beyondtheate.com/08_R_ipsi.html?utm_source=chatgpt.com \"Incremental Propensity Score Interventions â€“ Beyond the Average ...\" [19]: https://ideas.repec.org/a/bla/biomet/v68y2012i2p541-549.html?utm_source=chatgpt.com \"Population Intervention Causal Effects Based on Stochastic I\" [20]: https://epiresearch.org/wp-content/uploads/2024/04/Nonparametric-Causal-Effects-Based-on-Longitudinal-Modified-Treatment-Policies.pdf?utm_source=chatgpt.com \"Nonparametric Causal Effects Based on Longitudinal Modified Treatment ...\" [21]: https://www.jstor.org/stable/pdf/2006592?utm_source=chatgpt.com \"Nonparametric Bounds on Treatment Effects - JSTOR\" [22]: https://arxiv.org/pdf/2505.24296?utm_source=chatgpt.com \"Data Fusion for Partial Identification of Causal Effects\" [23]: https://tlverse.org/tlverse-handbook/shift.html?utm_source=chatgpt.com \"Chapter 9 Stochastic Treatment Regimes Targeted Learning in R - tlverse\" [24]: https://www.degruyterbrill.com/document/doi/10.1515/em-2012-0001/pdf?utm_source=chatgpt.com \"Identification, Estimation and Approximation of Risk under ...\"",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-partial-identification",
    "title": "Partial Identificationì— ëŒ€í•œ ì„¤ëª…",
    "date": "2025-08-19",
    "excerpt": "Partial Identificationì´ ë¬´ì—‡ì´ê³ , ì–‘ìˆ˜ì„±ì´ ìœ„ë°˜ë˜ì—ˆì„ ë•Œ ì–´ë–»ê²Œ ì“¸ ìˆ˜ ìˆëŠ”ì§€",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "Positivity assumption - partial identification Treatment positivity assumption (positivity / overlap assumption) ì¸ê³¼ì¶”ë¡ (causal inference) ì—ì„œ í•„ìˆ˜ ê°€ì • ì¤‘ í•˜ë‚˜ì˜ˆìš”. ì •ì˜: $$ 0 < P(T=1 \\mid X=x) < 1 \\quad \\text{for all } x $$ ì¦‰, ëª¨ë“  ê³µë³€ëŸ‰ $x$ ì— ëŒ€í•´ ì²˜ë¦¬ë¥¼ ë°›ì„ í™•ë¥ ë„ 0ë³´ë‹¤ í¬ê³ , ë°›ì§€ ì•Šì„ í™•ë¥ ë„ 0ë³´ë‹¤ ì»¤ì•¼ í•œë‹¤ëŠ” ì¡°ê±´ì…ë‹ˆë‹¤. ì˜ë¯¸: ì–´ë–¤ $x$ ê°’ì„ ê°€ì§„ ì§‘ë‹¨ë„ ì™„ì „íˆ í•œìª½ ì²˜ìš°(treated/untreated)ì—ë§Œ ì†í•˜ë©´ ì•ˆ ëœë‹¤ëŠ” ê²ƒ. ê·¸ë˜ì•¼ ë¹„êµ(ë°˜ì‚¬ì‹¤ì  ì¶”ì •)ê°€ ê°€ëŠ¥í•´ì§€ì£ . ğŸ‘‰ í•˜ì§€ë§Œ í˜„ì‹¤ì—ì„œëŠ” positivity ìœ„ë°˜(violation) ì´ ìì£¼ ë°œìƒí•©ë‹ˆë‹¤. ì˜ˆ: íŠ¹ì • ì—°ë ¹ëŒ€(ì•„ì£¼ ì–´ë¦° ì•„ì´)ëŠ” ì–´ë–¤ ì•½ì„ ì ˆëŒ€ ì²˜ë°©í•˜ì§€ ì•ŠëŠ” ê²½ìš°. ê·¸ëŸ¬ë©´ $P(T=1 \\mid X=x) = 0$ì´ ë˜ì–´ë²„ë¦¬ì£ . --- Partial identification ì™„ì „í•œ ì‹ë³„(full identification): ëª¨ë“  ê°€ì •(ignorability, positivity ë“±)ì´ ë§Œì¡±ë˜ë©´ ì¸ê³¼íš¨ê³¼(ATE ë“±)ë¥¼ ì ì¶”ì •(point estimate) í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶€ë¶„ ì‹ë³„(partial identification): ê°€ì •ì´ ì¼ë¶€ ê¹¨ì ¸ì„œ ì •í™•í•œ ì ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•  ë•Œ, ê°€ëŠ¥í•œ ì¸ê³¼íš¨ê³¼ì˜ ë²”ìœ„(bound, interval) ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. â†’ â€œíš¨ê³¼ê°€ ì •í™•íˆ Î²ë‹¤â€ ëŒ€ì‹  â€œíš¨ê³¼ëŠ” \\[a, b] ì‚¬ì´ì— ìˆë‹¤â€ë¼ê³  ë§í•˜ëŠ” ë°©ì‹. --- Positivity ìœ„ë°˜ê³¼ partial identificationì˜ ê´€ê³„ Partial identification ì ‘ê·¼ì€ positivity assumptionì´ ìœ„ë°˜ëœ ìƒí™©ì„ ë‹¤ë£¨ëŠ” ëŒ€í‘œì ì¸ ë°©ë²• ì¤‘ í•˜ë‚˜ì˜ˆìš”. positivityê°€ ê¹¨ì§€ë©´ íŠ¹ì • $X$ ì˜ì—­ì—ì„œ ë°˜ì‚¬ì‹¤ì  ê²°ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ì—†ê²Œ ë˜ëŠ”ë°, ì´ë•Œ ê·¸ ì˜ì—­ì—ì„œ ì¸ê³¼íš¨ê³¼ê°€ ì–´ë–¤ ë²”ìœ„ì— ìˆì„ì§€ ë…¼ë¦¬ì Â·ìˆ˜í•™ì  ì œì•½ì„ í†µí•´ interval boundë¥¼ êµ¬í•©ë‹ˆë‹¤. Manski(1990s ì´í›„)ì˜ workë‚˜ ìµœê·¼ causal inference ë¬¸í—Œì—ì„œëŠ” ì´ëŸ° ìƒí™©ì„ \"bounded causal effects\" ë¬¸ì œë¡œ ë‹¤ë£¹ë‹ˆë‹¤. --- ì˜ˆì‹œ (ì§ê´€ì ìœ¼ë¡œ) ì–´ë–¤ ì•½ì´ ì–´ë¦°ì´ì—ê²ŒëŠ” ì ˆëŒ€ íˆ¬ì—¬ë˜ì§€ ì•Šì•˜ë‹¤ê³  í•´ë´…ì‹œë‹¤. â†’ positivity violation. ê·¸ëŸ¼ ì–´ë¦°ì´ ì§‘ë‹¨ì—ì„œëŠ” â€œíˆ¬ì—¬ ì‹œ ê²°ê³¼â€ë¥¼ ê´€ì¸¡í•  ìˆ˜ ì—†ì£ . Partial identification ì ‘ê·¼ì€ (a) ì´ ì§‘ë‹¨ì—ì„œì˜ ê²°ê³¼ê°€ ìµœì†Œí•œ ê´€ì¸¡ëœ ë²”ìœ„ ì•ˆì— ìˆì„ ê²ƒì´ë¼ëŠ” ê°€ì •, (b) monotonicity ê°™ì€ ì¶”ê°€ ê°€ì • ë“±ì„ ì‚¬ìš©í•´ì„œ \"íš¨ê³¼ëŠ” ìµœì†Œ 0 ì´ìƒ, ìµœëŒ€ 5 ì´í•˜\" ì²˜ëŸ¼ êµ¬ê°„ ì¶”ì •ì„ ì œì‹œí•©ë‹ˆë‹¤. --- partial identification êµ¬ì²´ì ì¸ ì´ë¡ ì ì¸ ë‚´ìš© ì‹ë³„(Identification)ì˜ í° ê·¸ë¦¼ ì™„ì „ ì‹ë³„(full identification): ì¸ê³¼íš¨ê³¼(ì˜ˆ: ATE, ATT ë“±)ë¥¼ ì ì¶”ì •(point estimate) í•  ìˆ˜ ìˆëŠ” ìƒí™©. ì¡°ê±´: Ignorability (unconfoundedness): $Y(1), Y(0) \\perp T \\mid X$ Positivity (overlap): $0 < P(T=1 \\mid X=x) < 1$ for all $x$ ì´ ë‘ ì¡°ê±´ì´ ë§ìœ¼ë©´, ATE = $\\mathbb{E}[Y(1)-Y(0)]$ë¥¼ ê´€ì¸¡ ë°ì´í„°ë¡œ ê³„ì‚° ê°€ëŠ¥. ë¶€ë¶„ ì‹ë³„(partial identification): ìœ„ ì¡°ê±´ë“¤ì´ ì™„ì „íˆ ì„±ë¦½í•˜ì§€ ì•Šê±°ë‚˜, ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ì ì¶”ì • ë¶ˆê°€ëŠ¥í•  ë•Œ â†’ ê°€ëŠ¥í•œ ê°’ì˜ ë²”ìœ„(bounds, intervals) ë¥¼ ì°¾ëŠ” ì ‘ê·¼. ë”°ë¼ì„œ \"íš¨ê³¼ê°€ Î²ë‹¤\" ëŒ€ì‹  \"íš¨ê³¼ëŠ” \\[a, b] ë²”ìœ„ ì•ˆì— ìˆë‹¤\"ë¼ê³  ë§í•˜ëŠ” ê²Œ í•µì‹¬. --- ë¶€ë¶„ ì‹ë³„ì´ í•„ìš”í•œ ëŒ€í‘œì  ìƒí™© (1) Positivity violation (overlap ìœ„ë°˜) ì–´ë–¤ $X$ ê°’ì—ì„œ ì²˜ë¦¬(Treatment)ê°€ ì „í˜€ ì£¼ì–´ì§€ì§€ ì•ŠëŠ” ê²½ìš°. ì˜ˆ: ì–´ë–¤ ë‚˜ì´ëŒ€ ì•„ì´ë“¤ì—ê²ŒëŠ” ì ˆëŒ€ ì•½ë¬¼ì´ íˆ¬ì—¬ë˜ì§€ ì•Šì•˜ë‹¤ë©´ $P(T=1 X)=0$. ì´ë•Œ ê·¸ êµ¬ê°„ì—ì„œ ë°˜ì‚¬ì‹¤ì (counterfactual) ê²°ê³¼ëŠ” ê´€ì¸¡ ë¶ˆê°€ëŠ¥. í•´ê²°: ê·¸ êµ¬ê°„ì—ì„œ ê°€ëŠ¥í•œ ê²°ê³¼ ë²”ìœ„ë¥¼ ê°€ì •í•˜ì—¬ bound ì œì‹œ. â†’ Manski bounds (1989, 1990): ê´€ì°° ê°€ëŠ¥í•œ ë¶€ë¶„ê³¼ ë¶ˆê°€ëŠ¥í•œ ë¶€ë¶„ì„ ë¶„ë¦¬í•´, ë…¼ë¦¬ì ìœ¼ë¡œ ê°€ëŠ¥í•œ ìµœëŒ€Â·ìµœì†Œ íš¨ê³¼ë¥¼ ì œì‹œ. --- (2) Unobserved confounding (ë¹„ê´€ì¸¡ êµë€) êµë€ë³€ìˆ˜ $U$ê°€ ê´€ì¸¡ë˜ì§€ ì•Šìœ¼ë©´ ignorabilityê°€ ê¹¨ì§. ì´ ê²½ìš° â€œíš¨ê³¼ì˜ ì ì¶”ì •â€ì€ ë¶ˆê°€ëŠ¥í•˜ì§€ë§Œ, ì¶”ê°€ ê°€ì •(ì˜ˆ: êµë€ì˜ ìµœëŒ€ íš¨ê³¼ í¬ê¸° ì œí•œ)ì„ ë„£ìœ¼ë©´ bounded effectë¥¼ ì œì‹œí•  ìˆ˜ ìˆìŒ. ì˜ˆ: Rosenbaumì˜ sensitivity analysis â†’ êµë€ì´ ì–¼ë§ˆë‚˜ ê°•í•´ì•¼ ì¶”ì •ëœ íš¨ê³¼ê°€ ë¬´ë„ˆì§ˆì§€ë¥¼ boundë¡œ ì œì‹œ. --- (3) Instrumental Variables (IV) ë„êµ¬ë³€ìˆ˜ ì ‘ê·¼ì—ì„œë„ ë¶€ë¶„ ì‹ë³„ì´ ìì£¼ ë“±ì¥. ê°•í•œ ê°€ì •(ì™„ì „í•œ ë…ë¦½ì„±, exclusion restriction ë“±)ì´ ë§Œì¡±ë˜ë©´ LATE(Local Average Treatment Effect)ë¥¼ ì ì¶”ì •í•  ìˆ˜ ìˆìŒ. í•˜ì§€ë§Œ ê°€ì •ì„ ì™„í™”í•˜ë©´ â†’ LATE ìì²´ëŠ” interval identificationìœ¼ë¡œë§Œ ê°€ëŠ¥. ëŒ€í‘œì : Imbens & Manski (2004) bounds. --- (4) Selection bias / Missing data ë°ì´í„°ê°€ ë¬´ì‘ìœ„ë¡œ ë¹ ì§€ì§€ ì•Šê³  ì„ íƒì  ê²°ì¸¡(non-ignorable missingness)ì´ ìˆì„ ë•Œ. ì´ ê²½ìš°ë„ ê´€ì°° ë¶ˆê°€ëŠ¥í•œ ë¶€ë¶„ì— ëŒ€í•´ ìµœì†ŒÂ·ìµœëŒ€ ë²”ìœ„ë¥¼ ê³„ì‚°í•˜ëŠ” Manskiâ€™s missing data boundsê°€ ìˆìŒ. --- ëŒ€í‘œì ì¸ Bound ê¸°ë²•ë“¤ Manski bounds (worst-case bounds) ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœ. ê°€ì •ì„ ìµœì†Œí™”í•˜ê³ , ê´€ì¸¡ëœ ë°ì´í„°ë§Œìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ê°€ëŠ¥í•œ ì¸ê³¼íš¨ê³¼ì˜ ìƒí•œ/í•˜í•œì„ ê³„ì‚°. ë‹¨ì : ë³´í†µ intervalì´ ë„ˆë¬´ ë„“ì–´ì„œ ì‹¤ìš©ì„±ì´ ë–¨ì–´ì§. Lee bounds (2009) \\\\ì„ íƒì  ì´íƒˆ(selection)\\\\ì´ ìˆëŠ” í”„ë¡œê·¸ë¨ í‰ê°€ì—ì„œ ì‚¬ìš©. monotonicity (ë‹¨ì¡°ì„±) ê°€ì •ì„ ì¶”ê°€í•´ì„œ boundë¥¼ ì¢í˜. Imbens-Manski bounds (2004) ë„êµ¬ë³€ìˆ˜ ê¸°ë°˜ ì¶”ë¡ ì—ì„œ treatment effect ë²”ìœ„ ì œì‹œ. Sensitivity analysis ê¸°ë°˜ bounds ë¹„ê´€ì¸¡ êµë€ì´ ìˆì„ ë•Œ, íŠ¹ì • â€œê°ë„(sensitivity)â€ ê°€ì •ì„ í†µí•´ íš¨ê³¼ ë²”ìœ„ë¥¼ ì œì‹œ. --- ì´ë¡ ì  ì² í•™ ì ì¶”ì •(point identification) ì€ ê°•í•œ ê°€ì •(positivity, ignorability ë“±)ì„ ìš”êµ¬. í˜„ì‹¤ì—ì„œëŠ” ì´ ê°€ì •ë“¤ì´ ìì£¼ ê¹¨ì§ â†’ ê°•í•œ ê°€ì • ëŒ€ì‹  ì•½í•œ ê°€ì •ë§Œ ë‘ê³ , ì¶”ì • ê°€ëŠ¥í•œ ë¶€ë¶„ë§Œìœ¼ë¡œ inference â†’ ë¶€ë¶„ ì‹ë³„. ë”°ë¼ì„œ partial identificationì€ \"robust inference under weaker assumptions\" ë¼ê³  í•  ìˆ˜ ìˆì–´ìš”. ManskiëŠ” ì´ë¥¼ \"honest inference\" ë¼ê³  ë¶€ë¦…ë‹ˆë‹¤. (ë¶ˆí•„ìš”í•˜ê²Œ ê°•í•œ ê°€ì •ìœ¼ë¡œ ì˜ëª»ëœ ì ì¶”ì •ì„ í•˜ëŠë‹ˆ, ë²”ìœ„ë¥¼ ì œì‹œí•˜ëŠ” ê²ƒì´ ë” ì •ì§í•˜ë‹¤). --- ìš”ì•½ Partial identification = ì •ë³´ ë¶€ì¡±/ê°€ì • ìœ„ë°˜ ìƒí™©ì—ì„œ ê°€ëŠ¥í•œ ì¸ê³¼íš¨ê³¼ì˜ ë²”ìœ„ë¥¼ ì œì‹œí•˜ëŠ” ë°©ë²•. í™œìš©ë˜ëŠ” ì´ë¡ ì  ë§¥ë½: positivity ìœ„ë°˜ unobserved confounding instrumental variables (ì•½í•œ ê°€ì •í•˜ì—) selection bias / missing data ì£¼ìš” ë„êµ¬: Manski bounds, Lee bounds, Imbens-Manski bounds, sensitivity analysis ---",
    "public": true
  },
  {
    "id": "Data Science/Statistics/confidence-interval-vs-prediction-interval",
    "title": "Confidence Interval vs Prediction Interval",
    "date": "2025-08-18",
    "excerpt": "CIì™€ PI ê°ê°ì— ëŒ€í•œ ì •ë¦¬ì™€, ì´ ê°œë… ê°„ì˜ ì°¨ì´ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "Confidence-Interval",
      "Prediction-Interval",
      "statistics"
    ],
    "content": "Linear Regressionì—ì„œì˜ CI ì™€ PI ê°œë… Confidence Interval for the Mean Response ì°¸ê³ ìë£Œ 1: psu edu > In this section, we are concerned with the confidence interval, called a \"t-interval,\" for the mean response when the predictor value is $x_h$. formula in words: $$ \\text{Sample estimate} \\pm (t\\text{-multiplier} \\times \\text{standard error}) $$ formula in notation: $$ \\hat{y}_h \\pm t_{(1-\\alpha/2, n-2)} \\times \\sqrt{MSE \\times \\left( \\frac{1}{n} + \\dfrac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right)} $$ where: $\\hat{y}_h$ is the \"fitted value\" or \"predicted value\" of the response when the predictor is $x_h$ $t_{(1-\\alpha/2, n-2)}$ is the \"t-multiplier.\" Note that the t-multiplier has $n-2$ (not $n-1$) degrees of freedom because the confidence interval uses the mean square error (MSE) whose denominator is $n-2$. The expression under the square root is the \"standard error of the fit,\" which depends on the mean square error (MSE), the sample size ($n$), how far in squared units the predictor value $x_h$ is from the average of the predictor values $\\bar{x}$, and the sum of the squared distances of the predictor values $x_i$ from the average of the predictor values $\\bar{x}$. ìˆ˜ì‹ì„ ì•Œì•„ì•¼í•˜ëŠ” ì´ìœ ëŠ” width of CIë¥¼ ê²°ì •í•˜ëŠ”ê²Œ ë­”ì§€ ì•Œê¸° ìœ„í•´ì„œ. Prediction Interval for a New Response Mean Response vs New Response Mean Response (í‰ê·  ì‘ë‹µ) ì •ì˜: íŠ¹ì • $x_h$ ê°’ì—ì„œ ë¬´í•œíˆ ë§ì€ ê´€ì¸¡ê°’ë“¤ì˜ í‰ê·  ì˜ˆì‹œ: í‚¤ê°€ 170cmì¸ ëª¨ë“  ì‚¬ëŒë“¤ì˜ í‰ê·  ëª¸ë¬´ê²Œ íŠ¹ì§•: ê³ ì •ëœ ê°’ (ë³€í•˜ì§€ ì•ŠìŒ) ëª¨ì§‘ë‹¨ì˜ íŠ¹ì„± Confidence Intervalì˜ ëŒ€ìƒ New Response (ìƒˆë¡œìš´ ì‘ë‹µ) ì •ì˜: íŠ¹ì • $x_h$ ê°’ì—ì„œ ìƒˆë¡œìš´ ê°œë³„ ê´€ì¸¡ê°’ ì˜ˆì‹œ: í‚¤ê°€ 170cmì¸ íŠ¹ì • í•œ ì‚¬ëŒì˜ ì‹¤ì œ ëª¸ë¬´ê²Œ íŠ¹ì§•: ëœë¤í•œ ê°’ (ë§¤ë²ˆ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ) ê°œë³„ ê´€ì¸¡ê°’ì˜ íŠ¹ì„± Prediction Intervalì˜ ëŒ€ìƒ > CIëŠ” mean estimatorì˜ ë¶ˆí™•ì‹¤ì„±(ì£¼ë¡œ epistemic)ì„, PIëŠ” mean ì£¼ë³€ì˜ noise(aleatoric)ê¹Œì§€ í¬í•¨í•´ â€œê°œë³„ ì˜ˆì¸¡ê°’â€ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‹¤ë£¸. ë”°ë¼ì„œ ë™ì¼í•œ ì„¤ì •ì—ì„œ PIì˜ í­ì´ CIë³´ë‹¤ ë„“ìŒ. ìˆ˜ì‹ì—ì„œì˜ í‘œí˜„ formula in words: $$ \\text{Sample estimate} \\pm (t\\text{-multiplier} \\times \\text{standard error}) $$ formula in notation: $$ \\hat{y}_h \\pm t_{(1-\\alpha/2, n-2)} \\times \\sqrt{MSE \\times \\left( 1 + \\frac{1}{n} + \\dfrac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right)} $$ PI: í‰ê·  ì¶”ì • ë¶ˆí™•ì‹¤ì„± + ê°œë³„ ê´€ì¸¡ê°’ì˜ ë³€ë™ì„± ê°œë³„ ê´€ì¸¡ê°’ì€ í‰ê·  ì£¼ë³€ì—ì„œ RMSEë§Œí¼ ë³€ë™ RMSEì˜ ì •ì˜ $$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}i)^2}{n-2}}$$ RMSEì˜ ì˜ë¯¸ Residualì˜ í‘œì¤€í¸ì°¨ ì‹¤ì œ ê´€ì¸¡ê°’($y_i$)ê³¼ ì˜ˆì¸¡ê°’($\\hat{y}i$) ì‚¬ì´ì˜ ì°¨ì´ ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ëŠ” ë¶€ë¶„ = ê°œë³„ ê´€ì¸¡ê°’ì˜ ê³ ìœ í•œ ë³€ë™ì„± ì„ í˜•íšŒê·€ì—ì„œëŠ” ì˜¤ì°¨í•­ $\\epsilon_i$ê°€ í‰ê·  0, ë¶„ì‚° $\\sigma^2$ì¸ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê¸° ë•Œë¬¸ì— ìœ„ì™€ ê°™ì€ ë‚´ìš©ì´ ì„±ë¦½í•¨.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-calibrated-probability",
    "title": "ë³´ì •ëœ í™•ë¥ (Calibrated Probability)ì´ë€?",
    "date": "2025-08-18",
    "excerpt": "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡ì„ ì‹¤ì œ ê´€ì°° ë¹ˆë„ì™€ ì¼ì¹˜ì‹œí‚¤ëŠ” ë°©ë²•ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "calibration",
      "probability",
      "machine-learning",
      "logistic-regression",
      "platt-scaling"
    ],
    "content": "ì°¸ê³ ìë£Œ 1: Scikit-learn - Probability Calibration 2: nannyML ë³´ì •ëœ í™•ë¥ (Calibrated Probability)ì´ë€? --- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ì´ ì‹¤ì œ ê´€ì°°ëœ ë¹ˆë„ì™€ ì¼ì¹˜í•˜ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë§ì€ ê²½ìš° ëª¨ë¸ì˜ ì¶œë ¥ì€ í™•ë¥ ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ë³´ì •ë˜ì§€ ì•Šì€(uncalibrated) ìƒíƒœì…ë‹ˆë‹¤. > ë³´ì •ë˜ì§€ ì•Šì€ í™•ë¥ : ëª¨ë¸ì´ ì˜ˆì¸¡í•œ í™•ë¥ ì´ ì‹¤ì œì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ > ë³´ì •ëœ í™•ë¥ : ëª¨ë¸ì˜ ì˜ˆì¸¡ í™•ë¥ ì´ ì‹¤ì œ ê´€ì°° ë¹ˆë„ì™€ ì¼ì¹˜ ì™œ ë³´ì •ì´ í•„ìš”í•œê°€? Logistic Regressionì˜ ê²½ìš° Logistic Regressionì€ 0~1 ì‚¬ì´ì˜ ê°’ì„ ì¶œë ¥í•˜ì§€ë§Œ, ì´ëŠ” ì‹¤ì œ í™•ë¥ ê³¼ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤: `` ì˜ˆì¸¡ í™•ë¥  ì˜ˆì¸¡ íšŸìˆ˜ ì‹¤ì œ ë°œìƒ íšŸìˆ˜ ì‹¤ì œ ë¹„ìœ¨ 80% 100 65 65% (overconfident) 30% 100 45 45% (underconfident) 90% 100 95 95% (well-calibrated) ` ë³´ì •ë˜ì§€ ì•Šì€ í™•ë¥ ì˜ ì›ì¸ ì •ê·œí™”(Regularization): L1/L2 ì •ê·œí™”ê°€ í™•ë¥  ì¶”ì •ì„ ì™œê³¡ í´ë˜ìŠ¤ ë¶ˆê· í˜•: ì†Œìˆ˜ í´ë˜ìŠ¤ì— ëŒ€í•´ underconfident ëª¨ë¸ ë³µì¡ì„±: ë³µì¡í•œ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•© íŠ¹ì„± ìŠ¤ì¼€ì¼ë§: íŠ¹ì„±ì˜ ìŠ¤ì¼€ì¼ì´ í™•ë¥  ì¶”ì •ì— ì˜í–¥ ë³´ì • ë°©ë²• Platt Scaling ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì‚¬ìš©í•œ í›„ì²˜ë¦¬ ë°©ë²•ì…ë‹ˆë‹¤: $$P(y=1 x) = \\frac{1}{1 + \\exp(-a \\cdot f(x) + b)}$$ ì—¬ê¸°ì„œ: $f(x)$: ì›ë³¸ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ $a, b$: ë³´ì • íŒŒë¼ë¯¸í„° (ê²€ì¦ ë°ì´í„°ë¡œ í•™ìŠµ) Isotonic Regression ë‹¨ì¡°ì„± ì œì•½ì„ ê°€ì§„ ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì…ë‹ˆë‹¤: $$P(y=1 x) = m(f(x))$$ ì—¬ê¸°ì„œ $m$ì€ monotonic increasing functionì…ë‹ˆë‹¤. Temperature Scaling Softmax ì¶œë ¥ì„ ì¡°ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤: $$P(y=k x) = \\frac{\\exp(z_k/T)}{\\sum_{i=1}^{K} \\exp(z_i/T)}$$ ì—¬ê¸°ì„œ $T$ëŠ” temperature parameterì…ë‹ˆë‹¤. ì‹¤ì œ ëª¨ë¸ë³„ ë³´ì • í•„ìš”ì„± Tree-based Models (LightGBM, XGBoost) Tree-based ëª¨ë¸ë“¤ì€ íŠ¹íˆ ë³´ì •ì´ í•„ìš”í•©ë‹ˆë‹¤: ` ì˜ˆì¸¡ í™•ë¥  ì˜ˆì¸¡ íšŸìˆ˜ ì‹¤ì œ ë°œìƒ íšŸìˆ˜ ì‹¤ì œ ë¹„ìœ¨ 90% 100 70 70% (overconfident) 10% 100 30 30% (underconfident) ` ì™œ ì´ëŸ° í˜„ìƒì´ ë°œìƒí• ê¹Œ? Boosting íŠ¹ì„±: ì—¬ëŸ¬ weak learnerë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê²°í•©í•˜ë©´ì„œ í™•ë¥  ì¶”ì •ì´ ì™œê³¡ë¨ ê³¼ì í•©: ê°•ë ¥í•œ ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ì í•©ë˜ì–´ ê³¼ì‹ (overconfident)í•˜ê²Œ ë¨ Loss function: ë¶„ë¥˜ ì •í™•ë„ì— ìµœì í™”ë˜ì–´ ìˆì§€ í™•ë¥  ì¶”ì •ì—ëŠ” ìµœì í™”ë˜ì§€ ì•ŠìŒ í•´ê²° ë°©ë²•: `python from sklearn.calibration import CalibratedClassifierCV import lightgbm as lgb LightGBM ëª¨ë¸ lgb_model = lgb.LGBMClassifier() ë³´ì •ëœ ëª¨ë¸ (isotonicì´ tree-based ëª¨ë¸ì— ë” ì í•©) calibrated_model = CalibratedClassifierCV( lgb_model, method='isotonic', # LightGBMì—ëŠ” isotonicì´ ë” ì í•© cv=5 ) ë³´ì •ëœ í™•ë¥  (ì´ì œ ì‚¬ìš© ê°€ëŠ¥) calibrated_probs = calibrated_model.predict_proba(X_test)[:, 1] ` ë³´ì • í‰ê°€ ë°©ë²• Reliability Plot (Reliability Diagram) ì˜ˆì¸¡ í™•ë¥ ê³¼ ì‹¤ì œ ê´€ì°° ë¹ˆë„(observed frequency)ë¥¼ ë¹„êµí•˜ëŠ” ì‹œê°í™”: ` ì˜ˆì¸¡ í™•ë¥  êµ¬ê°„ ìƒ˜í”Œ ìˆ˜ ì‹¤ì œ ì–‘ì„± ë¹„ìœ¨ [0.0, 0.1) 100 0.05 [0.1, 0.2) 150 0.12 ... [0.9, 1.0) 80 0.92 ` Calibration Error ë³´ì • í’ˆì§ˆì„ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •: $$\\text{Calibration Error} = \\sum_{i=1}^{B} \\frac{n_i}{N} p_i - \\hat{p}_i $$ ì—¬ê¸°ì„œ: $B$: number of bins $n_i$: number of samples in i-th bin $N$: total number of samples $p_i$: predicted probability in i-th bin $\\hat{p}_i$: observed frequency in i-th bin ì‹¤ì œ êµ¬í˜„ ì˜ˆì‹œ Python ì½”ë“œ `python from sklearn.calibration import CalibratedClassifierCV from sklearn.linear_model import LogisticRegression ì›ë³¸ ëª¨ë¸ base_model = LogisticRegression() ë³´ì •ëœ ëª¨ë¸ (Platt Scaling ì‚¬ìš©) calibrated_model = CalibratedClassifierCV( base_model, method='sigmoid', # Platt Scaling cv=5 ) í•™ìŠµ calibrated_model.fit(X_train, y_train) ë³´ì •ëœ í™•ë¥  ì˜ˆì¸¡ calibrated_probs = calibrated_model.predict_proba(X_test)[:, 1] ` sklearn.calibration ëª¨ë“ˆ sklearn.calibrationì€ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ê³µì‹ ëª¨ë“ˆì…ë‹ˆë‹¤: `python ì£¼ìš” import from sklearn.calibration import CalibratedClassifierCV from sklearn.calibration import calibration_curve calibration_curve ì‚¬ìš© ì˜ˆì‹œ fraction_of_positives, mean_predicted_value = calibration_curve( y_true, y_pred, n_bins=10 ) ` CalibratedClassifierCVì˜ ì£¼ìš” íŒŒë¼ë¯¸í„°: method='sigmoid': Platt Scaling (ë¡œì§€ìŠ¤í‹± íšŒê·€ ê¸°ë°˜) method='isotonic': Isotonic Regression (ë¹„ëª¨ìˆ˜ì  ë°©ë²•) cv: Cross-validation fold ìˆ˜ ë³´ì •ì˜ ì¤‘ìš”ì„± ì˜ì‚¬ê²°ì • ì§€ì› í™•ë¥  ê¸°ë°˜ ì˜ì‚¬ê²°ì •ì—ì„œ ì‹ ë¢°ì„±(reliability) í™•ë³´ ë¦¬ìŠ¤í¬ í‰ê°€ì˜ ì •í™•ì„± í–¥ìƒ ë¹„ì¦ˆë‹ˆìŠ¤ ì‘ìš© ì˜ë£Œì§„ë‹¨: ì§ˆë³‘ ë°œìƒ í™•ë¥ ì˜ ì •í™•í•œ ì¶”ì • ê¸ˆìœµë¦¬ìŠ¤í¬: ëŒ€ì¶œ ìƒí™˜ í™•ë¥ ì˜ ì‹ ë¢°ì„± ì¶”ì²œì‹œìŠ¤í…œ: ì‚¬ìš©ì ì„ í˜¸ë„ í™•ë¥ ì˜ ì •í™•ì„± ëª¨ë¸ í‰ê°€ ë‹¨ìˆœ ì •í™•ë„ ì™¸ì— í™•ë¥ ì  ì„±ëŠ¥(probabilistic performance) ì¸¡ì • ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„±(uncertainty)ì— ëŒ€í•œ ì´í•´ ì£¼ì˜ì‚¬í•­ ë°ì´í„° ëˆ„ìˆ˜(Data Leakage): ë³´ì • ì‹œ ê²€ì¦ ë°ì´í„°ë¥¼ ë³„ë„ë¡œ ì‚¬ìš© ë„ë©”ì¸ ë³€í™”(Domain Shift): ìƒˆë¡œìš´ ë„ë©”ì¸ì—ì„œëŠ” ì¬ë³´ì • í•„ìš” ê³„ì‚° ë¹„ìš©(Computational Cost): ë³´ì • ê³¼ì •ì—ì„œ ì¶”ê°€ì ì¸ ê³„ì‚° ì˜¤ë²„í—¤ë“œ ê²°ë¡  ë³´ì •ëœ í™•ë¥ ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì˜ ì¶œë ¥ì„ ì‹¤ì œ í™•ë¥ ê³¼ ì¼ì¹˜ì‹œí‚¤ëŠ” ì¤‘ìš”í•œ ê³¼ì •ì…ë‹ˆë‹¤. íŠ¹íˆ ì˜ì‚¬ê²°ì •ì´ ì¤‘ìš”í•œ ì‘ìš© ë¶„ì•¼ì—ì„œëŠ” ëª¨ë¸ì˜ í™•ë¥  ì˜ˆì¸¡ì´ ì‹ ë¢°í•  ìˆ˜ ìˆì–´ì•¼ í•˜ë¯€ë¡œ, ì ì ˆí•œ ë³´ì • ë°©ë²•ì„ ì ìš©í•˜ëŠ” ê²ƒì´ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ì¤‘ìš”í•œ ì : Logistic Regression: ê¸°ë³¸ì ìœ¼ë¡œ ë³´ì •ì´ í•„ìš” Tree-based Models (LightGBM, XGBoost): íŠ¹íˆ ë³´ì •ì´ í•„ìˆ˜ì  sklearn.calibration: ê³µì‹ì ìœ¼ë¡œ ì œê³µë˜ëŠ” ë³´ì • ë„êµ¬ method ì„ íƒ: sigmoid: ì„ í˜• ëª¨ë¸ì— ì í•© isotonic: tree-based ëª¨ë¸ì— ë” ì í•© ë”°ë¼ì„œ predict_probaë¥¼ ì§ì ‘ í™•ë¥ ë¡œ ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ê³ , ë°˜ë“œì‹œ ë³´ì • ê³¼ì •ì„ ê±°ì³ì•¼ í•©ë‹ˆë‹¤. --- Bad Machine Learning models can still be well-calibrated (2ë²ˆ ê¸€ ì •ë¦¬) > Machine learning models are often evaluated based on their performance, measured by how close some metric is to zero or one (depending on the metric), > but this is not the only factor that determines their usefulness. explore the difference between good calibration and good performance and when one might be preferred over the other. Probability calibration? > Probability calibration, in its strong definition, is the degree to which the probabilities predicted by a classification model match the true frequencies of the target classes in a dataset > A model with strong calibration guarantees that its predictions satisfy the frequentist definition of probability (as opposed to the Bayesian one), which states that an eventâ€™s probability is the limit of its relative frequency in many trials. ëŒ€ë¶€ë¶„ì˜ ML ì•Œê³ ë¦¬ì¦˜ì€ ill-calibratedí•˜ë‹¤. í™•ë¥ ë¡œ ì“°ê¸°ëŠ” ì–´ë ¤ì›€. > Most machine learning models are ill-calibrated, and the reasons depend on the learning algorithm Tree-based Ensemblesì˜ ë¬¸ì œì  Random Forest: ì—¬ëŸ¬ ê°œë³„ íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ í‰ê· ë‚´ì–´ ìµœì¢… ì˜ˆì¸¡ ìƒì„± í™•ë¥  ì¶”ì •ì˜ í•œê³„: 0ê³¼ 1ì— ê°€ê¹Œìš´ í™•ë¥ ì„ ì–»ê¸° ì–´ë ¤ì›€ ì´ìœ : ê°œë³„ íŠ¸ë¦¬ë“¤ ê°„ì— í•­ìƒ ë¶„ì‚°(variance)ì´ ì¡´ì¬ í™•ë¥  ì¶”ì •ì˜ ì™œê³¡ í˜„ìƒ 0 ê·¼ì²˜: ì‹¤ì œë³´ë‹¤ ë†’ê²Œ ì¶”ì • (overestimate) 1 ê·¼ì²˜: ì‹¤ì œë³´ë‹¤ ë‚®ê²Œ ì¶”ì • (underestimate) ê²°ê³¼: ê·¹ë‹¨ì ì¸ í™•ë¥ (0% ë˜ëŠ” 100%)ì„ ì‹ ë¢°í•  ìˆ˜ ì—†ìŒ ëª¨ë¸ ìµœì í™”ì˜ ë¬¸ì œ Binary metrics: ì •í™•ë„(accuracy) ë“±ì€ ë‹¨ìˆœíˆ ë§ë‹¤/í‹€ë¦¬ë‹¤ë§Œ íŒë‹¨ í™•ì‹¤ì„± ë¬´ì‹œ: ëª¨ë¸ì´ ì–¼ë§ˆë‚˜ í™•ì‹ í•˜ëŠ”ì§€ëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ Gini-impurity: ì˜ì‚¬ê²°ì •íŠ¸ë¦¬ê°€ ë¶„í• ì„ ê²°ì •í•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì§€í‘œ ëª©í‘œ: ê°€ëŠ¥í•œ í•œ ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ ë¶„ë¥˜í•˜ëŠ” ê²ƒ ì™œ ì´ëŸ° í˜„ìƒì´ ë°œìƒí• ê¹Œ? Ensemble íŠ¹ì„±: ì—¬ëŸ¬ íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ í‰ê· ë‚´ë©´ì„œ ê·¹ë‹¨ê°’ì´ ì™„í™”ë¨ ë¶„ì‚° ì¡´ì¬: ê° íŠ¸ë¦¬ê°€ ì„œë¡œ ë‹¤ë¥¸ íŒ¨í„´ì„ í•™ìŠµ ìµœì í™” ëª©í‘œ: ë¶„ë¥˜ ì •í™•ë„ì— ì§‘ì¤‘, í™•ë¥  ì¶”ì •ì€ ë¶€ì°¨ì  > The consequence of this is that while the scores produced by most machine learning models preserve order (the higher the number, the more likely the positive class), they cannot be interpreted as frequentist probabilities. Do you even need calibration? > When training a classification model, you need to ask yourself a crucial question: do you actually need the model to be well-calibrated? > The answer will depend on how the model will be used. Letâ€™s take a look at some examples. ë¬¸ì œì— ë”°ë¼ calibrationì´ í•„ìš”í•œì§€ ì •í•´ì§„ë‹¤. ranking ê°™ì€ ë¬¸ì œì—ì„œëŠ” í•„ìš” ì—†ì„ ê²ƒì´ê³ , ì€í–‰ì—ì„œ ëŒ€ì¶œì„ ê°šì„ í™•ë¥  ê°™ì€ ê²½ìš°ì—ëŠ” ì—„ë°€í•œ calibrationì´ í•„ìš”í•  ê²ƒì´ë‹¤. A story of a poor, well-calibrated model > But guess what, getting accurate predictions is not what we need! Just like in the credit line assignment and performance estimation examples, here too, the game is all about getting the probabilities right. ëª¨ë¸ì˜ ì„±ëŠ¥ì´ ì•ˆì¢‹ì„ë•Œë„ ìœ ìš©í•  ìˆ˜ ìˆë‹¤. > The modelâ€™s test accuracy was 63%. This is certainly better than a dummy model, which always predicts the home team to win; such a model would score 46% as the hosts tend to win almost half of the games. That said, 63% does not seem to be a great result. When itâ€™s impossible to get a good performance > Now consider an attempt to predict dice rolls. Our model should produce a probability of the die facing up to six after it has been rolled. ì£¼ì‚¬ìœ„ë¥¼ êµ´ë ¤ì„œ 6ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ë†’ì¼ ìˆ˜ê°€ ì—†ë‹¤. ì–´ë–¤ ê°’ì— ì˜í•´ ê²°ì •ë˜ëŠ”ê²Œ ì•„ë‹Œ ëœë¤ì´ë‹ˆê¹Œ. > Consider these two competing approaches. Model A is a dummy binary classifier that always predicts with full confidence that the rolled number is not a six; that is, it predicts a six 0% of the time and a not-six 100% of the time. Model B also never predicts a six, but the probabilities it outputs are different: it always predicts a six with the probability of â…™ and a not-six with the probability of â…š. > In the long run, both models feature the same accuracy: they are correct 5 out of 6 times. And this is as good as any model can get. However, an important fact differentiates between the two models: model B is perfectly calibrated while Model A is not calibrated at all. ë˜ê²Œ ì¢‹ì€ ì˜ˆì‹œë„¤. ì˜ˆì¸¡ ì„±ëŠ¥ë§Œìœ¼ë¡œëŠ” í‰ê°€í•  ìˆ˜ ì—†ëŠ” ì´ìœ . > As far as calibration is concerned, the two models couldnâ€™t be more different. How about the usefulness of the two models? > Model A doesnâ€™t really deliver any value. > Model B, on the other hand, allows us to accurately predict the target frequency in the long run. > It also allows us to run simulations to answer more complex questions such as: what is the probability of rolling four not-six and seven sixes in 11 rolls? Once again, despite the poor predictive performance, good calibration yields the model useful! Calibration probability ì„±ëŠ¥ í™•ì¸í•˜ëŠ” ë²• ğŸ” Calibration ì„±ëŠ¥ í‰ê°€ ê´€ì  2ê°€ì§€ Calibration quality (í™•ë¥  ë³´ì • í’ˆì§ˆ) ì—¬ê¸°ì„œ calibration curveê°€ ê°€ì¥ ì§ê´€ì ì¸ ë„êµ¬ì˜ˆìš”. í•˜ì§€ë§Œ curveëŠ” ì‹œê°ì  í™•ì¸ì´ë¯€ë¡œ, ì •ëŸ‰ì ì¸ ì§€í‘œë„ í•¨ê»˜ ì”ë‹ˆë‹¤: ECE (Expected Calibration Error) ì˜ˆì¸¡í™•ë¥  binë³„ í¸ì°¨ë¥¼ ê°€ì¤‘í‰ê· í•œ ê°’. ì‘ì„ìˆ˜ë¡ í™•ë¥ ì´ ì˜ ë³´ì •ë¨. MCE (Maximum Calibration Error) binë³„ ìµœëŒ€ í¸ì°¨. Brier Score í™•ë¥  ì˜ˆì¸¡ê³¼ ì‹¤ì œ ë¼ë²¨(0/1)ì˜ í‰ê· ì œê³±ì˜¤ì°¨. ë¶„ë¥˜ ì„±ëŠ¥ + ë³´ì • í’ˆì§ˆ ëª¨ë‘ ë°˜ì˜. ë”°ë¼ì„œ â€œcurve + ECE/Brier ê°™ì€ ìˆ˜ì¹˜â€ë¥¼ í•¨ê»˜ ë´ì•¼ í•¨. --- Discrimination ability (ë¶„ë¥˜ë ¥, ìˆœìœ„ ìœ ì§€ ì„±ëŠ¥) ëª¨ë¸ì´ ë³´ì •ë˜ì—ˆë”ë¼ë„, ë¶„ë¥˜ë ¥(ì˜ˆ: AUC, Accuracy, F1)ì´ ë‚˜ë¹ ì§€ë©´ ì˜ë¯¸ê°€ ì—†ì£ . calibrationì€ ë³´í†µ \\\\ë‹¨ì¡° ë³€í™˜(sigmoid, isotonic)\\\\ì´ë¼ì„œ ìˆœìœ„ ê¸°ë°˜ metric(AUC)ì€ ê±°ì˜ ìœ ì§€ë˜ì§€ë§Œ, ì„ê³„ê°’ ê¸°ë°˜ ì •í™•ë„Â·ì •ë°€ë„ ë“±ì€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆì–´ìš”. ë”°ë¼ì„œ calibration í›„ì—ë„ AUCë‚˜ Accuracyë¥¼ í™•ì¸í•´ì•¼ í•©ë‹ˆë‹¤. --- Calibration curveë§Œìœ¼ë¡œëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤. ì¶”ì²œ í‰ê°€ ë°©ë²•: Calibration curve â†’ ì§ê´€ì  ëª¨ì–‘ í™•ì¸ ECE / Brier score â†’ ì •ëŸ‰ì  calibration í’ˆì§ˆ AUC / Accuracy / F1 â†’ ë¶„ë¥˜ë ¥ ìœ ì§€ ì—¬ë¶€ --- > í™•ë¥  ë³´ì •ì˜ ì„±ëŠ¥ì€ â€œí™•ë¥ ì˜ ì‹ ë¢°ë„â€ì™€ â€œë¶„ë¥˜ë ¥â€ ë‘ ì¶•ì„ ëª¨ë‘ í™•ì¸í•´ì•¼ í•˜ê³ , ë”°ë¼ì„œ curve í•˜ë‚˜ë¡œë§Œ íŒë‹¨í•˜ë©´ ìœ„í—˜ Method ìì„¸í•˜ê²Œ ì •ë¦¬ Platt Scaling (method='sigmoid') Platt Scalingì€ ë¶„ë¥˜ê¸°ì˜ raw scoreë¥¼ ë³´ì •ëœ í™•ë¥ ë¡œ ë³€í™˜í•˜ëŠ” ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” calibration ë°©ë²• ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ğŸ“Œ í•µì‹¬ ê°œë… Platt Scalingì€ ë¶„ë¥˜ê¸°ì˜ \"ì ìˆ˜(score)\" $s$ë¥¼ í™•ë¥ ë¡œ ë³€í™˜í•˜ëŠ” sigmoid í˜•íƒœì˜ ë³´ì • í•¨ìˆ˜ì…ë‹ˆë‹¤: $$ \\hat{p}(y=1\\mid s) = \\sigma(As+B) = \\frac{1}{1+\\exp(-(As+B))} $$ ì—¬ê¸°ì„œ $A$, $B$ëŠ” ê²€ì¦ ë°ì´í„°ì—ì„œ ë¡œê·¸ ì†ì‹¤(log loss)ì„ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµë©ë‹ˆë‹¤. ğŸ¤” ì™œ í•„ìš”í•œê°€? ë§ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤ì´ ê²ªëŠ” ê³µí†µì ì¸ ë¬¸ì œ: SVM, Random Forest ë“±ì€ í™•ë¥ ì´ ì•„ë‹Œ \"ê²°ì • ì ìˆ˜\"ë¥¼ ì¶œë ¥ ì´ ì ìˆ˜ë“¤ì€ ìˆœìœ„ëŠ” ì •í™•í•˜ì§€ë§Œ(ë†’ì€ ROC-AUC) í™•ë¥ ë¡œëŠ” ë¶€ì •í™• ê³¼ì‹ (overconfident) ë˜ëŠ” ê³¼ì†Œì‹ (underconfident) ë¬¸ì œ ë°œìƒ Platt Scalingì€ ì´ë¥¼ Sì ê³¡ì„  í•˜ë‚˜ë¡œ í•´ê²°í•˜ì—¬ ìˆœìœ„ëŠ” ë³´ì¡´í•˜ë©´ì„œ í™•ë¥  í’ˆì§ˆì„ ê°œì„ í•©ë‹ˆë‹¤. ğŸ” ìˆ˜í•™ì  ë°°ê²½ ë³¸ì§ˆì ìœ¼ë¡œ íŠ¹ì§•ì´ 1ê°œ(ì ìˆ˜ $s$)ì¸ ë¡œì§€ìŠ¤í‹± íšŒê·€ì…ë‹ˆë‹¤: ë°ì´í„°: ê²€ì¦ ì…‹ $\\{(s_i, y_i)\\}_{i=1}^n$ where $y_i \\in \\{0,1\\}$ ëª¨ë¸: $\\text{logit}\\, \\hat{p}_i = As_i + B$ ëª©ì í•¨ìˆ˜: ë¡œê·¸ ì†ì‹¤ ìµœì†Œí™” $$ \\mathcal{L}(A,B) = -\\sum_{i=1}^n [y_i\\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)] $$ íŒŒë¼ë¯¸í„° í•´ì„ $B$: ì „ì²´ ê¸°ì €ìœ¨(base rate) ë³´ì • â†’ ì¢Œìš° ì´ë™ $A$: ê¸°ìš¸ê¸°(ì˜¨ë„) ì¡°ì ˆ $ A $ ì‘ìŒ â†’ ì™„ë§Œí•œ Sê³¡ì„  â†’ ê³¼ì‹  êµì • $ A $ í¼ â†’ ê°€íŒŒë¥¸ Sê³¡ì„  â†’ ê³¼ì†Œì‹  êµì • ğŸ’¡ ì‹¤ì „ ì‚¬ìš©ë²• 1) ë°ì´í„° ë¶„í•  (âš ï¸ ëˆ„ìˆ˜ ë°©ì§€ í•„ìˆ˜!) ê¶Œì¥ ë°©ë²•: K-fold Cross Validation `python from sklearn.calibration import CalibratedClassifierCV ìë™ìœ¼ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ calibrated = CalibratedClassifierCV(base_model, method='sigmoid', cv=5) calibrated.fit(X, y) ` ëŒ€ì•ˆ: ë…ë¦½ ê²€ì¦ì…‹ `python X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) base_model.fit(X_train, y_train) calibrated = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit') calibrated.fit(X_valid, y_valid) ` 2) ì¶”ë¡  ê³¼ì • ìƒˆ ìƒ˜í”Œ $x$ â†’ ë² ì´ìŠ¤ ëª¨ë¸ ì ìˆ˜ $s = f(x)$ â†’ ë³´ì •ëœ í™•ë¥  $\\hat{p} = \\sigma(As+B)$ ğŸ“Š ê°œì„  íš¨ê³¼ Platt Scaling ì ìš© í›„ ê°œì„ ë˜ëŠ” ì§€í‘œë“¤: ë¡œê·¸ ì†ì‹¤(Log Loss) âœ… ê±°ì˜ í•­ìƒ ê°œì„  ë¸Œë¼ì´ì–´ ì ìˆ˜(Brier Score) âœ… í™•ë¥  ì˜ˆì¸¡ì˜ MSE ê°ì†Œ ì‹ ë¢°ë„ ê³¡ì„  & ECE âœ… ë¹ˆë„-í™•ë¥  ì¼ì¹˜ ê°œì„  âš–ï¸ ì¥ë‹¨ì  ë¹„êµ âœ… ì¥ì  íŒŒë¼ë¯¸í„° 2ê°œë§Œ â†’ ì ì€ ë°ì´í„°ë¡œë„ ì•ˆì •ì  ë‹¨ì¡°ì„± ë³´ì¥ â†’ ìˆœìœ„/ROC ìœ ì§€ êµ¬í˜„ ê°„ë‹¨ â†’ ë¹ ë¥¸ í•™ìŠµ/ì¶”ë¡  âŒ í•œê³„ Sì í˜•íƒœ ì œì•½ â†’ ë³µì¡í•œ ì™œê³¡ íŒ¨í„´ í•œê³„ ê·¹ë‹¨ì  ë¶ˆê· í˜• â†’ ì¶©ë¶„í•œ ê²€ì¦ ìƒ˜í”Œ í•„ìš” ğŸ”„ ë‹¤ë¥¸ ë°©ë²•ê³¼ ë¹„êµ ë°©ë²• íŠ¹ì§• ì í•©í•œ ìƒí™© ------ ------ ------------- Platt Scaling ëª¨ìˆ˜ì , Sì ê³¡ì„  ì¼ë°˜ì ì¸ ê²½ìš°, ì ì€ ë°ì´í„° Isotonic Regression ë¹„ëª¨ìˆ˜ì , ë‹¨ì¡° ë§ì€ ë°ì´í„°, ë³µì¡í•œ íŒ¨í„´ Temperature Scaling ìŠ¤ì¼€ì¼ë§ë§Œ ë©€í‹°í´ë˜ìŠ¤, ê³¼ì‹ ë§Œ êµì • ğŸ’» ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python from sklearn.svm import LinearSVC from sklearn.calibration import CalibratedClassifierCV from sklearn.metrics import log_loss, brier_score_loss ê¸°ë³¸ ëª¨ë¸ (í™•ë¥  ì¶œë ¥ ì—†ìŒ) base = LinearSVC() base.fit(X_train, y_train) Platt Scaling ì ìš© calibrated = CalibratedClassifierCV(base, method='sigmoid', cv='prefit') calibrated.fit(X_valid, y_valid) ì„±ëŠ¥ ë¹„êµ raw_scores = base.decision_function(X_test) p_uncalibrated = 1 / (1 + np.exp(-raw_scores)) # ë‹¨ìˆœ sigmoid p_calibrated = calibrated.predict_proba(X_test)[:, 1] # Platt scaling print(f\"Log Loss (ë³´ì • ì „): {log_loss(y_test, p_uncalibrated):.4f}\") print(f\"Log Loss (ë³´ì • í›„): {log_loss(y_test, p_calibrated):.4f}\") ` âœ… ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸ [ ] ê²€ì¦ ë°ì´í„°ë¡œ $A$, $B$ í•™ìŠµ (ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€) [ ] ë³´ì • ì „í›„ ì„±ëŠ¥ ë¹„êµ (Log Loss, Brier Score, ì‹ ë¢°ë„ ê³¡ì„ ) [ ] í´ë˜ìŠ¤ ë¶ˆê· í˜• ì‹¬í•œ ê²½ìš° â†’ íƒ€ê¹ƒ ìŠ¤ë¬´ë”© ê³ ë ¤ [ ] ë©€í‹°í´ë˜ìŠ¤ â†’ one-vs-rest ë˜ëŠ” ë‹¤ë¥¸ ë°©ë²• ê²€í†  [ ] ë°ì´í„° ë¶„í¬ ë³€í™” â†’ ì£¼ê¸°ì  ì¬ë³´ì • > í•µì‹¬: Platt Scalingì€ \"ì ìˆ˜ì˜ ë¡œê·¸-ì˜¤ì¦ˆê°€ ì„ í˜•\"ì´ë¼ëŠ” ê°€ì • í•˜ì— ê²€ì¦ ë°ì´í„°ë¡œ ìµœì ì˜ ë³€í™˜ì„ í•™ìŠµí•˜ëŠ” ê°€ì„±ë¹„ ë†’ì€ í™•ë¥  ë³´ì • ë°©ë²•ì…ë‹ˆë‹¤. Isotonic Regression (method='isotonic') Isotonic Regressionì€ \"ë‹¨ì¡°ì„±\"ë§Œì„ ê°€ì •í•˜ê³  ë°ì´í„°ê°€ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ ë³€í™˜ í˜•íƒœë¥¼ ì°¾ë„ë¡ í•˜ëŠ” ë¹„ëª¨ìˆ˜ì  í™•ë¥  ë³´ì • ë°©ë²•ì…ë‹ˆë‹¤. ğŸ“Œ í•µì‹¬ ê°œë… Isotonic Regressionì€ ì ìˆ˜ $s$ì™€ í™•ë¥  ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ë‹¨ì¡° ì¦ê°€ ì œì•½ë§Œìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ìœ ì—°í•œ ë³´ì •ë²•ì…ë‹ˆë‹¤: $$ \\min_{g:\\, \\text{nondecreasing}} \\sum_{i=1}^n (g(s_i) - y_i)^2 $$ ì—¬ê¸°ì„œ $g$ëŠ” ê³„ë‹¨í˜• ë‹¨ì¡° í•¨ìˆ˜ì´ë©°, ê° ê³„ë‹¨ì˜ ë†’ì´ëŠ” í•´ë‹¹ êµ¬ê°„ ìƒ˜í”Œë“¤ì˜ ì‹¤ì œ ì–‘ì„±ë¥  í‰ê· ì…ë‹ˆë‹¤. ğŸ”„ Platt vs Isotonic ë¹„êµ íŠ¹ì„± Platt Scaling Isotonic Regression ------ --------------- ------------------- ê°€ì • Sì ê³¡ì„  í˜•íƒœ ê³ ì • ë‹¨ì¡°ì„±ë§Œ ê°€ì • ìœ ì—°ì„± ì œí•œì  ë§¤ìš° ë†’ìŒ ë°ì´í„° ìš”êµ¬ëŸ‰ ì ìŒ (ì•ˆì •ì ) ë§ìŒ (ê³¼ì í•© ìœ„í—˜) êµ­ì†Œ ë³´ì • ì „ì—­ì  êµ­ì†Œì  ìš°ìˆ˜ ê²°ê³¼ í˜•íƒœ ë¶€ë“œëŸ¬ìš´ ê³¡ì„  ê³„ë‹¨í˜• í•¨ìˆ˜ ğŸ› ï¸ ì‘ë™ ì›ë¦¬ (PAVA ì•Œê³ ë¦¬ì¦˜) Pool Adjacent Violators Algorithmì„ í†µí•´ íš¨ìœ¨ì ìœ¼ë¡œ í•´ê²°: ì •ë ¬: ì ìˆ˜ë¥¼ ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬ ìœ„ë°˜ ê°ì§€: ì™¼ìª½ ë¸”ë¡ í‰ê·  > ì˜¤ë¥¸ìª½ ë¸”ë¡ í‰ê· ì¸ ê²½ìš° ì°¾ê¸° ë³‘í•©: ìœ„ë°˜í•˜ëŠ” ì¸ì ‘ ë¸”ë¡ë“¤ì„ í•©ì³ì„œ ìƒˆë¡œìš´ í‰ê·  ê³„ì‚° ë°˜ë³µ: ëª¨ë“  ìœ„ë°˜ì´ í•´ê²°ë  ë•Œê¹Œì§€ ë°˜ë³µ ğŸ” ë³‘í•© ê³¼ì • ì˜ˆì‹œ Before ë³‘í•© (ë‹¨ì¡°ì„± ìœ„ë°˜ ìƒí™©): ` ì ìˆ˜ êµ¬ê°„: [0.1-0.2] [0.2-0.3] [0.3-0.4] ê° êµ¬ê°„ í‰ê· : 0.2 0.4 0.3 â† ìœ„ë°˜! 0.4 > 0.3 ` After ë³‘í•© (ë‹¨ì¡°ì„± ë³µêµ¬): ` ì ìˆ˜ êµ¬ê°„: [0.1-0.2] [0.2-0.4] â† ë³‘í•©ë¨ ê° êµ¬ê°„ í‰ê· : 0.2 0.35 â† ìƒˆë¡œìš´ ê°€ì¤‘í‰ê·  ` ê²°ê³¼: ì´ì œ 0.2~0.4 ë²”ìœ„ì˜ ëª¨ë“  ì ìˆ˜ë“¤ì€ ë™ì¼í•˜ê²Œ 0.35ì˜ í™•ë¥ ì„ ë°›ê²Œ ë©ë‹ˆë‹¤. `python ì˜ˆì‹œ: ë³‘í•©ëœ êµ¬ê°„ ë‚´ì˜ ì ìˆ˜ë“¤ scores = [0.25, 0.28, 0.32, 0.38] # ëª¨ë‘ ë³‘í•©ëœ êµ¬ê°„ ì•ˆ predicted_probs = [0.35, 0.35, 0.35, 0.35] # ëª¨ë‘ ê°™ì€ ê°’! ` > ğŸ’¡ ì§ê´€: \"ë†’ì€ ì ìˆ˜ì¼ìˆ˜ë¡ ë†’ì€ í™•ë¥ \"ì´ë¼ëŠ” ìˆœì„œëŠ” ì§€í‚¤ë˜, êµ¬ì²´ì ì¸ ê´€ê³„ëŠ” ë°ì´í„°ê°€ ê²°ì •í•˜ê²Œ í•¨. ë³‘í•©ëœ êµ¬ê°„ì—ì„œëŠ” ì‹¤ì œ ê´€ì¸¡ëœ ì–‘ì„±ë¥  í‰ê· ìœ¼ë¡œ í†µì¼ë©ë‹ˆë‹¤. ğŸ¯ ì™œ \"ë” í™•ë¥ ì \"ì¸ê°€? Isotonic Regressionì´ Plattë³´ë‹¤ \"ë” í™•ë¥ ì \"ì¸ í•µì‹¬ ì´ìœ ëŠ” ì‹¤ì œ ê´€ì¸¡ ë¹ˆë„ë¥¼ ê·¸ëŒ€ë¡œ í™•ë¥ ë¡œ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤: Plattì˜ ì ‘ê·¼ (í˜•íƒœ ìš°ì„ ): `python ë¯¸ë¦¬ ì •í•´ì§„ Sì í˜•íƒœì— ë°ì´í„°ë¥¼ ë§ì¶¤ p = 1 / (1 + exp(-(A*score + B))) \"ëª¨ë“  ì ìˆ˜-í™•ë¥  ê´€ê³„ëŠ” Sìì—¬ì•¼ í•´!\" ` Isotonicì˜ ì ‘ê·¼ (ê´€ì¸¡ ìš°ì„ ): `python ì‹¤ì œ ë°ì´í„°ì—ì„œ ê´€ì°°ëœ íŒ¨í„´ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš© if score in range_1: p = observed_positive_rate_in_range_1 # ì§„ì§œ ê´€ì¸¡ í™•ë¥ ! elif score in range_2: p = observed_positive_rate_in_range_2 # ì§„ì§œ ê´€ì¸¡ í™•ë¥ ! ` ì‹¤ì œ ì˜ˆì‹œ: `python ì–´ë–¤ ì ìˆ˜ êµ¬ê°„ì—ì„œ scores_in_range = [0.3, 0.32, 0.35, 0.38] actual_labels = [0, 1, 1, 1] ì‹¤ì œ ì–‘ì„±ë¥  = 3/4 = 0.75 predicted_probability = 0.75 # â† ì´ê²ƒì´ ë°”ë¡œ ê²½í—˜ì  í™•ë¥ ! ` ì˜ë£Œ ì§„ë‹¨ ì˜ˆì‹œë¡œ ë³´ë©´: Platt: \"ëª¨ë“  ì ìˆ˜-í™•ë¥  ê´€ê³„ëŠ” Sìì—¬ì•¼ í•´!\" â†’ ì‹¤ì œ ê¸‰ê²©í•œ ë³€í™”ë„ Sìë¡œ ìŠ¤ë¬´ë”© Isotonic: \"ì ìˆ˜ 0.7~0.8ëŒ€ì—ì„œ ì‹¤ì œë¡œ 90%ê°€ ì–‘ì„±? ê·¸ëŸ¼ í™•ë¥  0.9!\" â†’ ì‹¤ì œ ê´€ì¸¡ì„ ìˆëŠ” ê·¸ëŒ€ë¡œ ë°˜ì˜ > í•µì‹¬: Isotonicì€ \"ì‹¤ì œ ì„¸ìƒì˜ í™•ë¥  ë¶„í¬\"ë¥¼ ë” ì •í™•í•˜ê²Œ ë°˜ì˜í•˜ë©°, ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ê²½í—˜ì ìœ¼ë¡œ ê°€ì¥ ì •í™•í•œ í™•ë¥  ì¶”ì •ì„ ì œê³µí•©ë‹ˆë‹¤. ğŸ¯ ì–¸ì œ ì‚¬ìš©í•˜ë‚˜? âœ… Isotonicì´ ìœ ë¦¬í•œ ê²½ìš° ì¶©ë¶„í•œ êµì • ë°ì´í„° (ìˆ˜ë°±~ìˆ˜ì²œ ìƒ˜í”Œ) Tree-based ëª¨ë¸ (Random Forest, XGBoost, LightGBM) ë³µì¡í•œ ë¹„ì„ í˜• ì™œê³¡ íŒ¨í„´ êµ­ì†Œì  ë¯¸ìŠ¤ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë¬¸ì œ âŒ Plattì´ ë” ë‚˜ì€ ê²½ìš° ì ì€ êµì • ë°ì´í„° (ì•ˆì •ì„± ì¤‘ìš”) ì „ì—­ì  ê³¼ì‹ /ê³¼ì†Œì‹  íŒ¨í„´ ë¶€ë“œëŸ¬ìš´ í™•ë¥  ê³¡ì„  í•„ìš” ğŸ’» ì‹¤ì œ êµ¬í˜„ ê¸°ë³¸ ì‚¬ìš©ë²• `python from sklearn.calibration import CalibratedClassifierCV from sklearn.ensemble import RandomForestClassifier ìë™ êµì°¨ê²€ì¦ìœ¼ë¡œ ë°ì´í„° ëˆ„ìˆ˜ ë°©ì§€ base_model = RandomForestClassifier(n_estimators=300) calibrated = CalibratedClassifierCV(base_model, method='isotonic', cv=5) calibrated.fit(X, y) ë³´ì •ëœ í™•ë¥  ì˜ˆì¸¡ probabilities = calibrated.predict_proba(X_test)[:, 1] ` ì§ì ‘ ì œì–´ ë°©ë²• `python from sklearn.isotonic import IsotonicRegression ê²€ì¦ ë°ì´í„°ë¡œ ë³´ì •ê¸° í•™ìŠµ iso_regressor = IsotonicRegression( y_min=0.0, y_max=1.0, increasing=True, out_of_bounds='clip' ) iso_regressor.fit(scores_valid, y_valid) í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš© scores_test = base_model.decision_function(X_test) calibrated_probs = iso_regressor.predict(scores_test) ` ğŸ“Š ê¸°ëŒ€ íš¨ê³¼ Isotonic Regression ì ìš© í›„ ê°œì„ ë˜ëŠ” ì§€í‘œë“¤: ë¸Œë¼ì´ì–´ ì ìˆ˜ âœ… ì§ì ‘ ìµœì í™” ëŒ€ìƒ â†’ ê±°ì˜ í•­ìƒ ê°œì„  ë¡œê·¸ ì†ì‹¤ âœ… ë³´ì •ì´ ì˜ ë˜ë©´ ê°œì„  ECE & ì‹ ë¢°ë„ ê³¡ì„  âœ… ë¹ˆë„-í™•ë¥  ì¼ì¹˜ë„ í–¥ìƒ ROC-AUC âš¡ ë‹¨ì¡°ì„±ìœ¼ë¡œ ìˆœìœ„ ë³´ì¡´ â†’ ìœ ì§€ âš–ï¸ ì¥ë‹¨ì  ìš”ì•½ âœ… ì¥ì  ìµœì†Œ ê°€ì •: ë‹¨ì¡°ì„±ë§Œ ìš”êµ¬í•˜ëŠ” ìœ ì—°í•œ ì ‘ê·¼ êµ­ì†Œ ë³´ì •: íŠ¹ì • ì ìˆ˜ êµ¬ê°„ì˜ ì„¸ë°€í•œ êµì • ê°€ëŠ¥ í‘œí˜„ë ¥: ë³µì¡í•œ ë¹„ì„ í˜• íŒ¨í„´ë„ í•™ìŠµ ê°€ëŠ¥ âŒ ë‹¨ì  ë°ì´í„° ì˜ì¡´ì„±: ì¶©ë¶„í•œ ìƒ˜í”Œ ì—†ìœ¼ë©´ ê³¼ì í•© ìœ„í—˜ ê³„ë‹¨í˜• ê²°ê³¼: ì—°ì†ì„±ì´ ë¶€ì¡±í•œ êµ¬ê°„ë³„ ìƒìˆ˜ í•¨ìˆ˜ ì™¸ì‚½ í•œê³„: í•™ìŠµ ë²”ìœ„ ë°–ì—ì„œëŠ” ê²½ê³„ê°’ìœ¼ë¡œ ê³ ì • ğŸ” ì‹¤ë¬´ ì˜ˆì‹œ Tree ëª¨ë¸ì˜ ì „í˜•ì ì¸ ë¬¸ì œ: ê³¼ì‹  ë¬¸ì œ `python Before: Random Forest ì›ì‹œ í™•ë¥  rf_probs = rf.predict_proba(X_test)[:, 1] print(f\"Log Loss (ë³´ì • ì „): {log_loss(y_test, rf_probs):.4f}\") After: Isotonic ë³´ì • cal_rf = CalibratedClassifierCV(rf, method='isotonic', cv=5) cal_rf.fit(X_train, y_train) calibrated_probs = cal_rf.predict_proba(X_test)[:, 1] print(f\"Log Loss (ë³´ì • í›„): {log_loss(y_test, calibrated_probs):.4f}\") `` âœ… ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸ [ ] ë°ì´í„° ë¶„í• : êµì°¨ê²€ì¦ ë˜ëŠ” ë…ë¦½ ê²€ì¦ì…‹ìœ¼ë¡œ ëˆ„ìˆ˜ ë°©ì§€ [ ] ìƒ˜í”Œ ìˆ˜ í™•ì¸: í´ë˜ìŠ¤ë³„ë¡œ ì¶©ë¶„í•œ êµì • ìƒ˜í”Œ (ìˆ˜ë°±ê°œ ì´ìƒ) [ ] ì„±ëŠ¥ ë¹„êµ: ë³´ì • ì „í›„ Brier Score, Log Loss, ECE ì¸¡ì • [ ] ì‹ ë¢°ë„ ê³¡ì„ : ì‹œê°ì ìœ¼ë¡œ êµì • í’ˆì§ˆ í™•ì¸ [ ] í´ë¦¬í•‘ ì„¤ì •: 0/1 ê·¹ê°’ ë°©ì§€ë¡œ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´ [ ] ì •ê¸° ì¬ë³´ì •: ë°ì´í„° ë¶„í¬ ë³€í™” ì‹œ ì¬í•™ìŠµ ê³ ë ¤ ğŸ“ˆ íˆìŠ¤í† ê·¸ë¨ ë¹ˆë‹ê³¼ì˜ ì°¨ì´ ë°©ë²• êµ¬ê°„ ê²°ì • ë‹¨ì¡°ì„± ì ì‘ì„± ------ ----------- -------- -------- íˆìŠ¤í† ê·¸ë¨ ë¹ˆë‹ ë¯¸ë¦¬ ê³ ì • ë³´ì¥ ì•ˆë¨ ë‚®ìŒ Isotonic Regression ë°ì´í„° ê¸°ë°˜ ìë™ ë³´ì¥ë¨ ë†’ìŒ > í•µì‹¬: Isotonic Regressionì€ \"ë‹¨ì¡°ì„±\"ì´ë¼ëŠ” ìµœì†Œí•œì˜ ì œì•½ë§Œìœ¼ë¡œ ë°ì´í„°ê°€ ìŠ¤ìŠ¤ë¡œ ìµœì ì˜ í™•ë¥  ë³€í™˜ì„ ì°¾ê²Œ í•˜ëŠ” ê°•ë ¥í•˜ë©´ì„œë„ ìœ ì—°í•œ ë³´ì • ë°©ë²•ì…ë‹ˆë‹¤. ì¶©ë¶„í•œ ë°ì´í„°ë§Œ ìˆë‹¤ë©´ Platt Scalingë³´ë‹¤ í›¨ì”¬ ì •êµí•œ êµì •ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ë¹„êµë°ë¹„ìœ /08-17-ê¸°ìš´ì°¨ë¦¬ëŠ”ë°ëŠ”ê¿€ì ì´ìµœê³ ",
    "title": "ê¸°ìš´ ì°¨ë¦¬ëŠ” ë°ëŠ” ê¿€ì ì´ ìµœê³ ",
    "date": "2025-08-17",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ë¹„êµ ë° ë¹„ìœ í•  ë•Œ ì“°ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë°°ìš´ í‘œí˜„ë“¤ There is nothing as motivating as a deadline. There is nothing as motivating as a good salary. There is nothing as demotivating as a rejection letter. There is noting as encouring as a supportive family. There is noting as refresing as a goodnight's sleep. He is not only my husband but also my bestfriend. He is not only intelligence but also atheletic. ê°„ë‹¨í•œ ì‘ë¬¸ Today I learned expressions using \"nothing as ~ as \" and \"not only ~ but only\". These are used to explain something. When I express what it is amazing, I can say like that there is nothing as motivating as mindfulness. It is so helpful to me, it makes me more calm and focused. It is not only helpful for healthy mind but also helpful for succesful life. I can make more success when I can focused. Mindfulness make me more focused to kill monkey-mind. And if my mind be more peaceful, I can be more focused. In my opinion, focusness (ëª°ì…) is component of happy. We can be happy when we feel focusness. So mindfulness make us more happy. --- í”¼ë“œë°± ë° ìˆ˜ì •ëœ ë²„ì „ í”¼ë“œë°± ì „ë°˜ì ìœ¼ë¡œ mindfulnessì™€ focusì— ëŒ€í•œ ì¢‹ì€ ìƒê°ë“¤ì„ í‘œí˜„í–ˆìŠµë‹ˆë‹¤! ë¬¸ë²•ê³¼ ì–´íœ˜ ì„ íƒ ë¶€ë¶„ì—ì„œ ëª‡ ê°€ì§€ ê°œì„ ì ì´ ìˆìŠµë‹ˆë‹¤: ì£¼ìš” ê°œì„  í¬ì¸íŠ¸: ë™ì‚¬ ì‹œì œ ì¼ì¹˜: \"makes\", \"is\", \"helps\" ë“± í˜„ì¬í˜• ë™ì‚¬ì˜ ì¼ê´€ì„± ê´€ì‚¬ ì‚¬ìš©: \"a\" ë˜ëŠ” \"the\" ì ì ˆí•œ ì‚¬ìš© ì–´íœ˜ ì„ íƒ: ë” ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ í‘œí˜„ë“¤ ë¬¸ì¥ êµ¬ì¡°: ì¢€ ë” ë§¤ë„ëŸ¬ìš´ ì—°ê²° ìˆ˜ì •ëœ ë²„ì „ Today I learned expressions using \"nothing as ~ as\" and \"not only ~ but also\". These expressions are used to emphasize and compare things. When I want to express something amazing, I can say that there is nothing as motivating as mindfulness. It is so helpful to me - it makes me calmer and more focused. It is not only helpful for a healthy mind but also helpful for a successful life. I can achieve more success when I am focused. Mindfulness helps me focus better and quiet my monkey-mind. When my mind becomes more peaceful, I can concentrate better. In my opinion, focus (ëª°ì…) is a component of happiness. We can be happy when we experience deep focus. So mindfulness makes us happier.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/diner-recommendation",
    "title": "ìŒì‹ì  ì¶”ì²œ ì„œë¹„ìŠ¤ ê°œë°œ",
    "date": "2025-08-17",
    "excerpt": "",
    "category": "Career",
    "tags": [
      "ì´ë ¥ì„œ"
    ],
    "content": "ì •ì˜í–ˆë˜ ë¬¸ì œë“¤, í•´ê²°í•˜ëŠ” ê³¼ì •, í•´ê²°í•œ ê²°ê³¼",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-17-toss-platform",
    "title": "Toss Platform íŒ€ ê´€ë ¨ ì •ë¦¬",
    "date": "2025-08-17",
    "excerpt": "Toss Platformì— ëŒ€í•œ ì¸í„°ë·°, ì±„ìš©ê³µê³  ë“±ì„ ì •ë¦¬",
    "category": "Career",
    "tags": [
      "Toss"
    ],
    "content": "ì¸í„°ë·° ì •ë¦¬ ì¸í„°ë·° ë§í¬ > ddd",
    "public": true
  },
  {
    "id": "Self_Development/Career/25-08-17-how-to-write-resume",
    "title": "ì´ë ¥ì„œë¥¼ ì—…ë°ì´íŠ¸í•˜ë©´ì„œ ì•Œê²Œ ëœ ì´ë ¥ì„œ ì ëŠ” ë²•ë“¤",
    "date": "2025-08-17",
    "excerpt": "ì´ë ¥ì„œëŠ” ì–´ë–»ê²Œ ì ëŠ”ê²Œ ì¢‹ì„ì§€ì— ëŒ€í•œ ìƒê°ë“¤",
    "category": "Career",
    "tags": [
      "ì´ë ¥ì„œ"
    ],
    "content": "ì´ë ¥ì„œì—ëŠ” ë¬¸ì œë¥¼ ì–´ë–»ê²Œ ì •ì˜í–ˆê³ , ì´ë¥¼ ì–´ë–»ê²Œ í•´ê²°í•´ì„œ, ì–´ë–¤ ê²°ê³¼ë¥¼ ì–»ì—ˆëŠ”ì§€ì— ëŒ€í•œ ë…¼ë¦¬ì ì¸ ì‚¬ê³  íë¦„ì´ ë³´ì—¬ì•¼ í•œë‹¤. í† ìŠ¤ ì±„ìš© ì„¤ëª…íšŒì—ì„œë„ ë¹„ìŠ·í•œ ì´ì•¼ê¸°ë¥¼ í–ˆë‹¤. ì–´ë–¤ ê²½í—˜ì„ í–ˆëŠ”ì§€ë³´ë‹¤ í•´ë‹¹ ê²½í—˜ë“¤ì—ì„œ ë…¼ë¦¬ì ì¸ ì‚¬ê³ ê°€ ë³´ì´ê³  ê¹Šì´ìˆëŠ” ê³ ë¯¼ì„ í–ˆì—ˆëŠ”ì§€ê°€ ë” ì¤‘ìš”í•˜ë‹¤ê³ . ìµœê·¼ì— ë°›ì€ ì´ë ¥ì„œ í”¼ë“œë°±ì—ì„œë„ ê·¸ëŸ° ì´ì•¼ê¸°ê°€ ë‚˜ì™”ë‹¤. ë‹¨ìˆœí•œ ì„±ê³¼ë¥¼ ë‚˜ì—´í•˜ëŠ”ê²Œ ì•„ë‹Œ, í•´ë‹¹ ì„±ê³¼ê°€ ì–´ë–¤ ë§¥ë½ì—ì„œ ì§„í–‰ë˜ì—ˆê³  ì–´ë–¤ ê°€ì¹˜ë¥¼ ì§€ë‹ˆëŠ”ì§€ë¥¼ í’ë¶€í•˜ê²Œ ì„¤ëª…í•˜ëŠ”ê²Œ ì¤‘ìš”í•˜ë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/deadlock",
    "title": "Deadlockì— ëŒ€í•œ ì •ë¦¬",
    "date": "2025-08-17",
    "excerpt": "Deadlockì€ ë­ê³  ì™œ ë°œìƒí•˜ëŠ”ê°€? OCR í”„ë¡œì íŠ¸ì—ì„œ ë°œìƒí•œ ë¬¸ì œë„ ê°™ì´ ì •ë¦¬",
    "category": "Data Engineering",
    "tags": [
      "Deadlock"
    ],
    "content": "Deadlockì´ë€? > Deadlock(êµì°©ìƒíƒœ)ì€ ë‘ ê°œ ì´ìƒì˜ í”„ë¡œì„¸ìŠ¤ë‚˜ íŠ¸ëœì­ì…˜ì´ ì„œë¡œê°€ ê°€ì§„ ë¦¬ì†ŒìŠ¤ë¥¼ ê¸°ë‹¤ë¦¬ë©´ì„œ ë¬´í•œì • ëŒ€ê¸°í•˜ëŠ” ìƒíƒœë¥¼ ë§í•©ë‹ˆë‹¤. <figure> <img src=\"/post/DataEngineering/Deadlock.png\" alt=\"Deadlock\" width=\"80%\" /> <figcaption>Deadlock ì˜ˆì‹œ</figcaption> </figure> P1ê³¼ P2ê°€ ë¦¬ì†ŒìŠ¤ A, B ë‘˜ ë‹¤ë¥¼ ì–»ì–´ì•¼ í•œë‹¤ê³  ê°€ì •í•  ë•Œ, t1ì— P1ì´ ë¦¬ì†ŒìŠ¤ë¥¼ Aë¥¼ ì–»ê³  P2ê°€ ë¦¬ì†ŒìŠ¤ Bë¥¼ ì–»ì—ˆë‹¤ë©´ t2ë•Œ P1ì€ ë¦¬ì†ŒìŠ¤ Bë¥¼, P2ëŠ” ë¦¬ì†ŒìŠ¤ Aë¥¼ ê¸°ë‹¤ë¦¬ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì„œë¡œ ì›í•˜ëŠ” ë¦¬ì†ŒìŠ¤ê°€ ìƒëŒ€ë°©ì—ê²Œ í• ë‹¹ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— ì´ ë‘ í”„ë¡œì„¸ìŠ¤ëŠ” ë¬´í•œì • ê¸°ë‹¤ë¦¬ê²Œ ë˜ëŠ”ë° ì´ëŸ¬í•œ ìƒíƒœì„ DeadLockìƒíƒœë¼ê³  í•©ë‹ˆë‹¤. ì¶œì²˜: https://jwprogramming.tistory.com/12 [ê°œë°œìë¥¼ ê¿ˆê¾¸ëŠ” í”„ë¡œê·¸ë˜ë¨¸:í‹°ìŠ¤í† ë¦¬] --- Microsoft Datalocks guide ë§í¬ ë‚´ìš©ì´ ìì„¸í•˜ê²Œ ì„¤ëª…ë˜ì–´ìˆëŠ”ê²Œ ë§ë„¤. ì‹œê°„ë‚  ë•Œ ì½ì–´ë³´ì. A deadlock occurs when two or more tasks permanently block each other by each task having a lock on a resource that the other tasks are trying to lock. For example: Transaction A acquires a shared lock on row 1. Transaction B acquires a shared lock on row 2. Transaction A now requests an exclusive lock on row 2, and is blocked until transaction B finishes and releases the shared lock it has on row 2. Transaction B now requests an exclusive lock on row 1, and is blocked until transaction A finishes and releases the shared lock it has on row 1. The SQL Server Database Engine deadlock monitor periodically checks for tasks that are in a deadlock. If the monitor detects a cyclic dependency, it chooses one of the tasks as a victim and terminates its transaction with an error. OCR í”„ë¡œì íŠ¸ì—ì„œ ë°œìƒí•œ ì—ëŸ¬ ë©”ì„¸ì§€ê°€ ì´ ë¶€ë¶„ì— í•´ë‹¹í•˜ëŠ”êµ¬ë‚˜. This allows the other task to complete its transaction. The application with the transaction that terminated with an error can retry the transaction, which usually completes after the other deadlocked transaction finishes.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ë¹„êµë°ë¹„ìœ /08-15-ê±”ì™„ì „í—ˆì„¸ì•¼",
    "title": "ê±” ì™„ì „ í—ˆì„¸ì•¼",
    "date": "2025-08-15",
    "excerpt": "ê±” ì™„ì „ í—ˆì„¸ì•¼",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ë¹„êµ ë° ë¹„ìœ í•  ë•Œ ì“°ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "He is such a poser ê³µë¶€í•œ ê²ƒë“¤ ì •ë¦¬ so that such It's so sunny that you don't need to wear a jacket outside. He is so stubborn that he won't listen to me. ã…‹ã…‹ I'm so hungry that I could eat a horse. She is such a drama quen. drama quen : ëŠ˜ ì˜¤í•˜ë©´ì„œ ê´€ì‹¬ë°›ê³  ì‹·ã…ì–´í•˜ëŠ” ì‚¬ëŒ night owl : ì €ë…í˜• ì´ê°„ morning person : ì•„ì¹¨í˜• ì¸ê°„ cat person : ê³µì•¼ì´ ì¢‹ì•„í•˜ëŠ” ì‚¬ëŒ oportunitist : ê¸°íšŒì£¼ì˜ a know-it-all : ì•„ëŠ”ì²™ í•˜ëŠ” ì‚¬ëŒ hopeless romantic : ë‹µì—†ëŠ” ë‚­ë§Œì£¼ì˜ì be obssessed with ê°„ë‹¨í•œ ì‘ë¬¸í•˜ê¸° Today I learned representations(í‘œí˜„) using so ~ that and such. It is used to represent the situation or person. If someone ask me about me, I wll answer that I'm such a dog person. Or do you want to eat lunch, I will answer that I'm so hungry that I could eat a horse. So let's summary, when metapor somthing, I can use \"so ~ that\" and \"such\". It is very useful. --- í”¼ë“œë°± (Feedback) Your writing shows a good understanding of the \"so ~ that\" and \"such\" structures! Here are some suggestions for improvement: Grammar corrections: \"representations(í‘œí˜„)\" â†’ \"expressions\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"If someone ask me\" â†’ \"If someone asks me\" (3ì¸ì¹­ ë‹¨ìˆ˜) \"I wll answer\" â†’ \"I will answer\" (ì˜¤íƒ€) \"Or do you want to eat lunch\" â†’ \"Or if you ask do you want to eat lunch\" (ë¬¸ë§¥ìƒ ë” ìì—°ìŠ¤ëŸ¬ì›€) \"let's summary\" â†’ \"let's summarize\" (ë™ì‚¬í˜•) \"metapor somthing\" â†’ \"metaphor something\" (ì² ì) Structure improvements: ë¬¸ì¥ë“¤ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°í•  ìˆ˜ ìˆì–´ìš” ì˜ˆì‹œë¥¼ ë” ë‹¤ì–‘í•˜ê²Œ ë“¤ ìˆ˜ ìˆì–´ìš” ë” ìì„¸í•œ ë²„ì „ (Detailed Version) Understanding \"So ~ That\" and \"Such\" Expressions Today I learned about powerful expressions using \"so ~ that\" and \"such\" that help us emphasize situations or describe people more vividly. How these expressions work: \"So ~ that\" structure: Used with adjectives/adverbs Pattern: so + adjective/adverb + that + result/consequence Example: \"I'm so hungry that I could eat a horse.\" \"Such\" structure: Used with nouns (often with adjectives) Pattern: such + (adjective) + noun Example: \"He's such a poser\" or \"She's such a drama queen.\" Personal examples: When someone asks about my personality, I can confidently say \"I'm such a dog person\" because I absolutely love dogs and prefer their company to cats. This expression immediately conveys my strong preference. If someone invites me to lunch when I'm starving, I might respond, \"Yes! I'm so hungry that I could eat a horse!\" This dramatically emphasizes just how hungry I am. Why these expressions are useful: These metaphorical expressions add color and emphasis to our speech. Instead of simply saying \"I'm hungry,\" saying \"I'm so hungry that I could eat a horse\" creates a vivid image and shows the intensity of the feeling. Similarly, calling someone \"such a drama queen\" is much more expressive than just saying \"she's dramatic.\" Additional examples for practice: \"The movie was so boring that I fell asleep.\" \"He's such a night owl that he never goes to bed before 2 AM.\" \"The weather is so nice that we should go for a walk.\" \"She's such a know-it-all that nobody wants to work with her.\" These expressions help make our English more natural and emotionally expressive, allowing us to communicate not just facts but also our feelings and attitudes.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/what-is-index",
    "title": "Indexì— ëŒ€í•œ ì •ë¦¬",
    "date": "2025-08-14",
    "excerpt": "Indexê°€ ë­ê³ , ì–´ë–¤ ì›ë¦¬ë¡œ ë™ì‘í•˜ëŠ”ì§€ ìì„¸íˆ ì •ë¦¬",
    "category": "Data Engineering",
    "tags": [
      "DB",
      "Index"
    ],
    "content": "ì°¸ê³ ìë£Œ 1: ë¸”ë¡œê·¸ - Database ì¸ë±ìŠ¤(index)ë€? ìœ„ì—ì„œëŠ” ì¸ë±ìŠ¤ì— ëŒ€í•´ ê°„ë‹¨íˆ í•µì‹¬ë§Œ ë‹¤ë£¨ê³  ìˆì§€ë§Œ PK(ê¸°ë³¸í‚¤, Primary Key)ì™€ ì¸ë±ìŠ¤(Index)ì— ëŒ€í•´ì„œ ìì„¸íˆ ì•Œê³  ìˆëŠ” ê²ƒì€ êµ‰ì¥íˆ ì¤‘ìš”í•©ë‹ˆë‹¤. PK(ê¸°ë³¸í‚¤, Primary Key)ì™€ ì¸ë±ìŠ¤(Index)ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ í¬ìŠ¤íŒ…ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”! êµ‰ì¥íˆ ì¤‘ìš”í•œ ë¶€ë¶„ì¸ë§Œí¼ í•œë²ˆ ì¯¤ì€ ì½ì–´ë‘ì‹œëŠ” ê²ƒì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤! What is Index? > ì¸ë±ìŠ¤ë€ ì¶”ê°€ì ì¸ ì“°ê¸° ì‘ì—…ê³¼ ì €ì¥ ê³µê°„ì„ í™œìš©í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ í…Œì´ë¸”ì˜ ê²€ìƒ‰ ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ìë£Œêµ¬ì¡°ì´ë‹¤. (ì°¸ê³  1) <figure> <img src=\"/post/DataEngineering/index.png\" alt=\"What is Index\" /> <figcaption>ê·¸ë¦¼1. Index ì„¤ëª…</figcaption> </figure> ì¸ë±ìŠ¤ê°€ í•„ìš”í•œ ì´ìœ  í…Œì´ë¸” ìŠ¤ìº” vs ì¸ë±ìŠ¤ ìŠ¤ìº” í…Œì´ë¸” ìŠ¤ìº” (Table Scan) `` í…Œì´ë¸” ìŠ¤ìº”ì˜ ë¬¸ì œì : ëª¨ë“  í–‰ì„ ìˆœì°¨ì ìœ¼ë¡œ ê²€ì‚¬ ë°ì´í„°ê°€ ë§ì„ìˆ˜ë¡ ê²€ìƒ‰ ì‹œê°„ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ ì˜ˆ: 100ë§Œ ê±´ í…Œì´ë¸”ì—ì„œ íŠ¹ì • ê°’ì„ ì°¾ì„ ë•Œ â†’ ìµœì•…ì˜ ê²½ìš° 100ë§Œ ê±´ì„ ëª¨ë‘ ê²€ì‚¬í•´ì•¼ í•¨ â†’ í‰ê· ì ìœ¼ë¡œ 50ë§Œ ê±´ ê²€ì‚¬ í•„ìš” ` ì¸ë±ìŠ¤ ìŠ¤ìº” (Index Scan) ` ì¸ë±ìŠ¤ ìŠ¤ìº”ì˜ ì¥ì : ì •ë ¬ëœ ì¸ë±ìŠ¤ êµ¬ì¡°ë¥¼ í†µí•´ ë¹ ë¥¸ ê²€ìƒ‰ B-tree êµ¬ì¡°ë¡œ O(log n) ì‹œê°„ ë³µì¡ë„ ì˜ˆ: 100ë§Œ ê±´ í…Œì´ë¸”ì—ì„œ íŠ¹ì • ê°’ì„ ì°¾ì„ ë•Œ â†’ B-tree ë†’ì´ë§Œí¼ë§Œ ê²€ì‚¬ (ë³´í†µ 3-4íšŒ) â†’ 100ë§Œ ê±´ â†’ 20íšŒ ì´í•˜ì˜ ê²€ì‚¬ë¡œ ë‹¨ì¶• ` ì¸ë±ìŠ¤ì˜ ì¥ë‹¨ì  ì¥ì  ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ: íŠ¹ì • ì¡°ê±´ì˜ ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì°¾ì„ ìˆ˜ ìˆìŒ ì •ë ¬ ì„±ëŠ¥ í–¥ìƒ: ORDER BY ì ˆì˜ ì„±ëŠ¥ ê°œì„  ì¡°ì¸ ì„±ëŠ¥ í–¥ìƒ: í…Œì´ë¸” ê°„ ì—°ê²° ì‹œ ì¸ë±ìŠ¤ í™œìš© ê³ ìœ ì„± ë³´ì¥: UNIQUE ì¸ë±ìŠ¤ë¡œ ë°ì´í„° ë¬´ê²°ì„± ë³´ì¥ ë‹¨ì  ì €ì¥ ê³µê°„ ì¦ê°€: ì¸ë±ìŠ¤ ë°ì´í„°ë¥¼ ìœ„í•œ ì¶”ê°€ ì €ì¥ ê³µê°„ í•„ìš” ì“°ê¸° ì„±ëŠ¥ ì €í•˜: INSERT, UPDATE, DELETE ì‹œ ì¸ë±ìŠ¤ë„ í•¨ê»˜ ìˆ˜ì • ìœ ì§€ë³´ìˆ˜ ì˜¤ë²„í—¤ë“œ: ë°ì´í„° ë³€ê²½ ì‹œ ì¸ë±ìŠ¤ ì¬êµ¬ì„± í•„ìš” ë³µì¡ì„± ì¦ê°€: ì¸ë±ìŠ¤ ì„¤ê³„ì™€ ê´€ë¦¬ì˜ ë³µì¡ì„± ì¸ë±ìŠ¤ ì‚¬ìš© ì‹œë‚˜ë¦¬ì˜¤ ì¸ë±ìŠ¤ê°€ íš¨ê³¼ì ì¸ ê²½ìš° ` WHERE ì ˆì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ ì˜ˆ: user_id, email, created_date JOIN ì¡°ê±´ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ ì˜ˆ: ì™¸ë˜í‚¤, ìì£¼ ì¡°ì¸ë˜ëŠ” ì»¬ëŸ¼ ORDER BY, GROUP BYì—ì„œ ì‚¬ìš©ë˜ëŠ” ì»¬ëŸ¼ ì˜ˆ: ì •ë ¬ì´ë‚˜ ê·¸ë£¹í™” ê¸°ì¤€ì´ ë˜ëŠ” ì»¬ëŸ¼ ê³ ìœ í•œ ê°’ì„ ê°€ì§„ ì»¬ëŸ¼ ì˜ˆ: ì£¼ë¯¼ë“±ë¡ë²ˆí˜¸, ì´ë©”ì¼ ì£¼ì†Œ ë²”ìœ„ ê²€ìƒ‰ì´ ìì£¼ ë°œìƒí•˜ëŠ” ì»¬ëŸ¼ ì˜ˆ: ë‚ ì§œ, ë‚˜ì´, ê°€ê²© ë²”ìœ„ ` ì¸ë±ìŠ¤ê°€ ë¹„íš¨ê³¼ì ì¸ ê²½ìš° ` ë°ì´í„°ê°€ ì ì€ í…Œì´ë¸” (ìˆ˜ë°± ê±´ ì´í•˜) í…Œì´ë¸” ìŠ¤ìº”ì´ ë” ë¹ ë¥¼ ìˆ˜ ìˆìŒ NULL ê°’ì´ ë§ì€ ì»¬ëŸ¼ ì¸ë±ìŠ¤ íš¨ìœ¨ì„±ì´ ë–¨ì–´ì§ ìì£¼ ë³€ê²½ë˜ëŠ” ì»¬ëŸ¼ ì¸ë±ìŠ¤ ìœ ì§€ ë¹„ìš©ì´ ë†’ìŒ ë°ì´í„° ë¶„í¬ê°€ ê· ë“±í•˜ì§€ ì•Šì€ ì»¬ëŸ¼ ì˜ˆ: ì„±ë³„ (ë‚¨/ì—¬ë§Œ ìˆëŠ” ê²½ìš°) í…ìŠ¤íŠ¸ê°€ ê¸´ ì»¬ëŸ¼ ì¸ë±ìŠ¤ í¬ê¸°ê°€ ì»¤ì§€ê³  íš¨ìœ¨ì„± ì €í•˜ ` ì¸ë±ìŠ¤ì˜ ê¸°ë³¸ ì›ë¦¬ ì±…ì˜ ëª©ì°¨ì™€ ê°™ì€ ê°œë… ` ì±…ì—ì„œ íŠ¹ì • ë‚´ìš©ì„ ì°¾ëŠ” ë°©ë²•: ëª©ì°¨(ì¸ë±ìŠ¤)ë¥¼ ë³´ê³  í•´ë‹¹ í˜ì´ì§€ ë²ˆí˜¸ í™•ì¸ í•´ë‹¹ í˜ì´ì§€ë¡œ ë°”ë¡œ ì´ë™ í˜ì´ì§€ ë‚´ì—ì„œ ì„¸ë¶€ ë‚´ìš© ê²€ìƒ‰ í…Œì´ë¸”ì—ì„œ íŠ¹ì • ë°ì´í„°ë¥¼ ì°¾ëŠ” ë°©ë²•: ì¸ë±ìŠ¤ë¥¼ ë³´ê³  í•´ë‹¹ ë°ì´í„°ì˜ ìœ„ì¹˜ í™•ì¸ í•´ë‹¹ ìœ„ì¹˜ë¡œ ë°”ë¡œ ì´ë™ ì •í™•í•œ ë°ì´í„° ì¶”ì¶œ ` ì¸ë±ìŠ¤ì˜ í•µì‹¬ ì•„ì´ë””ì–´ ì •ë ¬ëœ êµ¬ì¡°: ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ì •ë ¬í•˜ì—¬ ë¹ ë¥¸ ê²€ìƒ‰ ê°€ëŠ¥ í¬ì¸í„° í™œìš©: ì‹¤ì œ ë°ì´í„° ìœ„ì¹˜ë¥¼ ê°€ë¦¬í‚¤ëŠ” í¬ì¸í„° ì €ì¥ ê³„ì¸µì  êµ¬ì¡°: B-tree ë“±ì„ í†µí•œ íš¨ìœ¨ì ì¸ ê²€ìƒ‰ ê²½ë¡œ ì œê³µ ì„ íƒì  ì ‘ê·¼: í•„ìš”í•œ ë°ì´í„°ë§Œ ì •í™•í•˜ê²Œ ì ‘ê·¼ Index data structure Hash Table > í•´ì‹œ í…Œì´ë¸”ì€ (Key, Value)ë¡œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ìë£Œêµ¬ì¡° ì¤‘ í•˜ë‚˜ë¡œ ë¹ ë¥¸ ë°ì´í„° ê²€ìƒ‰ (ì‹œê°„ë³µì¡ë„ O(1)) ì´ í•„ìš”í•  ë•Œ ìœ ìš©í•˜ë‹¤ <figure> <img src=\"/post/DataEngineering/hash_table.png\" alt=\"What is Hash Table\" /> <figcaption>ê·¸ë¦¼2. Hash Table ì„¤ëª…</figcaption> </figure> > í•˜ì§€ë§Œ DB ì¸ë±ìŠ¤ì—ì„œ í•´ì‹œ í…Œì´ë¸”ì´ ì‚¬ìš©ë˜ëŠ” ê²½ìš°ëŠ” ì œí•œì ì¸ë°, ê·¸ëŸ¬í•œ ì´ìœ ëŠ” í•´ì‹œê°€ ë“±í˜¸(=) ì—°ì‚°ì—ë§Œ íŠ¹í™”ë˜ì—ˆê¸° ë•Œë¬¸ì´ë‹¤. > í•´ì‹œ í•¨ìˆ˜ëŠ” ê°’ì´ 1ì´ë¼ë„ ë‹¬ë¼ì§€ë©´ ì™„ì „íˆ ë‹¤ë¥¸ í•´ì‹œ ê°’ì„ ìƒì„±í•˜ëŠ”ë°, ì´ëŸ¬í•œ íŠ¹ì„±ì— ì˜í•´ ë¶€ë“±í˜¸ ì—°ì‚°(>, <)ì´ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ì„ ìœ„í•´ì„œëŠ” í•´ì‹œ í…Œì´ë¸”ì´ ì í•©í•˜ì§€ ì•Šë‹¤. Hash Tableì˜ ì œí•œì  ì‚¬ìš© ì´ìœ  í•´ì‹œ í•¨ìˆ˜ì˜ íŠ¹ì„± ` í•´ì‹œ í•¨ìˆ˜ì˜ ê²°ì •ì  íŠ¹ì„±: ì…ë ¥ê°’ì´ 1ì´ë¼ë„ ë‹¬ë¼ì§€ë©´ ì™„ì „íˆ ë‹¤ë¥¸ í•´ì‹œê°’ ìƒì„± ì˜ˆì‹œ: \"user123\" â†’ í•´ì‹œ í•¨ìˆ˜ â†’ 8472 \"user124\" â†’ í•´ì‹œ í•¨ìˆ˜ â†’ 1956 \"user125\" â†’ í•´ì‹œ í•¨ìˆ˜ â†’ 3921 ì´ íŠ¹ì„±ì˜ ë¬¸ì œì : ì—°ì†ëœ ê°’ë“¤ì´ ì „í˜€ ë‹¤ë¥¸ í•´ì‹œê°’ì„ ê°€ì§ ìˆœì„œë‚˜ í¬ê¸° ê´€ê³„ê°€ í•´ì‹œê°’ì— ë°˜ì˜ë˜ì§€ ì•ŠìŒ ë²”ìœ„ ê²€ìƒ‰ì´ë‚˜ ì •ë ¬ì´ ë¶ˆê°€ëŠ¥ ` ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ íŒ¨í„´ê³¼ì˜ ë¶ˆì¼ì¹˜ ` ì¼ë°˜ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰: ì •í™•í•œ ê°’ ê²€ìƒ‰: WHERE user_id = 'user123' âœ“ (Hash Table ì í•©) ë²”ìœ„ ê²€ìƒ‰: WHERE age BETWEEN 20 AND 30 âœ— (Hash Table ë¶€ì í•©) ì •ë ¬: ORDER BY created_date âœ— (Hash Table ë¶€ì í•©) ë¶€ë¶„ ê²€ìƒ‰: WHERE name LIKE 'John%' âœ— (Hash Table ë¶€ì í•©) ìµœëŒ€/ìµœì†Œê°’: WHERE price > 1000 âœ— (Hash Table ë¶€ì í•©) Hash Tableì´ ì§€ì›í•˜ëŠ” ê²€ìƒ‰: ë“±í˜¸(=) ì—°ì‚°ë§Œ ì§€ì› ì •í™•í•œ í‚¤ ê°’ìœ¼ë¡œì˜ ì§ì ‘ ì ‘ê·¼ ìˆœì„œë‚˜ ê´€ê³„ ì •ë³´ ì—†ìŒ ` ì‹¤ì œ ì‚¬ìš© ì‚¬ë¡€ì˜ ì œí•œ ` Hash Tableì´ íš¨ê³¼ì ì¸ ê²½ìš°: ì‚¬ìš©ì ë¡œê·¸ì¸: WHERE email = 'user@example.com' ì„¸ì…˜ ì¡°íšŒ: WHERE session_id = 'abc123' ìºì‹œ ì¡°íšŒ: WHERE cache_key = 'user_profile_123' Hash Tableì´ ë¹„íš¨ê³¼ì ì¸ ê²½ìš°: ë‚˜ì´ëŒ€ë³„ ì‚¬ìš©ì: WHERE age BETWEEN 20 AND 30 ê°€ê²©ëŒ€ë³„ ìƒí’ˆ: WHERE price > 1000 AND price < 5000 ë‚ ì§œë³„ ì£¼ë¬¸: WHERE order_date >= '2024-01-01' ì´ë¦„ìˆœ ì •ë ¬: ORDER BY name ASC ` B-tree vs Hash Tableì˜ ê²€ìƒ‰ ëŠ¥ë ¥ ë¹„êµ ë“±í˜¸ ê²€ìƒ‰ (=) ` Hash Table: ì„±ëŠ¥: O(1) - í•´ì‹œê°’ ê³„ì‚° í›„ ì§ì ‘ ì ‘ê·¼ ì˜ˆì‹œ: WHERE user_id = 'user123' ê²°ê³¼: ë§¤ìš° ë¹ ë¦„ B-tree: ì„±ëŠ¥: O(log n) - íŠ¸ë¦¬ íƒìƒ‰ í•„ìš” ì˜ˆì‹œ: WHERE user_id = 'user123' ê²°ê³¼: ë¹ ë¦„ (í•˜ì§€ë§Œ Hash Tableë³´ë‹¤ëŠ” ëŠë¦¼) ` ë²”ìœ„ ê²€ìƒ‰ (>, <, BETWEEN) ` Hash Table: ì„±ëŠ¥: ì§€ì›í•˜ì§€ ì•ŠìŒ ì˜ˆì‹œ: WHERE age BETWEEN 20 AND 30 ê²°ê³¼: ë¶ˆê°€ëŠ¥ B-tree: ì„±ëŠ¥: O(log n + k) - këŠ” ê²°ê³¼ ê°œìˆ˜ ì˜ˆì‹œ: WHERE age BETWEEN 20 AND 30 ê²°ê³¼: íš¨ìœ¨ì  (ìˆœì„œëŒ€ë¡œ ì ‘ê·¼ ê°€ëŠ¥) ` ì •ë ¬ (ORDER BY) ` Hash Table: ì„±ëŠ¥: ì§€ì›í•˜ì§€ ì•ŠìŒ ì˜ˆì‹œ: ORDER BY created_date DESC ê²°ê³¼: ë¶ˆê°€ëŠ¥ B-tree: ì„±ëŠ¥: O(n log n) - ì •ë ¬ëœ ìˆœì„œë¡œ ì ‘ê·¼ ì˜ˆì‹œ: ORDER BY created_date DESC ê²°ê³¼: íš¨ìœ¨ì  (ì´ë¯¸ ì •ë ¬ëœ êµ¬ì¡°) ` Hash Tableì˜ ì ì ˆí•œ í™œìš© ë°©ì•ˆ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ ` ìµœì ì˜ ì¸ë±ìŠ¤ ì „ëµ: ìì£¼ ì‚¬ìš©ë˜ëŠ” ì •í™•í•œ ê°’ ê²€ìƒ‰: Hash Index ì˜ˆ: ì‚¬ìš©ì ID, ì´ë©”ì¼, ì„¸ì…˜ ID ë²”ìœ„ ê²€ìƒ‰ì´ë‚˜ ì •ë ¬ì´ í•„ìš”í•œ ê²½ìš°: B-tree Index ì˜ˆ: ë‚ ì§œ, ë‚˜ì´, ê°€ê²©, ì´ë¦„ ë³µí•© ì¸ë±ìŠ¤: B-tree í™œìš© ì˜ˆ: (category, price, created_date) ` ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œì˜ í™œìš© ` Redis, Memcached ë“±ì—ì„œ Hash Table í™œìš©: ë¹ ë¥¸ í‚¤-ê°’ ì¡°íšŒ ì„¸ì…˜ ì •ë³´ ì €ì¥ ìºì‹œ ë°ì´í„° ê´€ë¦¬ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ ì¥ì : ë©”ëª¨ë¦¬ ì ‘ê·¼ìœ¼ë¡œ ê·¹ë„ë¡œ ë¹ ë¥¸ ì†ë„ ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ë†’ì€ ì²˜ë¦¬ëŸ‰ í™•ì¥ ê°€ëŠ¥í•œ ë¶„ì‚° ì‹œìŠ¤í…œ ` ê²°ë¡  Hash Tableì€ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ì—ì„œ ì œí•œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì´ìœ ê°€ ëª…í™•í•©ë‹ˆë‹¤: ë“±í˜¸ ì—°ì‚°ì—ë§Œ íŠ¹í™”: ì •í™•í•œ ê°’ ê²€ìƒ‰ë§Œ ê°€ëŠ¥ ìˆœì„œ ì •ë³´ ë¶€ì¬: ë²”ìœ„ ê²€ìƒ‰ì´ë‚˜ ì •ë ¬ ë¶ˆê°€ í•´ì‹œ í•¨ìˆ˜ì˜ íŠ¹ì„±: ì—°ì†ëœ ê°’ë“¤ì´ ì „í˜€ ë‹¤ë¥¸ í•´ì‹œê°’ ìƒì„± ì‹¤ìš©ì„± ë¶€ì¡±: ëŒ€ë¶€ë¶„ì˜ ë°ì´í„°ë² ì´ìŠ¤ ê²€ìƒ‰ íŒ¨í„´ê³¼ ë§ì§€ ì•ŠìŒ ë”°ë¼ì„œ Hash Tableì€ íŠ¹ì • ìš©ë„(ë¹ ë¥¸ í‚¤-ê°’ ì¡°íšŒ, ë©”ëª¨ë¦¬ ê¸°ë°˜ ì‹œìŠ¤í…œ)ì—ë§Œ ì‚¬ìš©í•˜ê³ , ì¼ë°˜ì ì¸ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ë¡œëŠ” B-tree êµ¬ì¡°ê°€ í›¨ì”¬ ë” ì í•©í•©ë‹ˆë‹¤. Hash Tableì˜ ì‘ë™ ì›ë¦¬ í•´ì‹œ í•¨ìˆ˜ (Hash Function) ` í•´ì‹œ í•¨ìˆ˜ì˜ ì—­í• : ì…ë ¥ëœ í‚¤ë¥¼ ê³ ì •ëœ í¬ê¸°ì˜ í•´ì‹œê°’ìœ¼ë¡œ ë³€í™˜ ì˜ˆ: \"user123\" â†’ í•´ì‹œ í•¨ìˆ˜ â†’ 8472 ë™ì¼í•œ í‚¤ëŠ” í•­ìƒ ë™ì¼í•œ í•´ì‹œê°’ ìƒì„± ì„œë¡œ ë‹¤ë¥¸ í‚¤ê°€ ê°™ì€ í•´ì‹œê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ (í•´ì‹œ ì¶©ëŒ) í•´ì‹œ í•¨ìˆ˜ì˜ íŠ¹ì§•: ê²°ì •ì  (Deterministic): ê°™ì€ ì…ë ¥ì— ëŒ€í•´ í•­ìƒ ê°™ì€ ì¶œë ¥ ê· ë“± ë¶„í¬ (Uniform Distribution): í•´ì‹œê°’ì´ ê· ë“±í•˜ê²Œ ë¶„í¬ ë¹ ë¥¸ ê³„ì‚°: O(1) ì‹œê°„ì— í•´ì‹œê°’ ê³„ì‚° ` í•´ì‹œ ì¶©ëŒ (Hash Collision) ` í•´ì‹œ ì¶©ëŒì´ë€: ì„œë¡œ ë‹¤ë¥¸ í‚¤ê°€ ê°™ì€ í•´ì‹œê°’ì„ ê°€ì§€ëŠ” ê²½ìš° ì˜ˆ: \"user123\" â†’ 8472, \"admin456\" â†’ 8472 í•´ê²° ë°©ë²•: ì²´ì´ë‹ (Chaining): ê°™ì€ í•´ì‹œê°’ì— ì—°ê²° ë¦¬ìŠ¤íŠ¸ ì‚¬ìš© ê°œë°© ì£¼ì†Œë²• (Open Addressing): ë‹¤ë¥¸ ë¹ˆ ìŠ¬ë¡¯ ì°¾ê¸° ì´ì¤‘ í•´ì‹± (Double Hashing): ë‘ ë²ˆì§¸ í•´ì‹œ í•¨ìˆ˜ ì‚¬ìš© ` Hash Tableì˜ ì„±ëŠ¥ íŠ¹ì„± ì‹œê°„ ë³µì¡ë„ ` ê²€ìƒ‰ (Search): í‰ê· : O(1) - í•´ì‹œê°’ ê³„ì‚° í›„ ì§ì ‘ ì ‘ê·¼ ìµœì•…: O(n) - í•´ì‹œ ì¶©ëŒì´ ë§ì„ ê²½ìš° ì‚½ì… (Insert): í‰ê· : O(1) - í•´ì‹œê°’ ê³„ì‚° í›„ ì§ì ‘ ì €ì¥ ìµœì•…: O(n) - í•´ì‹œ ì¶©ëŒ í•´ê²° ê³¼ì • ì‚­ì œ (Delete): í‰ê· : O(1) - í•´ì‹œê°’ ê³„ì‚° í›„ ì§ì ‘ ì œê±° ìµœì•…: O(n) - í•´ì‹œ ì¶©ëŒ í•´ê²° ê³¼ì • ` ê³µê°„ ë³µì¡ë„ ` ì €ì¥ ê³µê°„: ê¸°ë³¸: O(n) - ë°ì´í„° ê°œìˆ˜ë§Œí¼ ê³µê°„ í•„ìš” í•´ì‹œ ì¶©ëŒ í•´ê²°: ì¶”ê°€ ê³µê°„ í•„ìš” ì²´ì´ë‹: ì—°ê²° ë¦¬ìŠ¤íŠ¸ ë…¸ë“œ ê³µê°„ ê°œë°© ì£¼ì†Œë²•: ë¹ˆ ìŠ¬ë¡¯ì„ ìœ„í•œ ì—¬ìœ  ê³µê°„ ` Hash Tableì˜ ì¥ë‹¨ì  ì¥ì  ë¹ ë¥¸ ê²€ìƒ‰: í‰ê·  O(1) ì‹œê°„ì— ë°ì´í„° ì ‘ê·¼ ë¹ ë¥¸ ì‚½ì…/ì‚­ì œ: í•´ì‹œê°’ ê³„ì‚° í›„ ì§ì ‘ ì¡°ì‘ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±: ë°ì´í„° í¬ê¸°ì— ë¹„ë¡€í•˜ëŠ” ê³µê°„ ì‚¬ìš© ë‹¨ìˆœí•œ êµ¬ì¡°: êµ¬í˜„ì´ ë¹„êµì  ê°„ë‹¨ ë‹¨ì  í•´ì‹œ ì¶©ëŒ: ì„±ëŠ¥ ì €í•˜ì˜ ì£¼ìš” ì›ì¸ ìˆœì°¨ ì ‘ê·¼ ë¹„íš¨ìœ¨: ë²”ìœ„ ê²€ìƒ‰ì´ë‚˜ ì •ë ¬ì— ë¶€ì í•© ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: í•´ì‹œ ì¶©ëŒ í•´ê²°ì„ ìœ„í•œ ì¶”ê°€ ê³µê°„ í•´ì‹œ í•¨ìˆ˜ ì˜ì¡´ì„±: ì¢‹ì€ í•´ì‹œ í•¨ìˆ˜ ì„ íƒì´ ì¤‘ìš” Hash Tableì˜ ì‹¤ì œ êµ¬í˜„ ì²´ì´ë‹ ë°©ì‹ ` êµ¬ì¡°: Bucket[0]: [Key1, Value1] â†’ [Key2, Value2] â†’ null Bucket[1]: [Key3, Value3] â†’ null Bucket[2]: null ... ì¥ì : êµ¬í˜„ì´ ê°„ë‹¨ í•´ì‹œ ì¶©ëŒì— ê°•í•¨ ë™ì  í™•ì¥ ê°€ëŠ¥ ë‹¨ì : ì¶”ê°€ ë©”ëª¨ë¦¬ ì‚¬ìš© ìºì‹œ ì§€ì—­ì„± ë–¨ì–´ì§ ` ê°œë°© ì£¼ì†Œë²• ` êµ¬ì¡°: Bucket[0]: [Key1, Value1] Bucket[1]: [Key2, Value2] (ì¶©ëŒë¡œ ì¸í•œ ì´ë™) Bucket[2]: [Key3, Value3] (ì¶©ëŒë¡œ ì¸í•œ ì´ë™) ì¥ì : ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìºì‹œ ì§€ì—­ì„± ì¢‹ìŒ ì¶”ê°€ ë©”ëª¨ë¦¬ ì‚¬ìš© ìµœì†Œ ë‹¨ì : êµ¬í˜„ì´ ë³µì¡ ì‚­ì œ ì‹œ ì²˜ë¦¬ ë³µì¡ í´ëŸ¬ìŠ¤í„°ë§ ë¬¸ì œ ` Hash Tableì˜ ì¸ë±ìŠ¤ í™œìš© ì í•©í•œ ì‚¬ìš© ì‚¬ë¡€ ` ì •í™•í•œ ê°’ ê²€ìƒ‰ (Point Query): ì˜ˆ: SELECT FROM users WHERE user_id = 'user123' í•´ì‹œê°’ìœ¼ë¡œ ì§ì ‘ ì ‘ê·¼í•˜ì—¬ O(1) ì„±ëŠ¥ ê³ ìœ  í‚¤ ê²€ìƒ‰: ì˜ˆ: ì‚¬ìš©ì ID, ì´ë©”ì¼ ì£¼ì†Œ ì¤‘ë³µì´ ì—†ëŠ” ê³ ìœ í•œ ê°’ ë©”ëª¨ë¦¬ ê¸°ë°˜ ì¸ë±ìŠ¤: Redis, Memcached ë“± ë¹ ë¥¸ ì‘ë‹µ ì‹œê°„ì´ ì¤‘ìš”í•œ ê²½ìš° ` ë¶€ì í•©í•œ ì‚¬ìš© ì‚¬ë¡€ ` ë²”ìœ„ ê²€ìƒ‰ (Range Query): ì˜ˆ: SELECT FROM users WHERE age BETWEEN 20 AND 30 í•´ì‹œê°’ì€ ìˆœì„œê°€ ì—†ì–´ ë²”ìœ„ ê²€ìƒ‰ ë¶ˆê°€ ì •ë ¬ì´ í•„ìš”í•œ ê²½ìš°: ì˜ˆ: ORDER BY created_date í•´ì‹œê°’ì€ ì •ë ¬ ìˆœì„œì™€ ë¬´ê´€ ë¶€ë¶„ ë¬¸ìì—´ ê²€ìƒ‰: ì˜ˆ: LIKE '%keyword%' í•´ì‹œê°’ì€ íŒ¨í„´ ë§¤ì¹­ì— ë¶€ì í•© ` Hash Table vs B-tree ë¹„êµ ì„±ëŠ¥ ë¹„êµ ` ê²€ìƒ‰ ì„±ëŠ¥: Hash Table: í‰ê·  O(1), ìµœì•… O(n) B-tree: í•­ìƒ O(log n) ë²”ìœ„ ê²€ìƒ‰: Hash Table: ì§€ì›í•˜ì§€ ì•ŠìŒ B-tree: O(log n + k) (këŠ” ê²°ê³¼ ê°œìˆ˜) ì •ë ¬: Hash Table: ì§€ì›í•˜ì§€ ì•ŠìŒ B-tree: ìˆœì„œëŒ€ë¡œ ì ‘ê·¼ ê°€ëŠ¥ ` ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ` ê³µê°„ íš¨ìœ¨ì„±: Hash Table: O(n) + ì¶©ëŒ í•´ê²° ê³µê°„ B-tree: O(n) + ë…¸ë“œ ì˜¤ë²„í—¤ë“œ í™•ì¥ì„±: Hash Table: ì¬í•´ì‹± í•„ìš” B-tree: ì ì§„ì  í™•ì¥ ê°€ëŠ¥ ` B+Tree <figure> <img src=\"/post/DataEngineering/InnoDB-B-Tree-Structure.png\" alt=\"InnoDBì˜ B+Tree êµ¬ì¡°\" /> <figcaption>ê·¸ë¦¼3. InnoDBì˜ B+Tree êµ¬ì¡°</figcaption> </figure> B+Treeì˜ ê°œë…ê³¼ íŠ¹ì§• B+Treeë€? B+TreeëŠ” B-Treeë¥¼ ê°œì„ í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ì— ìµœì í™”ëœ ìë£Œêµ¬ì¡°ì…ë‹ˆë‹¤. 1970ë…„ëŒ€ì— B-Treeì˜ ë‹¨ì ì„ ë³´ì™„í•˜ì—¬ ê°œë°œë˜ì—ˆìŠµë‹ˆë‹¤. B+Treeì˜ í•µì‹¬ êµ¬ì¡° ë°ì´í„° ì €ì¥ ë°©ì‹ ` B+Treeì˜ ë…¸ë“œ êµ¬ì¡°: Internal ë…¸ë“œ: í‚¤(Key)ì™€ í¬ì¸í„°ë§Œ ì €ì¥ [Key1, Pointer1, Key2, Pointer2, ...] ì‹¤ì œ ë°ì´í„°ëŠ” ì €ì¥í•˜ì§€ ì•ŠìŒ (ê²€ìƒ‰ ê²½ë¡œ ì œê³µ) Leaf ë…¸ë“œ: í‚¤(Key)ì™€ ë°ì´í„°(Value) ì €ì¥ [Key1, Value1, Key2, Value2, ...] ì‹¤ì œ ë°ì´í„°ê°€ ì €ì¥ë˜ëŠ” ê³³ ëª¨ë“  Leaf ë…¸ë“œëŠ” ë…¼ë¦¬ì  ìˆœì„œëŒ€ë¡œ ì—°ê²°ë˜ì–´ ë²”ìœ„/ìˆœì°¨ ì ‘ê·¼ì— ìœ ë¦¬ ` ë©”ëª¨ë¦¬/í˜ì´ì§€ ê´€ì ì˜ êµ¬ì¡°ì  ì´ì  ` Internal ë…¸ë“œê°€ ì‘ì•„ í•œ í˜ì´ì§€ì— ë” ë§ì€ í‚¤ë¥¼ ì ì¬ â†’ íŠ¸ë¦¬ ë†’ì´ ê°ì†Œ Leaf ë…¸ë“œëŠ” ë°ì´í„° ë°€ë„ê°€ ë†’ì•„ I/O íš¨ìœ¨ ì¦ê°€ í˜ì´ì§€ ë‹¨ìœ„(prefetch)ë¡œ ì½ì„ ë•Œ ìˆœì°¨ ì ‘ê·¼ ì´ì  í¼ ` B+Treeì˜ ê²€ìƒ‰ ê³¼ì • ì •í™•í•œ ê°’ ê²€ìƒ‰ (Point Query) ` 1) Root â†’ Internal ë…¸ë“œë“¤ì„ ë”°ë¼ í•˜í–¥ íƒìƒ‰ 2) ëª©ì  í‚¤ ë²”ìœ„ì˜ í¬ì¸í„°ë¥¼ ì„ íƒí•˜ë©° Leaf ë„ë‹¬ 3) Leafì—ì„œ í‚¤ë¥¼ ì°¾ì•„ ë°ì´í„° ë°˜í™˜ ì‹œê°„ ë³µì¡ë„: O(log n) ` ë²”ìœ„ ê²€ìƒ‰ (Range Query) ` 1) ì‹œì‘ í‚¤ë¥¼ ì°¾ê¸° ìœ„í•´ íŠ¸ë¦¬ íƒìƒ‰ (O(log n)) 2) í•´ë‹¹ Leafì—ì„œ ì‹œì‘í•˜ì—¬ ë‹¤ìŒ Leafë¡œ ìˆœì°¨ ì ‘ê·¼ 3) ë ì¡°ê±´ê¹Œì§€ ê²°ê³¼ ìˆ˜ì§‘ (O(k)) ì´ ì‹œê°„ ë³µì¡ë„: O(log n + k) ` B+Treeì˜ ì¥ë‹¨ì  ì¥ì  ` ë²”ìœ„/ìˆœì°¨ ì ‘ê·¼ ìµœì í™” (Leaf ê°„ ë…¼ë¦¬ì  ì—°ê²°) Internal ë…¸ë“œê°€ ì‘ì•„ íŠ¸ë¦¬ ë†’ì´ê°€ ë‚®ìŒ â†’ I/O ê°ì†Œ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ê²€ìƒ‰ ì„±ëŠ¥: O(log n) ` ë‹¨ì  ` í•­ìƒ Leafê¹Œì§€ ë‚´ë ¤ê°€ì•¼ ë°ì´í„° ì ‘ê·¼ ê°€ëŠ¥ ë…¸ë“œ ë¶„í• /ë³‘í•© ì‹œ ì—°ê²° ìœ ì§€ ê´€ë¦¬ ë¹„ìš© ì¡´ì¬ ` B+Treeì˜ ì‹¤ì œ í™œìš© ` MySQL InnoDB: í´ëŸ¬ìŠ¤í„°í˜• ì¸ë±ìŠ¤(Primary Key)ì™€ ë³´ì¡° ì¸ë±ìŠ¤(Secondary) ëª¨ë‘ B+Tree ê¸°ë°˜ PostgreSQL/Oracle: ê¸°ë³¸ ë²”ìš© ì¸ë±ìŠ¤ë¡œ B+Tree ì±„íƒ, ë³µí•© ì¸ë±ìŠ¤/ë²”ìœ„ ì¿¼ë¦¬ì— ìµœì  ` B+Treeì˜ ì„±ëŠ¥ íŠ¹ì„± ìš”ì•½ ` ì •í™•í•œ ê°’ ê²€ìƒ‰: O(log n) ë²”ìœ„ ê²€ìƒ‰: O(log n + k) ìˆœì°¨ ì ‘ê·¼: ì²« ì›ì†Œ O(log n) + í›„ì† O(1) (í˜ì´ì§€ ìˆœì°¨ ì ‘ê·¼) ì •ë ¬: ì¸ë±ìŠ¤ ìˆœì„œ ì ‘ê·¼ìœ¼ë¡œ ì¶”ê°€ ì •ë ¬ ìµœì†Œí™” `` ê²°ë¡  B+TreeëŠ” Internal/Leaf ë¶„ë¦¬ì™€ Leaf ê°„ ë…¼ë¦¬ì  ì—°ê²°ë¡œ ë²”ìœ„/ìˆœì°¨ ì ‘ê·¼ì— ë§¤ìš° ê°•í•˜ë©°, ë‚®ì€ íŠ¸ë¦¬ ë†’ì´ë¥¼ í†µí•´ ì¼ê´€ëœ O(log n) ì„±ëŠ¥ì„ ì œê³µí•˜ê¸° ë•Œë¬¸ì— í˜„ëŒ€ ë°ì´í„°ë² ì´ìŠ¤ì˜ í‘œì¤€ ì¸ë±ìŠ¤ êµ¬ì¡°ë¡œ ë„ë¦¬ ì“°ì…ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/what-is-B-Tree",
    "title": "B-Treeì— ëŒ€í•œ ì •ë¦¬",
    "date": "2025-08-14",
    "excerpt": "B-Treeê°€ ë­ê³ , ì–´ë–¤ ì›ë¦¬ë¡œ ë™ì‘í•˜ëŠ”ì§€ ìì„¸íˆ ì •ë¦¬",
    "category": "Data Engineering",
    "tags": [
      "DB",
      "SQL",
      "B-Tree"
    ],
    "content": "ì°¸ê³ ìë£Œ 1 : MySQL - B-Treeë¡œ ì¸ë±ìŠ¤(Index)ì— ëŒ€í•´ ì‰½ê³  ì™„ë²½í•˜ê²Œ ì´í•´í•˜ê¸° ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” ë°©ì‹ > ì¸ë±ìŠ¤ë¥¼ ì €ì¥í•˜ëŠ” ë°©ì‹(ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜)ì— ë”°ë¼ B-Tree ì¸ë±ìŠ¤, Hash ì¸ë±ìŠ¤, Fractal ì¸ë±ìŠ¤ ë“±ìœ¼ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ì¼ë°˜ì ìœ¼ë¡œ B-Tree êµ¬ì¡°ê°€ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì— B-Tree ì¸ë±ìŠ¤ë¥¼ í†µí•´ ì¸ë±ìŠ¤ì˜ ë™ì‘ ë°©ì‹ì— ëŒ€í•´ ì‚´í´ë³´ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. (ì°¸ê³  1) B-Tree êµ¬ì¡° <figure> <img src=\"/post/DataEngineering/B-Tree-Structure.png\" alt=\"B-Tree Structure\" /> <figcaption>ê·¸ë¦¼1. B-Tree Structure</figcaption> </figure> B-Treeêµ¬ì¡°ëŠ” ì´ì§„íŠ¸ë¦¬ë¥¼ í™•ì¥í•˜ì—¬ Nê°œì˜ ìì‹ì„ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ ê³ ì•ˆëœ ê²ƒ. Page or Block Page or Block : ë””ìŠ¤í¬ì™€ ë©”ëª¨ë¦¬(ë²„í¼í’€)ì— ë°ì´í„°ë¥¼ ì½ê³  ì“°ëŠ” ìµœì†Œ ì‘ì—… ë‹¨ìœ„ > ì¸ë±ìŠ¤ì˜ ì €ì¥ ë°©ì‹ì„ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” í˜ì´ì§€(ë˜ëŠ” ë¸”ëŸ­)ì— ëŒ€í•´ ì•Œì•„ì•¼ í•œë‹¤. > í˜ì´ì§€ë€ ë””ìŠ¤í¬ì™€ ë©”ëª¨ë¦¬(ë²„í¼í’€)ì— ë°ì´í„°ë¥¼ ì½ê³  ì“°ëŠ” ìµœì†Œ ì‘ì—… ë‹¨ìœ„ì´ë‹¤. ì¼ë°˜ì ì¸ ì¸ë±ìŠ¤ë¥¼ í¬í•¨í•´ PK(í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤)ì™€ í…Œì´ë¸” ë“±ì€ ëª¨ë‘ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬ëœë‹¤. > ë”°ë¼ì„œ ë§Œì•½ ì¿¼ë¦¬ë¥¼ í†µí•´ 1ê°œì˜ ë ˆì½”ë“œë¥¼ ì½ê³  ì‹¶ë”ë¼ë„ ê²°êµ­ì€ í•˜ë‚˜ì˜ ë¸”ë¡ì„ ì½ì–´ì•¼ í•˜ëŠ” ê²ƒì´ë‹¤ (ì°¸ê³  1) ê¸°ë³¸ ì •ì˜ í˜ì´ì§€/ë¸”ë¡: ë””ìŠ¤í¬ì™€ ë©”ëª¨ë¦¬ ê°„ ë°ì´í„° ì „ì†¡ì˜ ìµœì†Œ ë‹¨ìœ„ í¬ê¸°: ì¼ë°˜ì ìœ¼ë¡œ 4KB, 8KB, 16KB ë“±ìœ¼ë¡œ í‘œì¤€í™” ëª©ì : I/O íš¨ìœ¨ì„±ê³¼ ë©”ëª¨ë¦¬ ê´€ë¦¬ë¥¼ ìœ„í•œ ë‹¨ìœ„í™” ì™œ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ê´€ë¦¬í•˜ëŠ”ê°€? ë””ìŠ¤í¬ I/O íš¨ìœ¨ì„± ë””ìŠ¤í¬ íŠ¹ì„±: ë””ìŠ¤í¬ëŠ” ìˆœì°¨ ì ‘ê·¼ì— ìµœì í™”ë˜ì–´ ìˆìŒ Seek Time: ë””ìŠ¤í¬ í—¤ë“œê°€ íŠ¹ì • ìœ„ì¹˜ë¡œ ì´ë™í•˜ëŠ” ì‹œê°„ Transfer Time: ì‹¤ì œ ë°ì´í„°ë¥¼ ì½ê³  ì“°ëŠ” ì‹œê°„ í˜ì´ì§€ ë‹¨ìœ„: í•œ ë²ˆì˜ ë””ìŠ¤í¬ ì ‘ê·¼ìœ¼ë¡œ ì—¬ëŸ¬ ë ˆì½”ë“œ ì²˜ë¦¬ ë©”ëª¨ë¦¬ ê´€ë¦¬ íš¨ìœ¨ì„± ë²„í¼ í’€: ìì£¼ ì‚¬ìš©ë˜ëŠ” í˜ì´ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ LRU ì•Œê³ ë¦¬ì¦˜: ìµœê·¼ ì‚¬ìš©ëœ í˜ì´ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ í˜ì´ì§€ êµì²´: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì‚¬ìš© ë¹ˆë„ê°€ ë‚®ì€ í˜ì´ì§€ ì œê±° ë²„í¼í’€(Buffer Pool)ê³¼ ë©”ëª¨ë¦¬ ìºì‹± ì‹œìŠ¤í…œ ì •ì˜ì™€ ëª©ì  ë²„í¼í’€: ë°ì´í„°ë² ì´ìŠ¤ê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” í˜ì´ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€í•˜ëŠ” ìºì‹œ ì˜ì—­ í¬ê¸°: ì¼ë°˜ì ìœ¼ë¡œ ì „ì²´ ë©”ëª¨ë¦¬ì˜ 70-80% í• ë‹¹ ëª©ì : ë””ìŠ¤í¬ I/O ìµœì†Œí™”ë¡œ ì„±ëŠ¥ í–¥ìƒ í˜ì´ì§€ ë¡œë”© ê³¼ì • ì¿¼ë¦¬ ì‹¤í–‰ ìš”ì²­ ë²„í¼í’€ì—ì„œ í˜ì´ì§€ ê²€ìƒ‰ í˜ì´ì§€ê°€ ì—†ìœ¼ë©´ (Cache Miss): ë””ìŠ¤í¬ì—ì„œ í˜ì´ì§€ ë¡œë“œ ë²„í¼í’€ì— ì €ì¥ LRU ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€ í˜ì´ì§€ê°€ ìˆìœ¼ë©´ (Cache Hit): ë©”ëª¨ë¦¬ì—ì„œ ì§ì ‘ ë°ì´í„° ì ‘ê·¼ LRU ë¦¬ìŠ¤íŠ¸ì—ì„œ ìµœì‹  ìœ„ì¹˜ë¡œ ì´ë™ í˜ì´ì§€ êµì²´ ì •ì±… - LRU (Least Recently Used) ì•Œê³ ë¦¬ì¦˜: ê°€ì¥ ì˜¤ë˜ ì‚¬ìš©ë˜ì§€ ì•Šì€ í˜ì´ì§€ë¥¼ ë¨¼ì € ì œê±° ìƒˆë¡œ ë¡œë“œëœ í˜ì´ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ ë§¨ ì•ì— ë°°ì¹˜ ìì£¼ ì‚¬ìš©ë˜ëŠ” í˜ì´ì§€ëŠ” ë¦¬ìŠ¤íŠ¸ ë§¨ ë’¤ì— ìœ ì§€ ``text LRU ë¦¬ìŠ¤íŠ¸ êµ¬ì¡°: [MRU] â†â†’ [Page1] â†â†’ [Page2] â†â†’ [Page3] â†â†’ [LRU] â†‘ â†‘ ìµœê·¼ ì‚¬ìš© ì˜¤ë˜ëœ ì‚¬ìš© ` ë©”ëª¨ë¦¬ ì ‘ê·¼ vs ë””ìŠ¤í¬ ì ‘ê·¼ `text ì„±ëŠ¥ ë¹„êµ: ë©”ëª¨ë¦¬ ì ‘ê·¼: 100-1000ns (ë‚˜ë…¸ì´ˆ) ë””ìŠ¤í¬ ì ‘ê·¼: 10,000-100,000ns (ë§ˆì´í¬ë¡œì´ˆ) ì„±ëŠ¥ ì°¨ì´: ë©”ëª¨ë¦¬ê°€ ë””ìŠ¤í¬ë³´ë‹¤ 100-1000ë°° ë¹ ë¦„ `` B-Tree ì¸ë±ìŠ¤ì˜ êµ¬ì¡° > ì¸ë±ìŠ¤ëŠ” í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì €ì¥ë˜ë©°, ì¸ë±ìŠ¤ í‚¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•­ìƒ ì •ë ¬ëœ ìƒíƒœë¥¼ ìœ ì§€í•œë‹¤. > ì •ë ¬ëœ ì¸ë±ìŠ¤ í‚¤ë¥¼ ë”°ë¼ì„œ ë¦¬í”„ ë…¸ë“œì— ë„ë‹¬í•˜ë©´ (ì¸ë±ìŠ¤ í‚¤, PK) ìŒìœ¼ë¡œ ì €ì¥ë˜ì–´ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì´ ì„¤ê³„ëœ í…Œì´ë¸”ì´ ìˆë‹¤ê³  í•˜ì.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/sql-vs-nosql",
    "title": "SQL vs NoSQL",
    "date": "2025-08-14",
    "excerpt": "SQLê³¼ NoSQL ì¥ë‹¨ì  ë¹„êµ, ëŒ€í™”ê¸°ë¡ ì €ì¥í•  ë•Œ ë” ì ì ˆí•œ DBëŠ”?",
    "category": "Data Engineering",
    "tags": [
      "DB",
      "SQL",
      "NoSQL"
    ],
    "content": "SQLë³´ë‹¤ NoSQLì´ ë¹ ë¥´ë‹¤? ë‹¨ìˆœí•œ ë‹µë³€: \"ìƒí™©ì— ë”°ë¼ ë‹¤ë¥´ë‹¤\" SQLê³¼ NoSQLì˜ ì„±ëŠ¥ ë¹„êµëŠ” ë‹¨ìˆœíˆ \"ì–´ëŠ ê²ƒì´ ë¹ ë¥´ë‹¤\"ë¡œ ë‹µí•  ìˆ˜ ì—†ëŠ” ë³µì¡í•œ ë¬¸ì œì…ë‹ˆë‹¤. ê°ê°ì˜ ì¥ë‹¨ì ê³¼ ì í•©í•œ ì‚¬ìš© ì‚¬ë¡€ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì„±ëŠ¥ ë¹„êµì˜ í•µì‹¬ ìš”ì†Œ ì½ê¸° ì„±ëŠ¥ (Read Performance) SQL ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ ìµœì í™”: B-tree ì¸ë±ìŠ¤ë¡œ ë³µì¡í•œ ì¿¼ë¦¬ë„ ë¹ ë¥¸ ì²˜ë¦¬ ì¿¼ë¦¬ ìµœì í™”ê¸°: ì‹¤í–‰ ê³„íšì„ ìµœì í™”í•˜ì—¬ íš¨ìœ¨ì ì¸ ë°ì´í„° ì ‘ê·¼ ì¡°ì¸ ì„±ëŠ¥: ì •ê·œí™”ëœ ë°ì´í„°ë¡œ íš¨ìœ¨ì ì¸ ì¡°ì¸ ì—°ì‚° NoSQL ë°ì´í„°ë² ì´ìŠ¤ ë‹¨ìˆœí•œ ì¿¼ë¦¬: Key-Value ì¡°íšŒëŠ” ë§¤ìš° ë¹ ë¦„ ì¸ë©”ëª¨ë¦¬ ì²˜ë¦¬: Redis ê°™ì€ ì¸ë©”ëª¨ë¦¬ DBëŠ” ê·¹ë„ë¡œ ë¹ ë¦„ ìˆ˜í‰ì  ë¶„ì‚°: ì—¬ëŸ¬ ë…¸ë“œì— ë¶€í•˜ë¥¼ ë¶„ì‚°í•˜ì—¬ ì²˜ë¦¬ëŸ‰ í–¥ìƒ ì“°ê¸° ì„±ëŠ¥ (Write Performance) SQL ë°ì´í„°ë² ì´ìŠ¤ ACID íŠ¸ëœì­ì…˜: ë°ì´í„° ë¬´ê²°ì„±ì„ ìœ„í•œ ì˜¤ë²„í—¤ë“œ ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸: ì—¬ëŸ¬ ì¸ë±ìŠ¤ ë™ì‹œ ì—…ë°ì´íŠ¸ í•„ìš” ë¡œê¹…: íŠ¸ëœì­ì…˜ ë¡œê·¸ ê¸°ë¡ìœ¼ë¡œ ì¸í•œ ì§€ì—° NoSQL ë°ì´í„°ë² ì´ìŠ¤ ìµœì†Œí•œì˜ ê²€ì¦: ìŠ¤í‚¤ë§ˆ ê²€ì¦ ë¶€ë‹´ì´ ì ìŒ ë¹„ë™ê¸° ì²˜ë¦¬: ì¼ë¶€ NoSQLì€ ë¹„ë™ê¸°ì ìœ¼ë¡œ ë°ì´í„° ì €ì¥ ì¸ë±ìŠ¤ ë¶€ë‹´: ë³µì¡í•œ ì¸ë±ìŠ¤ êµ¬ì¡°ê°€ ì—†ì–´ ì“°ê¸° ì†ë„ í–¥ìƒ ì‹¤ì œ ì„±ëŠ¥ ë¹„êµ ì˜ˆì‹œ ë‹¨ìˆœ ì¡°íšŒ (Key-Value) `` SQL: SELECT value FROM table WHERE key = 'user123' NoSQL: db.get('user123') ê²°ê³¼: NoSQLì´ 2-5ë°° ë¹ ë¦„ (ì¸ë±ìŠ¤ ê²€ìƒ‰ ì˜¤ë²„í—¤ë“œ ì—†ìŒ) ` ë³µì¡í•œ ì¡°ì¸ ì¿¼ë¦¬ ` SQL: SELECT u.name, p.title FROM users u JOIN posts p ON u.id = p.user_id WHERE u.age > 25 AND p.created_at > '2024-01-01' NoSQL: ì—¬ëŸ¬ ë²ˆì˜ ê°œë³„ ì¿¼ë¦¬ í•„ìš” ê²°ê³¼: SQLì´ 10-100ë°° ë¹ ë¦„ (ìµœì í™”ëœ ì¡°ì¸ ì•Œê³ ë¦¬ì¦˜) ` ëŒ€ëŸ‰ ë°ì´í„° ì“°ê¸° ` SQL: 100ë§Œ ê±´ INSERT (íŠ¸ëœì­ì…˜ + ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸) NoSQL: 100ë§Œ ê±´ PUT (ìµœì†Œí•œì˜ ê²€ì¦) ê²°ê³¼: NoSQLì´ 3-10ë°° ë¹ ë¦„ (ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”) ` ì„±ëŠ¥ì´ ê²°ì •ë˜ëŠ” ìš”ì¸ë“¤ ë°ì´í„° í¬ê¸° ì†Œê·œëª¨ (GB ë‹¨ìœ„): SQLê³¼ NoSQL ì„±ëŠ¥ ì°¨ì´ ë¯¸ë¯¸ ì¤‘ê°„ ê·œëª¨ (TB ë‹¨ìœ„): NoSQLì˜ ìˆ˜í‰ì  í™•ì¥ì„± ìš°ìœ„ ëŒ€ê·œëª¨ (PB ë‹¨ìœ„): NoSQLì´ ì••ë„ì ìœ¼ë¡œ ìš°ìˆ˜ ì¿¼ë¦¬ ë³µì¡ë„ ë‹¨ìˆœ ì¡°íšŒ: NoSQL ìš°ìœ„ ë³µì¡í•œ ë¶„ì„: SQL ìš°ìœ„ ì‹¤ì‹œê°„ ì§‘ê³„: ìƒí™©ì— ë”°ë¼ ë‹¤ë¦„ ë°ì´í„° êµ¬ì¡° ì •ê·œí™”ëœ ë°ì´í„°: SQL ìš°ìœ„ ë¹„ì •ê·œí™”ëœ ë°ì´í„°: NoSQL ìš°ìœ„ ê³„ì¸µì  êµ¬ì¡°: NoSQL ìš°ìœ„ ê²°ë¡  \"SQLì´ ë¹ ë¥´ë‹¤\" vs \"NoSQLì´ ë¹ ë¥´ë‹¤\"ëŠ” ì˜ëª»ëœ ì§ˆë¬¸ì…ë‹ˆë‹¤. ì˜¬ë°”ë¥¸ ì ‘ê·¼ ë°©ë²•: ì‚¬ìš© ì‚¬ë¡€ ë¶„ì„: ì–´ë–¤ ì¢…ë¥˜ì˜ ì‘ì—…ì„ ì£¼ë¡œ ìˆ˜í–‰í•˜ëŠ”ê°€? ë°ì´í„° íŠ¹ì„± íŒŒì•…: êµ¬ì¡°í™”ëœ ë°ì´í„°ì¸ê°€, ìœ ì—°í•œ êµ¬ì¡°ê°€ í•„ìš”í•œê°€? í™•ì¥ì„± ìš”êµ¬ì‚¬í•­: í–¥í›„ ë°ì´í„° ì¦ê°€ê°€ ì˜ˆìƒë˜ëŠ”ê°€? ì¼ê´€ì„± ìš”êµ¬ìˆ˜ì¤€: ë°ì´í„° ì •í™•ì„±ì´ ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œê°€? ì„±ëŠ¥ì€ ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒì˜ í•œ ìš”ì†Œì¼ ë¿ì´ë©°, ì „ì²´ì ì¸ ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì†ë„ ì°¨ì´ì˜ ì´ë¡ ì  ë°°ê²½ ë°ì´í„° êµ¬ì¡°ì™€ ì ‘ê·¼ íŒ¨í„´ SQL: B-tree ê¸°ë°˜ ì¸ë±ìŠ¤ êµ¬ì¡° ` B-tree êµ¬ì¡°: [Root] / \\ [Left] [Right] / \\ / \\ [Leaf] [Leaf] [Leaf] [Leaf] íŠ¹ì§•: ê· í˜• ì¡íŒ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ O(log n) ê²€ìƒ‰ ì‹œê°„ ìˆœì°¨ ì ‘ê·¼ê³¼ ëœë¤ ì ‘ê·¼ ëª¨ë‘ íš¨ìœ¨ì  ì¸ë±ìŠ¤ ìœ ì§€ ë¹„ìš©: ì‚½ì…/ì‚­ì œ ì‹œ ì¬ê· í˜•í™” í•„ìš” ` NoSQL: Hash Table ê¸°ë°˜ êµ¬ì¡° ` Hash Table êµ¬ì¡°: Key: \"user123\" â†’ Hash Function â†’ Index: 8472 Bucket[8472] = {user123: \"ë°ì´í„°\"} íŠ¹ì§•: O(1) í‰ê·  ê²€ìƒ‰ ì‹œê°„ (í•´ì‹œ ì¶©ëŒ ì—†ëŠ” ê²½ìš°) ëœë¤ ì ‘ê·¼ì— ìµœì í™” ìˆœì°¨ ì ‘ê·¼ì€ ë¹„íš¨ìœ¨ì  ì¸ë±ìŠ¤ ìœ ì§€ ë¹„ìš©: ê±°ì˜ ì—†ìŒ ` ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°ì™€ I/O íŒ¨í„´ SQLì˜ I/O íŒ¨í„´ ` ì¿¼ë¦¬ ì‹¤í–‰ ê³¼ì •: SQL íŒŒì‹± â†’ 2. ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ â†’ 3. ì¸ë±ìŠ¤ íƒìƒ‰ â†’ 4. ë°ì´í„° í˜ì´ì§€ ë¡œë“œ ë©”ëª¨ë¦¬ ê³„ì¸µ: CPU Cache (L1/L2/L3) â†’ Buffer Pool â†’ Disk Storage I/O ìµœì í™”: Buffer Pool: ìì£¼ ì‚¬ìš©ë˜ëŠ” í˜ì´ì§€ë¥¼ ë©”ëª¨ë¦¬ì— ìœ ì§€ Prefetching: ì—°ì†ëœ í˜ì´ì§€ë¥¼ ë¯¸ë¦¬ ë¡œë“œ Write-Behind: ì“°ê¸° ì‘ì—…ì„ ë°°ì¹˜ë¡œ ì²˜ë¦¬ ` NoSQLì˜ I/O íŒ¨í„´ ` ë°ì´í„° ì ‘ê·¼ ê³¼ì •: Key í•´ì‹œ ê³„ì‚° â†’ 2. ë©”ëª¨ë¦¬/ë””ìŠ¤í¬ ìœ„ì¹˜ í™•ì¸ â†’ 3. ì§ì ‘ ë°ì´í„° ë¡œë“œ ë©”ëª¨ë¦¬ ê³„ì¸µ: CPU Cache â†’ In-Memory Storage â†’ Disk Storage (ì„ íƒì ) I/O ìµœì í™”: In-Memory First: ê°€ëŠ¥í•œ í•œ ë©”ëª¨ë¦¬ì—ì„œ ì²˜ë¦¬ Minimal Validation: ìµœì†Œí•œì˜ ë°ì´í„° ê²€ì¦ Async Writes: ë¹„ë™ê¸° ì“°ê¸°ë¡œ ì‘ë‹µ ì‹œê°„ ë‹¨ì¶• ` íŠ¸ëœì­ì…˜ê³¼ ACID ì†ì„±ì˜ ì˜¤ë²„í—¤ë“œ SQL: ACID íŠ¸ëœì­ì…˜ ë³´ì¥ ` íŠ¸ëœì­ì…˜ ì²˜ë¦¬ ê³¼ì •: BEGIN TRANSACTION â†’ Lock íšë“ (Row/Table Level) â†’ ë°ì´í„° ë³€ê²½ â†’ Undo Log ê¸°ë¡ â†’ Redo Log ê¸°ë¡ â†’ Lock í•´ì œ COMMIT ì˜¤ë²„í—¤ë“œ ìš”ì†Œ: Lock Management: ë™ì‹œì„± ì œì–´ë¥¼ ìœ„í•œ ë½ ì˜¤ë²„í—¤ë“œ Logging: ACID ë³´ì¥ì„ ìœ„í•œ ë¡œê·¸ ê¸°ë¡ Index Updates: ëª¨ë“  ê´€ë ¨ ì¸ë±ìŠ¤ ë™ì‹œ ì—…ë°ì´íŠ¸ Validation: ì œì•½ ì¡°ê±´ ê²€ì¦ ` NoSQL: Eventual Consistency ` ë°ì´í„° ì²˜ë¦¬ ê³¼ì •: PUT operation â†’ ë°ì´í„° ê²€ì¦ (ìµœì†Œí•œ) â†’ ë©”ëª¨ë¦¬ì— ì €ì¥ â†’ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë””ìŠ¤í¬ì— ì €ì¥ â†’ ë³µì œë³¸ì— ì „íŒŒ (ì„ íƒì ) ì˜¤ë²„í—¤ë“œ ìµœì†Œí™”: No Locking: ë½ ê¸°ë°˜ ë™ì‹œì„± ì œì–´ ì—†ìŒ Minimal Logging: íŠ¸ëœì­ì…˜ ë¡œê·¸ ì—†ìŒ Lazy Indexing: ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì§€ì—° Async Replication: ë¹„ë™ê¸° ë³µì œ ` ì¿¼ë¦¬ ìµœì í™”ì™€ ì‹¤í–‰ ê³„íš SQL: Cost-Based Query Optimizer ` ì¿¼ë¦¬ ìµœì í™” ê³¼ì •: SQL íŒŒì‹± â†’ 2. ë…¼ë¦¬ì  ê³„íš ìƒì„± â†’ 3. ë¬¼ë¦¬ì  ê³„íš ìƒì„± â†’ 4. ë¹„ìš© ê³„ì‚° â†’ 5. ìµœì  ê³„íš ì„ íƒ ìµœì í™” ìš”ì†Œ: Join Order: í…Œì´ë¸” ì¡°ì¸ ìˆœì„œ ìµœì í™” Index Selection: ì ì ˆí•œ ì¸ë±ìŠ¤ ì„ íƒ Access Path: í…Œì´ë¸” ìŠ¤ìº” vs ì¸ë±ìŠ¤ ìŠ¤ìº” Sort Elimination: ë¶ˆí•„ìš”í•œ ì •ë ¬ ì œê±° ë¹„ìš© ê³„ì‚°: CPU Cost: ì—°ì‚° ë¹„ìš© I/O Cost: ë””ìŠ¤í¬ ì ‘ê·¼ ë¹„ìš© Memory Cost: ë©”ëª¨ë¦¬ ì‚¬ìš© ë¹„ìš© ` NoSQL: Direct Access Pattern ` ë°ì´í„° ì ‘ê·¼ ë°©ì‹: Key ê¸°ë°˜ ì§ì ‘ ì ‘ê·¼ Secondary Index ì‚¬ìš© (ì œí•œì ) Map-Reduce ë˜ëŠ” Aggregation Pipeline ìµœì í™” ë¶€ì¡±: No Query Planning: ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì—†ìŒ Limited Indexing: ì œí•œì ì¸ ì¸ë±ìŠ¤ ì§€ì› Manual Optimization: ê°œë°œìê°€ ì§ì ‘ ìµœì í™” í•„ìš” ` ìˆ˜í‰ì  í™•ì¥ì„±ê³¼ ë¶„ì‚° ì²˜ë¦¬ SQL: ìˆ˜ì§ì  í™•ì¥ (Vertical Scaling) ` í™•ì¥ ë°©ì‹: ë” ê°•ë ¥í•œ CPU ì¶”ê°€ ë” í° ë©”ëª¨ë¦¬ ì¶”ê°€ ë” ë¹ ë¥¸ ë””ìŠ¤í¬ ì¶”ê°€ í•œê³„ì : ë‹¨ì¼ ì„œë²„ì˜ ë¬¼ë¦¬ì  í•œê³„ ë¹„ìš©ì˜ ê¸°í•˜ê¸‰ìˆ˜ì  ì¦ê°€ ì¥ì•  ì‹œ ì „ì²´ ì‹œìŠ¤í…œ ì¤‘ë‹¨ ` NoSQL: ìˆ˜í‰ì  í™•ì¥ (Horizontal Scaling) ` í™•ì¥ ë°©ì‹: ë” ë§ì€ ì„œë²„ ë…¸ë“œ ì¶”ê°€ ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚° ë¶€í•˜ë¥¼ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚° ì¥ì : ì„ í˜•ì  ì„±ëŠ¥ í–¥ìƒ ì¥ì•  ê²©ë¦¬ ë¹„ìš© íš¨ìœ¨ì„± ` ë„¤íŠ¸ì›Œí¬ ì§€ì—°ê³¼ ë¶„ì‚° ì‹œìŠ¤í…œ ì´ë¡  CAP ì´ë¡ ê³¼ ì„±ëŠ¥ íŠ¸ë ˆì´ë“œì˜¤í”„ ` CAP ì´ë¡ : Consistency (ì¼ê´€ì„±) Availability (ê°€ìš©ì„±) Partition Tolerance (ë¶„í•  í—ˆìš©ì„±) SQL ì„ íƒ: CP (ì¼ê´€ì„± + ë¶„í•  í—ˆìš©ì„±) ë„¤íŠ¸ì›Œí¬ ë¶„í•  ì‹œ ì¼ê´€ì„± ìœ ì§€ ì¼ë¶€ ë…¸ë“œ ì‚¬ìš© ë¶ˆê°€ë¡œ ê°€ìš©ì„± í¬ìƒ NoSQL ì„ íƒ: AP (ê°€ìš©ì„± + ë¶„í•  í—ˆìš©ì„±) ë„¤íŠ¸ì›Œí¬ ë¶„í•  ì‹œì—ë„ ì„œë¹„ìŠ¤ ì§€ì† ì¼ê´€ì„±ì€ Eventually Consistentë¡œ ë‹¬ì„± ` ë„¤íŠ¸ì›Œí¬ ì§€ì—°ì˜ ì˜í–¥ ` ë¶„ì‚° ì‹œìŠ¤í…œì—ì„œì˜ ì§€ì—°: Network Latency: ë…¸ë“œ ê°„ í†µì‹  ì§€ì—° Serialization: ë°ì´í„° ì§ë ¬í™”/ì—­ì§ë ¬í™” Consensus Protocol: ë¶„ì‚° í•©ì˜ í”„ë¡œí† ì½œ SQLì˜ ë¶„ì‚° íŠ¸ëœì­ì…˜: 2-Phase Commit: ëª¨ë“  ë…¸ë“œì˜ ë™ì‹œ ì»¤ë°‹ í•„ìš” Global Locking: ì „ì²´ ì‹œìŠ¤í…œì— ê±¸ì¹œ ë½ Synchronous Replication: ë™ê¸°ì  ë³µì œ NoSQLì˜ ë¶„ì‚° ì²˜ë¦¬: Local Operations: ë¡œì»¬ì—ì„œ ë¨¼ì € ì²˜ë¦¬ Async Replication: ë¹„ë™ê¸°ì  ë³µì œ Eventual Consistency: ìµœì¢…ì  ì¼ê´€ì„± ` ë©”ëª¨ë¦¬ ê´€ë¦¬ì™€ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ SQL: Buffer Pool ê´€ë¦¬ ` ë©”ëª¨ë¦¬ ê´€ë¦¬: LRU (Least Recently Used) ì•Œê³ ë¦¬ì¦˜ Dirty Page Management: ë³€ê²½ëœ í˜ì´ì§€ ì¶”ì  Checkpoint: ì£¼ê¸°ì ì¸ ë””ìŠ¤í¬ ë™ê¸°í™” ì˜¤ë²„í—¤ë“œ: Page Replacement: ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ í˜ì´ì§€ êµì²´ Dirty Page Flushing: ë³€ê²½ëœ í˜ì´ì§€ ë””ìŠ¤í¬ ì €ì¥ Buffer Pool Tuning: ë©”ëª¨ë¦¬ í¬ê¸° ìµœì í™” ` NoSQL: In-Memory ìµœì í™” ` ë©”ëª¨ë¦¬ ê´€ë¦¬: Direct Memory Access: ì§ì ‘ ë©”ëª¨ë¦¬ ì ‘ê·¼ Memory-Mapped Files: íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ì§ì ‘ ë§¤í•‘ Garbage Collection: ìë™ ë©”ëª¨ë¦¬ ì •ë¦¬ ìµœì í™”: Zero-Copy: ë¶ˆí•„ìš”í•œ ë©”ëª¨ë¦¬ ë³µì‚¬ ì œê±° Memory Pooling: ë©”ëª¨ë¦¬ í’€ì„ í†µí•œ í• ë‹¹ ìµœì í™” Cache-Aware Data Structures: ìºì‹œ ì¹œí™”ì  ë°ì´í„° êµ¬ì¡° ` ì´ëŸ¬í•œ ì´ë¡ ì  ë°°ê²½ì„ í†µí•´ SQLê³¼ NoSQLì˜ ì„±ëŠ¥ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ” ê·¼ë³¸ì ì¸ ì´ìœ ë¥¼ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ê°ì˜ ì„¤ê³„ ì² í•™ê³¼ ì•„í‚¤í…ì²˜ê°€ ì„±ëŠ¥ íŠ¹ì„±ì— ì§ì ‘ì ì¸ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìŠµë‹ˆë‹¤. ---- SQL vs NoSQL: ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒì˜ ê¸°ì¤€ ê°œìš” ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒì€ ì‹œìŠ¤í…œ ì„¤ê³„ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²°ì • ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. SQLê³¼ NoSQLì€ ê°ê° ë‹¤ë¥¸ íŠ¹ì„±ê³¼ ì¥ë‹¨ì ì„ ê°€ì§€ê³  ìˆì–´, ì‚¬ìš© ì‚¬ë¡€ì— ë”°ë¼ ì ì ˆí•œ ì„ íƒì´ í•„ìš”í•©ë‹ˆë‹¤. SQL ë°ì´í„°ë² ì´ìŠ¤ íŠ¹ì§• ACID íŠ¸ëœì­ì…˜: Atomicity, Consistency, Isolation, Durability ë³´ì¥ ì •ê·œí™”ëœ ìŠ¤í‚¤ë§ˆ: ë°ì´í„° ë¬´ê²°ì„±ê³¼ ì¼ê´€ì„± ìœ ì§€ ê´€ê³„í˜• ëª¨ë¸: í…Œì´ë¸” ê°„ ê´€ê³„ë¥¼ í†µí•œ ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì› ì¥ì  ë°ì´í„° ì¼ê´€ì„±ê³¼ ë¬´ê²°ì„± ë³´ì¥ ë³µì¡í•œ ì¡°ì¸ê³¼ ì§‘ê³„ ì¿¼ë¦¬ ì§€ì› ì„±ìˆ™í•œ ê¸°ìˆ ê³¼ í’ë¶€í•œ ìƒíƒœê³„ ë‹¨ì  ìˆ˜í‰ì  í™•ì¥ì„± ì œí•œ ìŠ¤í‚¤ë§ˆ ë³€ê²½ì˜ ì–´ë ¤ì›€ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ ì €í•˜ NoSQL ë°ì´í„°ë² ì´ìŠ¤ íŠ¹ì§• ìœ ì—°í•œ ìŠ¤í‚¤ë§ˆ: ë™ì  ë°ì´í„° êµ¬ì¡° ì§€ì› ìˆ˜í‰ì  í™•ì¥ì„±: ë¶„ì‚° í™˜ê²½ì—ì„œ ë†’ì€ ì„±ëŠ¥ ë‹¤ì–‘í•œ ë°ì´í„° ëª¨ë¸: Document, Key-Value, Column-family, Graph ì¥ì  ë†’ì€ í™•ì¥ì„±ê³¼ ì„±ëŠ¥ ìŠ¤í‚¤ë§ˆ ë³€ê²½ì˜ ìœ ì—°ì„± ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ì— ì í•© ë‹¨ì  ACID íŠ¸ëœì­ì…˜ ë³´ì¥ì˜ ì œí•œ ë³µì¡í•œ ì¿¼ë¦¬ì˜ ì–´ë ¤ì›€ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì˜ ì–´ë ¤ì›€ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì—ì„œì˜ ì¥ì  ìˆ˜í‰ì  í™•ì¥ì„± (Horizontal Scalability) ìƒ¤ë”©(Sharding): ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ì„œë²„ì— ë¶„ì‚° ì €ì¥ ìë™ íŒŒí‹°ì…”ë‹: ë°ì´í„° ì¦ê°€ì— ë”°ë¼ ìë™ìœ¼ë¡œ ë…¸ë“œ ì¶”ê°€ ì„ í˜•ì  ì„±ëŠ¥ í–¥ìƒ: ë…¸ë“œ ì¶”ê°€ ì‹œ ì„±ëŠ¥ì´ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€ ë¶„ì‚° ì•„í‚¤í…ì²˜ Master-Slave êµ¬ì¡°: ì½ê¸° ì‘ì—…ì„ ì—¬ëŸ¬ ë…¸ë“œì— ë¶„ì‚° Peer-to-Peer êµ¬ì¡°: ëª¨ë“  ë…¸ë“œê°€ ë™ë“±í•œ ì—­í•  ìˆ˜í–‰ ì§€ì—­ì  ë¶„ì‚°: ì§€ë¦¬ì ìœ¼ë¡œ ë¶„ì‚°ëœ ë°ì´í„°ì„¼í„° ì§€ì› ë©”ëª¨ë¦¬ ê¸°ë°˜ ì²˜ë¦¬ ì¸ë©”ëª¨ë¦¬ ìºì‹±: Redis, Memcached ë“±ì˜ ë¹ ë¥¸ ë©”ëª¨ë¦¬ ì ‘ê·¼ Lazy Loading: í•„ìš”í•œ ë°ì´í„°ë§Œ ë©”ëª¨ë¦¬ì— ë¡œë“œ Write-Behind: ì“°ê¸° ì‘ì—…ì„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬ ë°ì´í„° ëª¨ë¸ ìµœì í™” Column-Family DB: ì»¬ëŸ¼ ë‹¨ìœ„ë¡œ ë°ì´í„° ì €ì¥í•˜ì—¬ ì••ì¶• íš¨ìœ¨ì„± ì¦ëŒ€ Document DB: JSON í˜•íƒœë¡œ ì¤‘ì²© êµ¬ì¡° ë°ì´í„° íš¨ìœ¨ì  ì €ì¥ Key-Value DB: ë‹¨ìˆœí•œ êµ¬ì¡°ë¡œ ë¹ ë¥¸ ì½ê¸°/ì“°ê¸° ì„±ëŠ¥ CAP ì´ë¡  í™œìš© ì¼ê´€ì„±(Consistency) vs ê°€ìš©ì„±(Availability): ëŒ€ìš©ëŸ‰ í™˜ê²½ì—ì„œ ê°€ìš©ì„± ìš°ì„  ë„¤íŠ¸ì›Œí¬ ë¶„í• (Partition): ë„¤íŠ¸ì›Œí¬ ë¬¸ì œ ì‹œì—ë„ ì„œë¹„ìŠ¤ ì§€ì† Eventually Consistent: ìµœì¢…ì ìœ¼ë¡œ ì¼ê´€ì„± ë‹¬ì„± DynamoDB: AWSì˜ ê´€ë¦¬í˜• NoSQL ì„œë¹„ìŠ¤ íŠ¹ì§• ì™„ì „ ê´€ë¦¬í˜• ì„œë¹„ìŠ¤: ì„œë²„ ê´€ë¦¬, íŒ¨ì¹˜, ë°±ì—… ë“± AWSì—ì„œ ìë™ ì²˜ë¦¬ ë¬´ì œí•œ í™•ì¥ì„±: ìë™ìœ¼ë¡œ ìˆ˜ë°±ë§Œ ìš”ì²­/ì´ˆ ì²˜ë¦¬ ê°€ëŠ¥ ê¸€ë¡œë²Œ í…Œì´ë¸”: ì—¬ëŸ¬ ë¦¬ì „ì— ìë™ìœ¼ë¡œ ë°ì´í„° ë³µì œ ë°ì´í„° ëª¨ë¸ Key-Value + Document: ê¸°ë³¸ì ìœ¼ë¡œ Key-Valueì´ì§€ë§Œ JSON í˜•íƒœì˜ ë³µì¡í•œ ë°ì´í„°ë„ ì €ì¥ ê°€ëŠ¥ Primary Key: Partition Key (í•„ìˆ˜) + Sort Key (ì„ íƒ) Secondary Index: GSI(Global Secondary Index)ì™€ LSI(Local Secondary Index) ì§€ì› ì„±ëŠ¥ íŠ¹ì„± ì¼ê´€ëœ ì„±ëŠ¥: 10ms ì´í•˜ì˜ ì‘ë‹µ ì‹œê°„ ë³´ì¥ ìë™ ìŠ¤ì¼€ì¼ë§: íŠ¸ë˜í”½ì— ë”°ë¼ ìë™ìœ¼ë¡œ ìš©ëŸ‰ ì¡°ì • On-Demand vs Provisioned: ì‚¬ìš©ëŸ‰ ê¸°ë°˜ ë˜ëŠ” ì˜ˆì•½ ìš©ëŸ‰ ì„ íƒ ê°€ëŠ¥ ëŒ€í™”ê¸°ë¡ ì €ì¥ì—ì„œì˜ ì¥ì  ë†’ì€ ì“°ê¸° ì„±ëŠ¥: ì´ˆë‹¹ ìˆ˜ë§Œ ê±´ì˜ ì“°ê¸° ì²˜ë¦¬ ê°€ëŠ¥ ìë™ ë°±ì—…: Point-in-time recovery ì§€ì› TTL(Time To Live): ì˜¤ë˜ëœ ëŒ€í™”ê¸°ë¡ ìë™ ì‚­ì œ ìŠ¤íŠ¸ë¦¼ ê¸°ëŠ¥: ì‹¤ì‹œê°„ ë°ì´í„° ë³€ê²½ ê°ì§€ ë° ì²˜ë¦¬ ì‚¬ìš© ì‚¬ë¡€ ì‹¤ì‹œê°„ ì±„íŒ…: ë†’ì€ ì“°ê¸° ì„±ëŠ¥ê³¼ ë‚®ì€ ì§€ì—°ì‹œê°„ ê²Œì„ ë¦¬ë”ë³´ë“œ: ë¹ ë¥¸ ìˆœìœ„ ì¡°íšŒì™€ ì—…ë°ì´íŠ¸ IoT ë°ì´í„°: ëŒ€ëŸ‰ì˜ ì„¼ì„œ ë°ì´í„° ìˆ˜ì§‘ ë° ì €ì¥ ì‚¬ìš©ì ì„¸ì…˜: ë¹ ë¥¸ ì„¸ì…˜ ì •ë³´ ì ‘ê·¼ ë° ì—…ë°ì´íŠ¸ ëŒ€í™”ê¸°ë¡ ì €ì¥ ì‹œ ê³ ë ¤ì‚¬í•­ ëŒ€í™”ê¸°ë¡ì˜ íŠ¹ì„± êµ¬ì¡°ì  íŠ¹ì„±: ì‚¬ìš©ì ID, ì‹œê°„, ë©”ì‹œì§€ ë‚´ìš©, ì»¨í…ìŠ¤íŠ¸ ë°ì´í„° í¬ê¸°: í…ìŠ¤íŠ¸ ê¸°ë°˜ì´ì§€ë§Œ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í¬í•¨ ì ‘ê·¼ íŒ¨í„´: ì‚¬ìš©ìë³„ ì¡°íšŒ, ì‹œê°„ìˆœ ì •ë ¬, ê²€ìƒ‰ ê¸°ëŠ¥ SQL vs NoSQL ë¹„êµ SQL ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì‹œ ì¥ì : ì‚¬ìš©ìë³„ ëŒ€í™” ê¸°ë¡ì˜ ì¼ê´€ì„± ë³´ì¥ ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬ ì§€ì› (ì‚¬ìš©ì í–‰ë™ ë¶„ì„) íŠ¸ëœì­ì…˜ ê¸°ë°˜ì˜ ë°ì´í„° ë¬´ê²°ì„± ë‹¨ì : ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì™€ ê°™ì€ ìœ ì—°í•œ êµ¬ì¡° ì €ì¥ì˜ ì–´ë ¤ì›€ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ì‹œ ì„±ëŠ¥ ì €í•˜ NoSQL ë°ì´í„°ë² ì´ìŠ¤ ì„ íƒ ì‹œ ì¥ì : ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ì˜ ìœ ì—°í•œ ì €ì¥ ë†’ì€ ì“°ê¸° ì„±ëŠ¥ê³¼ í™•ì¥ì„± JSON í˜•íƒœì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë°ì´í„° ì €ì¥ ë‹¨ì : ë³µì¡í•œ ë¶„ì„ ì¿¼ë¦¬ì˜ ì–´ë ¤ì›€ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì˜ ì œí•œ ê¶Œì¥ì‚¬í•­ ëŒ€í™”ê¸°ë¡ ì €ì¥ì— ì í•©í•œ ì„ íƒ NoSQL (Document DB) ì¶”ì²œ: ì´ìœ : ëŒ€í™” ë°ì´í„°ì˜ ìœ ì—°í•œ êµ¬ì¡°ì™€ ë†’ì€ ì“°ê¸° ì„±ëŠ¥ ì¶”ì²œ DB: MongoDB, CouchDB ì ìš© ì‚¬ë¡€: ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜, ê³ ê° ì§€ì› ì‹œìŠ¤í…œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼: í•µì‹¬ ë°ì´í„°: SQL DB (ì‚¬ìš©ì ì •ë³´, ë©”íƒ€ë°ì´í„°) ëŒ€í™” ë‚´ìš©: NoSQL DB (ìœ ì—°í•œ êµ¬ì¡°, ë†’ì€ ì„±ëŠ¥) ê²°ë¡  SQLê³¼ NoSQLì€ ê°ê°ì˜ ì¥ë‹¨ì ì´ ìˆìœ¼ë©°, ëŒ€í™”ê¸°ë¡ ì €ì¥ê³¼ ê°™ì€ íŠ¹ì • ì‚¬ìš© ì‚¬ë¡€ì—ì„œëŠ” NoSQLì´ ë” ì í•©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹œìŠ¤í…œì˜ ì „ì²´ì ì¸ ìš”êµ¬ì‚¬í•­ê³¼ ë°ì´í„° ì¼ê´€ì„± ìš”êµ¬ ìˆ˜ì¤€ì„ ê³ ë ¤í•˜ì—¬ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ---- ë¶€ë¡: B-tree êµ¬ì¡°ì˜ ìƒì„¸ ë¶„ì„ B-treeì˜ ìˆ˜í•™ì  ê¸°ì´ˆ B-treeëŠ” 1970ë…„ëŒ€ì— Bayerì™€ McCreightì— ì˜í•´ ê°œë°œëœ ê· í˜• ì¡íŒ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ, SQL ë°ì´í„°ë² ì´ìŠ¤ì˜ í•µì‹¬ ì¸ë±ìŠ¤ êµ¬ì¡°ì…ë‹ˆë‹¤. B-treeì˜ ì •ì˜ì™€ ì†ì„± ` B-tree (Order = m)ì˜ ì •ì˜: ëª¨ë“  ë¦¬í”„ ë…¸ë“œëŠ” ê°™ì€ ë ˆë²¨ì— ìœ„ì¹˜ ë£¨íŠ¸ ë…¸ë“œë¥¼ ì œì™¸í•œ ëª¨ë“  ë…¸ë“œëŠ” ìµœì†Œ âŒˆm/2âŒ‰-1ê°œ, ìµœëŒ€ m-1ê°œì˜ í‚¤ë¥¼ ê°€ì§ ë£¨íŠ¸ ë…¸ë“œëŠ” ìµœì†Œ 1ê°œ, ìµœëŒ€ m-1ê°œì˜ í‚¤ë¥¼ ê°€ì§ ëª¨ë“  ë‚´ë¶€ ë…¸ë“œëŠ” ìµœì†Œ âŒˆm/2âŒ‰ê°œ, ìµœëŒ€ mê°œì˜ ìì‹ì„ ê°€ì§ ` B-treeì˜ ë†’ì´ ê³„ì‚° ` ìˆ˜í•™ì  ê³µì‹: ìµœì†Œ ë†’ì´: h_min = log_m(n+1) - 1 ìµœëŒ€ ë†’ì´: h_max = log_(âŒˆm/2âŒ‰)((n+1)/2) ì‹¤ì œ ê³„ì‚° ì˜ˆì‹œ: m = 100 (í•œ ë…¸ë“œë‹¹ ìµœëŒ€ 100ê°œ í‚¤) n = 1,000,000 (100ë§Œ ê°œ í‚¤) ìµœì†Œ ë†’ì´: log_100(1,000,001) - 1 â‰ˆ 2.5 ìµœëŒ€ ë†’ì´: log_50(500,000.5) â‰ˆ 3.8 ê²°ë¡ : 100ë§Œ ê°œ í‚¤ë¥¼ 3-4 ë ˆë²¨ì˜ B-treeë¡œ ì €ì¥ ê°€ëŠ¥ ` B-treeì˜ ë…¸ë“œ êµ¬ì¡°ì™€ ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ ë…¸ë“œ íƒ€ì…ë³„ êµ¬ì¡° ` Internal Node (ë‚´ë¶€ ë…¸ë“œ): [Pointer1] [Key1] [Pointer2] [Key2] [Pointer3] ... [PointerN] Leaf Node (ë¦¬í”„ ë…¸ë“œ): [Key1] [Data1] [Key2] [Data2] ... [KeyN] [DataN] [Next_Leaf_Pointer] ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒ: ê° ë…¸ë“œëŠ” ë””ìŠ¤í¬ í˜ì´ì§€ í¬ê¸°ì— ë§ì¶° ì„¤ê³„ ì¼ë°˜ì ìœ¼ë¡œ 4KB ë˜ëŠ” 8KB í˜ì´ì§€ ì‚¬ìš© í¬ì¸í„°ëŠ” 8ë°”ì´íŠ¸ (64ë¹„íŠ¸ ì‹œìŠ¤í…œ) í‚¤ëŠ” ê°€ë³€ ê¸¸ì´ (VARCHAR ë“±) ` í˜ì´ì§€ ìµœì í™” ` í˜ì´ì§€ í¬ê¸°ë³„ íŠ¹ì„±: 4KB í˜ì´ì§€: ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ìš°ì„ , ì‘ì€ ì¸ë±ìŠ¤ì— ì í•© 8KB í˜ì´ì§€: ì¼ë°˜ì ì¸ ìš©ë„, ê· í˜• ì¡íŒ ì„±ëŠ¥ 16KB í˜ì´ì§€: ìˆœì°¨ ì½ê¸° ì„±ëŠ¥ ìš°ì„ , í° ì¸ë±ìŠ¤ì— ì í•© Fill Factor (ì±„ì›€ë¥ ): ë†’ì€ Fill Factor (90%): ê³µê°„ íš¨ìœ¨ì„± ìš°ì„ , ì½ê¸° ì„±ëŠ¥ í–¥ìƒ ë‚®ì€ Fill Factor (70%): ì‚½ì… ì„±ëŠ¥ ìš°ì„ , ë…¸ë“œ ë¶„í•  ë¹ˆë„ ê°ì†Œ ` B-treeì˜ í•µì‹¬ ì•Œê³ ë¦¬ì¦˜ ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ (Search) ` ê²€ìƒ‰ ê³¼ì • ìƒì„¸ ë¶„ì„: ë£¨íŠ¸ ë…¸ë“œì—ì„œ ì‹œì‘ í˜„ì¬ ë…¸ë“œì˜ í‚¤ë“¤ê³¼ ë¹„êµí•˜ì—¬ ì ì ˆí•œ í¬ì¸í„° ì„ íƒ ë¦¬í”„ ë…¸ë“œì— ë„ë‹¬í•  ë•Œê¹Œì§€ 2ë‹¨ê³„ ë°˜ë³µ ë¦¬í”„ ë…¸ë“œì—ì„œ í‚¤ ê²€ìƒ‰ ì‹œê°„ ë³µì¡ë„: O(log_m n) mì´ í´ìˆ˜ë¡ íŠ¸ë¦¬ ë†’ì´ê°€ ë‚®ì•„ì§ ì¼ë°˜ì ìœ¼ë¡œ 3-4ë²ˆì˜ ë””ìŠ¤í¬ ì ‘ê·¼ìœ¼ë¡œ ì›í•˜ëŠ” ë°ì´í„° ì°¾ìŒ ì‹¤ì œ ì˜ˆì‹œ (í‚¤ = 35 ê²€ìƒ‰): Root: [50] â†’ 35 < 50, ì™¼ìª½ í¬ì¸í„° ì„ íƒ Internal: [20, 40] â†’ 20 < 35 < 40, ì¤‘ê°„ í¬ì¸í„° ì„ íƒ Leaf: [30, 35, 38] â†’ í‚¤ 35 ë°œê²¬! ` ì‚½ì… ì•Œê³ ë¦¬ì¦˜ (Insert) ` ì‚½ì… ê³¼ì • ë‹¨ê³„ë³„ ë¶„ì„: ì ì ˆí•œ ë¦¬í”„ ë…¸ë“œ ì°¾ê¸° (ê²€ìƒ‰ê³¼ ë™ì¼) ë…¸ë“œì— ê³µê°„ì´ ìˆëŠ” ê²½ìš°: ì§ì ‘ ì‚½ì… ë…¸ë“œê°€ ê°€ë“ ì°¬ ê²½ìš°: ë…¸ë“œ ë¶„í•  (Split) ë…¸ë“œ ë¶„í•  ê³¼ì •: ì›ë³¸ ë…¸ë“œ: [20, 25, 30, 35] ì¤‘ê°„ê°’ ì„ íƒ: 30 (âŒˆ4/2âŒ‰ = 2ë²ˆì§¸ ìœ„ì¹˜) ì™¼ìª½ ë…¸ë“œ: [20, 25] ì˜¤ë¥¸ìª½ ë…¸ë“œ: [35] ë¶€ëª¨ ë…¸ë“œë¡œ ì¤‘ê°„ê°’ 30 ì´ë™ ë¶„í• ì˜ ì˜í–¥: íŠ¸ë¦¬ ë†’ì´ê°€ ì¦ê°€í•  ìˆ˜ ìˆìŒ ëª¨ë“  ë¦¬í”„ ë…¸ë“œëŠ” ì—¬ì „íˆ ê°™ì€ ë ˆë²¨ ìœ ì§€ ë¶„í• ì€ ìƒí–¥ìœ¼ë¡œ ì „íŒŒë¨ (ë¶€ëª¨ ë…¸ë“œë„ ê°€ë“ ì°¬ ê²½ìš°) ` ì‚­ì œ ì•Œê³ ë¦¬ì¦˜ (Delete) ` ì‚­ì œ ê³¼ì •ì˜ ë³µì¡ì„±: í‚¤ ê²€ìƒ‰ ë° ì œê±° ë…¸ë“œê°€ ìµœì†Œ ì±„ì›€ë¥  ë¯¸ë§Œì¸ ê²½ìš° ì¬ê· í˜•í™” ì¬ê· í˜•í™” ì „ëµ: a) í‚¤ ë¹Œë¦¬ê¸° (Borrowing): í˜•ì œ ë…¸ë“œì—ì„œ í‚¤ë¥¼ ë¹Œë ¤ì™€ ìµœì†Œ ì±„ì›€ë¥  ìœ ì§€ ë¶€ëª¨ ë…¸ë“œì˜ í‚¤ë„ ì¡°ì • í•„ìš” b) ë…¸ë“œ ë³‘í•© (Merging): í˜•ì œ ë…¸ë“œì™€ ë³‘í•©í•˜ì—¬ í•˜ë‚˜ì˜ ë…¸ë“œë¡œ ë§Œë“¦ ë¶€ëª¨ ë…¸ë“œì˜ í‚¤ë„ ì œê±° í•„ìš” ì¬ê· í˜•í™”ì˜ ë³µì¡ì„±: ì‚­ì œëŠ” ì‚½ì…ë³´ë‹¤ ë³µì¡í•¨ ì—¬ëŸ¬ ë…¸ë“œì— ê±¸ì¹œ ì¡°ì •ì´ í•„ìš”í•  ìˆ˜ ìˆìŒ íŠ¸ë¦¬ ë†’ì´ ê°ì†Œ ê°€ëŠ¥ ` B-treeì˜ ì„±ëŠ¥ íŠ¹ì„±ê³¼ ìµœì í™” ì½ê¸° ì„±ëŠ¥ ë¶„ì„ ` ìˆœì°¨ ì½ê¸° (Range Query): ë¦¬í”„ ë…¸ë“œ ê°„ ì—°ê²° í¬ì¸í„°ë¡œ íš¨ìœ¨ì  ìˆœíšŒ ì˜ˆ: SELECT FROM users WHERE age BETWEEN 20 AND 30 ë¦¬í”„ ë…¸ë“œë§Œ ìˆœíšŒí•˜ë©´ ë˜ë¯€ë¡œ ë§¤ìš° ë¹ ë¦„ I/O íš¨ìœ¨ì„±: ì—°ì†ëœ í˜ì´ì§€ ì ‘ê·¼ìœ¼ë¡œ ë””ìŠ¤í¬ ìµœì í™” ëœë¤ ì½ê¸° (Point Query): íŠ¸ë¦¬ ë†’ì´ë§Œí¼ì˜ ë””ìŠ¤í¬ ì ‘ê·¼ í•„ìš” ì¼ë°˜ì ìœ¼ë¡œ 3-4ë²ˆì˜ I/Oë¡œ ì›í•˜ëŠ” ë°ì´í„° ì°¾ìŒ ì¸ë±ìŠ¤ê°€ ì˜ ì„¤ê³„ëœ ê²½ìš° ë§¤ìš° íš¨ìœ¨ì  ìºì‹œ ì¹œí™”ì : ìì£¼ ì ‘ê·¼í•˜ëŠ” ë…¸ë“œëŠ” ë©”ëª¨ë¦¬ì— ìœ ì§€ ` ì“°ê¸° ì„±ëŠ¥ ë¶„ì„ ` ì‚½ì… ì„±ëŠ¥: ë¦¬í”„ ë…¸ë“œê¹Œì§€ì˜ ê²½ë¡œ íƒìƒ‰: O(log n) ë…¸ë“œ ë¶„í•  ë°œìƒ ì‹œ: ì¶”ê°€ I/O í•„ìš” í‰ê· ì ìœ¼ë¡œ O(log n) ì‹œê°„ ë³µì¡ë„ ë¶„í•  ë¹ˆë„: Fill Factorì— ë”°ë¼ ê²°ì • ì‚­ì œ ì„±ëŠ¥: ê²€ìƒ‰ + ì¬ê· í˜•í™”: O(log n) ì¬ê· í˜•í™”ë¡œ ì¸í•œ ì¶”ê°€ I/O ë°œìƒ ì‚½ì…ë³´ë‹¤ ì•½ê°„ ëŠë¦¼ ì¬ê· í˜•í™” ì „ëµì— ë”°ë¼ ì„±ëŠ¥ ì°¨ì´ ` ìµœì í™” ê¸°ë²•ë“¤ B+tree (B-plus tree) ` B+treeì˜ í•µì‹¬ íŠ¹ì§•: ëª¨ë“  ë°ì´í„°ëŠ” ë¦¬í”„ ë…¸ë“œì—ë§Œ ì €ì¥ Internal ë…¸ë“œëŠ” í‚¤ì™€ í¬ì¸í„°ë§Œ í¬í•¨ ë¦¬í”„ ë…¸ë“œ ê°„ ì—°ê²° í¬ì¸í„°ë¡œ ìˆœì°¨ ì ‘ê·¼ ìµœì í™” B+tree vs B-tree ë¹„êµ: B-tree: ëª¨ë“  ë…¸ë“œì— ë°ì´í„° ì €ì¥ B+tree: ë¦¬í”„ ë…¸ë“œì—ë§Œ ë°ì´í„° ì €ì¥ B+treeì˜ ì¥ì : ìˆœì°¨ ì½ê¸° ì„±ëŠ¥ í–¥ìƒ Internal ë…¸ë“œ í¬ê¸° ê°ì†Œë¡œ ë” ë§ì€ í‚¤ ì €ì¥ ë²”ìœ„ ì¿¼ë¦¬ì— ìµœì í™” ë¦¬í”„ ë…¸ë“œë§Œ ìˆœíšŒí•˜ë©´ ëª¨ë“  ë°ì´í„° ì ‘ê·¼ ê°€ëŠ¥ ` ì••ì¶• ê¸°ë²• (Compression) ` Prefix Compression (ì ‘ë‘ì‚¬ ì••ì¶•): ì—°ì†ëœ í‚¤ì˜ ê³µí†µ ì ‘ë‘ì‚¬ ì œê±° ì˜ˆ: \"user_001\", \"user_002\", \"user_003\" ì••ì¶• í›„: \"user_\" + \"001\", \"002\", \"003\" ê³µê°„ ì ˆì•½: ê³µí†µ ì ‘ë‘ì‚¬ ê¸¸ì´ë§Œí¼ ì ˆì•½ Suffix Truncation (ì ‘ë¯¸ì‚¬ ì ˆë‹¨): í‚¤ì˜ ëë¶€ë¶„ì„ ì œê±°í•˜ì—¬ ê³µê°„ ì ˆì•½ ì˜ˆ: \"timestamp_2024_01_01_00_00_00\" ì ˆë‹¨ í›„: \"ts_20240101\" ì£¼ì˜: ê³ ìœ ì„± ë³´ì¥ í•„ìš” ì••ì¶•ì˜ íš¨ê³¼: ë…¸ë“œë‹¹ ë” ë§ì€ í‚¤ ì €ì¥ ê°€ëŠ¥ íŠ¸ë¦¬ ë†’ì´ ê°ì†Œë¡œ ê²€ìƒ‰ ì„±ëŠ¥ í–¥ìƒ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ ë””ìŠ¤í¬ I/O ê°ì†Œ ` ê³ ê¸‰ ìµœì í™” ê¸°ë²• ` Adaptive Merging: ì‚­ì œëœ í‚¤ê°€ ë§ì€ ë…¸ë“œë“¤ì„ ìë™ìœ¼ë¡œ ë³‘í•© ê³µê°„ íš¨ìœ¨ì„± í–¥ìƒ ì„±ëŠ¥ ì €í•˜ ë°©ì§€ Bulk Loading: ëŒ€ëŸ‰ ë°ì´í„° ì‚½ì… ì‹œ ìµœì í™” ë…¸ë“œ ë¶„í• ì„ ìµœì†Œí™” íŠ¸ë¦¬ êµ¬ì¡°ë¥¼ í•œ ë²ˆì— ìµœì í™” Parallel Processing: ì—¬ëŸ¬ ë…¸ë“œì˜ ë™ì‹œ ì²˜ë¦¬ ë©€í‹°ì½”ì–´ ì‹œìŠ¤í…œ í™œìš© ëŒ€ìš©ëŸ‰ ì¸ë±ìŠ¤ ë¹Œë“œ ì‹œê°„ ë‹¨ì¶• ` B-treeì˜ ì‹¤ì œ êµ¬í˜„ê³¼ ëª¨ë‹ˆí„°ë§ PostgreSQLì˜ B-tree êµ¬í˜„ `sql -- B-tree ì¸ë±ìŠ¤ ìƒì„± CREATE INDEX idx_users_age ON users(age); -- ì¸ë±ìŠ¤ ì‚¬ìš© í†µê³„ í™•ì¸ SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_stat_user_indexes WHERE indexname = 'idx_users_age'; -- ì¸ë±ìŠ¤ í¬ê¸° í™•ì¸ SELECT pg_size_pretty(pg_relation_size('idx_users_age')); -- ì¸ë±ìŠ¤ ìƒì„¸ ì •ë³´ SELECT FROM pg_stat_user_indexes WHERE indexname = 'idx_users_age'; -- ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  ë¶„ì„ SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch, CASE WHEN idx_scan > 0 THEN ROUND((idx_tup_fetch::float / idx_scan::float) 100, 2) ELSE 0 END as selectivity_percent FROM pg_stat_user_indexes WHERE schemaname = 'public'; ` MySQLì˜ B-tree êµ¬í˜„ `sql -- B-tree ì¸ë±ìŠ¤ ìƒì„± CREATE INDEX idx_users_age ON users(age); -- ì¸ë±ìŠ¤ ì‚¬ìš© í˜„í™© í™•ì¸ SHOW INDEX FROM users; -- ì¸ë±ìŠ¤ í†µê³„ ì •ë³´ SELECT FROM information_schema.statistics WHERE table_name = 'users' AND index_name = 'idx_users_age'; -- ì¸ë±ìŠ¤ í¬ê¸° í™•ì¸ SELECT table_name, index_name, ROUND(stat_value * @@innodb_page_size / 1024 / 1024, 2) as size_mb FROM mysql.innodb_index_stats WHERE stat_name = 'size' AND table_name = 'users'; -- ì¸ë±ìŠ¤ ì‚¬ìš©ë¥  ë¶„ì„ SELECT object_schema as database_name, object_name as table_name, index_name, count_read, count_write, count_fetch, count_insert, count_update, count_delete FROM performance_schema.table_io_waits_summary_by_index_usage WHERE object_schema = 'your_database'; ` B-treeì˜ í•œê³„ì™€ í˜„ëŒ€ì  ëŒ€ì•ˆ B-treeì˜ ê·¼ë³¸ì  í•œê³„ ` ë†’ì€ ì‚½ì…/ì‚­ì œ ë¹„ìš©: ë…¸ë“œ ë¶„í• /ë³‘í•©ìœ¼ë¡œ ì¸í•œ ì˜¤ë²„í—¤ë“œ ì¬ê· í˜•í™” ê³¼ì •ì˜ ë³µì¡ì„± ëœë¤ I/O íŒ¨í„´ ìˆœì°¨ ì‚½ì… ì‹œ ë¹„íš¨ìœ¨: ëœë¤í•œ í‚¤ ì‚½ì…ì— ìµœì í™” ìˆœì°¨ í‚¤ ì‚½ì… ì‹œ ë…¸ë“œ ë¶„í•  ë¹ˆë°œ ê³µê°„ ì§€ì—­ì„± ë¶€ì¡± ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: Internal ë…¸ë“œì˜ ë©”ëª¨ë¦¬ ì ìœ  í¬ì¸í„° ì˜¤ë²„í—¤ë“œ ìºì‹œ ë¯¸ìŠ¤ ë°œìƒ ìºì‹œ ì§€ì—­ì„±: ëœë¤ ì ‘ê·¼ìœ¼ë¡œ ì¸í•œ ìºì‹œ ë¯¸ìŠ¤ ë©”ëª¨ë¦¬ ê³„ì¸µ êµ¬ì¡°ì™€ì˜ ë¶ˆì¼ì¹˜ í˜„ëŒ€ CPU ì•„í‚¤í…ì²˜ì™€ì˜ ê¶í•© ` í˜„ëŒ€ì  ëŒ€ì•ˆ êµ¬ì¡°ë“¤ LSM Tree (Log-Structured Merge Tree) ` LSM Treeì˜ í•µì‹¬ ê°œë…: ì“°ê¸° ì„±ëŠ¥ì— ìµœì í™” ìˆœì°¨ ì“°ê¸°ë¡œ ë””ìŠ¤í¬ I/O ìµœì†Œí™” ì½ê¸° ì‹œ ì—¬ëŸ¬ ë ˆë²¨ ê²€ìƒ‰ í•„ìš” LSM Tree êµ¬ì¡°: Level 0 (Memory): ìµœì‹  ë°ì´í„°, ì •ë ¬ë˜ì§€ ì•ŠìŒ Level 1-N (Disk): ì •ë ¬ëœ ë°ì´í„°, í¬ê¸°ê°€ ì¦ê°€ ì¥ì : ë§¤ìš° ë†’ì€ ì“°ê¸° ì„±ëŠ¥ ìˆœì°¨ I/O íŒ¨í„´ìœ¼ë¡œ SSDì— ìµœì í™” ì••ì¶• íš¨ìœ¨ì„± ë‹¨ì : ì½ê¸° ì„±ëŠ¥ ì €í•˜ ê³µê°„ ì¦í­ (Space Amplification) ì»´íŒ©ì…˜ ì˜¤ë²„í—¤ë“œ ` Fractal Tree ` Fractal Treeì˜ íŠ¹ì§•: B-treeì˜ ë³€í˜•ìœ¼ë¡œ ì‚½ì… ì„±ëŠ¥ í–¥ìƒ ë²„í¼ë§ì„ í†µí•œ ë°°ì¹˜ ì²˜ë¦¬ ë©”ëª¨ë¦¬ì™€ ë””ìŠ¤í¬ì˜ ê· í˜•ì  ì°¾ê¸° ë™ì‘ ì›ë¦¬: ê° ë…¸ë“œì— ë²„í¼ ì¶”ê°€ ì‘ì€ ë³€ê²½ì‚¬í•­ì„ ë²„í¼ì— ëˆ„ì  ë²„í¼ê°€ ê°€ë“ ì°¨ë©´ ë°°ì¹˜ë¡œ ì²˜ë¦¬ ì¥ì : B-treeë³´ë‹¤ ë†’ì€ ì‚½ì… ì„±ëŠ¥ ìˆœì°¨ ì‚½ì…ì— ìµœì í™” ê¸°ì¡´ B-treeì™€ì˜ í˜¸í™˜ì„± ë‹¨ì : ë³µì¡í•œ êµ¬í˜„ ë©”ëª¨ë¦¬ ì˜¤ë²„í—¤ë“œ ë²„í¼ ê´€ë¦¬ì˜ ë³µì¡ì„± ` Bw-tree (Bw-tree) ` Bw-treeì˜ í˜ì‹ ì  íŠ¹ì§•: ë½ ì—†ëŠ” ë™ì‹œì„± ì œì–´ ë©”ëª¨ë¦¬ ë§¤í•‘ ê¸°ë°˜ êµ¬ì¡° ë¡œê·¸ êµ¬ì¡°í™”ëœ ì ‘ê·¼ í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜: Delta Node: ë³€ê²½ì‚¬í•­ì„ ë³„ë„ ë…¸ë“œì— ì €ì¥ Consolidation: ì£¼ê¸°ì ì¸ ë…¸ë“œ í†µí•© Split/Delete: ë…¼ë¦¬ì  ë¶„í• ê³¼ ì‚­ì œ ì¥ì : ë§¤ìš° ë†’ì€ ë™ì‹œì„± ë½ ê²½í•© ì—†ìŒ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ë‹¨ì : ë³µì¡í•œ êµ¬í˜„ ì½ê¸° ì„±ëŠ¥ ì €í•˜ ê°€ëŠ¥ ë©”ëª¨ë¦¬ ê´€ë¦¬ì˜ ë³µì¡ì„± ` B-treeì˜ ë¯¸ë˜ì™€ ë°œì „ ë°©í–¥ í˜„ëŒ€ì  ìš”êµ¬ì‚¬í•­ê³¼ì˜ ì¡°í™” ` ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬: í˜íƒ€ë°”ì´íŠ¸ ê·œëª¨ ë°ì´í„° ì§€ì› ë¶„ì‚° í™˜ê²½ì—ì„œì˜ íš¨ìœ¨ì„± í´ë¼ìš°ë“œ ë„¤ì´í‹°ë¸Œ ì•„í‚¤í…ì²˜ ì‹¤ì‹œê°„ ì²˜ë¦¬: ë§ˆì´í¬ë¡œì´ˆ ë‹¨ìœ„ ì‘ë‹µ ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ì²˜ë¦¬ ì´ë²¤íŠ¸ ê¸°ë°˜ ì•„í‚¤í…ì²˜ í•˜ì´ë¸Œë¦¬ë“œ ì›Œí¬ë¡œë“œ: ì½ê¸°/ì“°ê¸° ê· í˜• ë¶„ì„ê³¼ íŠ¸ëœì­ì…˜ì˜ í†µí•© ë‹¤ì–‘í•œ ë°ì´í„° íƒ€ì… ì§€ì› ` B-treeì˜ ì§„í™” ë°©í–¥ ` í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°: B-tree + LSM Tree ì¡°í•© ìƒí™©ì— ë”°ë¥¸ ë™ì  ì „í™˜ ìµœì ì˜ ì„±ëŠ¥ ë³´ì¥ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìµœì í™”: ì ‘ê·¼ íŒ¨í„´ í•™ìŠµ ìë™ ì¸ë±ìŠ¤ íŠœë‹ ì˜ˆì¸¡ì  ìµœì í™” í•˜ë“œì›¨ì–´ ìµœì í™”: NVMe SSD ìµœì í™” Persistent Memory í™œìš© GPU ê°€ì† ì²˜ë¦¬ `` B-treeëŠ” SQL ë°ì´í„°ë² ì´ìŠ¤ì˜ í•µì‹¬ ì¸ë±ìŠ¤ êµ¬ì¡°ë¡œ, 50ë…„ ì´ìƒ ê²€ì¦ëœ ì•ˆì •ì„±ê³¼ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ í˜„ëŒ€ì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„°ì™€ ê³ ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì— ë”°ë¼ LSM Tree, Fractal Tree, Bw-tree ê°™ì€ ëŒ€ì•ˆ êµ¬ì¡°ë“¤ì´ í•¨ê»˜ ì‚¬ìš©ë˜ê³  ìˆìœ¼ë©°, ì•ìœ¼ë¡œë„ í•˜ì´ë¸Œë¦¬ë“œ êµ¬ì¡°ì™€ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ìµœì í™”ë¥¼ í†µí•´ ê³„ì† ë°œì „í•  ê²ƒìœ¼ë¡œ ì˜ˆìƒë©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/aws-ec2-ebs",
    "title": "AWS EC2ì™€ EBS ì™„ë²½ ê°€ì´ë“œ",
    "date": "2025-08-14",
    "excerpt": "AWS EC2ì™€ EBSì˜ ê°œë…, ì‚¬ìš©ë²•, ê·¸ë¦¬ê³  EBS ë°±ì—… ê¸°ëŠ¥ì— ëŒ€í•œ ìƒì„¸í•œ ì„¤ëª…",
    "category": "Data Engineering",
    "tags": [
      "AWS",
      "EC2",
      "EBS",
      "Cloud Computing",
      "Infrastructure"
    ],
    "content": "AWS EC2ì™€ EBS ì™„ë²½ ê°€ì´ë“œ EC2 (Elastic Compute Cloud)ë€? EC2ëŠ” AWSì—ì„œ ì œê³µí•˜ëŠ” ê°€ìƒ ì„œë²„ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. ì‚¬ìš©ìê°€ í•„ìš”ì— ë”°ë¼ ì»´í“¨íŒ… ë¦¬ì†ŒìŠ¤ë¥¼ ìœ ì—°í•˜ê²Œ í™•ì¥í•˜ê±°ë‚˜ ì¶•ì†Œí•  ìˆ˜ ìˆëŠ” í´ë¼ìš°ë“œ ì»´í“¨íŒ… ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. EC2ì˜ ì£¼ìš” íŠ¹ì§• ê°€ìƒ ì„œë²„: ë¬¼ë¦¬ì  ì„œë²„ë¥¼ ê°€ìƒí™”í•˜ì—¬ ì œê³µ í™•ì¥ì„±: í•„ìš”ì— ë”°ë¼ ì¸ìŠ¤í„´ìŠ¤ ìˆ˜ë¥¼ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì¼ ìˆ˜ ìˆìŒ ë‹¤ì–‘í•œ ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…: CPU, ë©”ëª¨ë¦¬, ìŠ¤í† ë¦¬ì§€ ìš”êµ¬ì‚¬í•­ì— ë§ëŠ” ë‹¤ì–‘í•œ ì˜µì…˜ ì˜¨ë””ë§¨ë“œ ìš”ê¸ˆ: ì‚¬ìš©í•œ ë§Œí¼ë§Œ ë¹„ìš© ì§€ë¶ˆ EC2 ì‚¬ìš© ì‚¬ë¡€ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ í˜¸ìŠ¤íŒ… ì›¹ ì„œë²„, ì• í”Œë¦¬ì¼€ì´ì…˜ ì„œë²„ ìš´ì˜ ë¡œë“œ ë°¸ëŸ°ì„œì™€ í•¨ê»˜ ê³ ê°€ìš©ì„± í™•ë³´ ë°ì´í„° ì²˜ë¦¬ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë¶„ì„ ì‘ì—… ë°°ì¹˜ í”„ë¡œì„¸ì‹± ë° ETL ì‘ì—… ê°œë°œ ë° í…ŒìŠ¤íŠ¸ í™˜ê²½ ê°œë°œ, ìŠ¤í…Œì´ì§•, í…ŒìŠ¤íŠ¸ í™˜ê²½ êµ¬ì¶• CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê²Œì„ ì„œë²„ ì˜¨ë¼ì¸ ê²Œì„ ì„œë²„ ìš´ì˜ íŠ¸ë˜í”½ì— ë”°ë¥¸ ìë™ ìŠ¤ì¼€ì¼ë§ EBS (Elastic Block Store)ë€? EBSëŠ” EC2 ì¸ìŠ¤í„´ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ìˆëŠ” ë¸”ë¡ ë ˆë²¨ ìŠ¤í† ë¦¬ì§€ ì„œë¹„ìŠ¤ì…ë‹ˆë‹¤. í•˜ë“œ ë””ìŠ¤í¬ì™€ ìœ ì‚¬í•œ ê¸°ëŠ¥ì„ ì œê³µí•˜ë©°, EC2 ì¸ìŠ¤í„´ìŠ¤ì˜ ë¶€íŒ… ë””ìŠ¤í¬ë‚˜ ì¶”ê°€ ë°ì´í„° ìŠ¤í† ë¦¬ì§€ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. EBSì˜ ì£¼ìš” íŠ¹ì§• ë¸”ë¡ ë ˆë²¨ ìŠ¤í† ë¦¬ì§€: íŒŒì¼ ì‹œìŠ¤í…œì´ ì•„ë‹Œ ë¸”ë¡ ë‹¨ìœ„ë¡œ ë°ì´í„° ì €ì¥ ì˜êµ¬ ì €ì¥: ì¸ìŠ¤í„´ìŠ¤ê°€ ì¢…ë£Œë˜ì–´ë„ ë°ì´í„° ìœ ì§€ ë‹¤ì–‘í•œ ë³¼ë¥¨ íƒ€ì…: SSD, HDD ë“± ì„±ëŠ¥ê³¼ ë¹„ìš©ì— ë”°ë¥¸ ì„ íƒ ê°€ëŠ¥ ìŠ¤ëƒ…ìƒ· ê¸°ëŠ¥: ë°ì´í„° ë°±ì—… ë° ë³µêµ¬ ì§€ì› EBS ë³¼ë¥¨ íƒ€ì… SSD ê¸°ë°˜ ë³¼ë¥¨ gp3: ë²”ìš© SSD, ê· í˜•ì¡íŒ ì„±ëŠ¥ê³¼ ë¹„ìš© io2: ê³ ì„±ëŠ¥ SSD, ë†’ì€ IOPS ìš”êµ¬ì‚¬í•­ì— ì í•© HDD ê¸°ë°˜ ë³¼ë¥¨ st1: ì²˜ë¦¬ëŸ‰ ìµœì í™” HDD, ë¹…ë°ì´í„° ì›Œí¬ë¡œë“œì— ì í•© sc1: ì½œë“œ ìŠ¤í† ë¦¬ì§€, ìì£¼ ì ‘ê·¼í•˜ì§€ ì•ŠëŠ” ë°ì´í„°ì— ì í•© EBS ì‚¬ìš© ì‚¬ë¡€ ë¶€íŒ… ë””ìŠ¤í¬ EC2 ì¸ìŠ¤í„´ìŠ¤ì˜ ìš´ì˜ì²´ì œ ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì¹˜ ì‹œìŠ¤í…œ íŒŒì¼ ë° ì„¤ì • ì €ì¥ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í† ë¦¬ì§€ MySQL, PostgreSQL ë“±ì˜ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ ì €ì¥ ë†’ì€ IOPSê°€ í•„ìš”í•œ ì›Œí¬ë¡œë“œì— ì í•© íŒŒì¼ ì„œë²„ ì‚¬ìš©ì ì—…ë¡œë“œ íŒŒì¼ ì €ì¥ ê³µìœ  íŒŒì¼ ë° ë¬¸ì„œ ì €ì¥ ë°±ì—… ë° ë³µêµ¬ ì¤‘ìš”í•œ ë°ì´í„°ì˜ ìŠ¤ëƒ…ìƒ· ìƒì„± ì¬í•´ ë³µêµ¬ë¥¼ ìœ„í•œ ë°ì´í„° ë³´í˜¸ EC2ì™€ EBSì˜ ì—°ë™ ì—°ê²° ë°©ì‹ ë¶€íŒ… ë³¼ë¥¨: EC2 ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ì—°ê²°ë˜ëŠ” ë£¨íŠ¸ ë³¼ë¥¨ ì¶”ê°€ ë³¼ë¥¨: ì¸ìŠ¤í„´ìŠ¤ì— ì¶”ê°€ë¡œ ì—°ê²°í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ë³¼ë¥¨ ë³¼ë¥¨ ë¶„ë¦¬: ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ë³¼ë¥¨ì„ ë¶„ë¦¬í•˜ì—¬ ë‹¤ë¥¸ ì¸ìŠ¤í„´ìŠ¤ì— ì—°ê²° ê°€ëŠ¥ ì„±ëŠ¥ ìµœì í™” EBS ìµœì í™”: EC2 ì¸ìŠ¤í„´ìŠ¤ì™€ EBS ê°„ì˜ ë„¤íŠ¸ì›Œí¬ ì„±ëŠ¥ ìµœì í™” í”„ë¡œë¹„ì €ë‹ëœ IOPS: ì˜ˆì¸¡ ê°€ëŠ¥í•œ ì„±ëŠ¥ì„ ìœ„í•œ IOPS ì˜ˆì•½ ìŠ¤ë£¨í’‹ ìµœì í™”: ëŒ€ìš©ëŸ‰ ë°ì´í„° ì „ì†¡ì— ìµœì í™”ëœ ì²˜ë¦¬ëŸ‰ EBS ë°±ì—… ê¸°ëŠ¥ ìŠ¤ëƒ…ìƒ· (Snapshots) EBSì˜ í•µì‹¬ ë°±ì—… ê¸°ëŠ¥ìœ¼ë¡œ, íŠ¹ì • ì‹œì ì˜ ë³¼ë¥¨ ìƒíƒœë¥¼ ì €ì¥í•©ë‹ˆë‹¤. ìŠ¤ëƒ…ìƒ·ì˜ íŠ¹ì§• ì¦ë¶„ ë°±ì—…: ë³€ê²½ëœ ë¸”ë¡ë§Œ ë°±ì—…í•˜ì—¬ ì €ì¥ ê³µê°„ ì ˆì•½ ì§€ì—­ ê°„ ë³µì‚¬: ë‹¤ë¥¸ AWS ë¦¬ì „ìœ¼ë¡œ ë°±ì—… ë³µì‚¬ ê°€ëŠ¥ ìë™í™”: AWS Lambdaì™€ CloudWatchë¥¼ í†µí•œ ìë™ ìŠ¤ëƒ…ìƒ· ìƒì„± ë¹„ìš© íš¨ìœ¨ì„±: ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ìŠ¤ëƒ…ìƒ·ì€ ì‚­ì œí•˜ì—¬ ë¹„ìš© ì ˆì•½ ìŠ¤ëƒ…ìƒ· ìƒì„± ë°©ë²• ìˆ˜ë™ ìƒì„±: AWS ì½˜ì†”, CLI, SDKë¥¼ í†µí•œ ìˆ˜ë™ ìŠ¤ëƒ…ìƒ· ìƒì„± ìë™ ìƒì„±: ì •ì±… ê¸°ë°˜ ìë™ ìŠ¤ëƒ…ìƒ· ìƒì„± ë° ê´€ë¦¬ ìŠ¤ì¼€ì¤„ë§: ì •ê¸°ì ì¸ ë°±ì—… ìŠ¤ì¼€ì¤„ ì„¤ì • AWS Backup AWSì—ì„œ ì œê³µí•˜ëŠ” ì¤‘ì•™í™”ëœ ë°±ì—… ì„œë¹„ìŠ¤ë¡œ, EBS ë³¼ë¥¨ì„ í¬í•¨í•œ ë‹¤ì–‘í•œ AWS ë¦¬ì†ŒìŠ¤ë¥¼ ë°±ì—…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ í†µí•© ê´€ë¦¬: ì—¬ëŸ¬ AWS ì„œë¹„ìŠ¤ì˜ ë°±ì—…ì„ í•œ ê³³ì—ì„œ ê´€ë¦¬ ì •ì±… ê¸°ë°˜ ë°±ì—…: ë°±ì—… ì •ì±…ì„ ì •ì˜í•˜ì—¬ ìë™í™”ëœ ë°±ì—… ì‹¤í–‰ í¬ë¡œìŠ¤ ë¦¬ì „ ë°±ì—…: ì—¬ëŸ¬ ë¦¬ì „ì— ë°±ì—… ë³µì‚¬ë³¸ ì €ì¥ ê·œì • ì¤€ìˆ˜: ë‹¤ì–‘í•œ ê·œì • ë° í‘œì¤€ì— ë§ëŠ” ë°±ì—… ì •ì±… ì ìš© ë°±ì—… ì „ëµ 3-2-1 ë°±ì—… ê·œì¹™ 3ê°œì˜ ë°±ì—… ë³µì‚¬ë³¸ 2ê°œì˜ ë‹¤ë¥¸ ì €ì¥ ë§¤ì²´ 1ê°œì˜ ì˜¤í”„ì‚¬ì´íŠ¸ ë°±ì—… ë°±ì—… ì£¼ê¸° ì¼ì¼ ë°±ì—…: ì¤‘ìš”í•œ ë°ì´í„°ì˜ ê²½ìš° ì£¼ê°„ ë°±ì—…: ì¼ë°˜ì ì¸ ë°ì´í„°ì˜ ê²½ìš° ì›”ê°„ ë°±ì—…: ì¥ê¸° ë³´ê´€ì´ í•„ìš”í•œ ë°ì´í„°ì˜ ê²½ìš° ë°±ì—… ë³´ì¡´ ê¸°ê°„ ë‹¨ê¸° ë³´ì¡´: 30ì¼ ì´ë‚´ì˜ ë°±ì—… ì¤‘ê¸° ë³´ì¡´: 90ì¼ ì´ë‚´ì˜ ë°±ì—… ì¥ê¸° ë³´ì¡´: 1ë…„ ì´ìƒì˜ ë°±ì—… ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì¶• `` EC2 ì¸ìŠ¤í„´ìŠ¤ (t3.medium) â”œâ”€â”€ EBS gp3 ë³¼ë¥¨ (20GB) - ë¶€íŒ… ë””ìŠ¤í¬ â””â”€â”€ EBS io2 ë³¼ë¥¨ (100GB) - ë°ì´í„°ë² ì´ìŠ¤ ` ë°ì´í„° ë¶„ì„ í™˜ê²½ ` EC2 ì¸ìŠ¤í„´ìŠ¤ (r5.2xlarge) â”œâ”€â”€ EBS gp3 ë³¼ë¥¨ (50GB) - ë¶€íŒ… ë””ìŠ¤í¬ â”œâ”€â”€ EBS st1 ë³¼ë¥¨ (500GB) - ì›ì‹œ ë°ì´í„° â””â”€â”€ EBS gp3 ë³¼ë¥¨ (200GB) - ì²˜ë¦¬ëœ ë°ì´í„° `` ë¹„ìš© ìµœì í™” íŒ ì ì ˆí•œ ë³¼ë¥¨ íƒ€ì… ì„ íƒ: ì›Œí¬ë¡œë“œì— ë§ëŠ” ë³¼ë¥¨ íƒ€ì…ìœ¼ë¡œ ë¹„ìš© ì ˆì•½ ë¯¸ì‚¬ìš© ë³¼ë¥¨ ì •ë¦¬: ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” EBS ë³¼ë¥¨ ì‚­ì œ ìŠ¤ëƒ…ìƒ· ê´€ë¦¬: ì˜¤ë˜ëœ ìŠ¤ëƒ…ìƒ· ì •ë¦¬ ë° ë³´ì¡´ ê¸°ê°„ ìµœì í™” ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤: ì¥ê¸° ì‚¬ìš© ì‹œ ì˜ˆì•½ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë¹„ìš© ì ˆì•½ ê²°ë¡  AWS EC2ì™€ EBSëŠ” í´ë¼ìš°ë“œ ì¸í”„ë¼ì˜ í•µì‹¬ ì„œë¹„ìŠ¤ë¡œ, í•¨ê»˜ ì‚¬ìš©í•˜ì—¬ ì•ˆì •ì ì´ê³  í™•ì¥ ê°€ëŠ¥í•œ ì‹œìŠ¤í…œì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. EBSì˜ ê°•ë ¥í•œ ë°±ì—… ê¸°ëŠ¥ì„ í™œìš©í•˜ë©´ ë°ì´í„° ì†ì‹¤ ìœ„í—˜ì„ ìµœì†Œí™”í•˜ê³ , ë¹„ì¦ˆë‹ˆìŠ¤ ì—°ì†ì„±ì„ ë³´ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¬ë°”ë¥¸ EC2 ì¸ìŠ¤í„´ìŠ¤ íƒ€ì…ê³¼ EBS ë³¼ë¥¨ íƒ€ì…ì„ ì„ íƒí•˜ê³ , ì²´ê³„ì ì¸ ë°±ì—… ì „ëµì„ ìˆ˜ë¦½í•˜ì—¬ í´ë¼ìš°ë“œ í™˜ê²½ì„ íš¨ìœ¨ì ìœ¼ë¡œ ìš´ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ë¹„êµë°ë¹„ìœ /08-13-ì–´ì œì—ë¹„í•´ê³µê¸°ìƒíƒœê°€ë³„ë¡œì•¼",
    "title": "ì–´ì œì— ë¹„í•´ ê³µê¸° ìƒíƒœê°€ ë³„ë¡œì•¼",
    "date": "2025-08-13",
    "excerpt": "ë¹„êµí•  ë•Œ ì“°ëŠ” í‘œí˜„ë“¤ ì •ë¦¬",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "Compared to yesterday, the air quality is not so good. ë°°ìš´ ê²ƒë“¤ a dog person a cat person Are you a dog person or a cat person? morning person night owl Are you a morning person or a night owl? needy : ê¶í•í•œ, ê´€ì‹¬ì´ë‚˜ ì• ì •ì„ í•„ìš”ë¡œ í•˜ëŠ” social Compared to last week, the air quality is not so bad. outgoing : ì™¸í–¥ì ì¸ loyal : ì¶©ì„±ì‹¬ ìˆëŠ” Toy Story sequels are as good as the original sequel : í›„ì†ì‘ ë°œìŒíŒ! ê°•ì„¸ì‹ ê²½ì“°ê¸° SE-quel o-RI-ginal Sequels are not as good as original. Simple writing Today I learned",
    "public": true
  },
  {
    "id": "Data Science/Statistics/conformal-prediction-explained",
    "title": "Conformal Prediction: ë¶„í¬ ê°€ì • ì—†ì´ ìœ í•œ ìƒ˜í”Œ coverageë¥¼ ë³´ì¥í•˜ëŠ” ë°©ë²•",
    "date": "2025-08-13",
    "excerpt": "í†µê³„ì  ê°€ì • ì—†ì´ë„ finite-sample coverageë¥¼ ë³´ì¥í•˜ëŠ” conformal predictionì˜ ì›ë¦¬ì™€ ì‹¤ë¬´ ì ìš©ë²•",
    "category": "Data Science",
    "tags": [
      "statistics",
      "prediction-interval",
      "uncertainty",
      "calibration"
    ],
    "content": "ì°¸ê³ ìë£Œ 1 : ë¸”ë¡œê·¸ - Conformal Predictionìœ¼ë¡œ ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°í•˜ê¸° 2 : Paper - Conformal Prediction: A Gentle Introduction 3 : ë¸”ë¡œê·¸ - Conformal Prediction Paper Review - 2 --- Conformal Predictionì´ë€? Conformal predictionì€ ë¶„í¬ ê°€ì • ì—†ì´ë„ finite-sample coverageë¥¼ ë³´ì¥í•˜ëŠ” prediction intervalì„ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤. í•µì‹¬ íŠ¹ì§• ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”: ë°ì´í„°ê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ë“  ìƒê´€ì—†ìŒ Finite-sample coverage: ìœ í•œí•œ ìƒ˜í”Œì—ì„œë„ ëª©í‘œ coverage ë³´ì¥ ëª¨ë¸ agnostic: ì–´ë–¤ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ê³¼ë„ ì‚¬ìš© ê°€ëŠ¥ Calibration ìë™í™”: ë³„ë„ ì¡°ì • ì—†ì´ë„ coverage ë³´ì¥ ê¸°ë³¸ ì›ë¦¬ Exchangeability ê°€ì • Conformal predictionì˜ í•µì‹¬ ê°€ì •ì€ exchangeabilityì…ë‹ˆë‹¤: > ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„° $(x_{n+1}, y_{n+1})$ì™€ ê¸°ì¡´ ë°ì´í„° $(x_1, y_1), ..., (x_n, y_n)$ì´ exchangeableí•˜ë‹¤. Exchangeability: ë°ì´í„°ì˜ ìˆœì„œë¥¼ ë°”ê¿”ë„ í™•ë¥  ë¶„í¬ê°€ ë™ì¼ í•µì‹¬ ì•„ì´ë””ì–´ Nonconformity score ê³„ì‚°: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì¸¡ì • Calibration setì—ì„œ ë¶„ìœ„ìˆ˜ ê³„ì‚°: ëª©í‘œ coverageì— ë§ëŠ” ì„ê³„ê°’ ë„ì¶œ ìƒˆë¡œìš´ ì˜ˆì¸¡ì— ì ìš©: ê³„ì‚°ëœ ì„ê³„ê°’ìœ¼ë¡œ prediction interval êµ¬ì„± Split Conformal Prediction ê°€ì¥ ì‹¤ìš©ì ì´ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë‹¨ê³„ë³„ ê³¼ì • 1ë‹¨ê³„: ë°ì´í„° ë¶„í•  `` ì „ì²´ ë°ì´í„° â†’ Train Set + Calibration Set ` 2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ Train setìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ $\\hat{f}$ í•™ìŠµ 3ë‹¨ê³„: Nonconformity score ê³„ì‚° Calibration setì—ì„œ ê° ë°ì´í„°ì˜ nonconformity score ê³„ì‚° ëŒ€ì¹­ PIì˜ ê²½ìš°: $r_i = y_i - \\hat{f}(x_i) $ 4ë‹¨ê³„: ì„ê³„ê°’ ê³„ì‚° $\\hat{Q} = (1-\\alpha)(1+1/n_{cal})$-quantile of $\\{r_i\\}$ $n_{cal}$: calibration set í¬ê¸° 5ë‹¨ê³„: Prediction Interval êµ¬ì„± ìƒˆë¡œìš´ ì…ë ¥ $x$ì— ëŒ€í•´: $[\\hat{f}(x) - \\hat{Q}, \\hat{f}(x) + \\hat{Q}]$ Python êµ¬í˜„ ì˜ˆì‹œ `python import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor def split_conformal_prediction(X, y, alpha=0.1, test_size=0.2): # 1. ë°ì´í„° ë¶„í•  X_train, X_cal, y_train, y_cal = train_test_split( X, y, test_size=test_size, random_state=42 ) # 2. ëª¨ë¸ í•™ìŠµ model = RandomForestRegressor(random_state=42) model.fit(X_train, y_train) # 3. Calibration setì—ì„œ ì˜ˆì¸¡ y_cal_pred = model.predict(X_cal) # 4. Nonconformity scores ê³„ì‚° residuals = np.abs(y_cal - y_cal_pred) # 5. ì„ê³„ê°’ ê³„ì‚° n_cal = len(y_cal) quantile_level = (1 - alpha) (1 + 1/n_cal) threshold = np.quantile(residuals, quantile_level) return model, threshold ì‚¬ìš© ì˜ˆì‹œ model, threshold = split_conformal_prediction(X, y, alpha=0.1) ìƒˆë¡œìš´ ì˜ˆì¸¡ y_pred = model.predict(X_new) pi_lower = y_pred - threshold pi_upper = y_pred + threshold ` Conformalized Quantile Regression (CQR) Quantile regressionê³¼ conformal predictionì„ ê²°í•©í•œ ë°©ë²•ì…ë‹ˆë‹¤. CQR ê³¼ì • 1ë‹¨ê³„: Quantile ëª¨ë¸ í•™ìŠµ $\\tau = \\alpha/2$ (í•˜í•œ) $\\tau = 1 - \\alpha/2$ (ìƒí•œ) 2ë‹¨ê³„: Nonconformity score ê³„ì‚° $$ e_i = \\max\\big(\\hat{q}_{\\alpha/2}(x_i) - y_i, y_i - \\hat{q}_{1-\\alpha/2}(x_i), 0\\big) $$ 3ë‹¨ê³„: ì„ê³„ê°’ ê³„ì‚° $\\hat{Q} = (1-\\alpha)(1+1/n_{cal})$-quantile of $\\{e_i\\}$ 4ë‹¨ê³„: ìµœì¢… PI êµ¬ì„± $$ [\\hat{q}_{\\alpha/2}(x) - \\hat{Q}, \\hat{q}_{1-\\alpha/2}(x) + \\hat{Q}] $$ CQRì˜ ì¥ì  Heteroscedasticity ëŒ€ì‘: ë¶„ì‚°ì´ ì¼ì •í•˜ì§€ ì•Šì•„ë„ ì í•© Quantile ì •ë³´ í™œìš©: ë” ì •í™•í•œ PI êµ¬ì„± Coverage ë³´ì¥: Finite-sample marginal coverage CQR (Conformalized Quantile Regression) êµ¬í˜„ CQRì€ quantile regressionì„ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ prediction intervalsë¥¼ ìƒì„±í•©ë‹ˆë‹¤. `python import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.model_selection import train_test_split class ConformalizedQuantileRegression: def __init__(self, alpha=0.1): self.alpha = alpha self.lower_model = None self.upper_model = None self.tau = None def fit(self, X_train, y_train, X_cal, y_cal): \"\"\" CQR ëª¨ë¸ í•™ìŠµ ë° calibration (Romano et al., 2019) í•˜í•œ/ìƒí•œ quantile ëª¨ë¸ í•™ìŠµ calibration setìœ¼ë¡œ ì „ì—­ ì˜¤í”„ì…‹ tau ê³„ì‚° \"\"\" from sklearn.ensemble import GradientBoostingRegressor # 1) Quantile models: lower (Î±/2), upper (1-Î±/2) q_low = self.alpha / 2.0 q_high = 1.0 - self.alpha / 2.0 self.lower_model = GradientBoostingRegressor(loss=\"quantile\", alpha=q_low, random_state=42) self.upper_model = GradientBoostingRegressor(loss=\"quantile\", alpha=q_high, random_state=42) self.lower_model.fit(X_train, y_train) self.upper_model.fit(X_train, y_train) # 2) Calibration nonconformity scores lower_cal = self.lower_model.predict(X_cal) upper_cal = self.upper_model.predict(X_cal) # ì ìˆ˜ëŠ” interval ë°”ê¹¥ìœ¼ë¡œ ë‚˜ê°„ ì •ë„(ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ ì ˆë‹¨) scores = np.maximum.reduce([ lower_cal - y_cal, y_cal - upper_cal, np.zeros_like(y_cal) ]) # 3) Finite-sample quantile â†’ tau (ì „ì—­ ì˜¤í”„ì…‹) n_cal = len(scores) quantile_level = (1 - self.alpha) (1 + 1 / n_cal) self.tau = np.quantile(scores, quantile_level) return self def predict(self, X): \"\"\" ìµœì¢… Prediction Interval ìƒì„±: [q_low(X) - tau, q_high(X) + tau] \"\"\" ql = self.lower_model.predict(X) qh = self.upper_model.predict(X) # Quantile crossing ë°©ì§€: í•˜í•œ/ìƒí•œ ì •ë ¬ í›„ ì˜¤í”„ì…‹ ì ìš© q_low_aligned = np.minimum(ql, qh) q_high_aligned = np.maximum(ql, qh) lower = q_low_aligned - self.tau upper = q_high_aligned + self.tau return lower, upper def get_coverage(self, X_test, y_test): \"\"\"Test setì—ì„œ coverage ê³„ì‚°\"\"\" lower_bounds, upper_bounds = self.predict(X_test) # ì‹¤ì œê°’ì´ prediction interval ì•ˆì— ìˆëŠ” ë¹„ìœ¨ coverage = np.mean( (y_test >= lower_bounds) & (y_test <= upper_bounds) ) return coverage ì‚¬ìš© ì˜ˆì‹œ def demonstrate_cqr(): # ê°€ìƒ ë°ì´í„° ìƒì„± (heteroscedastic) np.random.seed(42) n_samples = 1000 X = np.random.uniform(0, 10, n_samples).reshape(-1, 1) # Heteroscedastic noise: Xê°€ í´ìˆ˜ë¡ noiseë„ ì»¤ì§ noise = np.random.normal(0, 0.1 + 0.05 X.flatten(), n_samples) y = 2 X.flatten() + noise # ë°ì´í„° ë¶„í•  X_train, X_temp, y_train, y_temp = train_test_split( X, y, test_size=0.4, random_state=42 ) X_cal, X_test, y_cal, y_test = train_test_split( X_temp, y_temp, test_size=0.5, random_state=42 ) # CQR ëª¨ë¸ í•™ìŠµ cqr = ConformalizedQuantileRegression(alpha=0.1) cqr.fit(X_train, y_train, X_cal, y_cal) # Prediction intervals ìƒì„± lower_bounds, upper_bounds = cqr.predict(X_test) # Coverage í™•ì¸ coverage = cqr.get_coverage(X_test, y_test) print(f\"CQR Coverage: {coverage:.3f}\") # Interval width í†µê³„ interval_widths = upper_bounds - lower_bounds print(f\"Average interval width: {np.mean(interval_widths):.3f}\") print(f\"Interval width std: {np.std(interval_widths):.3f}\") return cqr, X_test, y_test, lower_bounds, upper_bounds ì‹¤í–‰ if __name__ == \"__main__\": cqr_model, X_test, y_test, lower_bounds, upper_bounds = demonstrate_cqr() ` CQRì˜ í•µì‹¬ íŠ¹ì§• Quantile-based Approach Lower quantile (Î±/2)ì™€ upper quantile (1-Î±/2)ì„ ì§ì ‘ ëª¨ë¸ë§ ê° ì…ë ¥ Xì— ëŒ€í•´ conditional quantiles í•™ìŠµ Conformal Scores max(lower_pred - y, y - upper_pred) í˜•íƒœ ì‹¤ì œê°’ì´ prediction interval ë°–ì— ìˆëŠ” ì •ë„ë¥¼ ì¸¡ì • Adaptive Interval Width Quantile ëª¨ë¸: Xì— ë”°ë¼ ë‹¤ë¥¸ lower/upper bounds Adaptive Conformalization: Xì™€ ìœ ì‚¬í•œ calibration pointsë¡œ local threshold ê³„ì‚° ê²°ê³¼: Xì— ë”°ë¼ ë³€í•˜ëŠ” interval width + local coverage guarantee Heteroscedasticity ì²˜ë¦¬ Quantile ëª¨ë¸ì´ Xì— ë”°ë¥¸ noise ë³€í™”ë¥¼ í•™ìŠµ Local threshold: K-nearest neighborsë¡œ Xë³„ë¡œ ë‹¤ë¥¸ conformalization CRê³¼ì˜ ì°¨ì´: ê³ ì •ëœ widthê°€ ì•„ë‹Œ fully adaptive approach ì´ êµ¬í˜„ì€ Random Forestë¥¼ quantile regressionì— ì‚¬ìš©í–ˆì§€ë§Œ, Neural Networksë‚˜ Gradient Boosting ë“± ë‹¤ë¥¸ ëª¨ë¸ë¡œë„ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. ì‹¤ë¬´ ì ìš© íŒ ë°ì´í„° ë¶„í•  ì „ëµ Train/Calibration ë¹„ìœ¨: ë³´í†µ 80:20 ë˜ëŠ” 70:30 Stratified sampling: ë¶„ë¥˜ ë¬¸ì œì—ì„œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€ Time series: ì‹œê°„ ìˆœì„œ ê³ ë ¤í•œ ë¶„í•  Nonconformity Score ì„ íƒ íšŒê·€: $ y - \\hat{f}(x) $ (ì ˆëŒ“ê°’) ë¶„ë¥˜: $1 - \\hat{p}_y(x)$ (ì˜ˆì¸¡ í™•ë¥ ì˜ ë³´ì™„) ë¹„ëŒ€ì¹­: $y - \\hat{f}(x)$ (ë°©í–¥ì„± ê³ ë ¤) Coverage vs Width ê· í˜• ë†’ì€ coverage: ë” ë„“ì€ PI ì¢ì€ PI: ë‚®ì€ coverage ì‹¤ë¬´ì  trade-off: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ê³ ë ¤ ì¥ì ê³¼ í•œê³„ ì¥ì  ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”: ì–´ë–¤ ë°ì´í„°ë“  ì‚¬ìš© ê°€ëŠ¥ Finite-sample ë³´ì¥: ìœ í•œí•œ ìƒ˜í”Œì—ì„œë„ coverage ë³´ì¥ ëª¨ë¸ ë…ë¦½ì : ì–´ë–¤ ML ëª¨ë¸ê³¼ë„ ê²°í•© ê°€ëŠ¥ Calibration ìë™: ë³„ë„ ì¡°ì • ë¶ˆí•„ìš” í•œê³„ Marginal coverageë§Œ ë³´ì¥: Conditional coverageëŠ” ë³´ì¥í•˜ì§€ ì•ŠìŒ ë°ì´í„° ë¶„í•  í•„ìš”: í•™ìŠµ ë°ì´í„° ì¼ë¶€ë¥¼ calibrationì— ì‚¬ìš© Computational cost: ì¶”ê°€ ê³„ì‚° ë¹„ìš© Exchangeability ê°€ì •: ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•œ ë°ì´í„°ì—ëŠ” ë¶€ì í•© ìµœì‹  ë°œì „ ë™í–¥ Adaptive Conformal Prediction ì˜¨ë¼ì¸ í•™ìŠµ í™˜ê²½ì—ì„œ coverage ìœ ì§€ Concept driftì— ì ì‘ Conformal Prediction for Time Series ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„± ê³ ë ¤ Autocorrelation ì²˜ë¦¬ Multi-output Conformal Prediction ë‹¤ì¤‘ ì¶œë ¥ ë³€ìˆ˜ ë™ì‹œ ì²˜ë¦¬ ì¶œë ¥ ê°„ ìƒê´€ê´€ê³„ ê³ ë ¤ Conformal Prediction with Deep Learning ì‹ ê²½ë§ ëª¨ë¸ê³¼ì˜ ê²°í•© Uncertainty quantification ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸ êµ¬í˜„ ì „ í™•ì¸ì‚¬í•­ [ ] ë°ì´í„°ì˜ exchangeability ê°€ì • ê²€í†  [ ] ì ì ˆí•œ ë°ì´í„° ë¶„í•  ì „ëµ ìˆ˜ë¦½ [ ] Nonconformity score í•¨ìˆ˜ ì„ íƒ [ ] ëª©í‘œ coverage level ê²°ì • êµ¬í˜„ í›„ ê²€ì¦ì‚¬í•­ [ ] Calibration plotìœ¼ë¡œ coverage í™•ì¸ [ ] PI widthì˜ ì ì ˆì„± ê²€í†  [ ] Out-of-distribution ë°ì´í„°ì—ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ [ ] ê³„ì‚° ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì  í™•ì¸ ê²°ë¡  Conformal predictionì€ ë¶„í¬ ê°€ì • ì—†ì´ë„ reliableí•œ prediction intervalì„ ì œê³µí•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. ì£¼ìš” í™œìš© í¬ì¸íŠ¸: Uncertainty quantificationì´ ì¤‘ìš”í•œ ì‹¤ë¬´ ë¬¸ì œ ë¶„í¬ ê°€ì •ì„ í•˜ê¸° ì–´ë ¤ìš´ ìƒí™© Finite-sample coverage ë³´ì¥ì´ í•„ìš”í•œ ê²½ìš° ê¸°ì¡´ ML ëª¨ë¸ì— uncertainty ì •ë³´ ì¶”ê°€ ë‹¤ë§Œ, marginal coverageë§Œ ë³´ì¥í•œë‹¤ëŠ” í•œê³„ë¥¼ ì´í•´í•˜ê³ , ë°ì´í„° ë¶„í• ê³¼ computational costë¥¼ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. Coverageë€? CoverageëŠ” prediction intervalì´ ì‹¤ì œ ê°’ì„ í¬í•¨í•˜ëŠ” ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì˜ˆì‹œ: 90% prediction intervalì„ ë§Œë“¤ì—ˆë‹¤ë©´ 100ë²ˆì˜ ì˜ˆì¸¡ ì¤‘ 90ë²ˆì€ ì‹¤ì œ ê°’ì´ êµ¬ê°„ ì•ˆì— ë“¤ì–´ê°€ì•¼ í•¨ ì´ê²Œ 90% coverageì…ë‹ˆë‹¤ Marginal vs Conditional Coverage Marginalì€ \"ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ëª¨ë‘ í•©ì‚°(ë˜ëŠ” ì ë¶„)í•´ì„œ ì œê±°í•œ í›„ ë‚¨ì€ ê²ƒ\"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. Marginal Coverage ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í‰ê·  coverage ëª¨ë“  ì…ë ¥ê°’ $X$ì— ëŒ€í•´ í‰ê· ì ìœ¼ë¡œ ëª©í‘œ coverage ë‹¬ì„± ê°œë³„ $X$ ê°’ì—ì„œëŠ” coverageê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ Conditional Coverage ê° ì…ë ¥ê°’ $X$ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ ëª©í‘œ coverage ë‹¬ì„± ëª¨ë“  $X$ì—ì„œ ë™ì¼í•œ coverage ë³´ì¥ êµ¬ì²´ì  ì˜ˆì‹œ Marginal Coverage (90%): ` X=1ì¼ ë•Œ: 95% coverage (ì‹¤ì œ 95ë²ˆ/100ë²ˆ í¬í•¨) X=2ì¼ ë•Œ: 85% coverage (ì‹¤ì œ 85ë²ˆ/100ë²ˆ í¬í•¨) X=3ì¼ ë•Œ: 90% coverage (ì‹¤ì œ 90ë²ˆ/100ë²ˆ í¬í•¨) â†’ í‰ê· : 90% coverage âœ… ` Conditional Coverage (90%): ` X=1ì¼ ë•Œ: 90% coverage (ì •í™•íˆ 90ë²ˆ/100ë²ˆ í¬í•¨) X=2ì¼ ë•Œ: 90% coverage (ì •í™•íˆ 90ë²ˆ/100ë²ˆ í¬í•¨) X=3ì¼ ë•Œ: 90% coverage (ì •í™•íˆ 90ë²ˆ/100ë²ˆ í¬í•¨) â†’ ëª¨ë“  Xì—ì„œ 90% coverage âœ… `` ì™œ Marginal Coverageë§Œ ë³´ì¥í•˜ëŠ”ê°€? Conformal predictionì˜ í•œê³„: Marginal coverageëŠ” ë³´ì¥ âœ… Conditional coverageëŠ” ë³´ì¥í•˜ì§€ ì•ŠìŒ âŒ ì´ìœ : ë°ì´í„° ë¶„í• ì„ í†µí•´ ì „ì²´ì ì¸ coverageë§Œ ì¡°ì • ê°œë³„ $X$ ê°’ë³„ë¡œëŠ” coverageê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ ë” ì •í™•í•œ conditional coverageë¥¼ ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ ë°©ë²• í•„ìš” ì‹¤ë¬´ì  ì˜ë¯¸: ì „ì²´ì ìœ¼ë¡œëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” êµ¬ê°„ í•˜ì§€ë§Œ íŠ¹ì • ì…ë ¥ê°’ì—ì„œëŠ” ë„ˆë¬´ ë„“ê±°ë‚˜ ì¢ì„ ìˆ˜ ìˆìŒ ì´ëŠ” conformal predictionì˜ ê·¼ë³¸ì ì¸ í•œê³„ì…ë‹ˆë‹¤",
    "public": true
  },
  {
    "id": "Data Science/Paper-Review/A-Gentle-Introduction-to-Conformal-Prediction-and-Distribution-Free-Uncertainty-Quantification",
    "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification",
    "date": "2025-08-13",
    "excerpt": "Conformal Prediction ê´€ë ¨ Paper Review",
    "category": "Data Science",
    "tags": [
      "statistics",
      "prediction-interval",
      "uncertainty",
      "calibration",
      "paper-review"
    ],
    "content": "Abstact > Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. ë˜ê²Œ ê³µê°ì´ ë˜ëŠ” ë¶€ë¶„ì´ë‹¤. ëª¨ë¸ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ”ê²Œ êµ‰ì¥íˆ ì¤‘ìš”í•¨. > Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. distribution-free ì˜ íŠ¹ì§•ì„ ê°€ì§. --- Conformal Prediction <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure1.png\" alt=\"paper_figure1\" width=\"100%\" /> <figcaption>Figure 1: Prediction set examples on Imagenet. We show three progressively more difficult examples of the class fox squirrel and the prediction sets (i.e., C(Xtest)) generated by conformal prediction.</figcaption> </figure> ì´ë¯¸ì§€ë³„ë¡œ ì˜ˆì¸¡ê°’ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì´ ë‹¤ë¥¸ë°, ì´ê±¸ ì–´ë–»ê²Œ ìˆ˜ì¹˜ì ìœ¼ë¡œ ì •ëŸ‰í™”í•´ì„œ í‘œí˜„í•  ê²ƒì¸ê°€. ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•´ì„œ í‘œí˜„í•  ìˆ˜ ìˆëŠ”, prediction intervalì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆë‹¤. (ì‹ 1) $$ 1 - \\alpha \\leq \\mathbb{P}(Y_{test} \\in C(X_{test})) \\leq 1 - \\alpha + \\frac{1}{n + 1} \\tag{1} $$ > In words, the probability that the prediction set contains the correct label is almost exactly 1 âˆ’ Î±; we call this property marginal coverage, since the probability is marginal (averaged) over the randomness in the calibration and test points. calibration stepì„ í†µí•´ prediction setì„ ìƒì„±. calibration step : <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure2.png\" alt=\"paper_figure2\" width=\"100%\" /> <figcaption>Figure 2: Illustration of conformal prediction with matching Python code.</figcaption> </figure> set the conformal score $s_i = 1 - \\hat f(X_i)_{Y_i}$ : (ì˜ˆì‹œ) 1 - softmax output Scoreê°€ ë†’ì„ìˆ˜ë¡ ëª¨ë¸ì´ í•´ë‹¹ í´ë˜ìŠ¤ë¥¼ ì˜ëª» ì˜ˆì¸¡í–ˆë‹¤ëŠ” ì˜ë¯¸ define $\\hat q$ to be the $[(n+1)(1 - \\alpha)] / n$ empirical quantile of $\\{s_1, ..., s_n\\}$ $\\lceil \\cdot \\rceil$: ceiling function (ì˜¬ë¦¼ í•¨ìˆ˜) ë³¸ì§ˆì ìœ¼ë¡œëŠ” $(1-\\alpha)$ quantileì´ì§€ë§Œ ì‘ì€ ë³´ì •ê°’ í¬í•¨ creat a prediction set ìƒˆë¡œìš´ í…ŒìŠ¤íŠ¸ ë°ì´í„° $X_{test}$ì— ëŒ€í•´: $C(X_{test}) = \\{y : \\hat{f}(X_{test})_y \\geq 1 - \\hat{q}\\}$ ì¶©ë¶„íˆ ë†’ì€ softmax ì¶œë ¥ì„ ê°€ì§„ ëª¨ë“  í´ë˜ìŠ¤ë¥¼ í¬í•¨ Remarks : > Let us think about the interpretation of C Set-valued Function CëŠ” set-valued functionì…ë‹ˆë‹¤ Input: image Output: set of classes (Figure 1 ì°¸ì¡°) Softmax Output í™œìš© ëª¨ë¸ì˜ softmax outputì„ ì‚¬ìš©í•´ì„œ ì§‘í•©ì„ ìƒì„± ê° ì…ë ¥ì— ëŒ€í•´ adaptively different output setì„ êµ¬ì„± Uncertaintyì— ë”°ë¥¸ Set Size ë³€í™” Modelì´ uncertainí•  ë•Œ: setì´ ì»¤ì§ Imageê°€ intrinsically hardí•  ë•Œ: setì´ ì»¤ì§ ì´ëŠ” desirable propertyì…ë‹ˆë‹¤! Set Sizeì˜ ì˜ë¯¸ Set size = Model certainty indicator Setì´ í´ìˆ˜ë¡ â†’ Modelì´ uncertain Setì´ ì‘ì„ìˆ˜ë¡ â†’ Modelì´ certain Interpretation C(X_test)ëŠ” \"plausible classes that image X_test could be assigned to\" ì¦‰, ê°€ëŠ¥ì„±ì´ ìˆëŠ” classesë¥¼ ëª¨ë‘ í¬í•¨ Mathematical Guarantee CëŠ” valid: ìˆ˜ì‹ (1)ì„ ë§Œì¡± ìˆ˜ì‹ (1): $\\mathbb{P}(Y_{test} \\in C(X_{test})) \\geq 1-\\alpha$ Generalization ì´ëŸ¬í•œ Cì˜ propertiesë“¤ì€ ë‹¤ë¥¸ ML problemsì—ë„ ìì—°ìŠ¤ëŸ½ê²Œ ì ìš© Regression problemsì—ì„œë„ ë™ì¼í•œ ì›ë¦¬ ì‚¬ìš© ê°€ëŠ¥ > However, we had no guarantee that the softmax outputs were any good; they may have been arbitrarily overfit or otherwise untrustworthy. > Therefore, instead of taking the softmax outputs at face value, we used the holdout set to adjust for their deficiencies. Softmax Outputì˜ ë¬¸ì œì  Softmax outputì€ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•˜ëŠ”ë° heuristicí•˜ê²Œ ì‚¬ìš©ë¨ ì—„ë°€í•œ í™•ë¥ ì„ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ìŒ Overfittingì´ë‚˜ untrustworthyí•  ìˆ˜ ìˆìŒ Holdout Setì„ í†µí•œ ë³´ì • Holdout Setì˜ ì—­í• : n â‰ˆ 500ê°œì˜ fresh data points Training ì¤‘ì— ëª¨ë¸ì´ ë³´ì§€ ëª»í•œ ë°ì´í„° ëª¨ë¸ ì„±ëŠ¥ì˜ honest appraisal ì œê³µ Conformal Score ê³„ì‚°: Conformal score = $1 - \\hat{f}(X_i)_{Y_i}$ Modelì´ uncertainí•  ë•Œ scoreê°€ ì»¤ì§ Quantile ê¸°ë°˜ ì„ê³„ê°’ ì„¤ì • ì„ê³„ê°’ ê³„ì‚°: $\\hat{q} \\approx (1-\\alpha)$ quantile of scores Î± = 0.1ì¼ ë•Œ: 90%ì˜ ground truth softmax outputsê°€ $1-\\hat{q}$ level ì´ìƒì„ì„ ë³´ì¥ Test-time ì ìš©: ìƒˆë¡œìš´ ì´ë¯¸ì§€ X_testì˜ softmax outputs íšë“ $1-\\hat{q}$ ì´ìƒì¸ ëª¨ë“  classesë¥¼ prediction set C(X_test) ì— ìˆ˜ì§‘ ìƒˆë¡œìš´ true class Y_testì˜ softmax outputì´ $1-\\hat{q}$ ì´ìƒì¼ í™•ë¥ ì´ ìµœì†Œ 90% ë³´ì¥ --- 1.1 Instructions for Conformal Prediction --- Examples of Conformal Procedures 2.1 Classification with Adaptive Prediction Sets > The previous method produces prediction sets with the smallest average size [6], but it tends to undercover hard subgroups and overcover easy ones. > Here we develop a different method called adaptive prediction sets (APS) that avoids this problem. êµ¬ê°„ì˜ ë‘ê»˜ë¥¼ Xì— ë§ê²Œ ì¡°ì ˆí•˜ëŠ” ë°©ë²• ì œì‹œ <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure3.png\" alt=\"paper_figure3\" width=\"100%\" /> <figcaption>Figure 3: Python code for adaptive prediction sets.</figcaption> </figure> $$ C(x) = \\{\\pi_1(x), \\ldots, \\pi_k(x)\\}, \\text{ where } k = \\sup\\left\\{k_0 : \\sum_{j=1}^{k_0} \\hat{f}(x)_{\\pi_j(x)} < \\hat{q}\\right\\} + 1. \\tag{3} $$ > APSëŠ” \"ê° ì…ë ¥ì— ëŒ€í•´ 1-alpha coverageë¥¼ ì •í™•íˆ ë§Œì¡±í•˜ëŠ” ìµœì†Œí•œì˜ prediction set\"ì„ ë§Œë“œëŠ” ë°©ë²• Threshold ë°©ì‹ ê¸°ì¡´: ëª¨ë“  í´ë˜ìŠ¤ì— ë™ì¼í•œ threshold ì ìš© APS: ê° ì…ë ¥ xë§ˆë‹¤ ë‹¤ë¥¸ threshold (1-alpha ë§Œì¡±í•  ë•Œê¹Œì§€) Coverage ë³´ì¥ ê¸°ì¡´: Marginal coverageë§Œ ë³´ì¥ (ì „ì²´ í‰ê· ) APS: ê° ì…ë ¥ xì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ 1-alpha coverage ë‹¬ì„± Set Size ê¸°ì¡´: ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ ë¹„ìŠ·í•œ í¬ê¸°ì˜ prediction set APS: ì…ë ¥ë³„ë¡œ ì ì‘ì ìœ¼ë¡œ í¬ê¸° ì¡°ì ˆ ì˜ˆì‹œ ì‰¬ìš´ ì´ë¯¸ì§€: [0.8, 0.15, 0.05] â†’ APS: [Class 1] (1ê°œ) ì–´ë ¤ìš´ ì´ë¯¸ì§€: [0.4, 0.3, 0.2, 0.1] â†’ APS: [Class 1, Class 2, Class 3] (3ê°œ) ê¸°ì¡´ ë°©ë²• : > ê³ ì •ëœ quantile threshold ì‚¬ìš© ``python threshold = np.quantile(softmax_scores, 1 - alpha) prediction_set = [class for class in all_classes if softmax_score[class] >= threshold] ` ë¬¸ì œì : Undercover hard subgroups: ì–´ë ¤ìš´ ë°ì´í„°ëŠ” ë„ˆë¬´ ì ì€ í´ë˜ìŠ¤ í¬í•¨ â†’ coverage ë¶€ì¡± Overcover easy subgroups: ì‰¬ìš´ ë°ì´í„°ëŠ” ë„ˆë¬´ ë§ì€ í´ë˜ìŠ¤ í¬í•¨ â†’ ë¶ˆí•„ìš”í•˜ê²Œ ë„“ìŒ APS (Adaptive Prediction Sets) : <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure4.png\" alt=\"paper_figure4\" width=\"100%\" /> <figcaption>Figure 4: A visualization of the adaptive prediction sets algorithm in Eq. (3). Classes are included from most to least likely until their cumulative softmax output exceeds the quantile.</figcaption> </figure> `python APS: 1-alphaë¥¼ ë§Œì¡±í•  ë•Œê¹Œì§€ greedyí•˜ê²Œ ì¶”ê°€ def build_adaptive_set(softmax_scores, alpha): sorted_classes = np.argsort(softmax_scores)[::-1] # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ cumulative_prob = 0 prediction_set = [] for class_idx in sorted_classes: cumulative_prob += softmax_scores[class_idx] prediction_set.append(class_idx) if cumulative_prob >= (1 - alpha): # 1-alpha ë§Œì¡±í•˜ë©´ ì¤‘ë‹¨ break return prediction_set `` --- 2.2 Conformalized Quantile Regression <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure5.png\" alt=\"paper_figure5\" width=\"100%\" /> <figcaption>Figure 5: Python code for conformalized quantile regression.</figcaption> </figure> > However, because the fitted quantiles may be inaccurate, we will conformalize them. Python pseudocode for conformalized quantile regression is in Figure 5. ë¶„ë¥˜ ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ quantile regressionì´ ì •í™•í•˜ì§€ ì•Šìœ¼ë‹ˆ, conforming í•´ì¤€ë‹¤. $$ s(x, y) = \\max\\left\\{ \\hat{q}_{\\alpha/2}(x) - y, y - \\hat{q}_{1-\\alpha/2}(x) \\right\\} $$ > After computing the scores on our calibration set and setting $\\hat{q} = \\text{Quantile}(s_1, \\ldots, s_n; \\frac{\\lceil(n+1)(1-\\alpha)\\rceil}{n})$, we can form valid prediction intervals by taking $$ C(x) = \\left[\\hat{q}_{\\alpha/2}(x) - \\hat{q}, \\hat{q}_{1-\\alpha/2}(x) + \\hat{q}\\right]. \\tag{4} $$ <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure6.png\" alt=\"paper_figure6\" width=\"100%\" /> <figcaption>Figure 6: A visualization of the conformalized quantile regrssion algorithm in Eq. (4). We adjust the quantiles by the constant qË†, picked during the calibration step.</figcaption> </figure> --- 2.3 Conformalizing Scalar Uncertainty Estimates 2.3.1 The Estimated Standard Deviation > As an alternative to quantile regression, our next example is a different way of constructing prediction sets for continuous y with a less rich but more common notion of heuristic uncertainty: an estimate of the standard deviation $\\hat Ïƒ(x)$. ê¸°ì¡´ì˜ uncertainty quantification ë°©ë²•ì€ Y_test X_test = xê°€ ê°€ìš°ì‹œì•ˆ ë¶„í¬ N(Î¼(x), Ïƒ(x))ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³ , ëª¨ë¸ fÌ‚(x)ì™€ ÏƒÌ‚(x)ë¥¼ í•™ìŠµì‹œì¼œ ê°ê° í‰ê· ê³¼ ë¶„ì‚°ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì´ ë°©ë²•ì€ PyTorchì˜ GaussianNLLLoss ê°™ì€ ë‚´ì¥ ì†ì‹¤ í•¨ìˆ˜ë¡œ ì‰½ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆì–´ ë„ë¦¬ ì‚¬ìš©ë˜ì§€ë§Œ, ì‹¤ì œë¡œëŠ” Y_test X_testê°€ ê°€ìš°ì‹œì•ˆì´ ì•„ë‹Œ ê²½ìš°ê°€ ë§ì•„ì„œ ÏƒÌ‚(x)ê°€ ì‹ ë¢°í•  ìˆ˜ ì—†ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ conformal predictionì„ ì‚¬ìš©í•˜ì—¬ ì´ëŸ¬í•œ heuristicí•œ uncertainty ê°œë…ì„ fÌ‚(x) Â± qÌ‚ÏƒÌ‚(x) í˜•íƒœì˜ rigorousí•œ prediction intervalë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ 2.3.2 Other 1-D Uncertainty Estimates --- Evaluating Conformal Prediction ì´ ë‚´ìš©ì„ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•˜ë©´: Evaluating adaptivity: í‰ê·  set sizeê°€ ê°€ì¥ ì‘ì€ conformal prediction ì ˆì°¨ê°€ ë°˜ë“œì‹œ ìµœì„ ì€ ì•„ë‹ˆë¼ëŠ” ì ì„ ëª…ì‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì¢‹ì€ conformal prediction ì ˆì°¨ëŠ” ì‰¬ìš´ ì…ë ¥ì—ì„œëŠ” ì‘ì€ setì„, ì–´ë ¤ìš´ ì…ë ¥ì—ì„œëŠ” í° setì„ ì œê³µí•˜ì—¬ ëª¨ë¸ì˜ uncertaintyë¥¼ ì¶©ì‹¤íˆ ë°˜ì˜í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ì‘ì„±ì€ conformal predictionì˜ coverage guaranteeì—ì„œ ìë™ìœ¼ë¡œ ë³´ì¥ë˜ì§€ ì•Šì§€ë§Œ, ì‹¤ì œ ë°°í¬ì—ì„œëŠ” í•„ìˆ˜ì ì…ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì ì‘ì„±ì„ ê³µì‹í™”í•˜ê³ , ê·¸ ê²°ê³¼ë¥¼ íƒêµ¬í•˜ë©°, í‰ê°€ë¥¼ ìœ„í•œ ì‹¤ìš©ì ì¸ ì•Œê³ ë¦¬ì¦˜ì„ ì œì•ˆí•  ê²ƒì…ë‹ˆë‹¤. Correctness checks: ì •í™•ì„± ê²€ì‚¬ëŠ” conformal predictionì„ ì˜¬ë°”ë¥´ê²Œ êµ¬í˜„í–ˆëŠ”ì§€ í…ŒìŠ¤íŠ¸í•˜ëŠ” ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” coverageê°€ Theorem 1ì„ ë§Œì¡±í•˜ëŠ”ì§€ ê²½í—˜ì ìœ¼ë¡œ í™•ì¸í•  ê²ƒì…ë‹ˆë‹¤. ì´ ì†ì„±ì´ ì„±ë¦½í•˜ëŠ”ì§€ ì—„ê²©í•˜ê²Œ í‰ê°€í•˜ë ¤ë©´ ì‹¤ì œ ë°ì´í„°ì…‹ì˜ finite-sample ë³€ë™ì„±ì„ ì‹ ì¤‘í•˜ê²Œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” coverageì—ì„œ 1-Î±ë¡œë¶€í„°ì˜ í¸ì°¨ë¥¼ ê´€ì°°í•  ë•Œ ë°œìƒí•˜ëŠ” benign fluctuationsì˜ í¬ê¸°ì— ëŒ€í•œ ëª…ì‹œì  ê³µì‹ì„ ê°œë°œí•©ë‹ˆë‹¤. 3.1 Evaluating Adaptivity",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/quantile-regression-explained",
    "title": "Quantile Regression: í‰ê· ì´ ì•„ë‹Œ ë¶„ìœ„ìˆ˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•",
    "date": "2025-08-13",
    "excerpt": "ì „í†µì ì¸ íšŒê·€ë¶„ì„ê³¼ ë‹¬ë¦¬ íŠ¹ì • ë¶„ìœ„ìˆ˜ë¥¼ ì§ì ‘ ì˜ˆì¸¡í•˜ëŠ” quantile regressionì˜ ê°œë…ê³¼ í™œìš©ë²•",
    "category": "Data Science",
    "tags": [
      "statistics",
      "regression",
      "quantile",
      "prediction-interval"
    ],
    "content": "Quantile Regressionì´ë€? Quantile regressionì€ ì „í†µì ì¸ íšŒê·€ë¶„ì„ê³¼ ë‹¬ë¦¬ íŠ¹ì • ë¶„ìœ„ìˆ˜(quantile)ë¥¼ ì§ì ‘ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì „í†µì  íšŒê·€ë¶„ì„ vs Quantile Regression ì „í†µì  íšŒê·€ë¶„ì„ (OLS): ëª©í‘œ: ì¡°ê±´ë¶€ í‰ê·  $\\mathbb{E}[Y X]$ ì˜ˆì¸¡ Loss: Mean Squared Error (MSE) ê°€ì •: ì˜¤ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„ Quantile Regression: ëª©í‘œ: ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ $q_\\tau(Y X)$ ì˜ˆì¸¡ Loss: Pinball Loss (Quantile Loss) ê°€ì •: ë¶„í¬ ê°€ì • ì—†ìŒ, ë” robust Pinball Loss (Quantile Loss) Quantile regressionì˜ í•µì‹¬ì€ pinball lossì…ë‹ˆë‹¤: $$ \\mathcal{L}_\\tau(y, \\hat{q}_\\tau(x)) = \\max\\big(\\tau \\cdot [y - \\hat{q}_\\tau(x)], (1-\\tau) \\cdot [\\hat{q}_\\tau(x) - y]\\big) $$ Pinball Lossì˜ ì§ê´€ì  ì´í•´ $\\tau = 0.5$ (ì¤‘ì•™ê°’): ì–‘ìª½ ì˜¤ì°¨ì— ë™ì¼í•œ í˜ë„í‹° $\\tau = 0.9$ (90ë¶„ìœ„ìˆ˜): ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ë³´ë‹¤ ì‘ìœ¼ë©´ í° í˜ë„í‹° ì˜ˆì¸¡ê°’ì´ ì‹¤ì œê°’ë³´ë‹¤ í¬ë©´ ì‘ì€ í˜ë„í‹° $\\tau = 0.1$ (10ë¶„ìœ„ìˆ˜): ë°˜ëŒ€ íŒ¨í„´ Prediction Interval êµ¬ì„± Quantile regressionìœ¼ë¡œ prediction intervalì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤: ë‘ ê°œì˜ quantile ëª¨ë¸ í•™ìŠµ: $\\tau = \\alpha/2$ (í•˜í•œ) $\\tau = 1 - \\alpha/2$ (ìƒí•œ) Prediction Interval: $$[q_{\\alpha/2}(x), q_{1-\\alpha/2}(x)]$$ ì˜ˆì‹œ: 90% Prediction Interval $\\tau = 0.05$ ëª¨ë¸ë¡œ í•˜í•œ ì˜ˆì¸¡ $\\tau = 0.95$ ëª¨ë¸ë¡œ ìƒí•œ ì˜ˆì¸¡ ê²°ê³¼: $[q_{0.05}(x), q_{0.95}(x)]$ ì¥ì ê³¼ íŠ¹ì§• ì¥ì  ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”: ì •ê·œì„± ê°€ì • ì—†ì´ë„ ì‘ë™ Heteroscedasticity ëŒ€ì‘: ë¶„ì‚°ì´ ì¼ì •í•˜ì§€ ì•Šì•„ë„ ì í•© Outlier robust: ê·¹ê°’ì— ëœ ë¯¼ê° ì§ê´€ì  í•´ì„: ë¶„ìœ„ìˆ˜ëŠ” ë¹„ì„ í˜• ë³€í™˜ì—ë„ ë¶ˆë³€ íŠ¹ì§• ë¹„ëŒ€ì¹­ì„±: ë¶„í¬ê°€ ë¹„ëŒ€ì¹­ì¼ ë•Œ ë” ì •í™• ê³„ì‚° íš¨ìœ¨ì„±: ì„ í˜• í”„ë¡œê·¸ë˜ë°ìœ¼ë¡œ í•´ê²° ê°€ëŠ¥ í•´ì„ë ¥: ê° ë³€ìˆ˜ì˜ ì˜í–¥ë ¥ì„ ë¶„ìœ„ìˆ˜ë³„ë¡œ ë¶„ì„ ê°€ëŠ¥ êµ¬í˜„ ë°©ë²• Python ì˜ˆì‹œ (scikit-learn) ``python from sklearn.linear_model import QuantileRegressor import numpy as np ë°ì´í„° ì¤€ë¹„ X = np.random.randn(1000, 2) y = X[:, 0] + X[:, 1] + np.random.randn(1000) * (1 + abs(X[:, 0])) 10ë¶„ìœ„ìˆ˜ì™€ 90ë¶„ìœ„ìˆ˜ ëª¨ë¸ í•™ìŠµ q10_model = QuantileRegressor(quantile=0.1, alpha=0) q90_model = QuantileRegressor(quantile=0.9, alpha=0) q10_model.fit(X, y) q90_model.fit(X, y) Prediction interval q10_pred = q10_model.predict(X) q90_pred = q90_model.predict(X) 80% prediction interval pi_lower = q10_pred pi_upper = q90_pred ` LightGBMì—ì„œì˜ êµ¬í˜„ `python import lightgbm as lgb Quantile regressionì„ ìœ„í•œ íŒŒë¼ë¯¸í„° params = { 'objective': 'quantile', 'metric': 'quantile', 'quantile': 0.1, # 10ë¶„ìœ„ìˆ˜ 'boosting_type': 'gbdt' } ëª¨ë¸ í•™ìŠµ model_q10 = lgb.train(params, train_data) model_q90 = lgb.train(params, train_data) Prediction interval q10_pred = model_q10.predict(X_test) q90_pred = model_q90.predict(X_test) `` ì‹¤ë¬´ í™œìš© ì‚¬ë¡€ ê¸ˆìœµ ë¦¬ìŠ¤í¬ ê´€ë¦¬ VaR (Value at Risk): íŠ¹ì • ë¶„ìœ„ìˆ˜ë¡œ ìœ„í—˜ ì¸¡ì • Credit scoring: ëŒ€ì¶œ ìƒí™˜ í™•ë¥ ì˜ ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ ì˜ë£Œ ì§„ë‹¨ í˜ˆì•• ì˜ˆì¸¡: ì •ìƒ ë²”ìœ„ì˜ ìƒí•˜í•œ ì˜ˆì¸¡ ì•½ë¬¼ ë°˜ì‘: ê°œì¸ë³„ ë°˜ì‘ì˜ ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ ì œì¡°ì—… í’ˆì§ˆ ê´€ë¦¬ í’ˆì§ˆ ì§€í‘œ: ì œí’ˆ íŠ¹ì„±ì˜ í—ˆìš© ë²”ìœ„ ì˜ˆì¸¡ ë¶ˆëŸ‰ë¥  ì˜ˆì¸¡: íŠ¹ì • ì„ê³„ê°’ ì´í•˜/ì´ìƒ í™•ë¥  ë¶€ë™ì‚° ê°€ê²© ì˜ˆì¸¡ ê°€ê²© ë²”ìœ„: ì‹œì¥ ë³€ë™ì„±ì„ ê³ ë ¤í•œ ê°€ê²© êµ¬ê°„ íˆ¬ì ë¦¬ìŠ¤í¬: í•˜ë½ ê°€ëŠ¥ì„±ê³¼ ìƒìŠ¹ ê°€ëŠ¥ì„± ë™ì‹œ ê³ ë ¤ ì£¼ì˜ì‚¬í•­ê³¼ í•œê³„ ì£¼ì˜ì‚¬í•­ Quantile crossing: ë†’ì€ ë¶„ìœ„ìˆ˜ê°€ ë‚®ì€ ë¶„ìœ„ìˆ˜ë³´ë‹¤ ì‘ì•„ì§ˆ ìˆ˜ ìˆìŒ Calibration: ì‹¤ì œ coverageê°€ ëª©í‘œ coverageì™€ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ ë°ì´í„° ìš”êµ¬ëŸ‰: ê·¹ë‹¨ì  ë¶„ìœ„ìˆ˜ ì˜ˆì¸¡ì—ëŠ” ì¶©ë¶„í•œ ë°ì´í„° í•„ìš” í•œê³„ ê³„ì‚° ë³µì¡ë„: OLSë³´ë‹¤ ê³„ì‚° ë¹„ìš© ë†’ìŒ í•´ì„ ë³µì¡ì„±: ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ ëª¨ë¸ì˜ ë™ì‹œ í•´ì„ í•„ìš” ë¶ˆí™•ì‹¤ì„± ë¶„í•´: epistemic vs aleatoric uncertainty êµ¬ë¶„ ì–´ë ¤ì›€ ìµœì‹  ë°œì „ ë™í–¥ Deep Quantile Regression ì‹ ê²½ë§ ê¸°ë°˜ quantile regression ë³µì¡í•œ ë¹„ì„ í˜• ê´€ê³„ ëª¨ë¸ë§ Conformal Quantile Regression Finite-sample coverage ë³´ì¥ Calibration ìë™í™” Multi-task Quantile Learning ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ë¥¼ ë™ì‹œì— í•™ìŠµ Quantile crossing ë°©ì§€ ê²°ë¡  Quantile regressionì€ ì „í†µì ì¸ íšŒê·€ë¶„ì„ì˜ í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. íŠ¹íˆ: Prediction interval êµ¬ì„±ì— ë§¤ìš° ìœ ìš© ë¶„í¬ ê°€ì • ì—†ì´ robustí•œ ì˜ˆì¸¡ ê°€ëŠ¥ ì‹¤ë¬´ì  í•´ì„ì´ ì§ê´€ì ì´ê³  ëª…í™• ë‹¤ë§Œ, ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ ëª¨ë¸ì˜ ë™ì‹œ í•™ìŠµê³¼ calibration ë“± ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­ì„ ì˜ íŒŒì•…í•˜ê³  ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. --- ì°¸ê³ ìë£Œ: Quantile Regression - Wikipedia scikit-learn QuantileRegressor LightGBM Quantile Regression",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì†Œë¬¸ë°ê°€ì‹­/08-12-ë¦¬ë·°ë°ì´",
    "title": "ë¦¬ë·° ë°ì´",
    "date": "2025-08-12",
    "excerpt": "ì†Œë¬¸ ë° ê°€ì‹­ì— ì‚¬ìš©ë˜ëŠ” í‘œí˜„ë“¤ ë¦¬ë·°",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ í‘œí˜„ë“¤ ì •ë¦¬ Believe it or not Believe it or not, I bumped into Jessica Alba This maybe a surprise, but You may not know this, but Are you telling me that There's no way that No wonder I knew it I would have never ~ From what I've heard From what I know As far as I understand,",
    "public": true
  },
  {
    "id": "Self_Development/Career/toss-ì±„ìš©ì„¤ëª…íšŒ",
    "title": "í† ìŠ¤ ì±„ìš©ì„¤ëª…íšŒ ì •ë¦¬",
    "date": "2025-08-12",
    "excerpt": "í† ìŠ¤ ì±„ìš©ì„¤ëª…íšŒ ë“£ê³  ê´€ë ¨ ë‚´ìš©ë“¤ ì •ë¦¬",
    "category": "Career",
    "tags": [
      "Toss"
    ],
    "content": "Head of Data Tossê°€ ëŒ€ê·œëª¨ ì±„ìš© ì¤‘ì¸ ì´ìœ  ì„±ì¥í•˜ëŠ” ì¤‘ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì„±ì¥í•˜ê¸° ìœ„í•´ ë°ì´í„° ì§êµ°ì´ ì¤‘ìš”í•˜ë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸. Data ì¡°ì§ì˜ ë¯¸ì…˜ê³¼ ë¹„ì „ ê³ ê°ì—ê²Œ ìµœê³ ì˜ ê²½í—˜. ë¹„ì¦ˆë‹ˆìŠ¤ ê²½í—˜. ê³ ê°ì„ ë” ì˜ ì´í•´í•˜ê³ . ì„±ê³¼ì˜ ì‹œì‘. Data / ML ì§êµ° Data Engineering Data Analytics Data Ops Manager ML Engineer AI Engineer ML Manager Data Manager DAta Architect í† ìŠ¤ê°€ ì§‘ì¤‘í•˜ê³  ìˆëŠ” ê²ƒì€ ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ ë‚´ëŠ”ë° ì§‘ì¤‘í•˜ê³  ìˆë‹¤. ë°ì´í„° ê±°ë²„ë„ŒìŠ¤ í”„ë¡œì íŠ¸. ê´‘ê³ /ì»¤ë¨¸ìŠ¤ ì„±ì¥ . ì¶”ì²œ ê²€ìƒ‰. LLM. ìë™í™”. í˜ì‹ . Ads Domain ML Engineer ìµœì˜ë¯¼ë‹˜. í† ìŠ¤ ê´‘ê³ . ë‹¤ì–‘í•œ ì§€ë©´ì— ê´‘ì£¼ ìš”êµ¬ ìˆ˜ìš©. ê²°ì¬ ê¸ˆì•¡, ìœ ì € í–‰ë™ ë°ì´í„° ê¸°ë°˜ ê°œì¸í™”ëœ ê´‘ê³  ì œê³µ. í•´ê²°í•˜ê³  ìˆëŠ” ë¬¸ì œ CTR/CVR ì˜ˆì¸¡. ê´‘ê³  ë­í‚¹ì—ëŠ” ì¶”ì²œê³¼ ë‹¤ë¥´ê²Œ ë¹„ê¸´ê°€? eCPM ê³„ì‚°. ìë™ ì…ì°° ì‹œìŠ¤í…œ.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Resumes/25-08-12-feedback-hyundai-resume",
    "title": "[Feedback] - [íƒˆë½!] Hyundai Autoever - ML Engineer Resume",
    "date": "2025-08-12",
    "excerpt": "í˜„ëŒ€ìë™ì°¨ ì´ë ¥ì„œ",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³ ",
      "ì´ë ¥ì„œ"
    ],
    "content": "í”¼ë“œë°± ë°›ì€ ì´ë ¥ì„œ í”¼ë“œë°± ë°›ì€ ë‚´ìš©ë“¤ ê¸ì–´ì˜¤ê¸° ì´ì§ì‚¬ìœ ë¥¼ ì–´ë–»ê²Œ ë§í•´ì•¼í•˜ë‚˜ ê³ ë¯¼ì´ë„¤ìš” cvë‘ llm ê²½í—˜ì„ í•œ íšŒì‚¬ì—ì„œ í•˜ì‹ ê±´ê°€ìš” ë„ë©”ì¸ì´ ë‹¤ì–‘í•´ì„œ ì‹ ê¸°í•˜ë„¤ìš” ì„¸ë¶€ í¬íŠ¸í´ë¦¬ì˜¤ë‚˜, ë­”ê°€ ë‹¤ë¥¸ ìë£Œê°™ì€ê±´ ë” ì—†ìœ¼ì‹ ê°€ìš”? ì•„ë¬´ë˜ë„ ì§€ê¸ˆ ë°”ì˜ì‹  ê²ƒ ê°™ìœ¼ë‹ˆ ì œê°€ ë´¤ì„ ë•Œì˜ ë‚´ìš©ì„ ì ì–´ë³´ìë©´, ì§€ì›ì‚¬ìœ  ì—†ìŒ ê²½ë ¥ 3ë…„ì°¨ ê³µê³ ì¸ë° ë¹„í•´, ê²½ë ¥ 1ë…„ì°¨(ë‹¤ë¥¸ ìš”ì†Œë¡œ ì»¤ë²„ ê°€ëŠ¥) ìš°ëŒ€ì‚¬í•­ 2ê°œ ì¤‘ 3.1. \"í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§\"ëŠ” ì–´ë µì§€ ì•Šì•„ì„œ ì–´í•„ì´ ì–´ë ¤ì›€. 3.2. \"ë°ì´í„° í’ˆì§ˆ\" ì—­ëŸ‰ì´ ì œê°€ ë³´ê¸°ì—ëŠ” ì–´í•„ì´ ë  ì •ë„ì˜ ë‚œì´ë„ë¡œ ë³´ì´ì§€ ì•Šì•„ìš”. ì„±ê³¼ì˜ ê²½ìš°, ê¸°ì¡´ ëª¨ë¸ ê°œë¹„(ëŒ€ë¹„ ì˜¤íƒ€) ì¼ë°˜í™” ì„±ëŠ¥ 10~20% í–¥ìƒ ë° Dice Score 0.8 ë‹¬ì„±ì´ë¼ê³  í•˜ëŠ”ë°, ì´ê²Œ ì–´ëŠì •ë„ì˜ ê°€ì¹˜ê°€ ìˆëŠ”ì§€ë„ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ì´ê¸° ë•Œë¬¸ì— ëª…í™•í•˜ê²Œ íŒë‹¨í•˜ê¸° ì–´ë µê³ , OCR íŒŒì´í”„ë¼ì¸ ìµœì í™”, ì •í™•ë„ 99% ë‹¬ì„± ë° ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œ ì´ë¼ê³  í•˜ëŠ” ê²ƒë„ ì–¼í• ë³´ë©´ ì¢‹ì€ ì„±ê³¼ê°™ì€ë°, ì–´ë–¤ ë°ì´í„°ë¡œ ì–´ë–»ê²Œ í…ŒìŠ¤íŠ¸ í–ˆì„ ë•Œ ì´ë ‡ê²Œ ë‹¬ì„±ëœê±°ì§€?ê°€ ì—†ì–´ìš”. RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë° ìµœì í™”ë¡œ ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ëŒ€ì²´ ë° ì‚¬ìš©ì ë§Œì¡±ë„ 90% ì´ìƒ ë‹¬ì„± í•´ë‹¹ ì„±ê³¼ë„ ê¸°ì¡´ ìƒë‹´ì˜ ë§Œì¡±ë„ëŠ” ì–´ë• ê³  ë³€ê²½ ì´í›„ 90%ë¥¼ ë‹¬ì„±í•œê±´ì§€, ë§Œì¡±ë„ ì¡°ì‚¬ëŠ” ì–´ë–¤ì‹ìœ¼ë¡œ ì§„í–‰í–ˆìœ¼ë©° ëª¨ìˆ˜ê°€ ëª‡ê°œì¸ì§€ê°€ ì—†ë„¤ìš”. ì „ë°˜ì ìœ¼ë¡œ ë‚´ìš©ì— ëŒ€í•œ ê·¼ê±°ê°€ ë¶ˆí™•ì‹¤í•´ì„œ, ì œê°€ ì±„ìš© ë‹´ë‹¹ìë¼ë©´ ë‚´ìš©ì„ ë¶€í’€ë ¸ì„ê±°ë¼ê³  ìƒê°ë  ê²ƒ ê°™ê³ , ê²½ë ¥ë„ ë‚®ê¸° ë•Œë¬¸ì— ë–¨ì–´ëœ¨ë¦´ ê²ƒ ê°™ì•„ìš”. íŠ¹íˆë‚˜ \"ì—°êµ¬ê°œë°œ ì—­ëŸ‰\"ì„ ì›í•˜ê¸° ë•Œë¬¸ì— ì—°êµ¬ê°œë°œí•˜ë“¯ì´, ëª…í™•í•œ ë°°ê²½ê³¼ ê·¼ê±° ì„¤ëª…ì´ ë”ë”ìš± í•„ìš”í•  ê²ƒ ê°™êµ¬ìš” ë³€ìˆ˜í†µì œë‚˜ ê·¸ê²Œ ì—†ìœ¼ë©´ ì¢‹ì€ ì„±ê³¼ë”ë¼ë„ ì–»ì–´ê±¸ë¦°ê±°ë‚˜ ë‹¤ë¦„ ì—†ì„êº¼ì—ìš”, ì—¬ê¸°ì„  ì¶”ê°€ì ìœ¼ë¡œ ì´ì•¼ê¸°í•˜ìë©´, ë™ë£Œì˜ í•œë§ˆë””ë„ ì½ì–´ë³´ê³ , ì¸ì¬ìƒì´ë‚˜ íšŒì‚¬ ê°€ì¹˜ê°™ì€ ê²ƒë„ ê¼­ ì°¾ì•„ë³´ì„¸ìš”. ê³ ê¹ƒì§‘ ì‚¬ì¥ì´ê³ , ì‹¸ê³  ë§›ìˆëŠ” ê³ ê¸°ë¥¼ íŒ”ì•„ì•¼í•œë‹¤ëŠ” ì² í•™ì€ ê°–ëŠ”ë°, ì§ì›ì´ ê°‘ìê¸° ìš°ë¦¬ ê³ ê¸°ì •ë„ë©´ ë” ë°›ì„ ìˆ˜ ìˆë‹¤ê³  í•˜ë©´ ì–´ë–¤ ëŠë‚Œì´ê² ì–´ìš” ì² í•™ì€ ê°–ëŠ”ë° => ì² í•™ì„ ê°€ì§€ê³  ìˆëŠ”ë° ë§Œì¡±ë„ ì¡°ì‚¬ëŠ” ì–´ë–¤ì‹ìœ¼ë¡œ ì§„í–‰í–ˆìœ¼ë©° ëª¨ìˆ˜ê°€ ëª‡ê°œì¸ì§€ê°€ ì—†ë„¤ìš”. ì—°êµ¬ê°œë°œí•˜ë“¯ì´, ëª…í™•í•œ ë°°ê²½ê³¼ ê·¼ê±° ì„¤ëª…ì´ ë”ë”ìš± í•„ìš”í•  ê²ƒ ê°™êµ¬ìš” ì´ë¶€ë¶„ì„ ë³´ë©´ ì„±ê³¼ë‚˜ ì—­í• ì„ ì •ë¦¬í•˜ë©´ì„œ ì–´ëŠì •ë„ ë¶€ì—°ì„¤ëª…ê³¼ ë°°ê²½ì„ ì•Œë¦¬ëŠ”ê²Œ ì¢‹ë‹¤ëŠ” ë§ì”€ì´ì‹¤ê¹Œìš”? ì €ëŠ” ì›ë˜ êµ¬êµ¬ì ˆì ˆ ìŠ¤íƒ€ì¼ì´ì—ˆë‹¤ê°€ ì§€ê¸ˆ ì˜¬ë ¤ì£¼ì‹  ë¶„ ì²˜ëŸ¼ ê¹”ë”í•˜ê²Œ ì„±ê³¼ ìœ„ì£¼ë¡œ ì •ë¦¬í•˜ê³  ìˆì—ˆëŠ”ë° í”¼ë“œë°±ì´ ê³µê°ì´ ë¼ì„œìš” ë‹¨ì§€ ì´ëŸ° ë¶€ë¶„ë“¤ì´ ëª…í™•í•˜ì§€ ì•Šìœ¼ë‹ˆ, ê°œì„ ë˜ì–´ì•¼í• ê±°ë‹¤. ì¼ë‹¨ ì´ë ¥ì„œì—ì„œ ì•Œì•„ì•¼í•˜ëŠ”ê²Œ ì´ë ¥ì„œì˜ ëª©ì ì€ \"ë‹ˆê°€ ì°¾ë˜ ì‚¬ëŒì´ ë°”ë¡œ ë‚˜\" ê±°ë“ ìš”? 1ì°¨ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´ ì œê°€ ê°€ì§„ ê²½í—˜ì´ ë§ì„ ìˆ˜ë¡ ì¢‹ì„ê¹Œ, ì—†ì„ ìˆ˜ë¡ ì¢‹ì„ê¹Œë¥¼ ë¨¼ì € ìƒê°í•´ë³´ì‹¤ë˜ìš”? ì–´ë– ì‹¤ ê²ƒ ê°™ì•„ìš”? ë‚´ìš©ì´ êµ¬êµ¬ì ˆì ˆ í•˜ê³  ë§ê³ ë¥¼ ë– ë‚˜ì„œ ê·¼ê±°ì— ëŒ€í•´ì„  ì•„ë¬´ë˜ë„ ì €ìª½ì€ ì—°êµ¬ë¶„ì•¼ë‹¤ë³´ë‹ˆê¹Œ ë”ìš±ë” í•„ìš”í•˜ë‹¤ ìƒê°í•´ìš” ã…‹_ã…‹ ì‹¤í—˜ê³¼ ê²°ê³¼ë¡œ ì¦ëª…í•´ì•¼í•˜ëŠ” ê³³ì´ë‹¤ë³´ë‹ˆê¹Œ ìš°ë¦¬ íšŒì‚¬ë‘ ë§ëŠ” ê²½í—˜ì´ ë§ì€ ì‚¬ëŒ, ì ì€ ì‚¬ëŒ ê·¼ë° ì—¬ê¸° ì´ë ¥ì„œì—ì„œ ì–´í•„í•´ì•¼í•˜ëŠ” ë¶€ë¶„ì´ ì—°êµ¬ê±°ë“ ìš”? ë¬¼ë¡  ë‹¤ë¥¸ ê³³ì´ì–´ë„ ë¹„ìŠ·í•˜ê¸´ í•˜ì§€ë§Œ, í”íˆ ë°±ì—”ë“œ ê°œë°œìê°€ í•˜ëŠ” ì‹¤ìˆ˜ê°€ DB 10ì´ˆ ê±¸ë¦¬ë˜ ì¿¼ë¦¬ë¥¼ 0.1ì´ˆë¡œ ë‹¨ì¶• ì´ëŸ°ê±¸ ì ëŠ”ë° ì‚¬ì‹¤ ì´ê²Œ ì „í˜€ ê²½ìŸë ¥ì´ ì—†ì–´ìš” ë§Œì•½ì—, ì´ë¯¸ íŠœë‹ì´ ë‹¤ ë˜ì–´ìˆë˜ SQLì¸ë°ë„ 10ì´ˆê°€ ê±¸ë¦¬ë˜ SQLì¸ë° 0.1ì´ˆë¡œ ë°”ë€Œì—ˆìœ¼ë©´ ë‚˜ë¼ì—ì„œ ìƒì„ ì¤˜ë„ ë¶€ì¡±í•  ì—­ëŸ‰ì´ì§€ë§Œ ì•„ë¬´ ì¸ë±ìŠ¤ë„ ì•ˆê±¸ë ¤ìˆë˜ SQLì„ ì¸ë±ìŠ¤ ì¶”ê°€í•´ì„œ ê°œì„ í–ˆë‹¤ ì´ê±´ ìˆ˜ì¤€ì´ê±°ë“ ìš”., ê·¸ëƒ¥ SQL ì¸ë±ìŠ¤ì— ëŒ€í•œ ì§€ì‹ì„ ì•ˆë‹¤ \"ê²°êµ­ ê°™ì€ ì •ëŸ‰ì  ì„±ê³¼\" ë¼ê³  í•˜ë”ë¼ë„ ê·¸ ë’¤ì— ìˆëŠ” ë°°ê²½ì´ë‚˜, ìƒí™© ë“±ì— ë”°ë¼ì„œ ê°€ì¹˜ê°€ ì™„ì „íˆ ë‹¬ë¼ì ¸ìš”. ê·¸ë¦¬ê³  ê·¸ ê°€ì¹˜ë¥¼ ì´ë ¥ì„œì—ì„œ ëª…í™•í•˜ê²Œ ë³´ì—¬ì£¼ë ¤ë©´ ì—°êµ¬ ê²°ê³¼ì²˜ëŸ¼ ì„±ê³¼ë¥¼ ë³´ì—¬ì¤˜ì•¼ì£ . íŠ¹íˆë‚˜ AIì²˜ëŸ¼ ëŒë¦´ ë•Œë§ˆë‹¤ ë³€ìˆ˜ê°€ ìˆëŠ” ì¼€ì´ìŠ¤ì˜ ê²½ìš° ë³€ìˆ˜í†µì œê°€ ë“¤ì–´ê°€ê³ , ì‹œë‚˜ë¦¬ì˜¤ê°€ ìˆì–´ì•¼í•˜ëŠ”ë° ê·¸ëŸ°ê²Œ ì—†ëŠ” ìƒíƒœì—ì„œëŠ”, ì €ê²Œ ì–´ì©Œë‹¤ ìš´ì¢‹ê²Œ ì˜ ë‚˜ì˜¨ ì„±ê³¼ì¸ì§€, ì•„ë‹Œì§€ë„ í™•ì‹ í•˜ê¸° ì–´ë µêµ¬ìš”. @ëª¨ì£  ìš”ì•½í•˜ë©´, ì„±ê³¼ë¥¼ \"ì…ì¦í•˜ê¸° ìœ„í•œ ê·¼ê±°ëŠ” í•´ë‹¹ ì±„ìš©ê³µê³ ì—ì„œ ê¼­ í•„ìš”í•œ ë‚´ìš©ì„.\" ì´ë„¤ìš” AIìª½ì€ ì¢€ ì¡°ì‹¬ìŠ¤ëŸ½ê¸´í•œë° ê´€ë ¨ ì „ê³µìì—ê²Œ í”¼ë“œë°±ë“¤ìœ¼ì‹œëŠ”ê²ƒë„ ë„ì›€ë ê±°ê°™ìŠµë‹ˆë‹¤ ğŸ¤” ëª¨ë‘ì—° ì¶œì‹ ì´ì…”ì„œ ê·¸ìª½ì— ì»¤ë¦¬ì–´ í”¼ë“œë°±ì´ë¼ë“ ì§€ ì ê·¹ì ìœ¼ë¡œ í•´ì£¼ì‹¤ë¶„ì´ ë§ì•„ë³´ì—¬ì„œìš” ë¶„ëŸ‰ì´ ì• ë§¤í•  ê²ƒ ê°™ìœ¼ë©´ ì°¨ë¼ë¦¬ ê¸¸ê²Œ ì“°ì„¸ìš” ì—†ëŠ” ê²ƒë³´ë‹¨ ë‚˜ì•„ìš” ë©´ì ‘ê´€ìœ¼ë¡œ í•˜ì—¬ê¸ˆ ì•„ ì •ë§ë¡œ ì´ ì‚¬ëŒì´ 3ë…„ì°¨ ì´ìƒì˜ ì—­ëŸ‰ì„ ê°€ì¡Œêµ¬ë‚˜ ë¼ëŠ” ìƒê°ì´ ë“¤ë©´ ë©´ì ‘ê°€ì‹¤ ìˆ˜ ìˆì„êº¼ì—ìš” ê·¸ë¦¬ê³  ìœ„ì—ì„œ ë§ì”€ë“œë¦° ê²ƒì²˜ëŸ¼ ì¸ì¬ìƒì´ë‚˜, ë™ë£Œì˜ í•œë§ˆë”” ì´ëŸ°ê±° ì˜ ë³´ì‹œê³  ì–´í•„í•˜ì„¸ìš”. ì •ë¦¬ í•µì‹¬ ìš”ì•½: ì§€ì›ì‚¬ìœ  ë¶€ì¬, ê²½ë ¥ ë ˆë²¨ ê°­, ì„±ê³¼ ê·¼ê±° ë¶ˆì¶©ë¶„ â†’ ì‹ ë¢°ë„ í•˜ë½. ì—°êµ¬ê°œë°œ í¬ì§€ì…˜ íŠ¹ì„±ìƒ ì‹¤í—˜ ì„¤ê³„ì™€ ì¦ë¹™ì´ í•„ìˆ˜. ê°œì„  ë°©í–¥: ì§€ì›ì‚¬ìœ Â·íšŒì‚¬ ì í•©ì„±: ì¸ì¬ìƒ/ë™ë£Œì˜ í•œë§ˆë”” ê·¼ê±°ë¡œ ë™ê¸°ì™€ fit ëª…í™•í™” ê²½ë ¥ ê°­ ë³´ì™„: ë†’ì€ ë‚œì´ë„ ì´ìŠˆ í•´ê²°, ì˜¤ë„ˆì‹­, ì˜í–¥ ë²”ìœ„(ì¡°ì§/ì‚¬ìš©ì ì„íŒ©íŠ¸) ê°•ì¡° ìš°ëŒ€ì‚¬í•­ ì •ë ¬: Data Quality ì‹¬í™” ì‚¬ë¡€ ì¤‘ì‹¬, Prompt Engineeringì€ ì„íŒ©íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ìš”ì•½ ì„±ê³¼ ì„œìˆ  í…œí”Œë¦¿(ê° í•­ëª©ì— ê³µí†µ ì ìš©): Context: ë¬¸ì œ/ë°°ê²½, ë°ì´í„° íŠ¹ì„± Baseline: ê¸°ì¡´ ëª¨ë¸/í”„ë¡œì„¸ìŠ¤ì™€ ì§€í‘œ(ìˆ«ì) Design: ë³€ìˆ˜ í†µì œ, ì‹¤í—˜ ì‹œë‚˜ë¦¬ì˜¤/ì„¸íŒ… Metrics: ì§€í‘œ ì •ì˜, í‘œë³¸ìˆ˜(N), í‰ê°€ í”„ë¡œí† ì½œ Result: ê²°ê³¼ ìˆ˜ì¹˜ì™€ ê°œì„ í­, ë¶ˆí™•ì‹¤ì„±(ê°€ëŠ¥ ì‹œ) Role: ë³¸ì¸ ê¸°ì—¬ ë²”ìœ„ì™€ í˜‘ì—… í¬ì¸íŠ¸ Evidence: ì½”ë“œ/ëŒ€ì‹œë³´ë“œ/ë¦¬í¬íŠ¸/ìƒ˜í”Œ ë§í¬(ì¬í˜„ì„±) ì²´í¬ë¦¬ìŠ¤íŠ¸: ë°ì´í„°ì…‹/ë¶„í¬/ê¸°ê°„ ëª…ì‹œ, í…ŒìŠ¤íŠ¸ í”„ë¡œí† ì½œê³¼ ëª¨ìˆ˜ ê¸°ì¬ ê¸°ì¤€ì„  ëŒ€ë¹„ ê°œì„  ê·¼ê±°ì™€ ê°€ì • ì œì‹œ(ê³¼ì í•©/ìš°ì—° ê°€ëŠ¥ì„± ë°°ì œ) ì‚¬ìš©ì/ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¡œ ì—°ê²°(ìš´ì˜ ë¹„ìš©/ì‹œê°„/ë§Œì¡±ë„ ë“±) ë‹¤ìŒ ì•¡ì…˜: ì´ë ¥ì„œì˜ ê° ì„±ê³¼ í•­ëª©ì„ ìœ„ í…œí”Œë¦¿ìœ¼ë¡œ ì¬ì‘ì„± ë³„ë„ ì„¹ì…˜ìœ¼ë¡œ ì§€ì›ì‚¬ìœ  ì¶”ê°€(íšŒì‚¬ ê°€ì¹˜/ì¸ì¬ìƒ ì¸ìš©) í¬íŠ¸í´ë¦¬ì˜¤Â·ë ˆí¬Â·ì‚°ì¶œë¬¼ ë§í¬ ì‚½ì…(ì¬í˜„ì„± í™•ë³´) íšŒì‚¬ ìš©ì–´ì™€ ë¬¸ì²´ì— ë§ê²Œ ë¬¸êµ¬ ì†ì§ˆ, ë¶„ëŸ‰ ì• ë§¤í•˜ë©´ ìƒì„¸íˆ ì‘ì„± GPT ë„ˆê°€ ì£¼ëŠ” í”¼ë“œë°± (ë©´ì ‘ê´€ì´ë¼ê³  ìƒê°í•˜ê³ , ë‚´ ì´ë ¥ì„œë‘ ì±„ìš©ê³µê³  ë³´ê³ ) í•œëˆˆ í‰ê°€: JD í•µì‹¬ ìš”êµ¬ì™€ ë§¤ì¹­ ë†’ìŒ. ì‹¤ë¬´í˜• ì„±ê³¼ëŠ” ì¢‹ìœ¼ë‚˜ R&D íŒ€ íŠ¹ì„±ìƒ ì‹¤í—˜ ì„¤ê³„/ì¦ë¹™ì˜ ë°€ë„ëŠ” ë¶€ì¡±. ë©´ì ‘ ì¶”ì²œ. JD ë§¤ì¹­ë„ Must-have ê²€ìƒ‰/ì±—ë´‡ ì„œë¹„ìŠ¤ ê°œë°œ: RAG ê¸°ë°˜ ì±—ë´‡ êµ¬ì¶•Â·ìš´ì˜ ê²½í—˜ ëª…í™•. OK í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§/ë°ì´í„° ìš´ì˜Â·í’ˆì§ˆ ê´€ë¦¬: Prompt ìµœì í™”, ìë™ í‰ê°€Â·ëª¨ë‹ˆí„°ë§ êµ¬ì¶•. OK Python ì‹¤ë¬´: ë‹¤ìˆ˜ í”„ë¡œì íŠ¸ë¡œ ì¶©ë¶„. OK Nice-to-have LLM/Prompt: ë³´ìœ . OK Data Quality Process: OCR í’ˆì§ˆ ê²€ì¦Â·ëª¨ë‹ˆí„°ë§ ë¡œì§ ë³´ìœ . OK ë”¥ëŸ¬ë‹ ëª¨ë¸ë§: CV ì¤‘ì‹¬ì´ì§€ë§Œ ì‹¤ì „ ì„±ê³¼ ìˆìŒ. ì¤€ìˆ˜ NLP ë°ì´í„° êµ¬ì¶•/ì„¤ê³„: ì½”í¼ìŠ¤/ì–´ë…¸í…Œì´ì…˜/ì¦ê°• ì„¤ê³„ì˜ êµ¬ì²´ ì‚¬ë¡€ ì–¸ê¸‰ ì•½í•¨ â†’ ë³´ì™„ í•„ìš” ê°•ì  ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ ê²½í—˜(ì¹´ì¹´ì˜¤ ì—°ë™, ìš´ì˜ ìë™í™” ë“±)ìœ¼ë¡œ ì‹¤ì „ì„± ë†’ìŒ ê²€ìƒ‰Â·RAG ì•„í‚¤í…ì²˜, ë²¡í„°DB ìš´ìš© ë“± JDì™€ ì§ê²° ì—­ëŸ‰ ë³´ìœ  ë°ì´í„° í’ˆì§ˆÂ·ìš´ì˜ ìë™í™” ê´€ì ì—ì„œ ê°•í•œ ì‹œê·¸ë„ ìš°ë ¤/ê°­ ì„±ê³¼ ìˆ˜ì¹˜ì˜ ê·¼ê±° ë¶€ì¡±: í‘œë³¸ìˆ˜(N), í‰ê°€ í”„ë¡œí† ì½œ(holdout/CV/ì‹œê°„ë¶„í• ), í†µê³„ì  ìœ ì˜ì„± ë¯¸ê¸°ì¬ RAGÂ·Prompt í‰ê°€ì˜ ê°ê´€ ì§€í‘œ ë¶€ì¡±: faithfulness/grounding, hallucination rate ë“± NLP ë°ì´í„° êµ¬ì¶•/ì¦ê°• ê²½í—˜ì˜ êµ¬ì²´ì„± ë¶€ì¡±(ìŠ¤í‚¤ë§ˆ ì„¤ê³„, ì–´ë…¸í…Œì´ì…˜ í’ˆì§ˆ, ì¦ê°• ì „ëµ ë“±) CV í”„ë¡œì íŠ¸(ì„¸ê·¸ë©˜í…Œì´ì…˜)ëŠ” ê°„ì ‘ ì—°ê´€ â†’ JD ë§¥ë½ìœ¼ë¡œ ë²ˆì—­ í•„ìš” ì´ë ¥ì„œ ë³´ì™„ ì œì•ˆ ê° ì„±ê³¼ë¥¼ ì•„ë˜ 1ì¤„ í¬ë§·ìœ¼ë¡œ ì¬ì‘ì„±: â€œBaseline X ëŒ€ë¹„ Metric Y +Î”%(N=ìƒ˜í”Œ), ë°©ë²• Z. Offline(CV/ì‹œê°„ë¶„í• )ì™€ Online(A/B) ì¼ì¹˜(p<0.05). ë¹„ìš©/latency Tâ†“.â€ ì±—ë´‡ íŒŒíŠ¸ ë³´ê°• ë°ì´í„°ì…‹ ê·œëª¨/ë¶„í¬, split ì „ëµê³¼ seed Retrieval ì§€í‘œ(Recall@k, MRR), ì‘ë‹µ ì§€í‘œ ì •ì˜Â·ìƒ˜í”Œìˆ˜, ë§Œì¡±ë„ ì¡°ì‚¬ ì„¤ê³„(ëª¨ìˆ˜/ë°©ë²•) Hallucination/faithfulness ì¸¡ì • ë°©ì‹, ì‹¤íŒ¨ ì‚¬ë¡€ ì—ëŸ¬ ë¶„ì„ ë°ì´í„° ì¦ê°• ì ìš© ì‚¬ë¡€(ì—­ë²ˆì—­, í…œí”Œë¦¿, í•©ì„± Q/A ë“±) êµ¬ì²´í™” OCR íŒŒíŠ¸ì— CER/WER ë“± í‘œì¤€ ì§€í‘œì™€ í…ŒìŠ¤íŠ¸ í”„ë¡œí† ì½œ ëª…ì‹œ JD í‚¤ì›Œë“œì™€ 1:1 ë§¤í•‘ Summary 3ì¤„ ì¶”ê°€(ëŒ€í™”í˜• ë°ì´í„° ì„¤ê³„/ìš´ì˜, Prompt/ì¦ê°•, í’ˆì§ˆ ê´€ë¦¬) ì˜ˆìƒ ì§ˆë¬¸(ì¤€ë¹„ ê¶Œì¥) RAGì—ì„œ grounding í‰ê°€ ì–´ë–»ê²Œ í–ˆë‚˜? negative mining/Hard negative ì „ëµì€? ë§Œì¡±ë„ 90%ì˜ ì„¤ê³„Â·ëª¨ìˆ˜Â·í¸í–¥ í†µì œëŠ”? ì˜¤í”„ë¼ì¸ vs ì˜¨ë¼ì¸ ìƒê´€ì€? ë°ì´í„° leakage ë°©ì§€/ë“œë¦¬í”„íŠ¸ ëª¨ë‹ˆí„°ë§ ì²´ê³„? Prompt ìµœì í™”ì˜ ì‹¤í—˜ ì„¤ê³„ì™€ ablation ìš”ì•½? OCR ì •í™•ë„ ì •ì˜(ì •í™•ë„ vs CER/WER) ì„ íƒ ì´ìœ ì™€ í•œê³„? ê²°ë¡  ì§€ì› ì í•©ë„: ë†’ìŒ(ì„œë¹„ìŠ¤Â·ìš´ì˜Â·í’ˆì§ˆ ê´€ë¦¬Â·RAG/Prompt ê°•ì ) R&D ì‹ ë¢°ë„: ë³´ì™„ í•„ìš”(ì‹¤í—˜ ì„¤ê³„/ì¦ë¹™/ë°ì´í„° êµ¬ì¶• ì„¤ê³„ì˜ êµ¬ì²´í™”) ì¶”ì²œ: ì„œë¥˜ í†µê³¼ ê°€ëŠ¥ì„± ìˆìŒ. ìœ„ ë³´ì™„ ë°˜ì˜ ì‹œ ë©´ì ‘ ì„¤ë“ë ¥ í¬ê²Œ ìƒìŠ¹",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-KDE",
    "title": "KDEì— ëŒ€í•œ ìì„¸í•œ ì •ë¦¬",
    "date": "2025-08-12",
    "excerpt": "KDEì—ì„œ ì‚¬ìš©ë˜ëŠ” ê°œë…ë“¤ì— ëŒ€í•œ ìì„¸íˆ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "ì°¸ê³ ìë£Œ 1 : ë¸”ë¡œê·¸, KDE-Kernel Density Estimationì´ë€? 2 : ë¸”ë¡œê·¸, Kernel Density Estimation? KDE ì´í•´ KDEì˜ í•µì‹¬ ì•„ì´ë””ì–´ \"ì ë³„ë¡œ ë°€ë„ë¥¼ êµ¬í•  ë•Œ, ëª¨ë“  ê¸°ì¡´ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ì„œ ê±°ë¦¬ë¡œ ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ” ê²ƒ\" êµ¬ì²´ì ì¸ ê³¼ì •: ìƒˆë¡œìš´ ì  xì—ì„œ ë°€ë„ë¥¼ êµ¬í•˜ê³  ì‹¶ìŒ ëª¨ë“  ê¸°ì¡´ ë°ì´í„° í¬ì¸íŠ¸ x_iì— ëŒ€í•´: ê±°ë¦¬ x - x_i ê³„ì‚° ê±°ë¦¬ì— ë”°ë¥¸ ê°€ì¤‘ì¹˜ K(x - x_i) ê³„ì‚° ëª¨ë“  ê°€ì¤‘ì¹˜ë¥¼ í•©ì¹¨ â†’ $\\sum_{i=1}^{n} K(x - x_i)$ ì •ê·œí™” â†’ $\\frac{1}{nh}$ ê³±í•˜ê¸° ì˜ˆì‹œ: ``python ìƒˆë¡œìš´ ì  x = 4ì—ì„œ ë°€ë„ êµ¬í•˜ê¸° ê¸°ì¡´ ë°ì´í„°: [1, 3, 5] ê° í¬ì¸íŠ¸ë³„ ê°€ì¤‘ì¹˜: K(4-1) = K(3) # ê±°ë¦¬ 3, ë‚®ì€ ê°€ì¤‘ì¹˜ K(4-3) = K(1) # ê±°ë¦¬ 1, ë†’ì€ ê°€ì¤‘ì¹˜ K(4-5) = K(-1) # ê±°ë¦¬ 1, ë†’ì€ ê°€ì¤‘ì¹˜ ìµœì¢… ë°€ë„ = (K(3) + K(1) + K(-1)) / (nh) ` í•µì‹¬: ê±°ë¦¬ ê¸°ë°˜ ê°€ì¤‘ì¹˜: ê°€ê¹Œìš´ ë°ì´í„° í¬ì¸íŠ¸ì¼ìˆ˜ë¡ ë” í° ì˜í–¥ ì „ì²´ í•©ì‚°: ëª¨ë“  ê¸°ì¡´ ë°ì´í„°ì˜ ì •ë³´ë¥¼ í™œìš© ë¶€ë“œëŸ¬ìš´ ì¶”ì •: ê°œë³„ í¬ì¸íŠ¸ê°€ ì•„ë‹Œ ì „ì²´ì ì¸ ë¶„í¬ íŒ¨í„´ ë°˜ì˜ --- PDFì™€ PMFë€? PMF (Probability Mass Function, í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜) : PMFëŠ” ì´ì‚°í™•ë¥ ë³€ìˆ˜(discrete random variable)ì—ì„œ íŠ¹ì • ê°’ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. íŠ¹ì§•: ì´ì‚°í™•ë¥ ë³€ìˆ˜ì—ë§Œ ì ìš© ê° ê°’ì— ëŒ€í•´ í™•ë¥ ì´ í• ë‹¹ë¨ ëª¨ë“  í™•ë¥ ì˜ í•©ì€ 1 ìŒìˆ˜ í™•ë¥ ì€ ì¡´ì¬í•˜ì§€ ì•ŠìŒ ìˆ˜í•™ì  í‘œí˜„: $$P(X = x_i) = p_i$$ ì˜ˆì‹œ: ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§ˆ ë•Œ P(X=1) = 1/6 P(X=2) = 1/6 ... P(X=6) = 1/6 PDF (Probability Density Function, í™•ë¥ ë°€ë„í•¨ìˆ˜) : PDFëŠ” ì—°ì†í™•ë¥ ë³€ìˆ˜(continuous random variable)ì—ì„œ í™•ë¥  ë°€ë„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. íŠ¹ì§•: ì—°ì†í™•ë¥ ë³€ìˆ˜ì—ë§Œ ì ìš© íŠ¹ì • ì ì—ì„œì˜ í™•ë¥ ì€ 0 êµ¬ê°„ì— ëŒ€í•œ í™•ë¥ ì€ PDFë¥¼ ì ë¶„í•˜ì—¬ ê³„ì‚° ì „ì²´ êµ¬ê°„ì—ì„œì˜ ì ë¶„ê°’ì€ 1 ìŒìˆ˜ ê°’ì´ ê°€ëŠ¥ (ë‹¨, í™•ë¥ ì€ í•­ìƒ ì–‘ìˆ˜) ìˆ˜í•™ì  í‘œí˜„: $$P(a \\leq X \\leq b) = \\int_a^b f(x) dx$$ ì˜ˆì‹œ: ì •ê·œë¶„í¬ N(0,1)ì˜ PDF $$f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}$$ ì£¼ìš” ì°¨ì´ì  êµ¬ë¶„ PMF PDF ------ ----- ----- ë³€ìˆ˜ íƒ€ì… ì´ì‚° ì—°ì† í™•ë¥  ê³„ì‚° ì§ì ‘ ê°’ ì ë¶„ í•„ìš” íŠ¹ì • ì  í™•ë¥  ì–‘ìˆ˜ ê°€ëŠ¥ í•­ìƒ 0 ê·¸ë˜í”„ ë§‰ëŒ€ ê·¸ë˜í”„ ê³¡ì„  np.histogramì˜ density íŒŒë¼ë¯¸í„° density=False (ê¸°ë³¸ê°’) ë°˜í™˜ê°’: ê° êµ¬ê°„(bin)ì— ì†í•˜ëŠ” ë°ì´í„°ì˜ ê°œìˆ˜(count) íŠ¹ì§•: ëª¨ë“  êµ¬ê°„ì˜ í•©ì´ ì „ì²´ ë°ì´í„° ê°œìˆ˜ Nê³¼ ê°™ìŒ $\\sum_{i} \\text{count}_i = N$ ìš©ë„: ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•  ë•Œ ì˜ˆì‹œ: `python import numpy as np data = [1, 2, 2, 3, 3, 3, 4, 4, 5] counts, bins = np.histogram(data, bins=5, density=False) counts: [1, 2, 3, 2, 1] (ê° êµ¬ê°„ì˜ ë°ì´í„° ê°œìˆ˜) sum(counts) = 9 (ì „ì²´ ë°ì´í„° ê°œìˆ˜) ` density=True ë°˜í™˜ê°’: ê° êµ¬ê°„ì˜ í™•ë¥ ë°€ë„(probability density) íŠ¹ì§•: ê° êµ¬ê°„ì˜ ê°’ Ã— êµ¬ê°„ ë„ˆë¹„ì˜ í•©ì´ 1ì´ ë¨ $\\sum_{i} \\text{density}_i \\times \\text{bin\\_width}_i = 1$ ìš©ë„: PDFì™€ ë¹„êµí•˜ê±°ë‚˜ í™•ë¥  ë¶„í¬ë¥¼ ë¶„ì„í•  ë•Œ ì˜ˆì‹œ: `python density, bins = np.histogram(data, bins=5, density=True) density: [0.25, 0.5, 0.75, 0.5, 0.25] (í™•ë¥ ë°€ë„) bin_width = 1.0 sum(density bin_width) = 1.0 `` ìˆ˜í•™ì  ê´€ê³„ density=False â†’ density=True ë³€í™˜: $$\\text{density}_i = \\frac{\\text{count}_i}{N \\times \\text{bin\\_width}_i}$$ density=True â†’ density=False ë³€í™˜: $$\\text{count}_i = \\text{density}_i \\times N \\times \\text{bin\\_width}_i$$ ì–¸ì œ ì‚¬ìš©í• ê¹Œ? density=False: ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬, ë¹ˆë„ ë¶„ì„ density=True: PDF ì¶”ì •, ë‹¤ë¥¸ ë¶„í¬ì™€ ë¹„êµ, í™•ë¥ ì  ë¶„ì„ --- Kernel functionì´ë€? ìˆ˜í•™ì ìœ¼ë¡œ ì›ì ì„ ì¤‘ì‹¬ìœ¼ë¡œ Symmetric, ì ë¶„ê°’ì´ 1ì¸ Non-Negative function Normalization: $$\\int_{-\\infty}^{\\infty} K(x) dx = 1$$ Symmetry: $$K(-x) = K(x)$$ Non-negativity: $$K(x) \\geq 0 \\text{ for all } x$$ ì£¼ìš” Kernel í•¨ìˆ˜ë“¤ <figure> <img src=\"/post/DataScience/kernel_functions.png\" alt=\"Kernel functions\" /> <figcaption>Kernel functions</figcaption> </figure> Kernelì˜ ì—­í•  Kernel functionì€ ê° ë°ì´í„° í¬ì¸íŠ¸ ì£¼ë³€ì— \"ê°€ì¤‘ì¹˜\"ë¥¼ ë¶€ì—¬í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤: ë°ì´í„° í¬ì¸íŠ¸ê°€ ìˆëŠ” ê³³ì—ì„œëŠ” ë†’ì€ ê°€ì¤‘ì¹˜ ë©€ì–´ì§ˆìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ê°ì†Œ ì „ì²´ì ìœ¼ë¡œ ë¶€ë“œëŸ¬ìš´ PDF ì¶”ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•¨ Density Estimation > Density Estimationì€ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° í™•ë¥ ë°€ë„í•¨ìˆ˜(PDF)ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ë°ì´í„°ì˜ ë¶„í¬ë¥¼ íŒŒì•…í•˜ê³  ìƒˆë¡œìš´ ë°ì´í„°ê°€ ì–´ë””ì— ìœ„ì¹˜í•  í™•ë¥ ì´ ë†’ì€ì§€ ì˜ˆì¸¡í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ì£¼ìš” ë°©ë²•ë“¤ Histogram (íˆìŠ¤í† ê·¸ë¨) ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ë²•ìœ¼ë¡œ, ë°ì´í„°ë¥¼ êµ¬ê°„(bin)ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ê° êµ¬ê°„ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. ì¥ì : êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì§ê´€ì  ê³„ì‚° ì†ë„ê°€ ë¹ ë¦„ ë‹¨ì : êµ¬ê°„ì˜ ìœ„ì¹˜ì™€ í¬ê¸°ì— ë¯¼ê° ë¶ˆì—°ì†ì ì´ê³  ë¶€ë“œëŸ½ì§€ ì•ŠìŒ êµ¬ê°„ ê²½ê³„ì—ì„œ ë¶ˆì—°ì†ì„± ë°œìƒ Kernel Density Estimation (KDE) ê° ë°ì´í„° í¬ì¸íŠ¸ì— kernel functionì„ ë°°ì¹˜í•˜ì—¬ ë¶€ë“œëŸ¬ìš´ PDFë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ìˆ˜ì‹: $$\\hat{f}(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)$$ ì—¬ê¸°ì„œ: $\\hat{f}(x)$: ì¶”ì •ëœ PDF $n$: ë°ì´í„° ê°œìˆ˜ $h$: bandwidth (ëŒ€ì—­í­) $K(\\cdot)$: kernel function $x_i$: ië²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸ ì¥ì : ë¶€ë“œëŸ½ê³  ì—°ì†ì ì¸ PDF ì¶”ì • ë°ì´í„°ì˜ ì‹¤ì œ ë¶„í¬ë¥¼ ì˜ ë°˜ì˜ bandwidth ì¡°ì ˆë¡œ smoothness ì¡°ì • ê°€ëŠ¥ ë‹¨ì : bandwidth ì„ íƒì´ ì¤‘ìš” ê³„ì‚° ë¹„ìš©ì´ ìƒëŒ€ì ìœ¼ë¡œ ë†’ìŒ Parametric Methods íŠ¹ì • ë¶„í¬(ì˜ˆ: ì •ê·œë¶„í¬, ì§€ìˆ˜ë¶„í¬)ë¥¼ ê°€ì •í•˜ê³  ëª¨ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ì˜ˆì‹œ: $$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ ì¥ì : ê³„ì‚°ì´ ë¹ ë¦„ í•´ì„ì´ ìš©ì´ ì ì€ ë°ì´í„°ë¡œë„ ì¶”ì • ê°€ëŠ¥ ë‹¨ì : ë¶„í¬ ê°€ì •ì´ í‹€ë¦¬ë©´ ë¶€ì •í™• ë³µì¡í•œ ë¶„í¬ë¥¼ ì˜ í‘œí˜„í•˜ì§€ ëª»í•¨ ì–¸ì œ ì–´ë–¤ ë°©ë²•ì„ ì‚¬ìš©í• ê¹Œ? Histogram: ë¹ ë¥¸ íƒìƒ‰ì  ë¶„ì„, ëŒ€ëµì ì¸ ë¶„í¬ íŒŒì•… KDE: ì •í™•í•œ PDF ì¶”ì •, ì‹œê°í™”, ë¹„ëª¨ìˆ˜ì  ë¶„ì„ Parametric: ë¶„í¬ê°€ ëª…í™•í•  ë•Œ, ë¹ ë¥¸ ê³„ì‚°ì´ í•„ìš”í•  ë•Œ --- Kernel Density Estimation (KDE) > \"ê° ë°ì´í„° í¬ì¸íŠ¸ ì£¼ë³€ì— ì‘ì€ ì–¸ë•(kernel)ì„ ë§Œë“¤ê³ , ì´ë“¤ì„ ëª¨ë‘ ë”í•´ì„œ ì „ì²´ì ì¸ ì§€í˜•(PDF)ì„ ë§Œë“œëŠ” ê²ƒ\" KDE ìˆ˜ì‹ $$\\hat{f}(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)$$ ì—¬ê¸°ì„œ: $\\hat{f}(x)$: ì¶”ì •ëœ PDF $n$: ë°ì´í„° ê°œìˆ˜ $h$: bandwidth (ëŒ€ì—­í­). í¬ê¸°ì— ë”°ë¼ kernelì˜ í˜•íƒœê°€ ì¡°ì ˆë˜ëŠ” parameter. scaled kernelì´ë¼ê³ ë„ í•¨. $K(\\cdot)$: kernel function $x_i$: ië²ˆì§¸ ë°ì´í„° í¬ì¸íŠ¸",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-KDE-Histogram",
    "title": "KDEì™€ Histogramì— ëŒ€í•œ ì •ë¦¬",
    "date": "2025-08-12",
    "excerpt": "KDEì™€ Histogramì´ ë­”ì§€, ì°¨ì´ì ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "What is Histogram? > íˆìŠ¤í† ê·¸ë¨ì€ ë°ì´í„°ì˜ ë¶„í¬(ì–´ë””ì— ê°’ì´ ë§ì´ ëª°ë ¤ ìˆëŠ”ì§€)ë¥¼ ë³´ê¸° ìœ„í•´ ë°ì´í„° ë²”ìœ„ë¥¼ ì—¬ëŸ¬ êµ¬ê°„(bin)ìœ¼ë¡œ ë‚˜ëˆ„ê³ , ê° êµ¬ê°„ì— ì†í•œ ë°ì´í„° ê°œìˆ˜ë¥¼ ì„¸ì–´ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œí˜„í•œ ê²ƒ íˆìŠ¤í† ê·¸ë¨ì˜ ì“°ì„ ë°ì´í„° ë¶„í¬ì˜ í˜•íƒœ íŒŒì•… (ì •ê·œë¶„í¬? ì¹˜ìš°ì¹¨? ë‘ ê°œì˜ ë´‰ìš°ë¦¬ê°€ ìˆìŒ?) ë¶„í¬ ë¹„êµ (ì˜ˆ: A ê·¸ë£¹ê³¼ B ê·¸ë£¹ì˜ ì¸¡ì •ê°’ ë¶„í¬ ì°¨ì´) í†µê³„/ë¨¸ì‹ ëŸ¬ë‹ ì „ì²˜ë¦¬(í™•ë¥ ë¶„í¬ ì¶”ì •, KL divergence ê³„ì‚° ë“±) íˆìŠ¤í† ê·¸ë¨ ê¸°ë°˜ í™•ë¥ ë¶„í¬ ì¶”ì •ì˜ ì¥ì  êµ¬í˜„ì´ ì‰½ê³  ë¹ ë¦„ np.histogram, np.histogramddë¡œ ë°”ë¡œ ê°€ëŠ¥ ì‹œê°í™”í•˜ê¸° ì¢‹ìŒ ì´ì‚°í˜• ë°ì´í„°ì—ì„œëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ì ìš© ê°€ëŠ¥ ë‹¨ì  bin í¬ê¸°(í­), bin ê°œìˆ˜ì— ë”°ë¼ ê²°ê³¼ê°€ ë¯¼ê°í•˜ê²Œ ë³€í•¨ ë°ì´í„°ê°€ ì ìœ¼ë©´ binì´ ë¹„ê²Œ ë˜ê³ , ë¶„í¬ê°€ ë¶€ì •í™•í•´ì§ ë‹¤ì°¨ì› ë°ì´í„°ì¼ìˆ˜ë¡ bin ê°œìˆ˜ê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ëŠ˜ì–´(í¬ì†Œí™”), KL ê°’ì´ ë¶ˆì•ˆì •í•´ì§ â†’ \"ì°¨ì›ì˜ ì €ì£¼\" ë¬¸ì œ ì—°ì†í˜• ë°ì´í„°ì—ì„œëŠ” ë¶€ë“œëŸ¬ìš´ ë¶„í¬ í˜•íƒœë¥¼ ì œëŒ€ë¡œ ëª» ì¡ì„ ìˆ˜ ìˆìŒ python code ``python import numpy as np data = [1, 2, 2, 3, 3, 3, 4] counts, bin_edges = np.histogram(data, bins=3, density=False) # ì°¨ì›ì´ ì—¬ëŸ¬ê°œë©´ np.ravel()ì„ ì¨ì„œ ëª¨ë“  ê°’ì„ 1ì°¨ì›ìœ¼ë¡œ í¼ì³ì„œ ê³„ì‚° print(counts) # [2 3 2] print(bin_edges) # [1. 2. 3. 4.] ` `python hist, edges = np.histogramdd(data.drop([\"size\"], axis=1).to_numpy(), bins=30, range=None, density=False) # ì°¨ì› ë³„ë¡œ binì„ ìª¼ê°œì¤Œ ` What is KDE? KDE(Kernel Density Estimation, ì»¤ë„ ë°€ë„ ì¶”ì •)ëŠ” íˆìŠ¤í† ê·¸ë¨ë³´ë‹¤ í•œ ë‹¨ê³„ ë” ì„¸ë ¨ëœ í™•ë¥ ë°€ë„ ì¶”ì • ë°©ë²• ì •ì˜ì™€ ìˆ˜ì‹ ë°ì´í„°ì˜ ê° ê´€ì¸¡ì¹˜ì— ì‘ì€ kernelì„ ì–¹ì–´ í‰ê· í•¨ìœ¼ë¡œì¨ ì—°ì†ì ì´ê³  ë¶€ë“œëŸ¬ìš´ í™•ë¥ ë°€ë„í•¨ìˆ˜(pdf)ë¥¼ ì¶”ì •í•˜ëŠ” ë¹„ëª¨ìˆ˜ì (non-parametric) ë°©ë²• $$ \\hat{f}_h(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\!\\left(\\frac{x - x_i}{h}\\right) $$ K(kernel): Gaussian, Epanechnikov, Top-hat ë“±. ì¼ë°˜ì ìœ¼ë¡œ kernel ì„ íƒë³´ë‹¤ bandwidth h ì„ íƒì´ ì„±ëŠ¥ì— ë” í° ì˜í–¥ h(bandwidth): ê° ë°ì´í„° í¬ì¸íŠ¸ì— ì–¹ëŠ” kernelì˜ í­ (width)ì„ ê²°ì •. ê³¡ì„ ì˜ ë§¤ë„ëŸ¬ì›€ ì¡°ì ˆ. ì‘ìœ¼ë©´ ê³¼ì í•©(ìš¸í‰ë¶ˆí‰), í¬ë©´ ê³¼í‰í™œ(ëª¨ë“œê°€ ì‚¬ë¼ì§) ì¥ì  íˆìŠ¤í† ê·¸ë¨ ëŒ€ë¹„ ë§¤ë„ëŸ¬ìš´ ë¶„í¬ ì¶”ì •, bin ê²½ê³„ ì¸ê³µíš¨ê³¼ê°€ ì ìŒ ì—°ì†í˜• ë°ì´í„°ì—ì„œ ëª¨ë“œ, ê¼¬ë¦¬(tail) êµ¬ì¡° í™•ì¸ì— ìœ ë¦¬ ë‹¨ì /ì£¼ì˜ ì„±ëŠ¥ì´ bandwidthì— ë§¤ìš° ë¯¼ê° â†’ Cross-Validation, Silverman's rule, Scott's rule ë“±ìœ¼ë¡œ ì„ íƒ ê²½ê³„(boundary) ê·¼ì²˜ bias ë°œìƒ ê°€ëŠ¥ â†’ reflection ë“± ë³´ì • ê¸°ë²• ê³ ë ¤ ê³ ì°¨ì›ì—ì„œëŠ” í‘œë³¸ í¬ì†Œí™”ë¡œ ì„±ëŠ¥ ê¸‰ê°(ì°¨ì›ì˜ ì €ì£¼) Python ì˜ˆì‹œ `python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity rng = np.random.default_rng(0) x = rng.normal(loc=0, scale=1, size=200)[:, None] KDE fit kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.3).fit(x) x_plot = np.linspace(-4, 4, 400)[:, None] density = np.exp(kde.score_samples(x_plot)) visualize vs histogram plt.hist(x.ravel(), bins=30, density=True, alpha=0.3, label=\"hist\") plt.plot(x_plot, density, label=\"KDE\") plt.legend(); plt.show() ` bandwidth ì„ íƒì€ GridSearchCV`ë¡œ êµì°¨ê²€ì¦í•´ ê²°ì •í•˜ëŠ” ê²ƒì´ ì‹¤ì „ì—ì„œ ê°€ì¥ ì•ˆì „í•¨. KDE vs Histogram íŠ¹ì§• íˆìŠ¤í† ê·¸ë¨ KDE ---- ---------------- ----------------- í˜•íƒœ ê³„ë‹¨ì‹ ë¶€ë“œëŸ¬ìš´ ê³¡ì„  íŒŒë¼ë¯¸í„° bin ê°œìˆ˜/í­ ë°´ë“œí­(bandwidth) ê³ ì°¨ì› bin í­ ì„¤ì • ì–´ë ¤ì›€ bandwidth ì„¤ì • ì–´ë ¤ì›€ ë¯¼ê°ë„ bin ê°œìˆ˜ì— ë¯¼ê° bandwidthì— ë¯¼ê° ì¥ì  êµ¬í˜„ ì‰¬ì›€, ì§ê´€ì  ë§¤ë„ëŸ½ê³  ë¶€ë“œëŸ¬ìš´ ë¶„í¬ ì¶”ì • ë‹¨ì  ê³ ì°¨ì›/ë°ì´í„° ì ì„ ë•Œ ë¶€ì •í™• bandwidth ì„ íƒì´ ì–´ë ¤ì›€",
    "public": true
  },
  {
    "id": "Data Science/Statistics/how-to-define-prediciton-interval",
    "title": "MLì˜ prediction intervalì€ ì–´ë–»ê²Œ ì •ì˜í•  ìˆ˜ ìˆì„ê¹Œ?",
    "date": "2025-08-12",
    "excerpt": "MLì˜ prediction interval ì •ì˜í•˜ëŠ” ë²• ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "ì°¸ê³ ìë£Œ 1 : Confidence vs Prediction Intervals: Understanding the Difference 2 : IBM Developer - Prediction intervals explained: A LightGBM tutorial Prediction Interval vs Confidence Interval Prediction Interval (PI): ìƒˆ ê´€ì¸¡ì¹˜ $Y$ê°€ ì£¼ì–´ì§„ $X$ì—ì„œ í¬í•¨ë  êµ¬ê°„. ì¡°ê±´ë¶€ coverageë¥¼ ëª©í‘œë¡œ í•¨. $$ \\mathbb{P}\\big( Y \\in [L_\\alpha(X),\\ U_\\alpha(X)]\\mid X\\big) \\ge 1-\\alpha $$ Confidence Interval (CI): ì¡°ê±´ë¶€ í‰ê·  $\\mu(X)=\\mathbb{E}[Y\\mid X]$ ê°™ì€ ëª¨ìˆ˜ì  targetì´ êµ¬ê°„ì— í¬í•¨ë  í™•ë¥ . $$ \\mathbb{P}\\big( \\mu(X)\\in \\hat C_\\alpha(X) \\big) \\ge 1-\\alpha $$ í•µì‹¬ ì°¨ì´: CIëŠ” mean estimatorì˜ ë¶ˆí™•ì‹¤ì„±(ì£¼ë¡œ epistemic)ì„, PIëŠ” mean ì£¼ë³€ì˜ noise(aleatoric)ê¹Œì§€ í¬í•¨í•´ â€œê°œë³„ ì˜ˆì¸¡ê°’â€ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ë‹¤ë£¸. ë”°ë¼ì„œ ë™ì¼í•œ ì„¤ì •ì—ì„œ PIì˜ í­ì´ CIë³´ë‹¤ ë„“ìŒ. MLì—ì„œì˜ ë¶ˆí™•ì‹¤ì„± (Uncertainty) Aleatoric uncertainty (ìš°ì—°ì  ë¶ˆí™•ì‹¤ì„±): ë°ì´í„° ìì²´ì˜ ë‚´ì¬ì  ë³€ë™ì„±(heteroscedastic noise í¬í•¨). ë” ë§ì€ ë°ì´í„°ë¡œë„ ì™„ì „íˆ ì œê±°ë˜ì§€ ì•ŠìŒ. ì˜ˆ: ì¸¡ì • ì˜¤ì°¨, ìì—°ì ì¸ ë³€ë™ì„±. Epistemic uncertainty (ì¸ì‹ë¡ ì  ë¶ˆí™•ì‹¤ì„±): ëª¨ë¸/íŒŒë¼ë¯¸í„°ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±. ë°ì´í„°ê°€ ë§ì•„ì§€ë©´ ê°ì†Œ. ì˜ˆ: ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì •ì˜ ë¶ˆí™•ì‹¤ì„±, ëª¨ë¸ êµ¬ì¡°ì˜ í•œê³„. CI vs PI: CIëŠ” ì£¼ë¡œ epistemic uncertaintyë¥¼, PIëŠ” epistemic + aleatoric uncertainty ëª¨ë‘ë¥¼ ë‹¤ë£¸. ë”°ë¼ì„œ ë™ì¼í•œ ì„¤ì •ì—ì„œ PIì˜ í­ì´ CIë³´ë‹¤ ë„“ìŒ. Calibration: ëª©í‘œ coverage(ì˜ˆ: 90%)ì— ì‹¤ì œ ì»¤ë²„ë¦¬ì§€ê°€ ë§ë„ë¡ ì¡°ì •. Coverage ìœ í˜•: marginal vs conditional. ì‹¤ë¬´ì—ì„œëŠ” ë¶„í¬ ê°€ì •ì´ ì•½í•  ë•Œë„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” marginal coverage ë„êµ¬(ì˜ˆ: conformal)ê°€ ìœ ìš©. LightGBMì—ì„œ Prediction Interval (ì°¸ê³  2) LGBMì—ì„œ prediction interval ê¸°ëŠ¥ë„ ì œê³µí•˜ëŠ”êµ¬ë‚˜. quantile-regression ê¸°ë°˜. > When you are performing regression tasks, you have the option of generating prediction intervals by using quantile regression, which is a fancy way of estimating the median value for a regression value in a specific quantile. > Simply put, a prediction interval is just about generating a lower and upper bound on the final regression value. > This is incredibly important for some tasks, which I explain in this article Why use them? (them -> prediction interval) > You can never be 100 percent certain about one prediction from one model. > Instead, the idea is to give an interval back to a person who ends up controlling the final decision based on the range that is given by the model. Quantile regression > In the typical linear regression model, you track the mean difference from the ground truth to optimize the model. > However, in quantile regression, as the name suggests, you track a specific quantile (also known as a percentile) against the median of the ground truth. Quantile and assumptions > Using the median approach lets you specify the quantiles ì¤‘ì•™ê°’(median) ì ‘ê·¼ë²•ì˜ ì¥ì : ë¶„ìœ„ìˆ˜ ì§€ì • ê°€ëŠ¥: 5% ë¶„ìœ„ìˆ˜(ë°ì´í„°ì˜ 5%ë¥¼ í¬í•¨)ì™€ 95% ë¶„ìœ„ìˆ˜(ë°ì´í„°ì˜ 95%ë¥¼ í¬í•¨)ë¥¼ ì§€ì •í•˜ì—¬ í•˜í•œê³¼ ìƒí•œ ê²½ê³„ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Outlierì— ê°•í•¨: í‰ê· (mean)ì„ ì‚¬ìš©í•  ë•ŒëŠ” outlierê°€ ìˆìœ¼ë©´ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í‰ê· ì€ outlier ê°’ì— í¬ê²Œ ì˜í–¥ì„ ë°›ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¤‘ì•™ê°’ì„ ì‚¬ìš©í•˜ë©´ outlierì— ëœ ë¯¼ê°í•˜ì—¬ ë” ë‚˜ì€ í•˜í•œê³¼ ìƒí•œ ê²½ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”: ì„ í˜• íšŒê·€ì™€ ë‹¬ë¦¬ ë°ì´í„°ì˜ ë¶„í¬ì— ëŒ€í•œ ê°€ì •ì„ í•˜ì§€ ì•Šì•„, íŠ¹ì • ìƒí™©ì—ì„œ ë” ìœ ìš©í•˜ê³  ì •í™•í•©ë‹ˆë‹¤. Regression $$ L = \\begin{cases} \\alpha(y - X\\theta), & \\text{if } (y - X\\theta) \\geq 0 \\\\ (\\alpha - 1)(y - X\\theta), & \\text{if } (y - X\\theta) < 0 \\end{cases} $$ > That changes in quantile regression because you must be able to account for the different quantiles. > - When the alpha is high (for example, 0.95), the errors that are less than zero receive a lower error value than if they are greater than zero. > - When the alpha is low (for example, 0.05), the errors that are less than zero receive a higher error value than if they are greater than zero, where they receive a smaller error value python code ``python import lightgbm as lgb regressor = lgb.LGBMRegressor() regressor.fit(x_train, y_train) regressor_pred = regressor.predict(x_test) lower = lgb.LGBMRegressor(objective = 'quantile', alpha = 1 - 0.95) lower.fit(x_train, y_train) lower_pred = lower.predict(x_test) upper = lgb.LGBMRegressor(objective = 'quantile', alpha = 0.95) upper.fit(x_train, y_train) upper_pred = upper.predict(x_test) plt.figure(figsize=(10, 6)) plt.scatter(x_test.MedInc, lower_pred, color='limegreen', marker='o', label='lower', lw=0.5, alpha=0.5) plt.scatter(x_test.MedInc, regressor_pred, color='aqua', marker='x', label='pred', alpha=0.7) plt.scatter(x_test.MedInc, upper_pred, color='dodgerblue', marker='o', label='upper', lw=0.5, alpha=0.5) plt.plot(sorted(x_test.MedInc), sorted(lower_pred), color='black') plt.plot(sorted(x_test.MedInc), sorted(regressor_pred), color='red') plt.plot(sorted(x_test.MedInc), sorted(upper_pred), color='black') plt.legend() plt.show() ` êµ¬í˜„ ë°©ë²•ë“¤ 1 Quantile Regression ì•„ì´ë””ì–´: $\\tau$-quantile í•¨ìˆ˜ $q_\\tau(x)$ë¥¼ ì§ì ‘ í•™ìŠµí•´ $[q_\\alpha(x), q_{1-\\alpha}(x)]$ë¥¼ PIë¡œ ì‚¬ìš©. Loss(â€œpinball lossâ€): $$ \\mathcal{L}_\\tau\\big(y, \\hat q_\\tau(x)\\big) = \\max\\big(\\, \\tau\\,[y-\\hat q_\\tau(x)],\\ (\\tau-1)[y-\\hat q_\\tau(x)]\\,\\big) $$ êµ¬í˜„ íŒ: ë‘ ëª¨ë¸(or multi-head)ë¡œ $q_\\alpha, q_{1-\\alpha}$ ë™ì‹œ í•™ìŠµ quantile crossing ë°©ì§€: monotonicity penalty ë˜ëŠ” joint training ì¥ì : heteroscedastic noiseì— ê°•í•¨, ë¶„í¬ ê°€ì • ìµœì†Œí™”. ë‹¨ì : calibration ë³´ì¥ì€ ì—†ìŒ(ì‚¬í›„ ì¡°ì • ê°€ëŠ¥). 2 Conformal Prediction ê°€ì •: ì˜ˆì¸¡ ì‹œì ê³¼ calibration ë°ì´í„°ê°€ exchangeable. Split conformal(ëŒ€ì¤‘ì  ì‹¤ë¬´ ì ˆì°¨): ë°ì´í„° ë¶„í• : train / calibration trainìœ¼ë¡œ predictor $\\hat f$ í•™ìŠµ calibrationì—ì„œ ë¹„ì í•©ë„(nonconformity) ê³„ì‚°. ëŒ€ì¹­ PIì˜ ê²½ìš° $r_i = y_i - \\hat f(x_i) $ $\\hat Q$ = $(1-\\alpha)(1+1/n_{cal})$-quantile of $\\{r_i\\}$ ìƒˆ ì…ë ¥ $x$ì— ëŒ€í•´ $[\\hat f(x)-\\hat Q,\\ \\hat f(x)+\\hat Q]$ Quantile ê¸°ë°˜(CQR): quantile regressors $\\hat q_\\alpha, \\hat q_{1-\\alpha}$ì˜ ë¹„ì í•©ë„ $$ e_i = \\max\\big(\\hat q_\\alpha(x_i)-y_i,\\ y_i-\\hat q_{1-\\alpha}(x_i),\\ 0\\big) $$ ë¥¼ í†µí•´ $\\hat Q$ë¥¼ êµ¬í•˜ê³ , ìµœì¢… êµ¬ê°„ì„ $[\\hat q_\\alpha(x)-\\hat Q,\\ \\hat q_{1-\\alpha}(x)+\\hat Q]$ë¡œ ì„¤ì •. ì¥ì : ë¶„í¬/ëª¨ë¸ì— ê±°ì˜ ë¹„ì˜ì¡´ì ì´ë©° finite-sample marginal coverage ë³´ì¥. ë‹¨ì : ë°ì´í„° ë¶„í•  í•„ìš”, conditional coverageëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë¯¸ë³´ì¥. 3 Ensemble Methods Bootstrap ensemble, Random Forest, Deep Ensemble ë“±ìœ¼ë¡œ ì—¬ëŸ¬ ì˜ˆì¸¡ $\\{\\hat f_k(x)\\}_{k=1}^K$ì„ ì–»ì–´ ë¶„ì‚°ì„ ì¶”ì •. ë‹¨ìˆœíˆ ensemble ë¶„ì‚°ìœ¼ë¡œ $\\hat f(x) \\pm z\\cdot \\hat\\sigma_{ens}(x)$ë¥¼ ë§Œë“¤ë©´ calibrationì´ ë³´ì¥ë˜ì§€ ì•ŠìŒ. ì‹¤ë¬´ íŒ: Jackknife+ / Bootstrap+ ê°™ì€ conformalized ensembleì„ ì‚¬ìš©í•˜ë©´ finite-sample coverageì— ê°€ê¹Œìš´ ë³´ì •ì„ ë‹¬ì„±. ì˜ˆ: Jackknife+ëŠ” leave-one-out ì˜ˆì¸¡ ë¶„í¬ë¥¼ ì´ìš©í•´ í•˜í•œ/ìƒí•œì˜ ì ì ˆí•œ order statisticì„ ì·¨í•´ êµ¬ê°„ êµ¬ì„±. ì¥ì : êµ¬í˜„ ìš©ì´, epistemic ë°˜ì˜. ë‹¨ì : ë°ì´í„°/ê³„ì‚° ë¹„ìš© ì¦ê°€, ë³„ë„ calibration ê¶Œì¥. 4 Bayesian Approaches ëª©í‘œ: posterior predictive $p(y\\mid x, \\mathcal{D})$ë¡œë¶€í„° êµ¬ê°„ ì‚°ì¶œ. $$ p(y\\mid x, \\mathcal{D}) = \\int p(y\\mid x, \\theta)\\, p(\\theta\\mid \\mathcal{D})\\, d\\theta $$ êµ¬í˜„ ì˜ˆ: Bayesian Linear Regression, Gaussian Process, MC Dropout(Bayesian approximation), Stochastic VI, Laplace Approximation ë“±. ì ˆì°¨: posterior predictiveì˜ quantileì„ ì·¨í•´ PI êµ¬ì„±. ì¥ì : ë¶ˆí™•ì‹¤ì„± ë¶„í•´ ê°€ëŠ¥(epistemic/aleatoric), í•´ì„ë ¥. ë‹¨ì : ëª¨ë¸/ì‚¬ì „ ë¶„í¬ì— ë¯¼ê°, ëŒ€ê·œëª¨ ë”¥ëŸ¬ë‹ì—ì„œ ê·¼ì‚¬ í•„ìš”. 5 MAE ê¸°ë°˜ Prediction Interval MAE(Mean Absolute Error)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ prediction intervalì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ë„ ìˆìŠµë‹ˆë‹¤. ê¸°ë³¸ ì•„ì´ë””ì–´ MAEì˜ ì˜ë¯¸: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ ê°„ì˜ í‰ê·  ì ˆëŒ€ ì˜¤ì°¨ ë¶„í¬ ê°€ì •: ì˜¤ì°¨ê°€ ëŒ€ì¹­ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•  ë•Œ, MAEë¥¼ ì´ìš©í•´ prediction intervalì„ êµ¬ì„±í•  ìˆ˜ ìˆìŒ êµ¬í˜„ ë°©ë²• ë‹¨ìˆœ MAE ê¸°ë°˜ `python MAE ê³„ì‚° mae = np.mean(np.abs(y_true - y_pred)) Prediction interval êµ¬ì„± (ëŒ€ì¹­ ë¶„í¬ ê°€ì •) 90% coverageë¥¼ ìœ„í•œ ê²½ìš° alpha = 0.1 z_score = norm.ppf(1 - alpha/2) # 1.645 for 90% coverage MAEë¥¼ í‘œì¤€í¸ì°¨ë¡œ ë³€í™˜ (Laplace ë¶„í¬ ê°€ì •) Laplace ë¶„í¬ì—ì„œ MAE = Ïƒâˆš2, ë”°ë¼ì„œ Ïƒ = MAE/âˆš2 sigma_est = mae / np.sqrt(2) Prediction interval lower_bound = y_pred - z_score sigma_est upper_bound = y_pred + z_score sigma_est ` Heteroscedastic MAE ê¸°ë°˜ `python ê° ì˜ˆì¸¡ê°’ë³„ë¡œ MAEë¥¼ ê³„ì‚° (ì˜ˆ: binning ë˜ëŠ” local averaging) def heteroscedastic_mae_based_pi(y_pred, y_true, x_features, bins=10): # ì˜ˆì¸¡ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ binning pred_bins = pd.cut(y_pred, bins=bins) # ê° binë³„ë¡œ MAE ê³„ì‚° bin_mae = {} for bin_name in pred_bins.cat.categories: mask = pred_bins == bin_name if mask.sum() > 0: bin_mae[bin_name] = np.mean(np.abs(y_true[mask] - y_pred[mask])) # ê° ì˜ˆì¸¡ê°’ì— ëŒ€í•´ í•´ë‹¹ binì˜ MAE ì‚¬ìš© pi_bounds = [] for i, pred in enumerate(y_pred): bin_name = pred_bins.iloc[i] if bin_name in bin_mae: mae_local = bin_mae[bin_name] sigma_local = mae_local / np.sqrt(2) # 90% coverage z_score = 1.645 lower = pred - z_score sigma_local upper = pred + z_score sigma_local pi_bounds.append((lower, upper)) else: pi_bounds.append((pred, pred)) return np.array(pi_bounds) ` ì¥ë‹¨ì  ì¥ì : êµ¬í˜„ ê°„ë‹¨: ë³µì¡í•œ ëª¨ë¸ë§ ì—†ì´ ê¸°ì¡´ MAE ì§€í‘œë¥¼ í™œìš© í•´ì„ ìš©ì´: MAEì˜ ì˜ë¯¸ë¥¼ ê·¸ëŒ€ë¡œ prediction intervalì— ë°˜ì˜ ê³„ì‚° íš¨ìœ¨: ì¶”ê°€ì ì¸ ëª¨ë¸ í•™ìŠµì´ë‚˜ ë³µì¡í•œ ì¶”ë¡  ê³¼ì • ë¶ˆí•„ìš” ë‹¨ì : ë¶„í¬ ê°€ì •: ì˜¤ì°¨ê°€ ëŒ€ì¹­ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì •ì´ í•„ìš” Coverage ë³´ì¥ ì—†ìŒ: ì‹¤ì œ coverageê°€ ëª©í‘œ coverageì™€ ì¼ì¹˜í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ Homoscedastic ê°€ì •: ê¸°ë³¸ì ìœ¼ë¡œëŠ” ëª¨ë“  ì˜ˆì¸¡ê°’ì— ë™ì¼í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì ìš© ê°œì„  ë°©ì•ˆ Calibration ì ìš© `python def calibrate_mae_based_pi(y_pred, y_true, target_coverage=0.9): # ì´ˆê¸° MAE ê¸°ë°˜ PI ìƒì„± mae = np.mean(np.abs(y_true - y_pred)) initial_pi = mae 1.645 # 90% coverage # Calibrationì„ ìœ„í•œ scaling factor ì°¾ê¸° empirical_coverage = np.mean( (y_true >= y_pred - initial_pi) & (y_true <= y_pred + initial_pi) ) # Scaling factor ì¡°ì • scaling_factor = target_coverage / empirical_coverage calibrated_pi = initial_pi scaling_factor return calibrated_pi ` Ensemble MAE í™œìš© `python def ensemble_mae_based_pi(ensemble_predictions, y_true): # Ensemble ì˜ˆì¸¡ê°’ë“¤ì˜ í‘œì¤€í¸ì°¨ ê³„ì‚° pred_std = np.std(ensemble_predictions, axis=0) # MAE ê³„ì‚° ensemble_mean = np.mean(ensemble_predictions, axis=0) mae = np.mean(np.abs(y_true - ensemble_mean)) # MAEì™€ ensemble ë¶„ì‚°ì„ ê²°í•© combined_uncertainty = np.sqrt(mae2 + pred_std2) # Prediction interval êµ¬ì„± z_score = 1.645 # 90% coverage lower_bound = ensemble_mean - z_score combined_uncertainty upper_bound = ensemble_mean + z_score combined_uncertainty return lower_bound, upper_bound `` ì‹¤ë¬´ ì ìš© ì‹œ ê³ ë ¤ì‚¬í•­ ë°ì´í„° íŠ¹ì„±: ì˜¤ì°¨ì˜ ë¶„í¬ê°€ ëŒ€ì¹­ì— ê°€ê¹Œìš´ì§€ í™•ì¸ Calibration í•„ìˆ˜: ì‹¤ì œ coverageì™€ ëª©í‘œ coverage ê°„ì˜ ì°¨ì´ë¥¼ ë³´ì • Heteroscedasticity: ì˜ˆì¸¡ê°’ì— ë”°ë¼ ë¶ˆí™•ì‹¤ì„±ì´ ë‹¬ë¼ì§€ëŠ” ê²½ìš° local MAE ì‚¬ìš© ê³ ë ¤ Ensembleê³¼ ê²°í•©: MAEì˜ ì¥ì ê³¼ ensembleì˜ epistemic uncertainty ì¶”ì •ì„ ê²°í•©í•˜ì—¬ ë” robustí•œ PI êµ¬ì„± ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸ Coverage vs Width: ëª©í‘œ coverage(ì˜ˆ: 90%)ë¥¼ ë‹¬ì„±í•˜ë©´ì„œ êµ¬ê°„ í­ì„ ìµœì†Œí™”. Calibration plot: nominal vs empirical coverage ê³¡ì„  í™•ì¸. Shift robustness: OOD ìƒí™©ì—ì„œ coverage ë¶•ê´´ ê°€ëŠ¥ â†’ ìœ„í—˜êµ¬ê°„ íƒì§€, abstention, ìµœê·¼ì ‘-ë°€ë„ ê¸°ë°˜ ë³´ì™„ ê³ ë ¤. ë©”ì†Œë“œ ì„ íƒ ê°€ì´ë“œ ë°ì´í„°ê°€ í¬ê³  ë¶„í¬ ê°€ì •ì´ ì•½í•¨: Conformal(CQR) ìš°ì„  Heteroscedastic ëšœë ·: Quantile Regression(+ conformal calibration) Ensembleì„ ì´ë¯¸ ì‚¬ìš©: Jackknife+/Bootstrap+ë¡œ conformalize Probabilistic modeling ì„ í˜¸/ê°€ëŠ¥: Bayesian posterior predictive",
    "public": true
  },
  {
    "id": "Data Science/Statistics/distribution-distance-implementation_notebook",
    "title": "ë¶„í¬ê°„ ì°¨ì´ êµ¬í˜„ ë…¸íŠ¸ë¶",
    "date": "2025-08-12",
    "excerpt": "ë¶„í¬ê°„ ì°¨ì´ êµ¬í˜„ ë…¸íŠ¸ë¶",
    "category": "Data Science",
    "tags": [
      "KDE",
      "Distribution Distance",
      "KL",
      "JS",
      "Wasserstein"
    ],
    "content": "--- title: \"ë¶„í¬ê°„ ì°¨ì´ êµ¬í˜„ ë…¸íŠ¸ë¶\" date: \"2025-08-12\" category: \"Data Science\" tags: [\"KDE\", \"Distribution Distance\", \"KL\", \"JS\", \"Wasserstein\"] excerpt: \"ë¶„í¬ê°„ ì°¨ì´ êµ¬í˜„ ë…¸íŠ¸ë¶\" --- ê¸°ë³¸ì ì¸ ì„¤ì •ë“¤ notebook kernel : docker jupyter lab kernel ì‚¬ìš© ì‹¤í—˜ë‚´ìš© > ë¶„í¬ ê°„ì— ì°¨ì´ë¥¼ ì¸¡ì •í•  ë–„ ì‚¬ìš©í•˜ëŠ” ì§€í‘œ êµ¬í˜„ Kullback-Leibler Divergence Jensen-Shannon Divergence Wasserstein Distance KDEë¥¼ í†µí•´ ë¶„í¬ ì¶”ì • (ì‚¬ì´ì¦ˆë³„) ë¶„í¬ ê°„ ì°¨ì´ ì¸¡ì • # Estimate Density Function from sklearn.neighbors import KernelDensity from sklearn.preprocessing import StandardScaler import numpy as np ìë™ ëŒ€ì—­í­ ì„ íƒì„ ìœ„í•œ í•¨ìˆ˜ def auto_bandwidth(data): \"\"\" Silvermanì˜ ê·œì¹™ì„ ì‚¬ìš©í•˜ì—¬ ìë™ ëŒ€ì—­í­ ì„ íƒ \"\"\" n = len(data) std = np.std(data) # Silverman's rule of thumb bandwidth = 1.06 std (n * (-1 / 5)) return bandwidth def kde_density_estimation(kde_data, bandwidth=20, kernel=\"gaussian\"): \"\"\" KDEë¥¼ ì‚¬ìš©í•˜ì—¬ ë°€ë„ ì¶”ì • \"\"\" kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(kde_data) density_values = np.exp(kde.score_samples(kde_data)) return kde, density_values scaler = StandardScaler() kde_data = data.drop([\"size\"], axis=1) normalized_data = scaler.fit_transform(kde_data) auto_bw = auto_bandwidth(normalized_data) print(auto_bw) kde, density_values = kde_density_estimation(normalized_data, bandwidth=auto_bw, kernel=\"gaussian\") print(density_values.shape) type(normalized_data) ì¶”ì •í•œ ë°€ë„ ë¶„í¬ í†µê³„ê°’ ë½‘ì•„ì„œ í™•ì¸í•´ë³´ê¸° # ë°€ë„ ê°’ë“¤ì˜ ê¸°ë³¸ í†µê³„ í™•ì¸ print(\"=== ë°€ë„ ê°’ í˜„í™© ===\") print(f\"ë°€ë„ ê°’ ê°œìˆ˜: {len(density_values)}\") print(f\"ìµœì†Œê°’: {np.min(density_values):.6f}\") print(f\"ìµœëŒ€ê°’: {np.max(density_values):.6f}\") print(f\"í‰ê· ê°’: {np.mean(density_values):.6f}\") print(f\"ì¤‘ê°„ê°’: {np.median(density_values):.6f}\") print(f\"í‘œì¤€í¸ì°¨: {np.std(density_values):.6f}\") print() 0.1ë³´ë‹¤ í° ê°’ì´ ìˆëŠ”ì§€ í™•ì¸ above_threshold = density_values[density_values > 0.1] print(f\"0.1ë³´ë‹¤ í° ê°’ ê°œìˆ˜: {len(above_threshold)}\") print(f\"0.1ë³´ë‹¤ í° ê°’ë“¤: {above_threshold}\") íŠ¹ì • í¬ì¸íŠ¸ì— ëŒ€í•œ ë°€ë„ ê°’ì„ í•´ì„í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ def interpret_density_value(density_values, target_density): \"\"\" íŠ¹ì • ë°€ë„ ê°’ì˜ ì˜ë¯¸ í•´ì„ \"\"\" print(f\"=== ë°€ë„ ê°’ {target_density} í•´ì„ ===\") # 1. ìƒëŒ€ì  ìœ„ì¹˜ percentile = np.mean(density_values <= target_density) 100 print(f\"ìƒëŒ€ì  ìœ„ì¹˜: í•˜ìœ„ {percentile:.1f}%\") # 2. ì¼ë°˜ì„± ì •ë„ if percentile < 25: generality = \"ë§¤ìš° íŠ¹ì´í•¨\" elif percentile < 50: generality = \"íŠ¹ì´í•¨\" elif percentile < 75: generality = \"ì¼ë°˜ì \" else: generality = \"ë§¤ìš° ì¼ë°˜ì \" print(f\"ì¼ë°˜ì„± ì •ë„: {generality}\") # 3. ìœ ì‚¬í•œ ë°€ë„ë¥¼ ê°€ì§„ ë°ì´í„° ìˆ˜ similar_count = np.sum(np.abs(density_values - target_density) < 0.005) print(f\"ìœ ì‚¬í•œ ë°€ë„ ë°ì´í„°: {similar_count}ê°œ\") return percentile, generality í•´ì„ ì‹¤í–‰ percentile, generality = interpret_density_value(density_values, 0.02) # ì‚¬ì´ì¦ˆ ë³„ ë¶„í¬ ì°¨ì´ í™•ì¸í•˜ê¸° # 1. ì‚¬ì´ì¦ˆë³„ ë°€ë„ ì¶”ì • import numpy as np from scipy.stats import entropy from sklearn.preprocessing import StandardScaler from src.utils.dist_diff import auto_bandwidth, kde_density_estimation scaler = StandardScaler() data_12_1 = data.loc[data[\"size\"] == \"12.1\"].drop([\"size\"], axis=1) data_12_1 = scaler.fit_transform(data_12_1) scaler = StandardScaler() data_12_6 = data.loc[data[\"size\"] == \"12.6\"].drop([\"size\"], axis=1) data_12_6 = scaler.fit_transform(data_12_6) scaler = StandardScaler() data_13_2 = data.loc[data[\"size\"] == \"13.2\"].drop([\"size\"], axis=1) data_13_2 = scaler.fit_transform(data_13_2) kde_12_1, density_values_12_1 = kde_density_estimation(data_12_1, bandwidth=auto_bandwidth(data_12_1), kernel=\"gaussian\") kde_12_6, density_values_12_6 = kde_density_estimation(data_12_6, bandwidth=auto_bandwidth(data_12_6), kernel=\"gaussian\") kde_13_2, density_values_13_2 = kde_density_estimation(data_13_2, bandwidth=auto_bandwidth(data_13_2), kernel=\"gaussian\") # 2. KL Divergence ê³„ì‚° np.random.seed(42) epsilon = 1e-10 p_hist = np.clip(density_values_12_1, epsilon, None) q_hist = np.clip(density_values_12_6, epsilon, None) q_hist = np.random.choice(q_hist, size=p_hist.shape[0]) # ì‚¬ì´ì¦ˆ ë§ì¶°ì£¼ê¸° ìœ„í•´ì„œ kl_value = entropy(p_hist, q_hist) # scipyì˜ entropyê°€ KL divergence ê³„ì‚° print(\"KL Divergence (12.1 vs 12.6):\", kl_value) kl_value = entropy(q_hist, p_hist) # scipyì˜ entropyê°€ KL divergence ê³„ì‚° print(\"KL Divergence (12.6 vs 12.1):\", kl_value) KL Divergence íŠ¹ì§•. ìˆœì„œ ë°”ê¾¸ë©´ ê°’ì´ ë‹¬ë¼ì§ noiseì— ì·¨ì•½í•¨ í˜¹ì€ ë¯¼ê°í•¨ # 3. JS Divergence ê³„ì‚° import numpy as np from scipy.stats import entropy def js_divergence(p: np.ndarray, q: np.ndarray) -> float: \"\"\" Jensen-Shannon Divergence ê³„ì‚° Args: p, q: ë‘ í™•ë¥  ë¶„í¬ (1ì°¨ì› ë°°ì—´) Returns: float: JS Divergence ê°’ \"\"\" # ë°°ì—´ ê¸¸ì´ ë§ì¶”ê¸° min_length = min(len(p), len(q)) p = np.random.choice(p, min_length) q = np.random.choice(q, min_length) # í™•ë¥  ë¶„í¬ë¡œ ì •ê·œí™” (í•©ì´ 1ì´ ë˜ë„ë¡) p_norm = p / np.sum(p) q_norm = q / np.sum(q) # ì¤‘ê°„ ë¶„í¬ m = (p + q) / 2 m = (p_norm + q_norm) / 2 # JS Divergence = (KL(p m) + KL(q m)) / 2 js_div = (entropy(p_norm, m) + entropy(q_norm, m)) / 2 return js_div ì‚¬ìš© ì˜ˆì‹œ js_div_value = js_divergence(p_hist, q_hist) print(f\"JS Divergence: {js_div_value:.6f}\") # 4. JS distance ê³„ì‚° def js_distance(p: np.ndarray, q: np.ndarray) -> float: \"\"\" Jensen-Shannon Distance ê³„ì‚° Args: p, q: ë‘ í™•ë¥  ë¶„í¬ (1ì°¨ì› ë°°ì—´) Returns: float: JS Distance ê°’ (0~1 ë²”ìœ„) \"\"\" # JS Divergence ê³„ì‚° js_div = js_divergence(p, q) # JS Distance = sqrt(JS Divergence) js_dist = np.sqrt(js_div) return js_dist ì‚¬ìš© ì˜ˆì‹œ js_dist_value = js_distance(p_hist, q_hist) print(f\"JS Distance: {js_dist_value:.6f}\") JS DistanceëŠ” 0~1 ë²”ìœ„ print(\"=== JS Distance í•´ì„ ê°€ì´ë“œ ===\") if js_dist_value < 0.1: print(\"0.0 ~ 0.1: ë§¤ìš° ìœ ì‚¬í•œ ë¶„í¬ (ê±°ì˜ ë™ì¼)\") elif js_dist_value < 0.2: print(\"0.1 ~ 0.2: ìœ ì‚¬í•œ ë¶„í¬ (ë¹„ìŠ·í•¨)\") elif js_dist_value < 0.3: print(\"0.2 ~ 0.3: ì•½ê°„ ë‹¤ë¥¸ ë¶„í¬ (ì°¨ì´ê°€ ìˆìŒ)\") elif js_dist_value < 0.5: print(\"0.3 ~ 0.5: ìƒë‹¹íˆ ë‹¤ë¥¸ ë¶„í¬ (ëª…í™•í•œ ì°¨ì´)\") elif js_dist_value < 0.7: print(\"0.5 ~ 0.7: ë§¤ìš° ë‹¤ë¥¸ ë¶„í¬ (í° ì°¨ì´)\") else: print(\"0.7 ~ 1.0: ì™„ì „íˆ ë‹¤ë¥¸ ë¶„í¬ (ê·¹ë„ë¡œ ë‹¤ë¦„)\") from scipy.stats import wasserstein_distance def wasserstein_distance_scipy(p: np.ndarray, q: np.ndarray) -> float: \"\"\" scipyë¥¼ ì‚¬ìš©í•œ Wasserstein Distance ê³„ì‚° ìˆ˜ì‹: W(p,q) = Î£ x_i - y_i / n \"\"\" return wasserstein_distance(p, q) wasserstein_distance(p_hist, q_hist)",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-symbolic-regressor-model",
    "title": "Symbolic Regressor Modelì´ë€?",
    "date": "2025-08-11",
    "excerpt": "Symbolic Regressor Modelì˜ ê°œë…ê³¼ í™œìš©ë˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "statistics"
    ],
    "content": "Symbolic Regressor Modelì´ë€? --- ì°¸ê³ ìë£Œ Wikipedia - Symbolic regression PySR: High-Performance Symbolic Regression in Python gplearn (Genetic Programming in Python) [Koza (1992) Genetic Programming] ê°œë… ìš”ì•½ Symbolic Regressionì€ ë°ì´í„°ë¡œë¶€í„° â€œí•¨ìˆ˜ í˜•íƒœ(functional form)â€ ìì²´ë¥¼ íƒìƒ‰í•´ ë°œê²¬í•˜ëŠ” ë°©ë²•. ì¦‰, ê³ ì •ëœ ëª¨ë¸ í˜•íƒœì— íŒŒë¼ë¯¸í„°ë§Œ ë§ì¶”ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, operators (e.g., +, âˆ’, Ã—, Ã·, sin, exp, log)ì™€ variablesë¥¼ ì¡°í•©í•´ interpretable equationsë¥¼ ì°¾ëŠ”ë‹¤. ëª©í‘œ: predictive accuracyì™€ parsimony ì‚¬ì´ì˜ ê· í˜•ì„ ë§ì¶˜ interpretable model ë°œê²¬ í•µì‹¬ ì•„ì´ë””ì–´: operators/constants/variablesë¡œ ì´ë¤„ì§„ tree-structured expressionsë¥¼ íƒìƒ‰(ì£¼ë¡œ Genetic Programming)í•˜ì—¬ lossë¥¼ ìµœì†Œë¡œ í•˜ëŠ” ìˆ˜ì‹ ì°¾ê¸° ì–´ë–»ê²Œ ì‘ë™í•˜ë‚˜ (ì „í˜•ì  íŒŒì´í”„ë¼ì¸) 1) Expression space ì •ì˜: ê°€ëŠ¥í•œ operator set, constants, variables ëª©ë¡ ì •ì˜ 2) Initialize expressions: ë¬´ì‘ìœ„ë¡œ ì—¬ëŸ¬ expression trees ìƒì„± 3) Evaluate objective: data error (e.g., MSE)ì™€ complexity (ì˜ˆ: node count)ë¥¼ í•¨ê»˜ ê³ ë ¤ 4) Search (evolution): selection, crossover, mutationìœ¼ë¡œ ë” ì¢‹ì€ ìˆ˜ì‹ì„ ì ì§„ì ìœ¼ë¡œ íƒìƒ‰ 5) Pareto frontier: accuracyì™€ complexity ê°„ ìµœì  ì ˆì¶©ì˜ í›„ë³´ë¥¼ ì—¬ëŸ¬ ê°œ ìœ ì§€ loss functionì€ ì¼ë°˜ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆë‹¤. $$ \\mathcal{L}(f) = \\underbrace{\\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2}_{\\text{prediction error}} \\; + \\; \\lambda \\cdot \\underbrace{\\text{Complexity}(f)}_{\\text{expression complexity}} $$ ì¥ì ê³¼ ë‹¨ì  ì¥ì  Interpretability: ê²°ê³¼ê°€ ëª…ì‹œì  ìˆ˜ì‹ì´ë¼ ì„¤ëª…ì´ ì‰½ë‹¤ Expressiveness: ë¹„ì„ í˜• ê´€ê³„ë¥¼ ìœ ì—°í•˜ê²Œ í¬ì°© Knowledge discovery: ë°ì´í„°ì— ìˆ¨ê²¨ì§„ ê·œì¹™ì´ë‚˜ ë²•ì¹™ì„ ë“œëŸ¬ë‚´ê¸° ì¢‹ìŒ ë‹¨ì  Computational cost: í‘œí˜„ì‹ ê³µê°„ì´ ë°©ëŒ€, íƒìƒ‰ ë¹„ìš©ì´ í¼ Overfitting risk: operators/maximum depthë¥¼ ê³¼ë„í•˜ê²Œ í—ˆìš©í•˜ë©´ ë…¸ì´ì¦ˆì— ì í•© Reproducibility: ì§„í™”ì  íƒìƒ‰ì€ ë‚œìˆ˜ ì‹œë“œì— ë¯¼ê°í•  ìˆ˜ ìˆìŒ ì–¸ì œ ìœ ìš©í•œê°€ ê´€ê³„ë¥¼ ì„¤ëª…í•  â€œclosed-form equationâ€ì´ í•„ìš”í•œ ê²½ìš° (ê³¼í•™/ê³µí•™ ëª¨ë¸ë§) feature interactionsê°€ ë³µì¡í•´ ì „í†µì  ì„ í˜•ëª¨ë¸ì´ í•œê³„ì¼ ë•Œ black-box ì„±ëŠ¥ë„ ì¤‘ìš”í•˜ì§€ë§Œ, interpretabilityì™€ parsimonyê°€ í•µì‹¬ì¼ ë•Œ ê°„ë‹¨í•œ ì˜ˆì‹œ ì˜ˆë¥¼ ë“¤ì–´, ë°ì´í„°ê°€ ë‹¤ìŒê³¼ ê°™ì€ ê·œì¹™ì„ ë”°ë¥¸ë‹¤ê³  í•˜ì: $$ y = 2\\,\\sin(x) + 0.1\\,x^2 + \\varepsilon $$ Symbolic Regressionì€ ìœ„ì™€ ìœ ì‚¬í•œ í˜•íƒœì˜ ìˆ˜ì‹ì„ ë°ì´í„°ë§Œ ë³´ê³  ë°œê²¬í•˜ë ¤ ì‹œë„í•œë‹¤. ì‹¤ì œë¡œëŠ” coefficientsë‚˜ operator ì„ íƒì´ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. Pythonìœ¼ë¡œ ë§›ë³´ê¸° (PySR) ``python pip install pysr import numpy as np from pysr import PySRRegressor rng = np.random.default_rng(42) X = rng.uniform(-3.0, 3.0, size=(200, 1)) y = 2 np.sin(X[:, 0]) + 0.1 (X[:, 0] * 2) + rng.normal(0, 0.1, size=200) model = PySRRegressor( niterations=200, unary_operators=(\"sin\", \"cos\", \"exp\", \"log\"), binary_operators=(\"+\", \"-\", \"\", \"/\"), loss=\"loss(x, y) = (x - y)^2\", maxsize=20, populations=20, progress=False, random_state=42, ) model.fit(X, y) print(model) # íŒŒë ˆí†  í”„ë¡ í‹°ì–´ í›„ë³´ ìš”ì•½ print(model.sympy()) # ìµœê³  ì„±ëŠ¥/ë‹¨ìˆœì„± ì ˆì¶© ì‹ í•˜ë‚˜ ì¶œë ¥ ` ëª¨ë¸ì€ accuracy-complexity ì ˆì¶©ì„ ê³ ë ¤í•œ ì—¬ëŸ¬ ìˆ˜ì‹ì„ ì œì‹œí•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœê°€ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤. $$ \\hat{y} = 1.98\\,\\sin(x) + 0.12\\,x^2 $$ ì‹¤ë¬´ íŒ operator setì„ ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ ì œí•œí•˜ë©´ íƒìƒ‰ íš¨ìœ¨ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì´ í¬ê²Œ ê°œì„  expression complexity ì œì•½(maxsize, operator weights)ì„ ê°•í•˜ê²Œ ì£¼ì–´ overfitting ë°©ì§€ cross-validationìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ ì ê²€, random seed ê³ ì •ìœ¼ë¡œ reproducibility ê´€ë¦¬ íƒìƒ‰ ì‹œê°„ ì˜ˆì‚°ì„ ë‹¨ê³„ì ìœ¼ë¡œ ëŠ˜ë ¤ê°€ë©° Pareto frontierë¥¼ ë¹„êµ ê´€ë ¨ ëª¨ë¸/ê°œë… Genetic Programming Interpretable ML Sparse modeling(L1/L0)ê³¼ì˜ ë¹„êµ: ì „ìëŠ” ì£¼ì–´ì§„ í˜•íƒœ ë‚´ ê³„ìˆ˜ í¬ì†Œí™”, SRì€ í˜•íƒœ ìì²´ë¥¼ íƒìƒ‰ Genetic Programming(GP)ë€? Genetic Programmingì€ evolutionary algorithm ê³„ì—´ë¡œ, í”„ë¡œê·¸ë¨ ë˜ëŠ” ìˆ˜ì‹(expression)ì„ ì§ì ‘ â€œì§„í™”â€ì‹œì¼œì„œ ì£¼ì–´ì§„ ëª©ì í•¨ìˆ˜(fitness)ë¥¼ ìµœëŒ€í™”/ìµœì†Œí™”í•˜ëŠ” ê¸°ë²•ì´ë‹¤. Symbolic Regressionì—ì„œëŠ” operators, variables, constantsë¡œ êµ¬ì„±ëœ expression treeë¥¼ ê°œì²´ë¡œ ì·¨ê¸‰í•˜ì—¬ íƒìƒ‰í•œë‹¤. Expression representation: tree-structured expressions (nodes = operators, leaves = variables/constants) Search operators: Selection: tournament selection ë“±ìœ¼ë¡œ ìƒìœ„ ê°œì²´ ì„ ë³„ Crossover: subtree ë‹¨ìœ„ êµì°¨ë¡œ ë‘ ì‹ì„ ê²°í•© Mutation: point/subtree/hoist mutationìœ¼ë¡œ ì‹ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ ë³€ì´ Objective(Fitness): ì˜ˆì¸¡ ì˜¤ì°¨(MSE ë“±)ì™€ complexity(ë…¸ë“œ ìˆ˜, ê¹Šì´ ë“±)ë¥¼ í•¨ê»˜ ê³ ë ¤ $$ \\min_f \\; (\\, \\text{MSE}(f),\\; \\text{Complexity}(f) \\,) $$ Key hyperparameters: population size, generations, max depth/size, crossover rate, mutation rate, tournament size, parsimony coefficient Pros/Cons: Pros: ë§¤ìš° ìœ ì—°í•œ ê°€ì„¤ ê³µê°„, ë†’ì€ í•´ì„ ê°€ëŠ¥ì„±, ë³€ìˆ˜ ìƒí˜¸ì‘ìš© ìë™ ë°œê²¬ Cons: ê³„ì‚° ë¹„ìš© í¼, expression bloat(ë¶ˆí•„ìš”í•˜ê²Œ ì»¤ì§) ìœ„í—˜, í™•ë¥ ì  ë³€ë™ì„± Mitigation: parsimony pressure(ë³µì¡ë„ íŒ¨ë„í‹°), depth/size cap, ë„ë©”ì¸ priorsë¡œ ì´ˆê¸° ê°œì²´ ì‹œë”© ê´€ê³„ ì •ë¦¬: GPëŠ” SRì—ì„œ ê°€ì¥ ë„ë¦¬ ì“°ì´ëŠ” search backboneì´ë©°, ëŒ€ì•ˆìœ¼ë¡œ simulated annealing, MCMC, reinforcement learning ê¸°ë°˜ íƒìƒ‰ ë“±ì´ í™œìš©ë˜ê¸°ë„ í•œë‹¤. Genetic Programming ìƒì„¸ ì„¤ëª… Representation (tree-based expressions): ê° ê°œì²´ëŠ” expression treeë¡œ í‘œí˜„. ë‚´ë¶€ ë…¸ë“œ = operators(ì˜ˆ: +, âˆ’, Ã—, Ã·, sin, exp, log), ë¦¬í”„ = variables/constants(ephemeral random constants í¬í•¨). Function/Terminal set ì„¤ê³„: ë„ë©”ì¸ ì§€ì‹ìœ¼ë¡œ operator ì§‘í•©ê³¼ ë³€ìˆ˜, ìƒìˆ˜ ë²”ìœ„ë¥¼ ì œí•œí•˜ë©´ íƒìƒ‰ íš¨ìœ¨ê³¼ í•´ì„ ê°€ëŠ¥ì„±ì´ í–¥ìƒ. Initialization (ramped half-and-half): full/grow ë°©ì‹ì„ ì„ì–´ ë‹¤ì–‘í•œ ê¹Šì´/êµ¬ì¡°ì˜ ì´ˆê¸° ê°œì²´ë¥¼ ìƒì„±. Selection (tournament ë“±): ì—¬ëŸ¬ ê°œì²´ë¥¼ ë¬´ì‘ìœ„ë¡œ ë½‘ì•„ ìµœì  ê°œì²´ë¥¼ ì„ íƒ. ê°„ë‹¨í•˜ê³  ë³‘ë ¬í™” ìš©ì´. Variation operators: Crossover: ë‘ ë¶€ëª¨ì˜ subtreeë¥¼ êµí™˜í•´ ìì‹ ìƒì„± Mutation: point/subtree/hoist mutationìœ¼ë¡œ ì¼ë¶€ êµ¬ì¡°ë¥¼ ë¬´ì‘ìœ„ ë³€ê²½ Reproduction: ìš°ìˆ˜ ê°œì²´ë¥¼ ê·¸ëŒ€ë¡œ ë‹¤ìŒ ì„¸ëŒ€ë¡œ ë³µì‚¬(elitismì™€ í•¨ê»˜ ì‚¬ìš©) Fitness/objective: ë³´í†µ ì˜ˆì¸¡ ì˜¤ì°¨(MSE, MAE ë“±)ë¥¼ ìµœì†Œí™”í•˜ë©´ì„œ complexity(size, depth, description length ë“±)ë„ í•¨ê»˜ ìµœì†Œí™”í•˜ëŠ” multi-objective ê´€ì  $$ \\min_f \\; \\Big[\\; \\text{Error}(f),\\; \\text{Complexity}(f) \\;\\Big] $$ Scalarization (Parsimony pressure): multi-objectiveë¥¼ ë‹¨ì¼ ëª©ì ë¡œ í•©ì³ penaltyë¥¼ ì£¼ê¸°ë„ í•¨ $$ \\text{Fitness}_{\\text{penalized}}(f) = \\text{MSE}(f) + \\alpha\\,\\text{size}(f) $$ Non-dominated sorting (ì˜ˆ: NSGA-II): Pareto frontier ê¸°ë°˜ìœ¼ë¡œ dominance ê´€ê³„ë¡œ ìˆœìœ„ë¥¼ ë§¤ê²¨ ë‹¤ì–‘í•œ ì ˆì¶©í•´ë¥¼ ìœ ì§€ Bloat control: expression bloat ë°©ì§€ìš©ìœ¼ë¡œ max depth/size, lexicographic parsimony pressure, size-fair crossover ë“±ì„ ì‚¬ìš© Constants ì²˜ë¦¬: ephemeral random constants(ë¬´ì‘ìœ„ ìƒìˆ˜)ë¥¼ ë‘ê³ , í•„ìš” ì‹œ local search/gradient-free tuningìœ¼ë¡œ ê³„ìˆ˜ ë¯¸ì„¸ ì¡°ì • Typing/Grammar constraints: strongly typed GPë‚˜ grammar-guided GPë¡œ ìœ íš¨í•œ ìˆ˜ì‹ë§Œ ìƒì„±í•˜ë„ë¡ ì œí•œ Validation/Elitism: validation ì„±ëŠ¥ìœ¼ë¡œ ì¡°ê¸° ì¢…ë£Œì™€ model pickì„ ì•ˆì •í™”, ìš°ìˆ˜ ê°œì²´ë¥¼ ë‹¤ìŒ ì„¸ëŒ€ì— ë³´ì¡´ ê°„ë‹¨ ì•Œê³ ë¦¬ì¦˜ ìŠ¤ì¼€ì¹˜ `text Initialize population P0 with ramped half-and-half for gen in 1..G: Evaluate Error(f) and Complexity(f) for all f in P_{gen-1} Update Pareto frontier with elitism while P_gen < population_size: Select parents via tournament selection With prob pc: offspring = crossover(parent1, parent2) With prob pm: offspring = mutate(offspring) Enforce size/depth constraints on offspring Add offspring to P_gen Return best model on validation (accuracy-parsimony trade-off) ` ì‹¤ë¬´ ì…‹ì—… íŒ function set: ë„ë©”ì¸ì— ë§ëŠ” ì—°ì‚°ìë§Œ(ì˜ˆ: ì•ˆì „í•œ division, log1p) ì‚¬ìš©í•´ ìˆ˜ì‹ ì•ˆì •ì„± í™•ë³´ constraints: max depth/size, safe operators, domain boundsë¡œ íƒìƒ‰ ê³µê°„ ì œì–´ objectives: validation error + complexityë¥¼ í•¨ê»˜ ëª¨ë‹ˆí„°ë§(ë˜ëŠ” parsimony penalty) search budget: generations, population sizeë¥¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë¦¬ë©° Pareto set ë¹„êµ reproducibility: random seed ê³ ì •, ê²°ê³¼ ìˆ˜ì‹ì€ ì‹¬í”Œí•œ ê²ƒë¶€í„° ìš°ì„  ê²€í†  PySRì™€ì˜ ì—°ê²° PySRëŠ” multi-population evolutionary searchì™€ parsimony pressureë¥¼ í™œìš©í•´ ë¹ ë¥´ê²Œ Pareto frontierë¥¼ êµ¬ì¶• í•˜ì´í¼íŒŒë¼ë¯¸í„° ì˜ˆ: niterations, maxsize, binary_operators, unary_operators, loss, populations gplearnë¡œ êµ¬í˜„í•˜ê¸° (SymbolicRegressor) gplearnì€ scikit-learn ìŠ¤íƒ€ì¼ì˜ Symbolic Regression/Transformationì„ ì œê³µí•œë‹¤. ê¸°ë³¸ ì—°ì‚°ìëŠ” protected operatorsë¥¼ ì‚¬ìš©í•´ div(0 ë‚˜ëˆ—ì…ˆ ë³´í˜¸), log(ìŒìˆ˜/0 ë³´í˜¸) ë“± ìˆ˜ì¹˜ ì•ˆì •ì„±ì„ í™•ë³´í•œë‹¤. Quick start `python pip install gplearn import numpy as np from gplearn.genetic import SymbolicRegressor rng = np.random.default_rng(42) X = rng.uniform(-3.0, 3.0, size=(200, 1)) y = 2 np.sin(X[:, 0]) + 0.1 (X[:, 0] 2) + rng.normal(0, 0.1, size=200) est = SymbolicRegressor( population_size=2000, generations=30, function_set=(\"add\", \"sub\", \"mul\", \"div\", \"sin\", \"cos\", \"log\", \"sqrt\"), metric=\"mse\", parsimony_coefficient=0.001, p_crossover=0.8, p_subtree_mutation=0.05, p_hoist_mutation=0.01, p_point_mutation=0.05, init_depth=(2, 5), init_method=\"half and half\", const_range=(-2.0, 2.0), tournament_size=20, stopping_criteria=0.0, max_samples=0.9, n_jobs=-1, random_state=42, verbose=1, ) est.fit(X, y) print(est._program) # ë°œê²¬ëœ ìµœì¢… ìˆ˜ì‹ ë¬¸ìì—´ print(est.predict(X)[:5]) ` ì£¼ìš” ì¸ì ì„¤ëª… population_size: ì„¸ëŒ€ë‹¹ ê°œì²´ ìˆ˜. í´ìˆ˜ë¡ íƒìƒ‰ í­â†‘, ì‹œê°„/ë©”ëª¨ë¦¬ ë¹„ìš©â†‘ generations: ì„¸ëŒ€ ìˆ˜. ì˜ˆì‚°(budget)ê³¼ í•¨ê»˜ ì¡°ì ˆ function_set: ì‚¬ìš© ì—°ì‚°ì ì„¸íŠ¸. ë„ë©”ì¸ì— ë§ê²Œ ì¶•ì†Œ ê¶Œì¥. div, log, sqrt ë“±ì€ protected ë²„ì „ metric: \"mse\", \"mae\" ë˜ëŠ” ì‚¬ìš©ì ì •ì˜ í•¨ìˆ˜ ê°€ëŠ¥ parsimony_coefficient: ëª¨ë¸ í¬ê¸°(size)ì— ëŒ€í•œ íŒ¨ë„í‹° ê°•ë„. ê³¼ì í•©/ë¸”ë¡œíŠ¸ ì™„í™” p_crossover / p_subtree_mutation / p_hoist_mutation / p_point_mutation: variation í™•ë¥ . êµì°¨/ì„œë¸ŒíŠ¸ë¦¬/í˜¸ì´ìŠ¤íŠ¸/í¬ì¸íŠ¸ ë³€ì´ ë¹„ìœ¨ init_depth, init_method: ì´ˆê¸° íŠ¸ë¦¬ ê¹Šì´/ë°©ì‹(\"full\", \"grow\", \"half and half\") const_range: ephemeral random constants ë²”ìœ„. Noneì´ë©´ ìƒìˆ˜ ë¹„í™œì„±í™” tournament_size: selection ì••ë ¥ ê°•ë„. í´ìˆ˜ë¡ exploitationâ†‘, ë‹¤ì–‘ì„±â†“ stopping_criteria: metricì´ ì„ê³„ê°’ ì´í•˜ê°€ ë˜ë©´ ì¡°ê¸° ì¢…ë£Œ max_samples: ê° ì„¸ëŒ€ í•™ìŠµì— ì‚¬ìš©í•˜ëŠ” ìƒ˜í”Œ ë¹„ìœ¨(bootstrap/bagging íš¨ê³¼) n_jobs: ë³‘ë ¬ ì²˜ë¦¬ ìˆ˜. -1ì€ CPU ì „ë¶€ ì‚¬ìš© random_state: ì¬í˜„ì„± verbose: ì§„í–‰ ë¡œê·¸ ì¶œë ¥ ì°¸ê³ : SymbolicTransformerëŠ” íŠ¹ì§• ìƒì„±ê¸°ì²˜ëŸ¼ ìƒˆ featureë“¤ì„ ë§Œë“¤ì–´ downstream ëª¨ë¸ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŒ ì‹¤ë¬´ ê¶Œì¥ ì„¤ì • ì—°ì‚°ì: ë„ë©”ì¸ì— ë§ëŠ” ìµœì†Œ ì§‘í•©ìœ¼ë¡œ ì‹œì‘í•˜ê³  í•„ìš” ì‹œ ì¶”ê°€ íŒ¨ë„í‹°: parsimony_coefficientë¥¼ ì¶©ë¶„íˆ í¬ê²Œ ì„¤ì •í•´ ë‹¨ìˆœí•œ ì‹ ìš°ì„  ì œì•½: init_depth, const_range, tournament_sizeë¥¼ ì™„ë§Œíˆ ì¡°ì •í•˜ë©° overfitting/bloat ê· í˜• ê²€ì¦: validation ì„±ëŠ¥ìœ¼ë¡œ best programì„ ì„ íƒí•˜ê³  seed ê³ ì • gplearn ì‹¤í—˜ ê°€ì´ë“œ: í•˜ì´í¼íŒŒë¼ë¯¸í„° ì…‹ì—… ë¹ ë¥¸ ë² ì´ìŠ¤ë¼ì¸ ê¶Œì¥ê°’ Small data (< 10k rows): population_size=2000, generations=30 function_set=(\"add\",\"sub\",\"mul\",\"div\",\"sin\",\"cos\",\"log\",\"sqrt\") parsimony_coefficient=0.001 p_crossover=0.8, p_subtree_mutation=0.05, p_point_mutation=0.05, p_hoist_mutation=0.01 init_depth=(2,5), init_method=\"half and half\", const_range=(-2,2) tournament_size=20, metric=\"mse\", max_samples=1.0, n_jobs=-1, random_state=42 Medium data (10kâ€“100k rows): population_size=3000â€“5000, generations=50â€“100 parsimony_coefficient=0.003â€“0.01 (bloat ë°œìƒ ì‹œ ìƒí–¥) max_samples=0.7â€“0.9ë¡œ ì•½í•œ bagging ì ìš©í•´ generalization í–¥ìƒ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„° ìƒí˜¸ì‘ìš© population_size Ã— generations: íƒìƒ‰ ì˜ˆì‚°. ë‘˜ ë‹¤ ì¦ê°€í•˜ë©´ ì„±ëŠ¥ ì ì¬ë ¥â†‘, ì‹œê°„/ë©”ëª¨ë¦¬ ë¹„ìš©â†‘ tournament_size: selection ì••ë ¥. í¬ê²Œ í•˜ë©´ exploitationâ†‘, ë‹¤ì–‘ì„±â†“ â†’ ë„ˆë¬´ í¬ë©´ ì¡°ê¸° ìˆ˜ë ´ ìœ„í—˜ parsimony_coefficient: size penalty ê°•ë„. í´ìˆ˜ë¡ ë‹¨ìˆœí•œ ì‹ ì„ í˜¸, ê³¼ì í•©/ë¸”ë¡œíŠ¸ ì™„í™” function_set: ë„ë©”ì¸ ë§ì¶¤ ì¶•ì†Œ ê¶Œì¥. ë¶ˆí•„ìš” ì—°ì‚°ì ì œê±°ë¡œ íƒìƒ‰ ê³µê°„ ì¶•ì†Œ, í•´ì„ì„±â†‘ variation í™•ë¥ : p_crossoverëŠ” ë†’ê²Œ, mutation ë¹„ìœ¨ì€ ì‘ê²Œ ì‹œì‘. ì •ì²´ ì‹œ mutation ì†Œí­â†‘ init_depth / init_method: ì´ˆê¸° ë‹¤ì–‘ì„±ì— ì˜í–¥. ë„ˆë¬´ ì–•ìœ¼ë©´ íƒìƒ‰ ë¶€ì¡±, ë„ˆë¬´ ê¹Šìœ¼ë©´ ì´ˆê¸°ì— bloat max_samples: 1.0 ë¯¸ë§Œì´ë©´ ì„¸ëŒ€ë³„ subsamplingìœ¼ë¡œ bagging ìœ ì‚¬ íš¨ê³¼ â†’ ê³¼ì í•© ì™„í™” ì•ˆì •ì„±ê³¼ ì „ì²˜ë¦¬ gplearnì˜ div, log, sqrtëŠ” protected operatorsë¼ ìˆ˜ì¹˜ ì•ˆì •ì„± ì–‘í˜¸ feature scalingì€ í•„ìˆ˜ëŠ” ì•„ë‹ˆì§€ë§Œ íƒìƒ‰ ì•ˆì •í™”ì— ë„ì›€(í‘œì¤€í™”/ì •ê·œí™” ê¶Œì¥) íƒ€ê¹ƒ ìŠ¤ì¼€ì¼ë„ ì‹ ë³µì¡ë„ì— ì˜í–¥. í•„ìš” ì‹œ íƒ€ê¹ƒ ìŠ¤ì¼€ì¼ë§ í›„ ì¶”ë¡  ì‹œ ì—­ë³€í™˜ Validationê³¼ ì¬í˜„ì„± train/validation ë¶„ë¦¬ í›„ validation MSEë¡œ ì„¤ì • ë¹„êµ random_state ê³ ì •, ê²°ê³¼ ìˆ˜ì‹ ê¸¸ì´ì™€ ì„±ëŠ¥ì„ í•¨ê»˜ ê¸°ë¡í•´ ë‹¨ìˆœí•œ ì‹ ìš°ì„  `python from sklearn.model_selection import train_test_split from gplearn.genetic import SymbolicRegressor X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42) est = SymbolicRegressor( population_size=3000, generations=50, function_set=(\"add\",\"sub\",\"mul\",\"div\",\"sin\",\"cos\",\"log\",\"sqrt\"), metric=\"mse\", parsimony_coefficient=0.003, p_crossover=0.8, p_subtree_mutation=0.05, p_hoist_mutation=0.01, p_point_mutation=0.05, init_depth=(2,5), init_method=\"half and half\", const_range=(-2,2), tournament_size=20, max_samples=0.9, n_jobs=-1, random_state=42, ) est.fit(X_tr, y_tr) val_mse = ((est.predict(X_val) - y_val) 2).mean() ` Custom function ì¶”ê°€í•˜ê¸° make_functionìœ¼ë¡œ ë„ë©”ì¸ ì¹œí™”ì  ì—°ì‚°ìë¥¼ ì¶”ê°€í•˜ë©´ íƒìƒ‰ íš¨ìœ¨ì´ í¬ê²Œ í–¥ìƒë  ìˆ˜ ìˆë‹¤. `python import numpy as np from gplearn.functions import make_function def safe_exp(x): return np.exp(np.clip(x, -10, 10)) # í­ì£¼ ë°©ì§€ Exp = make_function(function=safe_exp, name=\"exp\", arity=1) est = SymbolicRegressor( function_set=(\"add\",\"sub\",\"mul\",\"div\",\"sin\",\"cos\",\"log\",\"sqrt\", Exp), # ... ê¸°íƒ€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ... ) ` ëª¨ë‹ˆí„°ë§ê³¼ ë¸”ë¡œíŠ¸ ì œì–´ verbose=1ë¡œ ì§„í–‰ ìƒí™© ëª¨ë‹ˆí„°ë§, ìˆ˜ì‹ ê¸¸ì´ê°€ ê³¼ë„í•˜ë©´ parsimony_coefficient ìƒí–¥ í•„ìš” ì‹œ function_set ì¶•ì†Œ ë˜ëŠ” tournament_size í•˜í–¥ìœ¼ë¡œ ë‹¤ì–‘ì„± íšŒë³µ ìµœì¢… ì‹ì€ est._program`ìœ¼ë¡œ ë¬¸ìì—´ í˜•íƒœ í™•ì¸ ê°€ëŠ¥",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-genetic-programming",
    "title": "Genetic Programmingì´ë€?",
    "date": "2025-08-11",
    "excerpt": "Genetic Programmingì˜ ê°œë…ê³¼ í™œìš©ë˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "statistics"
    ],
    "content": "Genetic Programming(GP)ì´ë€? --- ì°¸ê³ ìë£Œ [Koza (1992) Genetic Programming] Wikipedia - Genetic programming DEAP: Distributed Evolutionary Algorithms in Python gplearn: Genetic Programming in Python PySR: High-Performance Symbolic Regression in Python ê°œë… ìš”ì•½ > Genetic Programmingì€ evolutionary algorithmì˜ í•œ í˜•íƒœë¡œ, ê°œì²´ë¥¼ ê³ ì • ê¸¸ì´ ë²¡í„°ê°€ ì•„ë‹ˆë¼ â€œí”„ë¡œê·¸ë¨/ìˆ˜ì‹(expression)â€ ê·¸ ìì²´ë¡œ ê°„ì£¼í•˜ê³  selection, crossover, mutation ê°™ì€ ì—°ì‚°ì„ í†µí•´ ì ì§„ì ìœ¼ë¡œ ê°œì„ í•˜ëŠ” ë°©ë²•ì´ë‹¤. > íŠ¹íˆ tree-structured representationì„ ì‚¬ìš©í•œ ìˆ˜ì‹ íƒìƒ‰ì— ë„ë¦¬ ì“°ì´ë©°, Symbolic Regressionì˜ í•µì‹¬ backboneìœ¼ë¡œ í™œìš©ëœë‹¤. ëª©í‘œ: ì£¼ì–´ì§„ ëª©ì í•¨ìˆ˜(fitness)ë¥¼ ìµœì†Œí™”/ìµœëŒ€í™”í•˜ëŠ” programì„ ìë™ìœ¼ë¡œ ë°œê²¬ í‘œí˜„(Representation): ë³´í†µ treeë¡œ í‘œí˜„ëœ expression(ë‚´ë¶€ ë…¸ë“œ=operators, ë¦¬í”„=variables/constants) íƒìƒ‰(Search): population ê¸°ë°˜ evolutionary search (selection â†’ crossover/mutation â†’ replacement) ë‹¤ëª©ì  ìµœì í™”: ì •í™•ë„ì™€ ë³µì¡ë„(í•´ì„ì„±) ì‚¬ì´ì˜ ì ˆì¶©ì„ í•¨ê»˜ ê³ ë ¤ ëª©ì í•¨ìˆ˜ì™€ ë‹¤ëª©ì  ê´€ì  ì˜ˆì¸¡ ë¬¸ì œì—ì„œ í”í•œ ì„¤ì •ì€ ì˜ˆì¸¡ ì˜¤ì°¨ì™€ í‘œí˜„ ë³µì¡ë„ë¥¼ ë™ì‹œì— ì¤„ì´ëŠ” ê²ƒì´ë‹¤. $$ \\min_f \\; \\Big[\\; \\text{Error}(f),\\; \\text{Complexity}(f) \\;\\Big] $$ ë‹¨ì¼ ìŠ¤ì¹¼ë¼ë¡œ í•©ì¹˜ëŠ” ë°©ì‹(parsimony pressure)ë„ ë§ì´ ì“´ë‹¤. $$ \\text{Fitness}_{\\text{penalized}}(f) = \\text{Error}(f) + \\lambda\\,\\text{size}(f) $$ ì—¬ê¸°ì„œ size(f)ëŠ” ë…¸ë“œ ìˆ˜, íŠ¸ë¦¬ ê¹Šì´, description length ë“±ìœ¼ë¡œ ì •ì˜í•œë‹¤. \\lambdaëŠ” ë³µì¡ë„ íŒ¨ë„í‹° ê°•ë„ë¥¼ ì¡°ì ˆí•œë‹¤. ë˜ëŠ” NSGA-II ê°™ì€ non-dominated sortingìœ¼ë¡œ Pareto frontierë¥¼ ìœ ì§€í•œë‹¤. ì „í˜•ì  íŒŒì´í”„ë¼ì¸ 1) Function/Terminal set ì •ì˜: ì‚¬ìš©í•  operators(+, âˆ’, Ã—, Ã·, sin, exp, log, ...), variables, constants ë²”ìœ„ ì„¤ì • 2) ì´ˆê¸° ê°œì²´ ìƒì„±: ramped half-and-halfë¡œ ë‹¤ì–‘í•œ ê¹Šì´/ëª¨ì–‘ì˜ íŠ¸ë¦¬ë¥¼ ë§Œë“¤ê¸° 3) Fitness í‰ê°€: train(ë˜ëŠ” CV)ì—ì„œ Errorì™€ Complexity ì‚°ì¶œ 4) Selection: tournament ë“±ìœ¼ë¡œ ë¶€ëª¨ ì„ íƒ, elitismìœ¼ë¡œ ìµœìƒìœ„ ë³´ì¡´ 5) Variation: subtree crossover, mutation(point/subtree/hoist)ë¡œ ìƒˆ ê°œì²´ ìƒì„± 6) ì œì•½/ì•ˆì „ì¥ì¹˜: max depth/size, safe operators(0-division ë³´í˜¸ ë“±) 7) ì„¸ëŒ€ ë°˜ë³µ: budget(ì„¸ëŒ€ ìˆ˜, population size) ë‚´ì—ì„œ ë°˜ë³µ, validation ê¸°ì¤€ìœ¼ë¡œ best ì„ íƒ í•µì‹¬ ì—°ì‚°ìì™€ ì„¤ê³„ ìš”ì†Œ Selection: tournament selectionì´ ë‹¨ìˆœí•˜ë©´ì„œë„ íš¨ê³¼ì . tournament sizeê°€ í´ìˆ˜ë¡ exploitationâ†‘, ë‹¤ì–‘ì„±â†“ Crossover (subtree): ë‘ ë¶€ëª¨ì˜ ì„œë¸ŒíŠ¸ë¦¬ë¥¼ êµí™˜í•´ ìì‹ ìƒì„±. êµ¬ì¡°ì  íƒìƒ‰ì˜ í•µì‹¬ Mutation: point/subtree/hoist mutationìœ¼ë¡œ ë¶€ë¶„ êµ¬ì¡°ë¥¼ ë¬´ì‘ìœ„ ë³€í™”í•´ ë‹¤ì–‘ì„± í™•ë³´ Elitism/Reproduction: ìš°ìˆ˜ ê°œì²´ë¥¼ ë³´ì¡´í•´ í‡´í–‰ ë°©ì§€ Bloat control: í‘œí˜„ì´ ë¶ˆí•„ìš”í•˜ê²Œ ì»¤ì§€ëŠ” bloat ë°©ì§€ìš©ìœ¼ë¡œ max depth/size, lexicographic parsimony, size-fair crossover ë“±ì„ ì‚¬ìš© Ephemeral Random Constants(ERC): íŠ¸ë¦¬ ë§ë‹¨ì— ë‚œìˆ˜ ìƒìˆ˜ë¥¼ ë‘ê³  í•„ìš” ì‹œ ì§€ì—­ íƒìƒ‰ìœ¼ë¡œ ë¯¸ì„¸ ì¡°ì • Typing/Grammar constraints: strongly typed GPë‚˜ grammar-guided GPë¡œ ìœ íš¨í•œ í”„ë¡œê·¸ë¨ë§Œ ìƒì„±í•˜ë„ë¡ ì œí•œ ëŒ€í‘œ ë³€í˜•ë“¤ Tree-based GP: ê°€ì¥ í‘œì¤€ì  í˜•íƒœ. Symbolic Regressionì— ê´‘ë²”ìœ„ ì‚¬ìš© Strongly-Typed GP: ë…¸ë“œë³„ íƒ€ì… ì œì•½ìœ¼ë¡œ ë¶ˆë²• í‘œí˜„ ë°©ì§€, íƒìƒ‰ ì•ˆì •ì„±â†‘ Grammar-Guided GP (GGGP): BNF/CFG ë“± ë¬¸ë²•ìœ¼ë¡œ í•©ë²• í”„ë¡œê·¸ë¨ë§Œ ìƒì„± Linear GP / Cartesian GP: ë¹„íŠ¸ìŠ¤íŠ¸ë¦¼ ë˜ëŠ” graph-like í‘œí˜„ìœ¼ë¡œ ì½”ë“œ/íšŒë¡œ í˜•íƒœ íƒìƒ‰ ê°„ë‹¨ ì•Œê³ ë¦¬ì¦˜ ìŠ¤ì¼€ì¹˜ ``text Initialize population P0 with ramped half-and-half for gen in 1..G: Evaluate Error(f) and Complexity(f) for all f in P_{gen-1} Update Pareto frontier with elitism while P_gen < population_size: Select parents via tournament selection With prob pc: offspring = crossover(parent1, parent2) With prob pm: offspring = mutate(offspring) Enforce size/depth constraints on offspring Add offspring to P_gen Return best model on validation (accuracy-parsimony trade-off) ` Python ì˜ˆì‹œ 1: gplearnìœ¼ë¡œ Symbolic Regression `python pip install gplearn import numpy as np from gplearn.genetic import SymbolicRegressor rng = np.random.default_rng(42) X = rng.uniform(-3.0, 3.0, size=(200, 1)) y = 2 np.sin(X[:, 0]) + 0.1 (X[:, 0] * 2) + rng.normal(0, 0.1, size=200) est = SymbolicRegressor( population_size=2000, generations=30, function_set=(\"add\", \"sub\", \"mul\", \"div\", \"sin\", \"cos\", \"log\", \"sqrt\"), metric=\"mse\", parsimony_coefficient=0.001, p_crossover=0.8, p_subtree_mutation=0.05, p_hoist_mutation=0.01, p_point_mutation=0.05, init_depth=(2, 5), init_method=\"half and half\", const_range=(-2.0, 2.0), tournament_size=20, n_jobs=-1, random_state=42, ) est.fit(X, y) print(est._program) ` Python ì˜ˆì‹œ 2: DEAPìœ¼ë¡œ ìµœì†Œ êµ¬í˜„ ìŠ¤ì¼€ì¹˜ ì•„ë˜ëŠ” DEAPìœ¼ë¡œ ë§¤ìš° ì¶•ì•½ëœ Symbolic Regression ê³¨ê²©ì´ë‹¤. ì‹¤ì œë¡œëŠ” protected operators, depth ì œì•½, í‰ê°€/ì„ íƒ ë£¨í”„ë¥¼ ë” ë³´ê°•í•´ì•¼ í•œë‹¤. `python pip install deap import operator import numpy as np from deap import base, creator, gp, tools, algorithms Function/Terminal set pset = gp.PrimitiveSet(\"MAIN\", arity=1) pset.addPrimitive(operator.add, 2) pset.addPrimitive(operator.sub, 2) pset.addPrimitive(operator.mul, 2) pset.addPrimitive(np.sin, 1) pset.addEphemeralConstant(\"rand\", lambda: np.random.uniform(-1, 1)) pset.renameArguments(ARG0=\"x\") Fitness/Individual creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,)) creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin) toolbox = base.Toolbox() toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3) toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr) toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) Data rng = np.random.default_rng(0) X = rng.uniform(-3, 3, size=200) y = 2 np.sin(X) + 0.1 * (X 2) def eval_ind(individual): func = gp.compile(expr=individual, pset=pset) y_hat = np.array([func(x) for x in X]) return ((y_hat - y) 2).mean(), toolbox.register(\"evaluate\", eval_ind) toolbox.register(\"select\", tools.selTournament, tournsize=5) toolbox.register(\"mate\", gp.cxOnePoint) toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr, pset=pset) toolbox.decorate(\"mate\", gp.staticLimit(key=len, max_value=17)) toolbox.decorate(\"mutate\", gp.staticLimit(key=len, max_value=17)) pop = toolbox.population(n=200) hof = tools.HallOfFame(1) algorithms.eaSimple(pop, toolbox, cxpb=0.8, mutpb=0.2, ngen=30, halloffame=hof, verbose=False) print(hof[0]) ` ì¥ì ê³¼ ë‹¨ì  ì¥ì  í‘œí˜„ë ¥: ëª¨ë¸ í˜•íƒœ ìì²´ë¥¼ íƒìƒ‰í•˜ë¯€ë¡œ ë¹„ì„ í˜•/ìƒí˜¸ì‘ìš©ì„ ìœ ì—°í•˜ê²Œ í¬ì°© í•´ì„ì„±: ê²°ê³¼ê°€ ìˆ˜ì‹/ê·œì¹™ì´ë©´ ì„¤ëª…ê³¼ ì§€ì‹ ë°œê²¬ì— ìœ ë¦¬ ë„ë©”ì¸ priors í†µí•©: operators/grammar ì œì•½ìœ¼ë¡œ ë„ë©”ì¸ ì§€ì‹ ì£¼ì… ê°€ëŠ¥ ë‹¨ì  ê³„ì‚° ë¹„ìš©: íƒìƒ‰ ê³µê°„ì´ ë§¤ìš° í¬ê³  ì„¸ëŒ€ ë°˜ë³µì´ í•„ìš” Bloat/ê³¼ì í•©: ì œì•½ê³¼ íŒ¨ë„í‹° ì—†ìœ¼ë©´ í‘œí˜„ì´ ë¹„ëŒ€í•´ì§€ê³  ë…¸ì´ì¦ˆ ì í•© ìœ„í—˜ ì¬í˜„ì„±: í™•ë¥ ì  íƒìƒ‰ íŠ¹ì„±ìƒ ë‚œìˆ˜ ì‹œë“œ, ì„¤ì •ì— ë¯¼ê° ì–¸ì œ ìœ ìš©í•œê°€ ì‹ í˜•íƒœê°€ ì¤‘ìš”í•œ ê³¼í•™/ê³µí•™ ëª¨ë¸ë§(equation discovery) ë³µì¡í•œ feature interactionì„ ì„¤ëª… ê°€ëŠ¥í•œ í˜•íƒœë¡œ í¬ì°©í•˜ê³  ì‹¶ì„ ë•Œ ê·œì¹™/í”„ë¡œê·¸ë¨ í•©ì„±ì´ í•„ìš”í•œ ì»¨íŠ¸ë¡¤/íœ´ë¦¬ìŠ¤í‹± ì„¤ê³„ Feature engineering(SymbolicTransformer)ë¡œ í•´ì„ ê°€ëŠ¥í•œ ìƒˆ íŠ¹ì§• ìƒì„± ì‹¤ë¬´ íŒ Function set ìµœì†Œí™”: ë„ë©”ì¸ ì¹œí™”ì  operatorsë§Œ ë‘ê³  ì‹œì‘ â†’ íƒìƒ‰ ê³µê°„ ì¶•ì†Œ, í•´ì„ì„± í–¥ìƒ ì•ˆì „í•œ ì—°ì‚°ì: protected division/log/sqrt, bound-checkë¡œ ìˆ˜ì¹˜ ì•ˆì •ì„± í™•ë³´ ë³µì¡ë„ ì œì•½: max depth/size`, parsimony penalty, size-fair crossoverë¡œ bloat ì œì–´ Validation ê´€ë¦¬: train/validation ë¶„ë¦¬, Pareto setì—ì„œ ë‹¨ìˆœí•œ ì‹ ìš°ì„  ê²€í†  Search budget ì ì¦: generations/populationì„ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë¦¬ë©° ê°œì„  ì¶”ì  Seed ê³ ì • ë° ë¡œê¹…: reproducibilityë¥¼ ìœ„í•´ random stateì™€ ê²°ê³¼ ìˆ˜ì‹/ê¸¸ì´/ì„±ëŠ¥ ê¸°ë¡ ê´€ë ¨ í‚¤ì›Œë“œ Symbolic Regression, Interpretable ML, Program Synthesis êµ¬í˜„ ë¼ì´ë¸ŒëŸ¬ë¦¬: DEAP, gplearn(SymbolicRegressor/Transformer), PySR",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/what-is-redis",
    "title": "Redisë€?",
    "date": "2025-08-11",
    "excerpt": "Redisì˜ ê°œë…ê³¼ í™œìš©ë˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "statistics"
    ],
    "content": "ì¸ë©”ëª¨ë¦¬ ë°ì´í„° ì €ì¥ì†Œ: ë°ì´í„°ë¥¼ RAMì— ì €ì¥í•´ ë§ˆì´í¬ë¡œì´ˆ~ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì‘ë‹µì„ ì œê³µ í‚¤-ê°’ êµ¬ì¡°: í‚¤ë¡œ ë¹ ë¥´ê²Œ ì¡°íšŒ/ê°±ì‹ í•˜ëŠ” ë‹¨ìˆœí•˜ê³  íš¨ìœ¨ì ì¸ ë°ì´í„° ëª¨ë¸ ë‹¤ì–‘í•œ ìš©ë„ ìºì‹±: ìì£¼ ì¡°íšŒë˜ëŠ” ê²°ê³¼ë¥¼ ì €ì¥í•´ DB ë¶€í•˜ì™€ ì§€ì—°ì‹œê°„ì„ ì¤„ì„ ì„¸ì…˜ ê´€ë¦¬: ë¡œê·¸ì¸ ì„¸ì…˜/í† í° ë“± ìƒíƒœ ì •ë³´ë¥¼ TTLë¡œ ì•ˆì „í•˜ê²Œ ê´€ë¦¬ ë©”ì‹œì§€ í: ë¹„ë™ê¸° ì‘ì—… ì²˜ë¦¬ì™€ Producer-Consumer íŒ¨í„´ êµ¬í˜„ NoSQL ë°ì´í„°ë² ì´ìŠ¤: ìŠ¤í‚¤ë§ˆ ìœ ì—°ì„±, ë‹¤ì–‘í•œ ìë£Œêµ¬ì¡°, ìˆ˜í‰ í™•ì¥ ìš©ì´ ì˜¤í”ˆ ì†ŒìŠ¤: BSD ë¼ì´ì„ ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ì»¤ë®¤ë‹ˆí‹°ì™€ ìƒíƒœê³„ê°€ í™œë°œ TTLì´ë€? TTL(Time To Live)ì€ í‚¤ì— ì„¤ì •í•˜ëŠ” ìœ íš¨ê¸°ê°„ì…ë‹ˆë‹¤. ë§Œë£Œ ì‹œê°„ì´ ì§€ë‚˜ë©´ í‚¤ëŠ” ìë™ìœ¼ë¡œ ì‚­ì œë©ë‹ˆë‹¤. ì„¤ì •: SET key val EX <sec> ë˜ëŠ” PX <ms>, EXPIRE key <sec>, PEXPIRE key <ms> í•´ì œ: PERSIST key (ë§Œë£Œ ì œê±°) ì¡°íšŒ: TTL key(ì´ˆ), PTTL key(ms). -1ì€ ë§Œë£Œ ì—†ìŒ, -2ëŠ” í‚¤ ì—†ìŒ ë™ì‘: RedisëŠ” ì§€ì—°(lazy) + ì£¼ê¸°ì (active) ìŠ¤ìº”ìœ¼ë¡œ í‚¤ë¥¼ ë§Œë£Œ ì²˜ë¦¬ ì°¸ê³ : ë§Œë£Œ(expiration)ëŠ” ì‹œê°„ ê¸°ë°˜ ì‚­ì œ, evictionì€ ë©”ëª¨ë¦¬ ì••ë°• ì‹œ ì •ì±…ì— ë”°ë¥¸ ì œê±° ``bash SET token:123 abc EX 60 # 60ì´ˆ í›„ ë§Œë£Œ TTL token:123 # ë‚¨ì€ ì´ˆ í™•ì¸ PERSIST token:123 # ë§Œë£Œ ì œê±° ` í•œ ì¤„ ìš”ì•½ ë§¤ìš° ë¹ ë¥¸ in-memory data storeë¡œì„œ, ìºì‹œ/ì„¸ì…˜/í/ìˆœìœ„í‘œ/ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ… ë“± ì‹¤ì‹œê°„ì„±ì´ ì¤‘ìš”í•œ ë¬¸ì œì— íƒì›”í•©ë‹ˆë‹¤. ì˜ì†ì„±ê³¼ ê³ ê°€ìš©ì„± ê¸°ëŠ¥ì„ ê°–ì·„ì§€ë§Œ, ì „í†µì ì¸ RDBMSì˜ ì™„ì „í•œ ëŒ€ì²´ì¬ëŠ” ì•„ë‹™ë‹ˆë‹¤. ê°œìš” RedisëŠ” in-memory key-value data structure storeë¡œ, ë§¤ìš° ë‚®ì€ ì§€ì—°ì‹œê°„ê³¼ ë†’ì€ ì²˜ë¦¬ëŸ‰ì„ ì œê³µí•˜ëŠ” ìºì‹œ(Cache), ì„¸ì…˜ ì €ì¥ì†Œ(Session Store), ë©”ì‹œì§€ ë¸Œë¡œì»¤(Message Broker) ìš©ë„ë¡œ ë„ë¦¬ ì“°ì…ë‹ˆë‹¤. ë‹¨ì¼ ìŠ¤ë ˆë“œ ê¸°ë°˜ event loopë¡œ ë™ì‘í•˜ì§€ë§Œ I/O multiplexingì„ í™œìš©í•´ ê³ ì„±ëŠ¥ì„ ë‹¬ì„±í•©ë‹ˆë‹¤. ì›í•˜ëŠ” ê²½ìš° ë””ìŠ¤í¬ì— ì˜ì†í™”(RDB/AOF)ë„ ì§€ì›í•©ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§• In-memory: ë©”ëª¨ë¦¬ì— ë°ì´í„°ë¥¼ ë³´ê´€í•´ ë§ˆì´í¬ë¡œì´ˆ~ë°€ë¦¬ì´ˆ ë‹¨ìœ„ ì‘ë‹µ Rich data structures: String, List, Set, Sorted Set(ZSet), Hash, Stream, HyperLogLog, Bitmap, Geospatial TTL/Expiration: key ë‹¨ìœ„ ë§Œë£Œì™€ ë‹¤ì–‘í•œ eviction ì •ì±…(LRU/LFU ë“±) Persistence: RDB(snapshot), AOF(append only) ë˜ëŠ” í˜¼í•© ì‚¬ìš© Scalability/HA: Replication, Sentinel(ìë™ failover), Cluster(16384 hash slots) Atomic ops & Lua: ë‹¨ì¼ ëª…ë ¹ ì›ìì„±, EVALì„ í†µí•œ ì„œë²„ ì¸¡ Lua ìŠ¤í¬ë¦½íŒ… í•µì‹¬ ìë£Œêµ¬ì¡° í•œëˆˆì— String: ì¹´ìš´í„°, í† í°, í”Œë˜ê·¸ ë“± ë‹¨ìˆœ ê°’ ì €ì¥ Hash: ì‚¬ìš©ì í”„ë¡œí•„ ê°™ì€ field-value ë§µ List: í/ìŠ¤íƒ, ì‘ì—… ëŒ€ê¸°ì—´, ë¡œê·¸ ë²„í¼ Set: ê³ ìœ  ì§‘í•©, ì¤‘ë³µ ì œê±°, êµì§‘í•©/í•©ì§‘í•© ì—°ì‚° Sorted Set (ZSet): ì ìˆ˜ ê¸°ë°˜ ì •ë ¬(ë¦¬ë”ë³´ë“œ, ìˆœìœ„í‘œ) Stream: ë¡œê·¸/ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼, ì†Œë¹„ì ê·¸ë£¹ ê¸°ë°˜ ì²˜ë¦¬ ëŒ€í‘œ í™œìš© ì‹œë‚˜ë¦¬ì˜¤ Cache: DB ì§ˆì˜/ë Œë”ë§ ê²°ê³¼ ìºì‹±, TTLê³¼ invalidation ì „ëµ ê²°í•© Session Store: ì›¹ ì„¸ì…˜/ì¸ì¦ í† í° ì €ì¥, ë§Œë£Œ/ì—°ì¥ ê´€ë¦¬ Queue & Pub/Sub: ê°„ë‹¨í•œ ì‘ì—… í(List/BRPOP) ë˜ëŠ” ì‹¤ì‹œê°„ ë©”ì‹œì§•(Stream, Pub/Sub) Rate Limiting: ìš”ì²­ ë¹ˆë„ ì œì–´(ì¹´ìš´í„°, í† í° ë²„í‚·), ë¶„ì‚° ë½(SET key val NX PX) Leaderboard/Ranking: ZSetìœ¼ë¡œ ìƒìœ„ N, ìŠ¤ì½”ì–´ ë²”ìœ„ ì§ˆì˜ ìš´ìš© í¬ì¸íŠ¸ Persistence: RDB: ì£¼ê¸°ì  ìŠ¤ëƒ…ìƒ·, ë³µêµ¬ ë¹ ë¦„, ë°ì´í„° ì†ì‹¤ ê°€ëŠ¥ì„± ìˆìŒ(ë§ˆì§€ë§‰ ìŠ¤ëƒ…ìƒ· ì´í›„) AOF: ëª¨ë“  write ë¡œê·¸, ë‚´êµ¬ì„± ìš°ìˆ˜, íŒŒì¼ ì»¤ì§ˆ ìˆ˜ ìˆì–´ ë¦¬ë¼ì´íŠ¸ í•„ìš” Eviction ì •ì±…: noeviction, allkeys-lru, allkeys-lfu ë“± ì›Œí¬ë¡œë“œì— ë§ê²Œ ì„ íƒ Cluster: í‚¤ í•´ì‹œ ìŠ¬ë¡¯ ê¸°ë°˜ ìƒ¤ë”©. ë©€í‹°í‚¤ ì—°ì‚°ì€ ê°™ì€ ìŠ¬ë¡¯(= ê°™ì€ hash tag)ì—ì„œë§Œ ê°€ëŠ¥ ëª¨ë“ˆ: RediSearch(ê²€ìƒ‰), RedisJSON(JSON ë¬¸ì„œ), RedisBloom(í™•ë¥  ìë£Œêµ¬ì¡°), TimeSeries(ì‹œê³„ì—´) ê°„ë‹¨í•œ ëª…ë ¹ ì˜ˆì‹œ `bash String SET page:home \"rendered_html\" EX 60 GET page:home List queue (producer/consumer) LPUSH jobs \"task-1\" BRPOP jobs 5 Sorted Set leaderboard ZADD leaderboard 100 user:alice 120 user:bob ZREVRANGE leaderboard 0 1 WITHSCORES ` Python ì˜ˆì‹œ(redis-py) `python import redis client = redis.Redis(host=\"localhost\", port=6379, decode_responses=True) cache set/get with TTL client.set(\"user:42:profile\", \"{\\\\\"name\\\\\": \\\\\\\"Alice\\\\\\\"}\", ex=300) profile = client.get(\"user:42:profile\") rate limiting (simple counter) key = \"rl:ip:203.0.113.1:60s\" pipe = client.pipeline() pipe.incr(key) pipe.expire(key, 60) count, _ = pipe.execute() if count > 100: print(\"Too many requests\") leaderboard client.zadd(\"leaderboard\", {\"user:alice\": 100, \"user:bob\": 120}) top = client.zrevrange(\"leaderboard\", 0, 9, withscores=True) print(top) `` ì–¸ì œ í”¼í• ê¹Œ? ê°•í•œ íŠ¸ëœì­ì…˜ ì¼ê´€ì„±ê³¼ ë³µì¡í•œ ì¡°ì¸/ì¿¼ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°(ì „í†µ RDBMS ê¶Œì¥) ëŒ€ìš©ëŸ‰ ì¥ê¸° ë³´ê´€ì˜ ì£¼ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì“°ê¸°ì—ëŠ” ë¹„ìš©/ë‚´êµ¬ì„± íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤ í•„ìš” Redis, ì´ëŸ¬í•œ í”„ë¡œì íŠ¸ì— ì ìš©í•˜ì‹œë©´ ìœ ìš©í•©ë‹ˆë‹¤. ì¸ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì‘ë™í•˜ëŠ” RedisëŠ” ë°ì´í„°ë¥¼ ë¹ ë¥´ê²Œ ì²˜ë¦¬í•˜ë©° ì‹¤ì‹œê°„ ì‘ë‹µì´ ì¤‘ìš”í•œ í”„ë¡œì íŠ¸ì—ì„œ ë›°ì–´ë‚œ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ”ë°ìš”, Redisë¥¼ í™œìš©í•˜ë©´ ì¢‹ì€ ëŒ€í‘œì ì¸ í”„ë¡œì íŠ¸ë¥¼ 4ê°€ì§€ë¡œ ì •ë¦¬í•´ ë³´ì•˜ìŠµë‹ˆë‹¤. ì„¸ì…˜ ê´€ë¦¬ RedisëŠ” ì‚¬ìš©ì ì„¸ì…˜ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì €ì¥í•˜ì—¬ ë¹ ë¥´ê³  íš¨ìœ¨ì ì¸ ì„¸ì…˜ ê´€ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ì‚¬ìš©ì ì¸ì¦ ì†ë„ í–¥ìƒ: ë¡œê·¸ì¸ ì‹œ ì‚¬ìš©ì ë°ì´í„°ë¥¼ ì¦‰ê°ì ìœ¼ë¡œ ì¡°íšŒí•  ìˆ˜ ìˆì–´ ì¸ì¦ ì†ë„ë¥¼ í¬ê²Œ ë†’ì…ë‹ˆë‹¤. ì„¸ì…˜ ë§Œë£Œ ê´€ë¦¬: TTL(Time to Live)ì„ ì„¤ì •í•´ ë¶ˆí•„ìš”í•œ ë°ì´í„°ê°€ ë©”ëª¨ë¦¬ë¥¼ ì°¨ì§€í•˜ì§€ ì•Šë„ë¡ ìë™ìœ¼ë¡œ ë§Œë£Œì‹œí‚µë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë©”ëª¨ë¦¬ ì‚¬ìš©ì„ íš¨ìœ¨ì ìœ¼ë¡œ ê´€ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ì¤‘ ì„œë²„ í™˜ê²½ ì§€ì›: Redisë¥¼ ì¤‘ì•™ ì„¸ì…˜ ì €ì¥ì†Œë¡œ ì‚¬ìš©í•˜ë©´ ì—¬ëŸ¬ ì›¹ ì„œë²„ ê°„ ì„¸ì…˜ ë°ì´í„°ë¥¼ ì‰½ê²Œ ê³µìœ í•  ìˆ˜ ìˆì–´ ë¡œë“œë°¸ëŸ°ì‹± í™˜ê²½ì—ì„œë„ ì•ˆì •ì ì¸ ì„¸ì…˜ ê´€ë¦¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ìºì‹± ì‹œìŠ¤í…œ RedisëŠ” ìì£¼ ì¡°íšŒë˜ëŠ” ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ìºì‹± í•˜ì—¬ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë¶€í•˜ë¥¼ ì¤„ì´ê³  ë¹ ë¥¸ ì‘ë‹µ ì†ë„ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì½ê¸° ë¶€í•˜ ë¶„ì‚°: ì œí’ˆ ì •ë³´, í˜ì´ì§€ ì½˜í…ì¸  ê°™ì€ ê³ ë¹ˆë„ ì½ê¸° ìš”ì²­ì„ Redisì— ìºì‹± í•´ ë°ì´í„°ë² ì´ìŠ¤ì˜ ì½ê¸° ë¶€í•˜ë¥¼ ê°ì†Œì‹œí‚µë‹ˆë‹¤. ë³µì¡í•œ ì—°ì‚° ì†ë„ í–¥ìƒ: ìƒí’ˆ ì¶”ì²œì´ë‚˜ ê²€ìƒ‰ ê²°ê³¼ì²˜ëŸ¼ ê³„ì‚°ì´ ë§ì€ ë°ì´í„°ë¥¼ Redisì— ì €ì¥í•´ í´ë¼ì´ì–¸íŠ¸ ìš”ì²­ ì‹œ ë¹ ë¥´ê²Œ ë°˜í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œë£Œ ì •ì±… ì§€ì›: ìºì‹± ë°ì´í„°ì— TTLì„ ì„¤ì •í•´ ì˜¤ë˜ëœ ë°ì´í„°ë¥¼ ìë™ìœ¼ë¡œ ì œê±°, ìµœì‹  ìƒíƒœë¥¼ ìœ ì§€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ ë¶„ì„ ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ê°€ ì¤‘ìš”í•œ í™˜ê²½ì—ì„œ ë¹ ë¥¸ ë°ì´í„° ì¡°íšŒì™€ ì§‘ê³„ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤. ëŒ€ì‹œë³´ë“œ ì—…ë°ì´íŠ¸: íŠ¸ë˜í”½, ì‚¬ìš©ì í–‰ë™ ë°ì´í„° ê°™ì€ ì •ë³´ë¥¼ Redisì— ì €ì¥í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ëŒ€ì‹œë³´ë“œì— ë°˜ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹¤ì‹œê°„ í†µê³„ ê³„ì‚°: Redisì˜ Sorted Setì´ë‚˜ Hash êµ¬ì¡°ë¥¼ í™œìš©í•´ ì‚¬ìš©ì í™œë™ ì§‘ê³„ë‚˜ ìˆœìœ„ ê³„ì‚°ì„ ì¦‰ì‹œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë²¤íŠ¸ ì²˜ë¦¬: ì‹¤ì‹œê°„ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ì´ë²¤íŠ¸ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê³  ì²˜ë¦¬í•´ ì´ìƒ ì§•í›„ íƒì§€, íŠ¸ëœì­ì…˜ ëª¨ë‹ˆí„°ë§ ê°™ì€ ì‹œìŠ¤í…œì— í™œìš©ë©ë‹ˆë‹¤. ë©”ì‹œì§€ í Redisì˜ Pub/Sub ê¸°ëŠ¥ì€ ì‹¤ì‹œê°„ ë©”ì‹œì§€ ì „ì†¡ê³¼ ìˆ˜ì‹ ì„ ê°„ë‹¨í•˜ê³  íš¨ìœ¨ì ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì‹¤ì‹œê°„ ì•Œë¦¼ ì‹œìŠ¤í…œ: Redisë¥¼ í™œìš©í•˜ë©´ ì´ë©”ì¼ ì•Œë¦¼, í‘¸ì‹œ ì•Œë¦¼ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì±„íŒ… ì• í”Œë¦¬ì¼€ì´ì…˜: ì‚¬ìš©ì ê°„ ë©”ì‹œì§€ë¥¼ Redisë¥¼ í†µí•´ ì¦‰ì‹œ ì „ë‹¬í•´ ëŠê¹€ ì—†ëŠ” ì‹¤ì‹œê°„ ì±„íŒ… í™˜ê²½ì„ êµ¬í˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„° ê´€ë¦¬: Redis Streams ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ë©´ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„° ìŠ¤íŠ¸ë¦¬ë°ì„ ê´€ë¦¬í•˜ê³ , ì—¬ëŸ¬ ì†Œë¹„ì(Consumer)ì—ê²Œ ë°ì´í„°ë¥¼ ë¶„ë°°í•  ìˆ˜ ìˆì–´ í™•ì¥ì„±ì´ ë›°ì–´ë‚©ë‹ˆë‹¤. RedisëŠ” ì¸ ë©”ëª¨ë¦¬ ë°ì´í„°ë² ì´ìŠ¤ë¡œì„œ ë›°ì–´ë‚œ ë°ì´í„° ì²˜ë¦¬ ëŠ¥ë ¥ì„ ìë‘í•©ë‹ˆë‹¤. ë†’ì€ ì²˜ë¦¬ëŸ‰ê³¼ ì˜ì†ì„± ì˜µì…˜ ë•ë¶„ì— ì„¸ì…˜ ê´€ë¦¬, ìºì‹± ì‹œìŠ¤í…œ, ì‹¤ì‹œê°„ ë¶„ì„, ë©”ì‹œì§€ íì™€ ê°™ì€ í”„ë¡œì íŠ¸ì—ì„œ í™œìš©í•˜ë©´ ë”ìš± íš¨ê³¼ì ì¸ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì–´ì§€ëŠ” ë‚´ìš©ì—ì„œëŠ” Redisë¥¼ ë”ìš± íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. Redisë¥¼ ì‚¬ìš©í•˜ê¸° ì¢‹ì€ ê²½ìš° ì„¸ì…˜ ê´€ë¦¬: ë¡œê·¸ì¸ ì„¸ì…˜/í† í°ì„ ë¹ ë¥´ê²Œ ì¡°íšŒí•˜ê³  TTLë¡œ ìë™ ë§Œë£Œê°€ í•„ìš”í•  ë•Œ ìºì‹± ì‹œìŠ¤í…œ: ìì£¼ ì¡°íšŒë˜ëŠ” ë°ì´í„°ë¡œ DB ë¶€í•˜ë¥¼ ì¤„ì´ê³  ì‘ë‹µ ì‹œê°„ì„ ë‹¨ì¶•í•´ì•¼ í•  ë•Œ ì‹¤ì‹œê°„ ë¶„ì„: ëŒ€ì‹œë³´ë“œ/í†µê³„/í™œë™ ì§‘ê³„ë¥¼ ms~s ë‹¨ìœ„ë¡œ ê°±ì‹ í•´ì•¼ í•  ë•Œ ë©”ì‹œì§€ íÂ·ìŠ¤íŠ¸ë¦¬ë°: ê°„ë‹¨í•œ ì‘ì—… íë‚˜ ì‹¤ì‹œê°„ ì•Œë¦¼/ì±„íŒ…/ì´ë²¤íŠ¸ ìŠ¤íŠ¸ë¦¼ì´ í•„ìš”í•  ë•Œ ë ˆì´íŠ¸ ë¦¬ë¯¸íŒ…Â·ì¿¼í„°: ì‚¬ìš©ì/ IPë³„ ìš”ì²­ ë¹ˆë„ ì œí•œì„ ì›ì ì¹´ìš´í„°+TTLë¡œ êµ¬í˜„í•  ë•Œ ë­í‚¹Â·ìˆœìœ„í‘œ: ì ìˆ˜ ê¸°ë°˜ ìƒìœ„ N, ë²”ìœ„ ì§ˆì˜ê°€ ì¦ì„ ë•Œ(ZSet ì í•©) ì„ì‹œ ë°ì´í„°: OTP/ì¸ì¦ì½”ë“œ/ë‹¨ê¸° ìºì‹œ ë“± ë§Œë£Œê°€ ëª…í™•í•œ ë°ì´í„°ë¥¼ ë‹¤ë£° ë•Œ ì´ˆì €ì§€ì—° ì½ê¸°: DB ì•ë‹¨ ìºì‹œ ê³„ì¸µìœ¼ë¡œ p95 ì‘ë‹µ ì‹œê°„ì„ ë‚®ì¶°ì•¼ í•  ë•Œ",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/how-to-evaluate-model-in-selection",
    "title": "ì„ íƒ ë¬¸ì œì—ì„œ ëª¨ë¸ì„ ì–´ë–»ê²Œ í‰ê°€í•  ê²ƒì¸ê°€",
    "date": "2025-08-11",
    "excerpt": "ì„ íƒ ë¬¸ì œì—ì„œ ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "evaluation"
    ],
    "content": "ë¬¸ì œì •ì˜ <figure> <img src=\"/post/Causal_Inference/ì„ íƒë¬¸ì œ_í‰ê°€_ê³ ë¯¼.png\" alt=\"ì„ íƒ ë¬¸ì œ í‰ê°€ ê³ ë¯¼\" /> <figcaption>ê·¸ë¦¼1. ì„ íƒì˜ ë¬¸ì œì—ì„œ í‰ê°€ì— ëŒ€í•œ ê³ ë¯¼</figcaption> </figure> ì„ íƒì˜ ë¬¸ì œì—ì„œ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê°€í•˜ëŠ” ì¼ì€ ê½¤ë‚˜ ì–´ë ¤ìš´ ì¼ì…ë‹ˆë‹¤. ì™œëƒí•˜ë©´ ê³ ê°ì€ ì—¬ëŸ¬ê°€ì§€ ì„ íƒ ì¤‘ì—ì„œ í•˜ë‚˜ì˜ ì„ íƒë§Œ í•˜ê¸° ë•Œë¬¸ì´ì£ . ê·¸ë¦¼1ì€ ì œê°€ í’€ê³  ìˆëŠ” ë¬¸ì œë¥¼ ì‹œê°í™”í•œ ìë£Œì…ë‹ˆë‹¤. ì‹œë ¥êµì • ë„ë©”ì¸ì—ì„œ ì˜ì‚¬ëŠ” ê³ ê°ì—ê²Œ ìµœì í™”ëœ ë Œì¦ˆ í¬ê¸°ë¥¼ ì„ íƒí•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë•Œ ì„ íƒì— ë„ì›€ì„ ì£¼ê¸° ìœ„í•´ì„œ ê²°ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ì˜ ê²°ê³¼ ì¶”ì •ì¹˜ë¥¼ í‰ê°€í•˜ëŠ”ë° í•œ ê°€ì§€ ì´ìŠˆê°€ ìˆìŠµë‹ˆë‹¤. í•˜ë‚˜ì˜ ì˜ˆì¸¡ê°’ì— ëŒ€í•œ ì‹¤ì œ ê°’ë§Œì„ ì•Œ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¸ë¦¼1ì—ì„œëŠ” output 2ì— ëŒ€í•œ ì‹¤ì œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆê³ , output1ê³¼ output3ì— ëŒ€í•œ ì‹¤ì œ ê²°ê³¼ëŠ” í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ° ì„ íƒì˜ ë¬¸ì œì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆì„ê¹Œìš”? ì²˜ì¹˜ë³„ í™˜ìì˜ ë¶„í¬ê°€ ê²¹ì¹œë‹¤ë©´, RCT ìƒí™©ì—ì„œ ìƒê°í•´ë³´ê¸° *RCT(Randomized Controlled Trial) : ë¬´ì‘ìœ„ ë°°ì • ì‹œí—˜. ë¬´ì‘ìœ„ë¡œ ì²˜ì¹˜ì— ë°°ì •ëœ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. ë¬´ì‘ìœ„ë¡œ ë°°ì •ë˜ì—ˆê¸° ë•Œë¬¸ì— ì²˜ì¹˜ë³„ í™˜ìì˜ ë¶„í¬ê°€ ë¹„ìŠ·í•˜ê²Œ í˜•ì„±ë˜ì—ˆì„ ê²ƒì…ë‹ˆë‹¤. Aë¼ëŠ” ê³ ê°ì€ 12.1ë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ì§€ë§Œ, Aì™€ ë¹„ìŠ·í•œ A'ëŠ” 12.6ìœ¼ë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ì„ ê²ƒì´ê³ , Aì™€ ë¹„ìŠ·í•œ A\"ëŠ” 13.2ë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ì„ ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ Aì— ëŒ€í•´ì„œëŠ” 12.1 ë°–ì— ìˆ˜ìˆ  ê²°ê³¼ê°€ ì—†ì§€ë§Œ, ë‚˜ë¨¸ì§€ 12.6ê³¼ 13.2ì— ëŒ€í•œ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ê°„ì ‘ì ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ë¥´ê²Œ í‘œí˜„í•˜ë©´, Aë¼ëŠ” ê³ ê°ì´ ê° ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì´ ë¹„ìŠ·í•˜ë‹¤ë©´, ê·¸ ì²˜ì¹˜ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ê°„ì ‘ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆê³  ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì—ì„œëŠ” ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•©ë‹ˆë‹¤. > positivity/overlap assumption and the common support region. ì²˜ì¹˜ë³„ ê³ ê°ì˜ ë¶„í¬ê°€ ê²¹ì¹˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ì‹¤ì œ ìƒí™©ì—ì„œ ìƒê°í•´ë³´ê¸° ë‹¹ì—°í•˜ê²Œë„ ì‹¤ì œ ìƒí™©ì€ RCT ì™€ ë‹¤ë¦…ë‹ˆë‹¤. ì˜ì‚¬ëŠ” ê³ ê°ì˜ ìƒíƒœë¥¼ ë³´ê³  ì²˜ì¹˜ë¥¼ ê²°ì •í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„_Feature_DensityPlot.png\" alt=\"ì²˜ì¹˜ë³„ Feature Density Plot\"> <figcaption>ê·¸ë¦¼ 2: ì²˜ì¹˜ë³„ Feature ë¶„í¬ ë¹„êµ</figcaption> </figure> íŠ¹íˆ ëˆˆì˜ í¬ê¸°(ata)ë¥¼ í†µí•´ ë Œì¦ˆ í¬ê¸°ì˜ ë²”ìœ„ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. ê·¸ë¦¼ 2ë¥¼ í†µí•´ ì´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 12.1ì„ ì„ íƒí•˜ëŠ” ëˆˆì˜ í¬ê¸° (Cas_ata) ë¶„í¬ì™€ 13.2ë¥¼ ì„ íƒí•˜ëŠ” ëˆˆì˜ í¬ê¸° ë¶„í¬ëŠ” ê½¤ë‚˜ ë‹¤ë¦…ë‹ˆë‹¤. ê±°ì˜ ê²¹ì³ì ¸ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ 13.2ë¥¼ ë°›ì„ í™•ë¥ ì´ ë§¤ìš° ë‚®ì€ ê³ ê°ì˜ 13.2ì— ëŒ€í•œ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì€ ê°„ì ‘ì ìœ¼ë¡œë„ í‰ê°€í•  ìˆ˜ ì—†ê³ , ì´ë¥¼ ì‹ ë¢°í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒì…ë‹ˆë‹¤. ê²¹ì¹˜ëŠ” ì •ë„, ì²˜ì¹˜ë¥¼ ì„ íƒí•  í™•ë¥ ì„ êµ¬í•˜ëŠ” ë°©ë²• íŠ¹ì • ê³ ê°ì´ ì–´ë–¤ ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì€ ëª¨ë¸ì„ í†µí•´ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Xë¥¼ ê³ ê°ì˜ ëˆˆì˜ ìƒíƒœë¡œ, Yë¥¼ ì²˜ì¹˜ë¡œ ë‘ê³  ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë©´ ë©ë‹ˆë‹¤. <figure> <img src=\"/post/Causal_Inference/ì„ íƒë¬¸ì œ_í™•ë¥ ì¶”ì •ëª¨ë¸ê²°ê³¼_1.png\" alt=\"í™•ë¥ ì¶”ì •ê²°ê³¼\"> <figcaption>ê·¸ë¦¼ 3: 12.1ê³¼ 12.6ì´ ê²¹ì¹˜ëŠ” ê³ ê° (ì²˜ì¹˜ : size, ë‚˜ë¨¸ì§€ëŠ” ê³ ê° ëˆˆì˜ ì¡°ê±´)</figcaption> </figure> <figure> <img src=\"/post/Causal_Inference/ì„ íƒë¬¸ì œ_í™•ë¥ ì¶”ì •ëª¨ë¸ê²°ê³¼_2.png\" alt=\"í™•ë¥ ì¶”ì •ê²°ê³¼\"> <figcaption>ê·¸ë¦¼ 4: 12.6ê³¼ 13.2ê°€ ê²¹ì¹˜ëŠ” ê³ ê° (ì²˜ì¹˜ : size, ë‚˜ë¨¸ì§€ëŠ” ê³ ê° ëˆˆì˜ ì¡°ê±´)</figcaption> </figure> ê·¸ë¦¼3ê³¼ ê·¸ë¦¼4ëŠ” í™•ë¥ ì¶”ì • ëª¨ë¸ì„ ë§Œë“  í›„, ëª¨ë¸ì„ í†µí•´ í™•ë¥ ê°’ì„ êµ¬í•´ í•„í„°ë§í•œ ê²°ê³¼ì…ë‹ˆë‹¤. ê·¸ë¦¼3ì˜ ê²½ìš° 12.1ê³¼ 12.6ì„ ë°›ì„ í™•ë¥ ì´ 0.4 ì´ìƒì¸ ê³ ê°ë“¤ì…ë‹ˆë‹¤. ì´ëŸ° ê³ ê°ë“¤ì˜ ê²½ìš° 12.1ê³¼ 12.6ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì„ ê°„ì ‘ì ìœ¼ë¡œ í‰ê°€í•  ìˆ˜ ìˆê³ , ì‹ ë¢°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 13.2ì˜ ê²½ìš°ì—ëŠ” ì‹ ë¢°í•  ìˆ˜ ì—†ì„ ê²ƒì…ë‹ˆë‹¤. ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„±ì„ í‘œí˜„í•˜ëŠ” ë°©ë²• ê·¸ë ‡ë‹¤ë©´ ì´ëŸ° ì§ê´€ì„ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œìš”? ì˜ˆì¸¡ê°’ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì–´ë–»ê²Œ í‘œí˜„í•  ìˆ˜ ìˆì„ê¹Œìš”? GPT-5 ì˜ê²¬ ì‹ ë¢°ë„(confidence) = Overlap Ã— Predictive Uncertainty ë°˜ì˜ Overlap ê¸°ë°˜ ì‹ ë¢°ë„: ì¶”ì²œ í›„ë³´ ìƒìœ„ 2ê°œ ì²˜ì¹˜ t1, t2ì— ëŒ€í•´ min(P_hat(t1 x), P_hat(t2 x))ê°€ ë†’ì„ìˆ˜ë¡ ì‹ ë¢°ë„â†‘ ì˜ˆì¸¡ ë¶ˆí™•ì‹¤ì„±: ì•™ìƒë¸”/MC Dropout ë¶„ì‚°, ë˜ëŠ” Quantile Regressionìœ¼ë¡œ ì˜ˆì¸¡êµ¬ê°„ ì‚°ì¶œ í‘œì‹œ ë°©ë²•: ì‹ ë¢°ë„ ë‚®ì€ ì²˜ì¹˜ëŠ” íšŒìƒ‰ ì²˜ë¦¬ ë˜ëŠ” ì£¼ì„(â€œë°ì´í„° ë¶€ì¡±/overlap ë¶€ì¡±â€) ì‹ ë¢°ë„ì™€ í•¨ê»˜ ì˜ˆì¸¡êµ¬ê°„ í‘œì‹œ $$ \\text{Confidence}(x, t) \\;=\\; \\underbrace{\\min_{t \\in \\mathcal{C}(x)} \\hat{P}(t \\mid x)}_{\\text{overlap}} \\;\\times\\; \\underbrace{g\\!\\left(\\frac{1}{1 + \\sigma^2_{\\hat{y}_t}(x)}\\right)}_{\\text{uncertainty}} $$ ì—¬ê¸°ì„œ $\\mathcal{C}(x)$ëŠ” ì¶”ì²œ í›„ë³´ ì²˜ì¹˜ ì§‘í•©, $sigma^2_{\\hat{y}_t}(x)$ëŠ” (ì•™ìƒë¸”/MC Dropout/ë¶„ìœ„ìˆ˜ íšŒê·€ ë“±ìœ¼ë¡œ ì¶”ì •í•œ) ì˜ˆì¸¡ ë¶„ì‚°ì´ë©°, $g(\\cdot)$ëŠ” 0â€“1 ì •ê·œí™” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë‚´ ìƒê°, ë¶„ì‚°ìœ¼ë¡œ í‘œí˜„í•˜ê¸° ì œê°€ ì›í•˜ëŠ” ë°©í–¥ì€ prediction intervalì„ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì—¬ê¸°ì— overlap ì •ë„ê°€ ë°˜ì˜ë˜ì–´ì„œ, ì•ì„œ ì–¸ê¸‰í•œ ì˜ˆì‹œì²˜ëŸ¼ 13.2ë¥¼ ë°›ì„ í™•ë¥ ì´ ë§¤ìš° ë‚®ì€ ê³ ê°ì˜ 13.2ì— ëŒ€í•œ ë¶„ì‚°ì€ ë§¤ìš° í¬ê²Œ ê³„ì‚° ëìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤. ì´ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•˜ê³  ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ì§€ëŠ” ì•„ì§ ì°¾ì•„ë³´ë©´ì„œ ê³ ë¯¼í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/ì¥íš¨ì§„ê°•ì˜/6ê°•_í•˜ë³µë¶€ë§Œ ì¡°ì´ëŠ” ê°ê°ì„ í†µí•´ íš¡ê²½ë§‰ ì»¨íŠ¸ë¡¤í•˜ê¸°",
    "title": "6ê°• í•˜ë³µë¶€ë§Œ ì¡°ì´ëŠ” ê°ê°ì„ í†µí•´ íš¡ê²½ë§‰ ì»¨íŠ¸ë¡¤í•˜ê¸°",
    "date": "2025-08-10",
    "excerpt": "í•˜ë³µë¶€ë§Œ ì¡°ì´ëŠ” ê°ê°ì„ í†µí•´ íš¡ê²½ë§‰ ì»¨íŠ¸ë¡¤í•˜ê¸°. ê´€ë ¨ ì—°ìŠµë²• ì •ë¦¬",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "ì¥íš¨ì§„ ê°•ì˜",
      "ì—°ìŠµë°©ë²•"
    ],
    "content": "ìœ ë£Œ ì¸ê°• ë§í¬ ê°€ìŠ´ì€ ì„œë¸Œ. ê°€ìŠ´ì€ ìµœëŒ€í•œ ì•ˆì“°ëŠ”ê²Œ ì¢‹ìŒ. ê°ˆë¹„ë¼ˆ ì•„ë˜ ë¶€ë¶„ë§Œ ì¡°ì´ëŠ” ëŠë‚Œ ì•„ë˜ ìœ„ìª½ìœ¼ë¡œ > ì²˜ìŒìœ¼ë¡œ ë¶„ë¦¬í•˜ëŠ” ì—°ìŠµ!",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì†Œë¬¸ë°ê°€ì‹­/08-10-ë“£ê¸°ë¡œëŠ”ë‹¤ì‹œë§Œë‚˜ê³ ì‹¶ì–´í•œëŒ€",
    "title": "ë“£ê¸°ë¡œëŠ” ë‹¤ì‹œ ë§Œë‚˜ê³  ì‹¶ì–´ í•œëŒ€",
    "date": "2025-08-10",
    "excerpt": "ì†Œë¬¸ ë° ê°€ì‹­ íŒŒíŠ¸",
    "category": "English Study",
    "tags": [
      "SPEAK",
      "ì†Œë¬¸ ë° ê°€ì‹­"
    ],
    "content": "From what I've heard, he wants to get back together. I knew it ì˜¤ëŠ˜ ë°°ìš´ ê²ƒë“¤ ë‚´ê°€ ë“¤ì€ ë°”ë¡œëŠ” ë‚´ê°€ ì•„ëŠ” ë°”ë¡œëŠ” ë‚´ê°€ ì´í•´í•˜ëŠ” ë²”ìœ„ > From what I've haard From what I've heard, Steve wants to get back together. (ë‹¤ì‹œ ë§Œë‚˜ê³  ì‹¶ë”ë¼) From what I've heard, Andrew is seeing someone else. (ë‹¤ë¥¸ì‚¬ëŒì„ ë§Œë‚˜ê³  ìˆë‹¤) > From what I know : (í‹€ë¦´ ìˆ˜ë„ ìˆëŠ”ë°) ë‚´ê°€ ì•„ëŠ” ë°”ë¡œëŠ” know ìì²´ê°€ ìƒíƒœë¥¼ ë‚˜íƒ€ë‚´ê¸° ë•Œë¬¸ì— í˜„ì¬í˜•ìœ¼ë¡œ ì“¸ ìˆ˜ ìˆìŒ From what I know, Sean is from New York. From what I know, Sue used to work at Google. From what I know, Alex is from Hawai. > As far as I understand: ë‚´ê°€ ì´í•´í•˜ëŠ” ë²”ìœ„ ë‚´ì—ì„œ As far as I understand, Julie doesn't want to get back together. have a huge crash on someone ê°„ë‹¨í•œ ì‘ë¬¸ (ë°°ìš´ í‘œí˜„ë“¤ ì‚¬ìš©í•´ì„œ) I had a blind date yesterday. (I went on a blind date yesterday.) I heared that from matchmaker, from what she know, my blind date has really good personality. So I expected that our date would be fun. After date, I have a good emotion to her. As far as I unserstand, she is so ë¬´í•´í•˜ë‹¤ and cute. I wanna know more about her. --- ìˆ˜ì •ëœ ë²„ì „ I went on a blind date yesterday. I heard that from the matchmaker. According to what she knows, my blind date has a really good personality. So I expected that our date would be fun. After the date, I have good feelings toward her. As far as I understand, she is so harmless and cute. I want to know more about her. --- í”¼ë“œë°± ë° ìˆ˜ì •ì‚¬í•­ ë¬¸ë²•ì  ì˜¤ë¥˜ ìˆ˜ì •: \"heared\" â†’ \"heard\" (hearì˜ ê³¼ê±°í˜•) \"matchmaker\" â†’ \"the matchmaker\" (ê´€ì‚¬ í•„ìš”) \"she know\" â†’ \"she knows\" (3ì¸ì¹­ ë‹¨ìˆ˜ í˜„ì¬í˜•) \"my blind date has really good personality\" â†’ \"my blind date has a really good personality\" (ê´€ì‚¬ í•„ìš”) \"After date\" â†’ \"After the date\" (ê´€ì‚¬ í•„ìš”) \"I have a good emotion to her\" â†’ \"I have good feelings toward her\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"I wanna know\" â†’ \"I want to know\" (ê³µì‹ì ì¸ ì‘ë¬¸ì—ì„œëŠ” ì™„ì „í•œ í˜•íƒœ ì‚¬ìš©) ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„: \"According to what she knows\" - \"from what she know\"ë³´ë‹¤ ë” ìì—°ìŠ¤ëŸ¬ì›€ \"good feelings toward her\" - \"good emotion to her\"ë³´ë‹¤ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ \"harmless\" - \"ë¬´í•´í•˜ë‹¤\"ì˜ ì ì ˆí•œ ì˜ì–´ ë²ˆì—­",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-information-entropy",
    "title": "Information Entropyë€?",
    "date": "2025-08-10",
    "excerpt": "Information Entropy ê°œë…ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "ì°¸ê³ ìë£Œ 1: ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸ 2: ìœ„í‚¤í”¼ë””ì•„ Information content ì •ë³´ë€? > In information theory, the information content, self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable. > It can be thought of as an alternative way of expressing probability, much like odds or log-odds, but which has particular mathematical advantages in the setting of information theory. surprisal, information content ì´ëŸ° ë‹¨ì–´ë“¤ì€ í™•ë¥ ì„ ë³€í˜•í•´ì„œ í‘œí˜„í•˜ëŠ” ê²ƒë“¤. ë†€ë¼ì›€ ì •ë„ë¥¼ ì–‘ì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒ ê°™ë‹¤. > The Shannon information can be interpreted as quantifying the level of \"surprise\" of a particular outcome. > As it is such a basic quantity, it also appears in several other settings, such as the length of a message needed to transmit the event given an optimal source coding of the random variable. $$I(x) = -log_b(P(X))$$ $b$: $2, e, 10$ ì™œ logë¥¼ ë¶™ì—¬ì„œ ì •ì˜í–ˆì„ê¹Œ? í™•ë¥ ê°’ì— ë°˜ë¹„ë¡€ í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ë‘ ì‚¬ê±´ì˜ ì •ë³´ëŸ‰ì„ í•©ì¹˜ë©´ ê° ì‚¬ê±´ì˜ ì •ë³´ëŸ‰ì„ í•©ì¹œ ê²ƒê³¼ ê°™ì•„ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì—”íŠ¸ë¡œí”¼ (entropy) : í‰ê·  ì •ë³´ëŸ‰ $$H(X) = E[I(X)] = - \\sum_{i=1}^n P(x_i) log_b (P(x_i))$$",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-data-drift",
    "title": "Data Driftë€?",
    "date": "2025-08-10",
    "excerpt": "Data Driftì˜ ê°œë…ê³¼ í™œìš©ë˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "data-drift",
      "machine-learning",
      "statistics",
      "data-engineering"
    ],
    "content": "ì°¸ê³ í•œ ê¸€ 1: ë§í¬ë“œì¸ - Machine Learning Community (Moderated) --- How to choose Data Drift metrics in ML Production (ì°¸ì¡° 1)? Jensenâ€“Shannon(JS) Distance ì¥ì : symmetricì ì´ê³  ê°’ì´ 0~1ë¡œ boundedë¼ í•´ì„ì´ ì‰½ê³  threshold ì„¤ì •ì´ ì•ˆì •ì . noiseÂ·ê²½ë¯¸í•œ ë³€ë™ì— robust. ë‹¨ì : computational costê°€ ìƒëŒ€ì ìœ¼ë¡œ í¼(íŠ¹íˆ high-dimensional). ì–¸ì œ ì“°ë‚˜: í•´ì„ ìš©ì´ì„±ê³¼ stable monitoringì´ ì¤‘ìš”í•  ë•Œ. Kullbackâ€“Leibler(KL) Divergence ì¥ì : distributionì˜ subtle differencesê¹Œì§€ sensitiveí•˜ê²Œ ê°ì§€(early warningì— ìœ ë¦¬). ë‹¨ì : outlierÂ·ì‘ì€ probability massì— sensitiveí•´ spike/false alarm ê°€ëŠ¥, support set mismatch ì‹œ ë°œì‚°. ì–¸ì œ ì“°ë‚˜: early detectionì´ ìµœìš°ì„ ì´ê±°ë‚˜ ë†’ì€ sensitivityê°€ í•„ìš”í•œ ì‹œìŠ¤í…œ. Wasserstein Distance ì¥ì : í° distribution ë³€í™”ë„ smoothly tracking, referenceÂ·current distributionì´ ê²¹ì¹˜ì§€ ì•Šì•„ë„ ë™ì‘. ë‹¨ì : computational cost í¼(large-scale/real-time, high-dimensionalì—ì„œ íŠ¹íˆ ë¶€ë‹´). ì–¸ì œ ì“°ë‚˜: large/structural changesì˜ stable trackingì´ í•„ìš”í•  ë•Œ. í•œ ì¤„ ê°€ì´ë“œ interpretabilityÂ·threshold stability: JS early detection of subtle changes: KL smooth and robust tracking: Wasserstein <figure> <img src=\"/post/DataScience/DataDrift_plot_1.gif\" alt=\"Data Drift Plot\" /> <figcaption>ê·¸ë¦¼ 1. Data Drift Plot</figcaption> </figure> --- ì§€í‘œë³„ íŒŒì´ì¬ êµ¬í˜„ ì˜ˆì‹œ (JS/KL/Wasserstein) ì—°ì†í˜• featureì˜ distributionì„ ë™ì¼í•œ êµ¬ê°„ìœ¼ë¡œ discretizeí•œ ë’¤, KL/JSë¥¼ ê³„ì‚°í•˜ê³  Wasserstein distanceë¥¼ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œì…ë‹ˆë‹¤. ``python import numpy as np from scipy.stats import entropy, wasserstein_distance def _normalize_hist(values: np.ndarray, bins: np.ndarray, epsilon: float = 1e-8) -> np.ndarray: counts, _ = np.histogram(values, bins=bins) probs = counts.astype(float) probs = probs / max(probs.sum(), 1.0) probs = np.clip(probs, epsilon, None) probs = probs / probs.sum() return probs def kl_divergence_from_samples(reference: np.ndarray, current: np.ndarray, num_bins: int = 30, base: int = 2) -> float: \"\"\"KL(P Q): ê¸°ì¤€(reference)=P, í˜„ì¬(current)=Q\"\"\" bins = np.histogram_bin_edges(reference, bins=num_bins) p = _normalize_hist(reference, bins) q = _normalize_hist(current, bins) return float(entropy(p, qk=q, base=base)) def js_distance_from_samples(reference: np.ndarray, current: np.ndarray, num_bins: int = 30, base: int = 2) -> float: \"\"\"JS distance: sqrt(JS divergence), 0~1 ë²”ìœ„(base=2)\"\"\" bins = np.histogram_bin_edges(reference, bins=num_bins) p = _normalize_hist(reference, bins) q = _normalize_hist(current, bins) m = 0.5 (p + q) js_div = 0.5 entropy(p, qk=m, base=base) + 0.5 * entropy(q, qk=m, base=base) return float(np.sqrt(js_div)) def wasserstein_from_samples(reference: np.ndarray, current: np.ndarray) -> float: return float(wasserstein_distance(reference, current)) ì˜ˆì‹œ ë°ì´í„° ref = np.random.normal(0.0, 1.0, size=10_000) cur = np.random.normal(0.3, 1.2, size=2_000) metrics = { \"KL(P Q)\": kl_divergence_from_samples(ref, cur), \"JS distance\": js_distance_from_samples(ref, cur), \"Wasserstein\": wasserstein_from_samples(ref, cur), } print(metrics) ` ì°¸ê³ : ë²”ì£¼í˜• featureëŠ” ê° categoryì˜ relative frequencyë¥¼ probabilityë¡œ normalizeí•˜ì—¬ KL/JSë¥¼ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤(ë™ì¼í•œ category ìˆœì„œì™€ smoothing í•„ìš”). ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì§€í‘œ ë¹„êµí‘œ Metric ê°’ ë²”ìœ„ symmetry distribution overlap í•„ìš” sensitivity(subtle changes) ì¥ì  ë‹¨ì  computational cost ê¶Œì¥ ì‚¬ìš© --- --- --- --- --- --- --- --- --- JS distance 0~1 symmetric ë¶ˆí•„ìš” ì¤‘ê°„ interpretabilityÂ·threshold ì„¤ì • ìš©ì´, noise robust high-dimensionalì—ì„œ computational cost ì¦ê°€ ì¤‘ stable monitoring/threshold ìš´ì˜ KL divergence 0~âˆ asymmetric ë¶€ë¶„ overlap ê¶Œì¥ ë†’ìŒ subtle changes early detection outlier/sparse probabilityì— sensitive, false alarm ê°€ëŠ¥ ì¤‘ early warningì´ ì¤‘ìš”í•œ ì‹œìŠ¤í…œ Wasserstein â‰¥0 symmetric ë¶ˆí•„ìš” ì¤‘ê°„ large/structural changesë„ smoothly tracking, interpretation ì§ê´€ì  large-scale/real-timeÂ·high-dimensionalì—ì„œ cost í¼ ë†’ìŒ robust change tracking, overlap ì ì€ distribution --- ì§€í‘œ ìƒì„¸ ì„¤ëª… Kullbackâ€“Leibler(KL) Divergence relative entropy, I-divergence. > A simple interpretation of the KL divergence of P from Q is the expected excess surprisal from using Q as a model instead of P when the actual distribution is P. PëŒ€ì‹  Që¥¼ ì¼ì„ ë•Œ ì •ë³´ëŸ‰ì˜ ë³€í™”ì •ë„ê°€ KL divergence. $$ D_{KL}(P Q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} = H(P, Q) - H(P) \\tag{3} $$ distanceì˜ ì†ì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•´ì„œ distanceë¼ê³  í‘œí˜„í•  ìˆ˜ëŠ” ì—†ë‹¤. squared distancesì˜ ì†ì„±ì€ ë§Œì¡±í•œë‹¤ê³  í•œë‹¤. ì§ê´€: ì§„ì§œê°€ Pì¸ë° Që¡œ codingí•˜ë ¤ê³  í•  ë•Œ â€œì¶”ê°€ë¡œ ë‚´ì•¼ í•˜ëŠ” information costâ€. directionalityì´ ì¤‘ìš”í•´ KL(P Q)ì™€ KL(Q P)ëŠ” ë‹¤ë¥¸ ê°’ì„ ì¤€ë‹¤. ìˆ˜ì‹ $$ KL(P\\,\\ \\,Q) = \\sum\\limits_{x} p(x)\\,\\log\\frac{p(x)}{q(x)}\\quad(\\text{ì—°ì†í˜•ì€ ì ë¶„}) $$ íŠ¹ì§•: asymmetric, lower bound 0Â·upper bound ì—†ìŒ. Q(x)=0ì´ë©´ì„œ P(x)>0ì´ë©´ ë°œì‚°(infinity). sparse probabilityÂ·outlierì— ë§¤ìš° sensitive. ê³„ì‚°ë²•: histogram/KDEë¡œ p,q ì¶”ì • í›„ epsilon smoothing(ì˜ˆ: 1e-6). ìš´ì˜ì—ì„œëŠ” KL(P\\ \\ Q), KL(Q\\ \\ P) ë‘˜ ë‹¤ ë³´ê±°ë‚˜ symmetricization(KL sym) í˜¹ì€ JSë¡œ ëŒ€ì²´. threshold íŒíŠ¸: spikeê°€ ì¦ìœ¼ë¯€ë¡œ moving averageÂ·quantile ê¸°ë°˜ ê²½ë³´, permutation(label shuffle) nullë¡œ p-value ì¶”ì • ê¶Œì¥. ì‚¬ìš©ì²˜: early ë¯¸ì„¸ ë³€í™” íƒì§€, sensitiveë„ ìµœìš°ì„  ê²½ë³´ ì‹œìŠ¤í…œ, ë¦¬ìŠ¤í¬ê°€ í° ì˜ì—­ì˜ ì„¸ë°€ monitoring. í”í•œ í•¨ì •: sample ì ì„ ë•Œ unstable, bin ì„ íƒ/bandwidth sensitive, log ìˆ˜ì¹˜unstable â†’ smoothing/clippingÂ·normalization í•„ìš”. --- Jensenâ€“Shannon(JS) Divergence information radius(IRAD) or total divergence > It is based on the Kullback-Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. square root of Jensen-Shannon divergence == Jensen-Shannon distance ì§ê´€: ë‘ distributionì˜ â€œì¤‘ê°„ distributionâ€ Mê³¼ì˜ information differenceë¥¼ í‰ê· í•´ ì¸¡ì •í•˜ê³ , square rootì„ ì·¨í•´ ì‹¤ì œ distanceë¡œ ë§Œë“  ê°’. symmetricì´ê³  0~1 ë²”ìœ„ë¼ thresholdì„ ì •í•˜ê¸° ì‰½ë‹¤. ìˆ˜ì‹ $$ JS(P, Q) = \\tfrac{1}{2}\\,KL(P\\,\\ \\,M) + \\tfrac{1}{2}\\,KL(Q\\,\\ \\,M)\\\\ M = \\tfrac{1}{2}(P + Q)\\\\ JS\\_\\text{dist}(P, Q) = \\sqrt{JS(P, Q)}\\quad(\\text{log base}=2\\Rightarrow 0\\le JS\\_\\text{dist}\\le 1) $$ íŠ¹ì§•: symmetric, bounded(0~1), support set mismatchì—ë„ finite. rare eventì— over-sensitiveí•˜ì§€ ì•Šì•„ ìš´ì˜ monitoringì— ì•ˆì •ì . ê³„ì‚°ë²•: ì—°ì†í˜•ì€ ê³µí†µ bin ë˜ëŠ” KDEë¡œ í™•ë¥ ì¶”ì • í›„ ê³„ì‚°, ë²”ì£¼í˜•ì€ ë²”ì£¼ ì§‘í•©ì„ í•©ì§‘í•©ìœ¼ë¡œ ì •ë ¬í•´ smoothing(e.g., 1e-6) í›„ ê³„ì‚°. í‘œë³¸ ì ìœ¼ë©´ ë¶„ì‚°â†‘ â†’ ìœˆë„ìš° í¬ê¸° í™•ëŒ€. threshold íŒíŠ¸: ê¸°ì¤€ ê¸°ê°„ì—ì„œ distributionë¥¼ ë¶€íŠ¸ìŠ¤íŠ¸ë©í•´ ê²½í—˜ì  distributionë¥¼ ë§Œë“  ë’¤ ìƒìœ„ quantileë¡œ ê²½ë³´ì„  ì„¤ì •(ì¡°ì§ë³„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê¶Œì¥). ì‚¬ìš©ì²˜: í•´ì„ ìš©ì´Â·ì•ˆì •ì  ìŠ¤ì¼€ì¼ì´ í•„ìš”í•œ ëŒ€ì‹œë³´ë“œ, ë‹¤ìˆ˜ í”¼ì²˜ì˜ ì¼ê´€ëœ threshold ìš´ì˜. í”í•œ í•¨ì •: bin ê°œìˆ˜/ê²½ê³„, KDE bandwidthì— sensitive. ë²”ì£¼ í¬ë°•ë„ ë†’ì„ ë•Œ ê°’ì´ ê³¼ì†Œí‰ê°€ë  ìˆ˜ ìˆìŒ â†’ smoothing/ë³‘í•© ê³ ë ¤. --- Wasserstein Distance (Earth Mover's Distance, 1-Wasserstein) ì§ê´€: í•œ distributionì˜ ì§ˆëŸ‰ì„ ë‹¤ë¥¸ distributionë¡œ ì˜®ê¸¸ ë•Œ í•„ìš”í•œ â€œìµœì†Œ ì‘ì—…ëŸ‰â€. ê°’ì˜ ë‹¨ìœ„ê°€ ì› ë³€ìˆ˜ì™€ ê°™ì•„ í•´ì„ì´ ì§ê´€ì ì´ë‹¤. ìˆ˜ì‹(1ì°¨ì›) $$ W\\_1(P, Q) = \\int\\limits_{0}^{1} \\big F\\_P^{-1}(u) - F\\_Q^{-1}(u)\\big \\,du $$ íŠ¹ì§•: symmetric, distribution ê²¹ì¹¨ì´ ì—†ì–´ë„ ì •ì˜ ê°€ëŠ¥, support set ê²½ê³„ì— ê°•í•¨. ìœ„ì¹˜Â·ìŠ¤ì¼€ì¼ ì´ë™ì„ ë¶€ë“œëŸ½ê²Œ ë°˜ì˜. ê³„ì‚°ë²•: 1DëŠ” scipy.stats.wasserstein_distance`ë¡œ ì •ë ¬ ê¸°ë°˜ O(n log n). ë‹¤ë³€ëŸ‰ì€ í‘œì¤€í™” í›„ Sinkhorn(ì—”íŠ¸ë¡œí”¼ normalization) ë“±ìœ¼ë¡œ ê·¼ì‚¬(POT ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬). threshold íŒíŠ¸: ì› ë‹¨ìœ„ ê·¸ëŒ€ë¡œ í•´ì„í•˜ê±°ë‚˜ IQR/í‘œì¤€í¸ì°¨ë¡œ normalizationí•´ ë¬´ë‹¨ìœ„ distanceë¡œ ë¹„êµ. KPIì™€ ì§ì ‘ ì—°ê²°í•´ threshold ì„¸íŒ… ìš©ì´. ì‚¬ìš©ì²˜: í°/êµ¬ì¡°ì  ì´ë™ ì¶”ì , ê¸°ì¤€Â·í˜„ì¬ distributionê°€ ê±°ì˜ ê²¹ì¹˜ì§€ ì•ŠëŠ” ìƒí™©, ë¹„ì¦ˆë‹ˆìŠ¤ ë‹¨ìœ„ í•´ì„ì´ ì¤‘ìš”í•œ ê²½ìš°. í”í•œ í•¨ì •: ê³ ì°¨ì› ê³„ì‚°ëŸ‰ ë¶€ë‹´Â·ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€, í”¼ì²˜ ìŠ¤ì¼€ì¼ mismatch ì‹œ ì™œê³¡ â†’ í‘œì¤€í™” í•„ìˆ˜. ë‹¤ë³€ëŸ‰ OTëŠ” êµ¬í˜„Â·íŠœë‹ ë‚œì´ë„ ë†’ìŒ. ì§ê´€ì  ì´í•´ ğŸšš \"í™ì„ ì˜®ê¸°ëŠ” ë¹„ìš©\" ê°œë… í•œ ë¶„í¬ì˜ \"ì§ˆëŸ‰\"ì„ ë‹¤ë¥¸ ë¶„í¬ë¡œ ì˜®ê¸°ëŠ” ìµœì†Œ ì‘ì—…ëŸ‰ì„ ì˜ë¯¸í•©ë‹ˆë‹¤ ë§ˆì¹˜ ê±´ì„¤ í˜„ì¥ì—ì„œ í™ì„ í•œ ê³³ì—ì„œ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤ ê° ìœ„ì¹˜ì—ì„œ í•„ìš”í•œ í™ì˜ ì–‘ê³¼ ì˜®ê²¨ì•¼ í•  ê±°ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ ì‘ì—…ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤ ê°„ë‹¨í•œ ì˜ˆì‹œ ë¶„í¬ P: [0, 0, 1, 0, 0] (ì¤‘ì•™ì—ë§Œ ì§ˆëŸ‰) ë¶„í¬ Q: [0, 0, 0, 1, 0] (ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ ì´ë™) Wasserstein Distance = 1 (í•œ ì¹¸ ì˜®ê¸°ë©´ ë¨) KL divergence = âˆ (ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì—†ìŒ) ë°ì´í„° ë“œë¦¬í”„íŠ¸ì—ì„œì˜ í™œìš© ë°ì´í„° ë“œë¦¬í”„íŠ¸ë¥¼ ê°ì§€í•  ë•Œ: ì‹œê°„ì— ë”°ë¥¸ ë¶„í¬ ë³€í™”ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì • ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ ë¶„ì„ ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ì— í™œìš© ì´ ë°©ë²•ì€ íŠ¹íˆ ì—°ì†í˜• ë³€ìˆ˜ë‚˜ ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ì—ì„œ ë¶„í¬ ë³€í™”ë¥¼ ì¸¡ì •í•  ë•Œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤. ---",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-KL-divergence",
    "title": "KL Divergenceë€?",
    "date": "2025-08-10",
    "excerpt": "KL Divergence ê°œë…ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "ì°¸ê³ ìë£Œ 1: ê³µëŒì´ì˜ ìˆ˜í•™ì •ë¦¬ë…¸íŠ¸ entropy entropy ê°œë… ì •ë¦¬ $$ H(x) = - \\sum_{i=1}^n p(x_i)log(p(x_i)) \\tag{1} $$ cross entropy ì˜ˆì¸¡ê³¼ ë‹¬ë¼ì„œ ìƒê¸°ëŠ” ê¹œë†€ë„ (ì¦‰ ì •ë³´ëŸ‰) $$ H(p, q) = - \\sum_{i=1}^n p(x_i)log(q(x_i)) \\tag{2} $$ KL divergence > A simple interpretation of the KL divergence of P from Q is the expected excess surprisal from using Q as a model instead of P when the actual distribution is P. $$ D_{KL}(p q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} \\tag{3} $$ $$ = \\sum_{x \\in X} p(x) \\log p(x) - \\sum_{x \\in X} p(x) \\log q(x) \\tag{4} $$ $$ D_{KL}(p q) = H(p, q) - H(p) \\tag{5} $$",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì†Œë¬¸ë°ê°€ì‹­/08-09-ê²°êµ­-ë‘˜ì´-ì˜ë ì¤„-ì•Œì•˜ì–´",
    "title": "08-09 ê²°êµ­ ë‘˜ì´ ì˜ë ì¤„ ì•Œì•˜ì–´",
    "date": "2025-08-09",
    "excerpt": "ì†Œë¬¸ë°ê°€ì‹­ 08-09 ê²°êµ­ ë‘˜ì´ ì˜ë ì¤„ ì•Œì•˜ì–´",
    "category": "English Study",
    "tags": [
      "SPEAK",
      "ì†Œë¬¸ë°ê°€ì‹­"
    ],
    "content": "I knew they would end up together. ë°°ìš´ ê²ƒë“¤ No wonder : ë†€ëì§€ ì•Šì•„. No wonder he didn't call. No wonder he's great with kids. I knew it : ê·¸ëŸ´ ì¤„ ì•Œì•˜ì–´. I knew they would end up together. I knew they would break up. would have naver p.p : ì ˆëŒ€ ~í•˜ì§€ ëª»í–ˆì„ê±°ì•¼. I would have naver guessed it. I would have naver thought it.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/ì¥íš¨ì§„ê°•ì˜/5ê°•_ë§ˆì§€ë§‰ê¹Œì§€ ë±‰ëŠ” ë°©ë²•ì„ í†µí•œ ë³µë¶€ê°€ ì¡°ì´ëŠ” ê°ê° ìµíˆê¸°",
    "title": "5ê°• ë§ˆì§€ë§‰ê¹Œì§€ ë±‰ëŠ” ë°©ë²•ì„ í†µí•œ ë³µë¶€ê°€ ì¡°ì´ëŠ” ê°ê° ìµíˆê¸°",
    "date": "2025-08-07",
    "excerpt": "ë§ˆì§€ë§‰ê¹Œì§€ ë±‰ëŠ” ë°©ë²•ì„ í†µí•´ ë³µë¶€ê°€ ì¡°ì´ëŠ” ê°ê° ìµíˆê¸°. ê´€ë ¨ ì—°ìŠµë²• ì •ë¦¬",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "ì¥íš¨ì§„ ê°•ì˜",
      "ì—°ìŠµë°©ë²•"
    ],
    "content": "ìœ ë£Œ ì¸ê°• ë§í¬ 4ê°• í—€ë˜ ìƒí™©ì—ì„œ ì¡°ê¸ˆë§Œ í‹ˆì„ ë²Œë¦¬ê¸° íœ˜íŒŒëŒ ëª¨ì–‘ ë§Œë“¤ê¸° ë§ˆì§€ë§‰ê¹Œì§€ ë±‰ê¸°!! ì£¼ë¨¹ìœ¼ë¡œ ë§‰ê¸° í”¼ì§€ì»¬ í›ˆë ¨!! ë§ì´ ì—°ìŠµí•˜ë©´ ì—°ìŠµí• ìˆ˜ë¡ ë¹¨ë¦¬ ëŠ . ë‹¤ìŒ ìˆ˜ì—…ì„ ë“£ë”ë¼ë„ ê³„ì† í•´ì£¼ê¸°. ê°€ìˆ˜ë“¤ì€ ë…¸ë˜ë¥¼ ì˜í•˜ë‹ˆê¹Œ ê¸°ë³¸ë§Œ ë” ë§ì´ í•¨. ë” ë¹¡ì„¸ê²Œ.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/ì¥íš¨ì§„ê°•ì˜/4ê°•_ì…ì„ ë§‰ì•„ì„œ ë±‰ì„ ë•Œ ì“°ëŠ” ì••ë ¥ ëŠë¼ê¸°",
    "title": "4ê°• ì…ì„ ë§‰ì•„ì„œ ë±‰ì„ ë•Œ ì“°ëŠ” ì••ë ¥ ëŠë¼ê¸°",
    "date": "2025-08-07",
    "excerpt": "ì…ì„ ë§‰ì•„ì„œ ë±‰ì„ ë•Œ ì“°ëŠ” ì••ë ¥ ëŠë¼ê¸°. ê´€ë ¨ ì—°ìŠµë²• ì •ë¦¬",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "ì¥íš¨ì§„ ê°•ì˜",
      "ì—°ìŠµë°©ë²•"
    ],
    "content": "ìœ ë£Œ ì¸ê°• ë§í¬ 3ê°• í›ˆë ¨. ì•„ì¹¨ ì ì‹¬ ì €ë… ê¸°ì¤€ìœ¼ë¡œ 15ë²ˆì€ í•  ìˆ˜ ìˆì„ ë•Œ ë„˜ì–´ì˜¤ê¸°. ì…ì„ ë§‰ê³  ì••ë ¥ì„ ëŠê»´ë³´ëŠ” í›ˆë ¨ ë‚˜ê°ˆ ê¸¸ì´ ì—†ê¸° ë•Œë¬¸ì— ì••ì¶•ì— ê°€ê¹Œì›€. ì••ì¶•ì„ ëŠê»´ë³´ëŠ” í›ˆë ¨. ê°•í•˜ê²Œ ë±‰ì–´ë³´ê¸° ë±‰ë‹¤ê°€ ë§‰ì•„ë³´ê¸° ì €í•­ì´ ìƒê¹€. ì´ ëŠë‚Œ. í˜ì„ ê¾¹ ì£¼ê³  ìˆëŠ” ëŠë‚Œ. ë§‰ì€ ê±¸ ëš«ê³  ë‚˜ê°€ê² ë‹¤ëŠ” í˜ì„ ì£¼ê¸° ë°°ê°€ ì•ˆìœ¼ë¡œ ë°€ë ¤ ì˜¬ë¼ê°€ëŠ” ëŠë‚Œ. ì†ìœ¼ë¡œ. ìµœëŒ€í•œ ë§ì´. ì••ë ¥ = ì €í•­ë²•",
    "public": true
  },
  {
    "id": "Data Science/Statistics/confidence_interval_for_ML",
    "title": "MLì˜ ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•˜ëŠ” ë°©ë²•",
    "date": "2025-08-06",
    "excerpt": "MLì˜ ì‹ ë¢°êµ¬ê°„ì€ ì–´ë–»ê²Œ êµ¬í• ê¹Œ?",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "confidence-interval"
    ],
    "content": "ì°¸ê³ ìë£Œ 1: Confidence Intervals for Machine Learning 2: Donâ€™t Forget Confidence Intervals for Your ML Product 3: Creating Confidence Intervals for Machine Learning Classifiers 4: Confidence vs Prediction Intervals: Understanding the Difference --- ì‹ ë¢°êµ¬ê°„ì˜ ì •ì˜ > A confidence interval provides a range of values within which a population parameter is likely to fall, with a certain level of confidence. ì‹ ë¢°êµ¬ê°„ì€ ëª¨ì§‘ë‹¨ì˜ ëª¨ìˆ˜(parameter)ê°€ íŠ¹ì • ë²”ìœ„ ë‚´ì— ìˆì„ ê°€ëŠ¥ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” í†µê³„ì  êµ¬ê°„ --- metricì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ (ì°¸ê³ 1) ë³´í†µ MLì˜ ì‹ ë¢°êµ¬ê°„ì€ metricì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ì„ ì˜ë¯¸í•˜ëŠ” ê²ƒ ê°™ë‹¤. <figure> <img src=\"/post/DataScience/CI_for_ML/CI_example1.png\" alt=\"confidence_interval_for_ML\" style=\"width: 50%;\" /> <figcaption>ê·¸ë¦¼1. confidence_interval_for_ML (ì°¸ê³ 1)</figcaption> </figure> > For example, a 95% confidence interval of [85%, 90%] for model accuracy means that if we repeated the process multiple times, about 95% of the intervals would contain the true accuracy. This helps in assess model reliability and make informed decisions. ì •í™•ë„ì™€ ê°™ì€ í‰ê°€ì§€í‘œë¥¼ ì‹ ë¢°êµ¬ê°„ìœ¼ë¡œ ì œì‹œí•˜ë©´ ë°ì´í„°ê°€ ë°”ë€Œì—ˆì„ ë•Œì— ëŒ€í•œ ê³ ë ¤ë„ í•  ìˆ˜ ìˆê³ , ì´ëŸ¬í•œ ì´ìœ  ë–„ë¬¸ì— ì„ íƒì— ë” ë„ì›€ì´ ë¨. --- ì‹ ë¢°êµ¬ê°„ì˜ ì¤‘ìš”ì„± (ì°¸ê³ 1) ì‹ ë¢°êµ¬ê°„ì€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ë‹¤ì–‘í•œ ì˜ì—­ì—ì„œ ê°€ì¹˜ ìˆëŠ” ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤: ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ì •í™•ë„ë‚˜ F1-scoreì™€ ê°™ì€ ë‹¨ì¼ ì§€í‘œì—ë§Œ ì˜ì¡´í•˜ëŠ” ëŒ€ì‹ , ì‹ ë¢°êµ¬ê°„ì€ ì„œë¡œ ë‹¤ë¥¸ ìƒ˜í”Œì—ì„œì˜ ì„±ëŠ¥ ë³€í™”ë¥¼ ì´í•´í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì˜ˆì‹œ: ëª¨ë¸ì˜ ì •í™•ë„ê°€ 85%ì´ê³  95% ì‹ ë¢°êµ¬ê°„ì´ [82%, 88%]ë¼ë©´, ìƒˆë¡œìš´ ë°ì´í„°ì—ì„œ í…ŒìŠ¤íŠ¸í•  ë•Œ ëª¨ë¸ì˜ ì •í™•ë„ê°€ ì´ ë²”ìœ„ ë‚´ì—ì„œ ë³€ë™í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. íšŒê·€ ê³„ìˆ˜ í•´ì„ ì„ í˜• íšŒê·€ ëª¨ë¸ì—ì„œ ì‹ ë¢°êµ¬ê°„ì€ ì¶”ì •ëœ ê³„ìˆ˜ì˜ ì‹ ë¢°ì„±ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì˜ˆì‹œ: íŠ¹ì • íŠ¹ì„±ì˜ ê³„ìˆ˜ê°€ 2.5ì´ê³  95% ì‹ ë¢°êµ¬ê°„ì´ [1.8, 3.2]ë¼ë©´, í•´ë‹¹ íŠ¹ì„±ì´ ì–‘ì˜ íš¨ê³¼ë¥¼ ê°€ì§„ë‹¤ê³  í•©ë¦¬ì ìœ¼ë¡œ í™•ì‹ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ì˜ ë¶ˆí™•ì‹¤ì„± ì‹ ë¢°êµ¬ê°„ì€ ëª¨ë¸ ì˜ˆì¸¡ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„± ì¸¡ì •ì„ ì œê³µí•˜ë©°, íŠ¹íˆ í™•ë¥ ì  ëª¨ë¸ì—ì„œ ìœ ìš©í•©ë‹ˆë‹¤. ì˜ˆì‹œ: ì£¼íƒ ê°€ê²©ì„ [200K, 250K] ë²”ìœ„ë¡œ 95% ì‹ ë¢°ìˆ˜ì¤€ì—ì„œ ì˜ˆì¸¡í•œë‹¤ë©´, ì‹¤ì œ ê°€ê²©ì´ ì´ ë²”ìœ„ ë‚´ì— ìˆì„ ê°€ëŠ¥ì„±ì´ ë†’ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. A/B í…ŒìŠ¤íŠ¸ ë° ê°€ì„¤ ê²€ì • ë‘ ëª¨ë¸ì´ë‚˜ íŠ¹ì„±ì„ ë¹„êµí•  ë•Œ, ì‹ ë¢°êµ¬ê°„ì€ ì°¨ì´ê°€ í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œì§€ íŒë‹¨í•˜ëŠ” ë° ë„ì›€ì„ ì¤ë‹ˆë‹¤. ì˜ˆì‹œ: ëª¨ë¸ Aì˜ ì •í™•ë„ê°€ 90% [88%, 92%]ì´ê³  ëª¨ë¸ Bê°€ 87% [85%, 89%]ë¼ë©´, ëª¨ë¸ Aê°€ ìš°ìˆ˜í•˜ë‹¤ëŠ” ì¦ê±°ê°€ ìˆìŠµë‹ˆë‹¤. --- ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•˜ëŠ” ë²• (ì°¸ê³ 2) > So how can we create and add confidence intervals to our models? As the name implies, to build confidence intervals (or any other measure of confidence) you first need to define what your metric represents and what confidence means to you and your user. > There are many ways to derive a measure of confidence but all depend on some sort of knowledge which you can use to validate and test your predictions. --- ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•˜ëŠ” ë²• (ì°¸ê³ 3) > Confidence intervals are no silver bullet, but at the very least, they can offer an additional glimpse into the uncertainty of the reported accuracy and performance of a model. ì—¬ëŸ¬ ê¸€ì—ì„œ ì‹ ë¢°êµ¬ê°„ì„ ì œì‹œí•˜ëŠ”ê²Œ ì†Œí†µì„ í•˜ëŠ”ë° ë„ì›€ì„ ì¤€ë‹¤ê³  ì´ì•¼ê¸°í•˜ë„¤. ê³µê°ì´ ëœë‹¤. ì–´ë–¤ ì¶”ì •ê°’ì„ ì´ì•¼ê¸°í•  ë•Œ ë¶ˆí™•ì‹¤ì„±ì— ëŒ€í•œ ì´ì•¼ê¸°ëŠ” ê¼­ í¬í•¨ì´ ë˜ì–´ì•¼ í•œë‹¤ê³  ìƒê°í•œë‹¤. <figure> <img src=\"/post/DataScience/CI_for_ML/with_CI_without_CI.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>ê·¸ë¦¼2. with_CI_without_CI</figcaption> </figure> í™•ì‹¤íˆ í¬í•¨ëœê²Œ ë” ë³´ê¸° ì¢‹ìŒ ì´ ë¸”ë¡œê·¸ì—ì„œ CIë¥¼ êµ¬í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²•ë“¤ì„ ì†Œê°œí•˜ê³  ìˆë‹¤. ê¸°ë³¸ì ì¸ ì»¨ì…‰ë§Œ ì´í•´í•˜ê³ , ì •ë¦¬í•˜ë‹¤ê°€ ë„ˆë¬´ ë§ì•„ì„œ íŒ¨ìŠ¤. Confidence Intervals in a Nutshell > A confidence interval is a method that computes an upper and a lower bound around an estimated value. The actual parameter value is either insider or outside these bounds. ì¶”ì •ê°’ì— ëŒ€í•œ ë¶ˆí™•ì‹¤ì„±ì„ ì •ëŸ‰í™”í•œê²Œ ì‹ ë¢°êµ¬ê°„ì´ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆê² ë‹¤. <figure> <img src=\"/post/DataScience/CI_for_ML/95_percent_CI.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>ê·¸ë¦¼3. 95% ì‹ ë¢°êµ¬ê°„ì˜ ì˜ë¯¸</figcaption> </figure> 95% ì‹ ë¢°êµ¬ê°„ì˜ ì˜ë¯¸ëŠ”? > â€œ95% ì‹ ë¢°êµ¬ê°„â€ì´ë€, ë™ì¼í•œ ë°©ë²•ìœ¼ë¡œ í‘œë³¸ì„ ì—¬ëŸ¬ ë²ˆ ë½‘ì•„ì„œ ì‹ ë¢°êµ¬ê°„ì„ ê³„ì† ê³„ì‚°í•œë‹¤ë©´, ê·¸ì¤‘ ì•½ 95%ì˜ ì‹ ë¢°êµ¬ê°„ì´ ëª¨ì§‘ë‹¨ì˜ ì°¸ê°’ì„ í¬í•¨í•˜ê²Œ ëœë‹¤ëŠ” ì˜ë¯¸ ì˜¤í•´ì™€ ì •í™•í•œ í•´ì„ > ë§ì€ ì‚¬ëŒë“¤ì´ â€œì´ ì‹ ë¢°êµ¬ê°„ ì•ˆì— ì§„ì§œ í‰ê· ì´ 95% í™•ë¥ ë¡œ ë“¤ì–´ ìˆë‹¤â€ë¼ê³  ìƒê°í•˜ì§€ë§Œ, ì—„ë°€í•˜ê²ŒëŠ” í‹€ë¦° í•´ì„ì´ì•¼. ì°¸ê°’ì€ ì´ë¯¸ ê³ ì •ëœ ê°’ì´ë‹ˆê¹Œ, í™•ë¥ ë¡œ í•´ì„í•˜ì§€ ì•Šì•„. ì˜¬ë°”ë¥¸ í•´ì„ì€ â€œì´ ë°©ì‹ìœ¼ë¡œ êµ¬í•œ ì‹ ë¢°êµ¬ê°„ì´ ëª¨ì§‘ë‹¨ì˜ í‰ê· ì„ 95% ìˆ˜ì¤€ìœ¼ë¡œ í¬í•¨í•œë‹¤â€ëŠ” ê²ƒ. ì›í•˜ëŠ” ê±´ ëª¨ë¸ì˜ ì„±ëŠ¥ì— ëŒ€í•œ ê°’ë“¤. ì‹ ë¢°êµ¬ê°„ì€ ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ëª¨ë¸ì´ ì¶”ì •ê°’ì— ëŒ€í•œ ì–¼ë§Œí¼ì˜ ë¶ˆí™•ì‹¤ì„±ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸. A Note About Statistical Significance í†µê³„ì  ìœ ì˜ì„±ì´ë€? > í†µê³„ì  ìœ ì˜ì„±ì€ ê´€ì°°ëœ ê²°ê³¼ê°€ ë‹¨ìˆœí•œ ìš°ì—°ì´ë‚˜ í™•ë¥ ì  ë³€ë™ì´ ì•„ë‹Œ, ì‹¤ì œë¡œ ì˜ë¯¸ ìˆëŠ” ì°¨ì´ë‚˜ ê´€ê³„ê°€ ìˆë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆëŠ” ì •ë„ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. <figure> <img src=\"/post/DataScience/CI_for_ML/ci-overlap.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>ê·¸ë¦¼4. ì‹ ë¢°êµ¬ê°„ì˜ ì¤‘ë³µì„ í†µí•œ íŒë‹¨</figcaption> </figure> group1ê³¼ group2ê°€ ë‹¤ë¥¸ì§€ë¥¼ ì‹ ë¢°êµ¬ê°„ì„ í†µí•´ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤. ë‹¹ì—°í•˜ê²Œë„ ì™„ì „íˆ ê²¹ì¹˜ì§€ ì•Šìœ¼ë©´ í™•ì‹¤íˆ ë‹¤ë¥¸ ê²ƒì´ë¼ê³  íŒë‹¨í•  ìˆ˜ ìˆë‹¤. ê²¹ì¹œë‹¤ë©´ ê²¹ì¹œ ë¶„í¬ë¥¼ ê·¸ë¦¬ê³  ì—¬ê¸°ì„œ mu = 0ì´ ì‹ ë¢°êµ¬ê°„ì— í¬í•¨ë˜ëŠ”ì§€ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆë‹¤. Method 1 : Normal Approximation Interval Based on a Test Set > In our case, the sample mean $\\bar x$ is test set accuracy $\\text{ACC}_{test}$, a proportion of success (in the context of a Binomial proportion confidence interval). Binomial proportion confidence interval? > ì„±ê³µ/ì‹¤íŒ¨ì™€ ê°™ì€ ì´í•­ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë¹„ìœ¨(proportion)ì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ MLì—ì„œì˜ ì ìš©: ë¶„ë¥˜ ëª¨ë¸ì˜ ì •í™•ë„(accuracy)ëŠ” ì´í•­ ë¶„í¬ë¥¼ ë”°ë¦„ ê° ì˜ˆì¸¡ì´ ë§ìŒ(ì„±ê³µ) ë˜ëŠ” í‹€ë¦¼(ì‹¤íŒ¨)ì˜ ë‘ ê°€ì§€ ê²°ê³¼ë§Œ ê°€ì§ ì „ì²´ ì˜ˆì¸¡ ì¤‘ ë§ì€ ë¹„ìœ¨ì´ ì •í™•ë„ ì˜ˆì‹œ: í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ 100ê°œ ìƒ˜í”Œ ì¤‘ 85ê°œë¥¼ ë§ì·„ë‹¤ë©´ ì •í™•ë„ = 85/100 = 0.85 (85%) ì´ 85%ë¼ëŠ” ë¹„ìœ¨ì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•˜ëŠ” ê²ƒì´ ì´í•­ ë¹„ìœ¨ ì‹ ë¢°êµ¬ê°„ ê³„ì‚° ë°©ë²•: ì •ê·œê·¼ì‚¬ë²• (Normal Approximation) Wilson ë°©ë²• Clopper-Pearson ë°©ë²• ìˆ˜ì‹ ì •ì˜ standard error : $\\sqrt{\\frac{1}{n} \\text{ACC}_{test} (1 - \\text{ACC}_{test})}$ > You can find a description of this method in section 1.7 Confidence Intervals via Normal Approximation of my â€œModel Evaluation, Model Selection, and Algorithm Selection in Machine Learningâ€. <figure> <img src=\"/post/DataScience/CI_for_ML/normal-approx.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>ê·¸ë¦¼5. ì •í™•ë„ì— ëŒ€í•œ ì‹ ë¢°êµ¬ê°„ í‘œì‹œ</figcaption> </figure> Method 2: Bootstrapping Training Set - Setup Step > If we only have one estimate, like the accuracy from a single test set, we need to make assumptions about the distribution of this accuracy value. > For example, we may assume that the accuracy values (that we would compute from different samples) are normally distributed. ë¶€íŠ¸ìŠ¤íŠ¸ë© ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•œë‹¤ëŠ”ê±´ ì •ê·œë¶„í¬ë¥¼ ê°€ì •í•˜ëŠ”ê±´ê°€? > No! ë¶€íŠ¸ìŠ¤íŠ¸ë©ì€ ì •ê·œë¶„í¬ ê°€ì • ì—†ì´ë„ ì‹ ë¢°êµ¬ê°„ì„ êµ¬í•  ìˆ˜ ìˆëŠ” ê°•ë ¥í•œ ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì…ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ ë¶€íŠ¸ìŠ¤íŠ¸ë©ì˜ ê°€ì¥ í° ì¥ì  ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. Confidence vs Prediction Intervals: Understanding the Difference (ì°¸ê³ 4) > The root cause of this uncertainty is the sampling process. > It is unrealistic to consider the entire population when conducting a statistical analysis. ì¢‹ì€ ë¬¸ì¥ì´ë‹¤. > The true value of the population parameter is usually not exactly equal to the value estimated from the sample - this difference is the standard error. > To account for this error, it is conventional to estimate an expected value and then specify a range that is expected to contain the actual value. ì¶”ì •ê°’ì€ ì‹¤ì œê°’ê³¼ ì°¨ì´ê°€ ìˆì„ ê²ƒì´ê³ , ì´ë¥¼ ë²”ìœ„ë¡œ í‘œì‹œí•œë‹¤. What is a Prediction Interval? > A prediction interval is the range that is expected - with some level of confidence, to contain the true value of an individual data point, based on a prediction made using regression analysis. ë‚´ê°€ ì›í•˜ëŠ” ê°œë…ì´ prediction intervalì´êµ¬ë‚˜. ê°œë³„ ì¶”ì •ê°’ì— ëŒ€í•œ ì˜¤ì°¨ ë²”ìœ„. > However, you must account for the increased variability of individual data points because you are predicting an individual value (and not an average). > Thus, the prediction interval is larger than the confidence interval.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/sharding_vs_partitioning",
    "title": "sharding vs partitioning",
    "date": "2025-08-05",
    "excerpt": "DBê´€ë¦¬í•  ë•Œ shardingê³¼ partitioningì˜ ì°¨ì´ì— ëŒ€í•œ ê°œë… ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "DataBase"
    ],
    "content": "ì°¸ê³ í•œ ê¸€ ë§í¬ë“œì¸ ê²Œì‹œë¬¼ - Ashish Pratap Singh <figure> <img src=\"/post/DataScience/sharding_vs_partitioning.gif\" alt=\"Sharding vs Partitioning ì°¨ì´ì \" /> <figcaption>Sharding vs Partitioning</figcaption> </figure> êµ¬ë¶„ Partitioning Sharding ------ ------------- ---------- ë²”ìœ„ ë‹¨ì¼ ì„œë²„ ë‚´ ì—¬ëŸ¬ ì„œë²„ ê°„ ëª©ì  ì„±ëŠ¥ ìµœì í™” ìˆ˜í‰ì  í™•ì¥ ë³µì¡ë„ ìƒëŒ€ì ìœ¼ë¡œ ë‹¨ìˆœ ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ ì‚¬ìš© ì‹œê¸° ë‹¨ì¼ ë¨¸ì‹  ë‚´ ìµœì í™” ë‹¨ì¼ ë¨¸ì‹  í•œê³„ ì´ˆê³¼ ì‹œ Partitioningì´ë€? > í•˜ë‚˜ì˜ í° í…Œì´ë¸”ì„ ë” ì‘ì€ ì²­í¬(íŒŒí‹°ì…˜)ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ ëª©ì : ì„±ëŠ¥ í–¥ìƒê³¼ ìœ ì§€ë³´ìˆ˜ ë‹¨ìˆœí™” íŠ¹ì§•: ë…¼ë¦¬ì ìœ¼ë¡œëŠ” í•˜ë‚˜ì˜ í…Œì´ë¸”: ê°œë°œìê°€ ì¿¼ë¦¬í•  ë•ŒëŠ” ë§ˆì¹˜ í•˜ë‚˜ì˜ í…Œì´ë¸”ì²˜ëŸ¼ ì‚¬ìš© (íŒŒí‹°ì…˜ì„ ì‹ ê²½ ì“¸ í•„ìš” ì—†ìŒ) ë¬¼ë¦¬ì ìœ¼ë¡œëŠ” ì—¬ëŸ¬ íŒŒì¼ë¡œ ë¶„ì‚°: ì‹¤ì œë¡œëŠ” ê° íŒŒí‹°ì…˜ì´ ë³„ë„ì˜ íŒŒì¼/ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì €ì¥ë¨ ì¿¼ë¦¬ ì„±ëŠ¥ í–¥ìƒ (íŠ¹ì • íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº”) ë°ì´í„° ê´€ë¦¬ ìš©ì´ì„± (ì˜¤ë˜ëœ íŒŒí‹°ì…˜ ì‚­ì œ ë“±) ``sql -- íŒŒí‹°ì…”ë‹ëœ í…Œì´ë¸”ì„ ì¡°íšŒí•  ë•Œ SELECT FROM logs WHERE log_date >= '2024-01-01'; -- ê°œë°œìëŠ” ë§ˆì¹˜ í•˜ë‚˜ì˜ í…Œì´ë¸”ì„ ì¡°íšŒí•˜ëŠ” ê²ƒì²˜ëŸ¼ ì‚¬ìš© -- í•˜ì§€ë§Œ DB ì—”ì§„ì€ ë‚´ë¶€ì ìœ¼ë¡œ 2024ë…„ íŒŒí‹°ì…˜ë§Œ ìŠ¤ìº”í•¨ ` íŒŒí‹°ì…”ë‹ì€ ëŒ€ë¶€ë¶„ì˜ í˜„ëŒ€ì ì¸ DBMSì—ì„œ ì§€ì›í•˜ëŠ” ê¸°ëŠ¥ ì§€ì›í•˜ëŠ” DBMS MySQL: Range, List, Hash, Key íŒŒí‹°ì…”ë‹ ì§€ì› PostgreSQL: Range, List, Hash íŒŒí‹°ì…”ë‹ ì§€ì› Oracle: Range, List, Hash, Composite íŒŒí‹°ì…”ë‹ ì§€ì› SQL Server: Range, List, Hash íŒŒí‹°ì…”ë‹ ì§€ì› BigQuery: Date, Integer Range íŒŒí‹°ì…”ë‹ ì§€ì› Snowflake: ë‹¤ì–‘í•œ íŒŒí‹°ì…”ë‹ ì „ëµ ì§€ì› ì§€ì›í•˜ì§€ ì•ŠëŠ” DBMS SQLite: ê¸°ë³¸ì ìœ¼ë¡œ íŒŒí‹°ì…”ë‹ ë¯¸ì§€ì› ì¼ë¶€ NoSQL DB: íŒŒí‹°ì…”ë‹ ëŒ€ì‹  ìƒ¤ë”© ê°œë… ì‚¬ìš© MySQL íŒŒí‹°ì…”ë‹ ì˜ˆì‹œ `sql -- ì›”ë³„ Range íŒŒí‹°ì…”ë‹ CREATE TABLE logs ( id INT, log_date DATE, message TEXT ) PARTITION BY RANGE (YEAR(log_date) 100 + MONTH(log_date)) ( PARTITION p202401 VALUES LESS THAN (202402), PARTITION p202402 VALUES LESS THAN (202403), PARTITION p202403 VALUES LESS THAN (202404), PARTITION p_future VALUES LESS THAN MAXVALUE ); ` Shardingì´ë€? > ë°ì´í„°ë¥¼ ì—¬ëŸ¬ ë°ì´í„°ë² ì´ìŠ¤ ì„œë²„ì— ë¶„ì‚° ë°°ì¹˜ ëª©ì : ìˆ˜í‰ì  í™•ì¥(horizontal scaling)ì„ í†µí•œ ëŒ€ìš©ëŸ‰ ì²˜ë¦¬ íŠ¹ì§•: ê° ì„œë²„(ìƒ¤ë“œ)ê°€ ì „ì²´ ë°ì´í„°ì…‹ì˜ ì¼ë¶€ë¥¼ ë³´ìœ  ë‹¨ì¼ ë¨¸ì‹ ì˜ í•œê³„ë¥¼ ë„˜ì–´ì„œëŠ” í™•ì¥ì„± ì œê³µ ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ ê´€ë¦¬ í•„ìš” ì˜ˆì‹œ `sql -- ìƒ¤ë”©ëœ í™˜ê²½ì—ì„œ ì‚¬ìš©ì ì¡°íšŒ -- ì‚¬ìš©ì IDë¥¼ 4ë¡œ ë‚˜ëˆˆ ë‚˜ë¨¸ì§€ë¡œ ìƒ¤ë“œ ê²°ì • shard_number = user_id % 4 -- ì‚¬ìš©ì IDê°€ 123ì¸ ê²½ìš°: 123 % 4 = 3 â†’ shard_3ì—ì„œ ì¡°íšŒ SELECT * FROM users WHERE user_id = 123; -- ì‹¤ì œë¡œëŠ” shard_3 ì„œë²„ì—ì„œ ì‹¤í–‰ë¨ ` ìˆ˜í‰ì  í™•ì¥(Horizontal Scaling)ì´ë€? > ìˆ˜í‰ì  í™•ì¥ì€ \"ë” ë§ì€ ì„œë²„ë¥¼ ì¶”ê°€\"í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì‹œìŠ¤í…œì„ í™•ì¥í•˜ëŠ” ê²ƒì„ ì˜ë¯¸ ìˆ˜ì§ì  í™•ì¥ vs ìˆ˜í‰ì  í™•ì¥ êµ¬ë¶„ ìˆ˜ì§ì  í™•ì¥ (Vertical Scaling) ìˆ˜í‰ì  í™•ì¥ (Horizontal Scaling) ------ ------------------------------- ---------------------------------- ë°©ì‹ ê¸°ì¡´ ì„œë²„ì˜ ì„±ëŠ¥ì„ í–¥ìƒ ìƒˆë¡œìš´ ì„œë²„ë¥¼ ì¶”ê°€ ì˜ˆì‹œ CPU 4ì½”ì–´ â†’ 16ì½”ì–´, RAM 8GB â†’ 64GB ì„œë²„ 1ëŒ€ â†’ ì„œë²„ 10ëŒ€ ì¥ì  êµ¬í˜„ì´ ë‹¨ìˆœ ë¬´ì œí•œ í™•ì¥ ê°€ëŠ¥ ë‹¨ì  ë¬¼ë¦¬ì  í•œê³„ ì¡´ì¬, ë¹„ìš© ì¦ê°€ ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ ê´€ë¦¬ í•„ìš” ìƒ¤ë”©ì—ì„œì˜ ìˆ˜í‰ì  í™•ì¥ ` ì´ˆê¸° ìƒíƒœ: ì„œë²„ 1ëŒ€ (ëª¨ë“  ë°ì´í„° ë³´ìœ ) â”œâ”€â”€ ì‚¬ìš©ì 1~1000 â”œâ”€â”€ ì‚¬ìš©ì 1001~2000 â””â”€â”€ ì‚¬ìš©ì 2001~3000 ìƒ¤ë”© í›„: ì„œë²„ 3ëŒ€ (ë°ì´í„° ë¶„ì‚°) â”œâ”€â”€ ì„œë²„1: ì‚¬ìš©ì 1~1000 â”œâ”€â”€ ì„œë²„2: ì‚¬ìš©ì 1001~2000 â””â”€â”€ ì„œë²„3: ì‚¬ìš©ì 2001~3000 ë” ë§ì€ ì‚¬ìš©ìê°€ ëŠ˜ì–´ë‚˜ë©´: â”œâ”€â”€ ì„œë²„1: ì‚¬ìš©ì 1~1000 â”œâ”€â”€ ì„œë²„2: ì‚¬ìš©ì 1001~2000 â”œâ”€â”€ ì„œë²„3: ì‚¬ìš©ì 2001~3000 â”œâ”€â”€ ì„œë²„4: ì‚¬ìš©ì 3001~4000 â† ìƒˆë¡œ ì¶”ê°€ â””â”€â”€ ì„œë²„5: ì‚¬ìš©ì 4001~5000 â† ìƒˆë¡œ ì¶”ê°€ `` RDBMS MySQL: ProxySQL, MySQL Router ë“± ë¯¸ë“¤ì›¨ì–´ í•„ìš” PostgreSQL: ê¸°ë³¸ ë²„ì „ì€ ìƒ¤ë”© ë¯¸ì§€ì› Oracle: RACëŠ” ìˆì§€ë§Œ ìƒ¤ë”©ê³¼ëŠ” ë‹¤ë¦„ SQL Server: ê¸°ë³¸ ë²„ì „ì€ ìƒ¤ë”© ë¯¸ì§€ì› ìƒ¤ë”© êµ¬í˜„ ë°©ë²• ìƒ¤ë”©ì€ DBMSì—ì„œ ì§ì ‘ ì§€ì›í•˜ëŠ” ê²½ìš°ê°€ ì ì–´ì„œ, ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤. 3.1 DBMSë³„ ìƒ¤ë”© ì§€ì› í˜„í™© ë‚´ì¥ ìƒ¤ë”© ì§€ì› DBMS MongoDB: ìë™ ìƒ¤ë”© ì§€ì› (shard key ê¸°ë°˜) Cassandra: íŒŒí‹°ì…˜ í‚¤ ê¸°ë°˜ ìƒ¤ë”© Redis Cluster: í•´ì‹œ ìŠ¬ë¡¯ ê¸°ë°˜ ìƒ¤ë”© Elasticsearch: ì¸ë±ìŠ¤ ê¸°ë°˜ ìƒ¤ë”© ìƒ¤ë”© ë¯¸ì§€ì› DBMS (ìˆ˜ë™ êµ¬í˜„ í•„ìš”) MySQL: ProxySQL, MySQL Router ë“± ë¯¸ë“¤ì›¨ì–´ í•„ìš” PostgreSQL: ê¸°ë³¸ ë²„ì „ì€ ìƒ¤ë”© ë¯¸ì§€ì› (Citus í™•ì¥ìœ¼ë¡œ ê°€ëŠ¥) Oracle: RACëŠ” ìˆì§€ë§Œ ìƒ¤ë”©ê³¼ëŠ” ë‹¤ë¦„ SQL Server: ê¸°ë³¸ ë²„ì „ì€ ìƒ¤ë”© ë¯¸ì§€ì› 3.2 ìƒ¤ë”© êµ¬í˜„ ë°©ì‹ ì• í”Œë¦¬ì¼€ì´ì…˜ ë ˆë²¨ ìƒ¤ë”© ê°œë…: ê°œë°œìê°€ ì§ì ‘ ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œì—ì„œ ìƒ¤ë”© ë¡œì§ì„ êµ¬í˜„ ë°©ì‹: ì‚¬ìš©ì IDë‚˜ ë‹¤ë¥¸ í‚¤ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì–´ë–¤ ìƒ¤ë“œì— ì ‘ê·¼í• ì§€ ê²°ì • ì¥ì : ì™„ì „í•œ ì œì–´ ê°€ëŠ¥, ì„¸ë°€í•œ íŠœë‹ ê°€ëŠ¥ ë‹¨ì : ê°œë°œ ë³µì¡ë„ ì¦ê°€, ìœ ì§€ë³´ìˆ˜ ë¶€ë‹´ ë¯¸ë“¤ì›¨ì–´ë¥¼ í†µí•œ ìƒ¤ë”© ê°œë…: ë°ì´í„°ë² ì´ìŠ¤ì™€ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‚¬ì´ì— ì¤‘ê°„ ê³„ì¸µì„ ë‘ì–´ ìƒ¤ë”© ì²˜ë¦¬ ë°©ì‹: ì¿¼ë¦¬ë¥¼ ë°›ì•„ì„œ ì ì ˆí•œ ìƒ¤ë“œë¡œ ë¼ìš°íŒ… ì¥ì : ì• í”Œë¦¬ì¼€ì´ì…˜ ì½”ë“œ ë³€ê²½ ìµœì†Œí™”, ì „ë¬¸ì ì¸ ìƒ¤ë”© ê´€ë¦¬ ë‹¨ì : ì¶”ê°€ì ì¸ ì¸í”„ë¼ ë³µì¡ë„, ì„±ëŠ¥ ì˜¤ë²„í—¤ë“œ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ í™œìš© ê°œë…: í´ë¼ìš°ë“œ ì œê³µì—…ì²´ê°€ ê´€ë¦¬í•˜ëŠ” ìƒ¤ë”© ì„œë¹„ìŠ¤ ì‚¬ìš© ë°©ì‹: ì„¤ì •ë§Œìœ¼ë¡œ ìë™ ìƒ¤ë”© êµ¬ì„± ì¥ì : ê´€ë¦¬ ë¶€ë‹´ ìµœì†Œí™”, ìë™ í™•ì¥ ë‹¨ì : ë²¤ë” ì¢…ì†ì„±, ë¹„ìš© ì¦ê°€ 3.3 ìƒ¤ë”© ì „ëµ Hash ê¸°ë°˜ ìƒ¤ë”© ê°œë…: ë°ì´í„°ì˜ í•´ì‹œê°’ì„ ê¸°ë°˜ìœ¼ë¡œ ìƒ¤ë“œ ê²°ì • íŠ¹ì§•: ë°ì´í„°ê°€ ê· ë“±í•˜ê²Œ ë¶„ì‚°ë¨ ì˜ˆì‹œ: ì‚¬ìš©ì IDë¥¼ í•´ì‹œí•´ì„œ ìƒ¤ë“œ ë²ˆí˜¸ ê²°ì • Range ê¸°ë°˜ ìƒ¤ë”© ê°œë…: ë°ì´í„°ì˜ ë²”ìœ„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìƒ¤ë“œ ê²°ì • íŠ¹ì§•: ë²”ìœ„ ì¿¼ë¦¬ ì„±ëŠ¥ì´ ì¢‹ìŒ ì˜ˆì‹œ: ë‚ ì§œë³„, ID ë²”ìœ„ë³„ë¡œ ìƒ¤ë“œ í• ë‹¹ Directory ê¸°ë°˜ ìƒ¤ë”© ê°œë…: ë§¤í•‘ í…Œì´ë¸”ì„ ì‚¬ìš©í•´ì„œ ìƒ¤ë“œ ê²°ì • íŠ¹ì§•: ìœ ì—°í•œ ìƒ¤ë“œ í• ë‹¹ ê°€ëŠ¥ ì˜ˆì‹œ: íŠ¹ì • ì‚¬ìš©ìë¥¼ íŠ¹ì • ìƒ¤ë“œì— ê³ ì • ë°°ì¹˜ 3.4 ìƒ¤ë”© ì‹œ ê³ ë ¤ì‚¬í•­ ì¥ì  ë¬´ì œí•œ ìˆ˜í‰ì  í™•ì¥ ê°€ëŠ¥ ê° ìƒ¤ë“œë³„ ë…ë¦½ì ì¸ ì„±ëŠ¥ ìµœì í™” ì¥ì•  ê²©ë¦¬ (í•œ ìƒ¤ë“œ ì¥ì• ê°€ ì „ì²´ì— ì˜í–¥ ì—†ìŒ) ë‹¨ì  ë³µì¡í•œ ë¶„ì‚° ì‹œìŠ¤í…œ ê´€ë¦¬ í¬ë¡œìŠ¤ ìƒ¤ë“œ ì¿¼ë¦¬ì˜ ì„±ëŠ¥ ì´ìŠˆ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥ì˜ ì–´ë ¤ì›€ íŠ¸ëœì­ì…˜ ê´€ë¦¬ì˜ ë³µì¡ì„±",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/datapipeline_linkedin_shared",
    "title": "ë§í¬ë“œì¸ì—ì„œ ê³µìœ ëœ Data Pipeline êµ¬ì¡° ì •ë¦¬",
    "date": "2025-08-05",
    "excerpt": "ë§í¬ë“œì¸ì—ì„œ ê³µìœ ëœ Data Pipeline ì°¸ê³ í•´ì„œ ê·¸ë¦¼ ê·¸ë ¤ë³´ê³ , ì´í•´í•˜ëŠ” ê³¼ì •",
    "category": "Data Science",
    "tags": [
      "Data Pipeline"
    ],
    "content": "ì°¸ê³ í•œ ê¸€ ë§í¬ë“œì¸ - Aurimas Griclunas <figure> <img src=\"/post/DataScience/Datapipeline_linkedin.gif\" alt=\"Data Pipeline LinkedIn\" /> <figcaption>Data Pipeline LinkedIn</figcaption> </figure> ì›ë³¸ ê¸€ ì •ë¦¬ ML ì‹œìŠ¤í…œì˜ Data Pipeline êµ¬ì¡° ML ì‹œìŠ¤í…œì—ì„œ Data Qualityì™€ Integrityë¥¼ ë³´ì¥í•˜ëŠ” ê²ƒì€ ë§¤ìš° ì¤‘ìš”í•©ë‹ˆë‹¤. íŠ¹íˆ ML Training ë° Inference Pipelineì˜ upstreamì—ì„œ ì´ë¥¼ í™•ë³´í•´ì•¼ í•˜ë©°, downstream ì‹œìŠ¤í…œì—ì„œ ì´ë¥¼ ì‹œë„í•˜ë©´ ëŒ€ê·œëª¨ ì‘ì—… ì‹œ ë¶ˆê°€í”¼í•œ ì‹¤íŒ¨ë¥¼ ì•¼ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Data Lake ë˜ëŠ” LakeHouse layerì—ì„œ ìˆ˜í–‰í•´ì•¼ í•  ì‘ì—…ì´ ë§ìŠµë‹ˆë‹¤. ì•„ë˜ëŠ” production-grade end-to-end data flowì˜ ì˜ˆì‹œ ì•„í‚¤í…ì²˜ì…ë‹ˆë‹¤. Data Flow ë‹¨ê³„ë³„ ì„¤ëª… Schema ê´€ë¦¬ Schema changesëŠ” version controlì—ì„œ êµ¬í˜„ ìŠ¹ì¸ í›„ Applications, Databases, ì¤‘ì•™ Data Contract Registryì— í‘¸ì‹œ ApplicationsëŠ” ìƒì„±ëœ Dataë¥¼ Kafka Topicsë¡œ ì „ì†¡ Data ìˆ˜ì§‘ 2ë‹¨ê³„: Application Servicesì—ì„œ ì§ì ‘ ë°œìƒí•˜ëŠ” Events IoT Fleets ë° Website Activity Tracking í¬í•¨ 2.1ë‹¨ê³„: CDC streamsì„ ìœ„í•œ Raw Data Topics Data ê²€ì¦ ë° ì²˜ë¦¬ 3ë‹¨ê³„: Flink Application(s)ì´ Raw Data streamsì„ ì†Œë¹„í•˜ê³  Contract Registryì˜ schemasì— ëŒ€í•´ ê²€ì¦ 4ë‹¨ê³„: contractë¥¼ ì¶©ì¡±í•˜ì§€ ì•ŠëŠ” DataëŠ” Dead Letter Topicìœ¼ë¡œ ì „ì†¡ 5ë‹¨ê³„: contractë¥¼ ì¶©ì¡±í•˜ëŠ” DataëŠ” Validated Data Topicìœ¼ë¡œ ì „ì†¡ 6ë‹¨ê³„: Validated Data Topicì˜ DataëŠ” ì¶”ê°€ Validationì„ ìœ„í•´ object storageë¡œ ì „ì†¡ Data Warehouse ë° ë³€í™˜ 7ë‹¨ê³„: Object Storageì˜ DataëŠ” scheduleì— ë”°ë¼ Data Contractsì˜ ì¶”ê°€ SLAsì— ëŒ€í•´ ê²€ì¦ë˜ê³ , analytical purposesë¥¼ ìœ„í•´ transformed ë° modeledë˜ì–´ Data Warehouseë¡œ ì „ì†¡ Feature Store ë° ML Pipeline 8ë‹¨ê³„: Modeled ë° Curated dataëŠ” ì¶”ê°€ Feature Engineeringì„ ìœ„í•´ Feature Store Systemìœ¼ë¡œ ì „ì†¡ 8.1ë‹¨ê³„: Real Time FeaturesëŠ” Validated Data Topic(5ë‹¨ê³„)ì—ì„œ ì§ì ‘ Feature Storeë¡œ ingested SLA checksê°€ ì–´ë ¤ì›Œ Data Quality ë³´ì¥ì´ ë³µì¡í•¨ 9ë‹¨ê³„: High Quality DataëŠ” ML Training Pipelinesì—ì„œ ì‚¬ìš© 10ë‹¨ê³„: ë™ì¼í•œ DataëŠ” Inferenceì—ì„œ Feature Servingì— ì‚¬ìš© ì£¼ì˜ì‚¬í•­ ML SystemsëŠ” Data Driftì™€ Concept Driftì™€ ê°™ì€ ë‹¤ë¥¸ Data ê´€ë ¨ ë¬¸ì œë¡œ ê³ í†µë°›ìŠµë‹ˆë‹¤. ì´ëŠ” silent failuresì´ë©° monitoringí•  ìˆ˜ ìˆì§€ë§Œ Data Contractì— í¬í•¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. --- ì´ êµ¬ì¡°ëŠ” LLM ê¸°ë°˜ ì‹œìŠ¤í…œì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CDC streams? > CDC(Change Data Capture) Streamì€ ë°ì´í„°ë² ì´ìŠ¤ì˜ ë³€ê²½ì‚¬í•­(INSERT, UPDATE, DELETE)ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ê°ì§€í•˜ê³  ìŠ¤íŠ¸ë¦¬ë°í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë°ì´í„° ë™ê¸°í™”ì— ì‚¬ìš©ë˜ëŠ” ê°œë…? ì‹¤ì‹œê°„ ë°ì´í„° ë™ê¸°í™” ì•„í‚¤í…ì²˜: Kafka & CDC ê¸°ë°˜ ì„¤ê³„ ê²½í—˜ Kafka? > Apache KafkaëŠ” ê³ ì„±ëŠ¥ ë¶„ì‚° ìŠ¤íŠ¸ë¦¬ë° í”Œë«í¼ì…ë‹ˆë‹¤. ì‹¤ì‹œê°„ ë°ì´í„° íŒŒì´í”„ë¼ì¸ê³¼ ìŠ¤íŠ¸ë¦¬ë° ì• í”Œë¦¬ì¼€ì´ì…˜ì„ êµ¬ì¶•í•˜ê¸° ìœ„í•œ ì˜¤í”ˆì†ŒìŠ¤ í”Œë«í¼ì…ë‹ˆë‹¤. í•µì‹¬ ê°œë…: Producer: ë°ì´í„°ë¥¼ Kafkaë¡œ ì „ì†¡í•˜ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ Consumer: Kafkaì—ì„œ ë°ì´í„°ë¥¼ ì½ì–´ê°€ëŠ” ì• í”Œë¦¬ì¼€ì´ì…˜ Topic: ë°ì´í„°ê°€ ì €ì¥ë˜ëŠ” ì¹´í…Œê³ ë¦¬ (ë©”ì‹œì§€ íì™€ ìœ ì‚¬) Broker: Kafka ì„œë²„ (í´ëŸ¬ìŠ¤í„°ë¡œ êµ¬ì„±) Partition: Topicì„ ì—¬ëŸ¬ ê°œë¡œ ë‚˜ëˆˆ ë‹¨ìœ„ (ë³‘ë ¬ ì²˜ë¦¬)",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì†Œë¬¸ë°ê°€ì‹­/08-04-ë„ˆí•œí…Œ_ì‘ì—…_ê±¸ì—ˆë‹¤ë‹ˆ_ë§ë„ì•ˆë¼",
    "title": "ë„ˆí•œí…Œ ì‘ì—… ê±¸ì—ˆë‹¤ë‹ˆ ë§ë„ ì•ˆë¼",
    "date": "2025-08-04",
    "excerpt": "ë¯¿ì„ ìˆ˜ ì—†ë‹¤ëŠ” ë“¯ ë‹¤ì‹œ ë¬»ëŠ” í‘œí˜„ë“¤. ê·¸ëŸ´ë¦¬ ì—†ë‹¤ëŠ” ì´ì•¼ê¸°",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ ì •ë¦¬ Are you telling me that John got plastic surgery? There's no way : ê·¸ëŸ´ë¦¬ê°€ ì—†ì–´ There's no way that Mark is cheating on his girlfriend. hit on makes fun of someone : ~~ë¥¼ ë¹„ì›ƒë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Career/how-to-get-job",
    "title": "ì´ì§í•˜ëŠ” ë°©ë²• ì •ë¦¬",
    "date": "2025-08-04",
    "excerpt": "ì´ì§í•˜ëŠ” ë²• ì°¾ì•„ë³´ë©´ì„œ í•˜ë‚˜ì”© ì •ë¦¬í•œ ê²ƒë“¤",
    "category": "Career",
    "tags": [
      "ì´ì§ì„±ê³µí•˜ì!"
    ],
    "content": "ì¡í”Œë˜ë‹› ì´ì§ ìë£Œ",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-04-í•˜ì´í¼ì»¤ë„¥íŠ¸",
    "title": "í•˜ì´í¼ì»¤ë„¥íŠ¸ ML Engineer",
    "date": "2025-08-04",
    "excerpt": "í•˜ì´í¼ì»¤ë„¥íŠ¸ ML Engineer ì ì–´ë³´ê¸°",
    "category": "Career",
    "tags": [
      "í•˜ì´í¼ì»¤ë„¥íŠ¸ ì±„ìš©ê³µê³ "
    ],
    "content": "í•˜ì´í¼ì»¤ë„¥íŠ¸ ML Engineer ì±„ìš©ê³µê³  ì±„ìš©ê³µê³  ë§í¬: LinkedIn Job Posting íšŒì‚¬ ì •ë³´ Legal Entity íšŒì‚¬ëª…: Hyperconnect ë¸Œëœë“œ: Match Group AI Match Group AI ì†Œê°œ Match Group AIëŠ” ì¤‘ì•™ AI ì¡°ì§ìœ¼ë¡œì„œ, AI ê¸°ìˆ ì„ í™œìš©í•˜ì—¬ Tinderì™€ Hingeë¥¼ ë¹„ë¡¯í•œ Match Groupì˜ ë‹¤ì–‘í•œ ì œí’ˆêµ°ì— ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ì˜ ê¸°ìˆ ë¡œëŠ” ì ‘ê·¼í•˜ê¸° ì–´ë µì§€ë§Œ AI ê¸°ìˆ ì„ í†µí•´ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì„ ì°¾ì•„ë‚´ê³  ì‚¬ìš©ì ê²½í—˜ì„ í˜ì‹ í•©ë‹ˆë‹¤. ì£¼ìš” ì—­í• : ì‚¬ìš©ìë“¤ì´ ìì‹ ì„ ë” ì˜ í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ë„êµ¬ ê°œë°œ ì˜ë¯¸ ìˆëŠ” ë§Œë‚¨ì„ ë°œê²¬í•˜ëŠ” ê³¼ì •ì—ì„œ ë” ë§Œì¡±ìŠ¤ëŸ¬ìš´ ê²½í—˜ì„ ì œê³µí•˜ëŠ” ê¸°ëŠ¥ ê°œë°œ ML Team ì†Œê°œ ML Teamì€ Match Groupì˜ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì— AI ê¸°ìˆ ì„ ì ìš©í•˜ëŠ” ML Engineerë¡œ êµ¬ì„±ëœ íŒ€ì…ë‹ˆë‹¤. ì´ë“¤ì€ ìµœì‹  AI ê¸°ìˆ ì„ ì—°êµ¬í•˜ê³  ì ìš©í•˜ëŠ” ê³¼í•™ìì´ì, ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì— ë§ê²Œ ëª¨ë¸ ë° ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ê³  ìµœì í™”í•˜ì—¬ ì‹¤ì§ˆì ì¸ ì„íŒ©íŠ¸ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ” ì—”ì§€ë‹ˆì–´ì´ê¸°ë„ í•©ë‹ˆë‹¤. ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ë¬¸ì œ ë°œêµ´ ë° ì •ì˜: ì‹¤ì œ ì„œë¹„ìŠ¤ í˜„ì¥ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ë°œêµ´í•˜ê³  ì •ì˜ ëª¨ë¸ ê°œë°œ: ë¬¸ì œ í•´ê²°ì— ê°€ì¥ ì í•©í•œ SotA(State-of-the-Art) ëª¨ë¸ì„ ê°œë°œí•˜ê±°ë‚˜ ì¬í˜„ ë°°í¬: ì™„ì„±ëœ ëª¨ë¸ì„ ëª¨ë°”ì¼ê³¼ ì„œë²„ í™˜ê²½ì— ì•ˆì •ì ì´ê³  íš¨ìœ¨ì ìœ¼ë¡œ ë°°í¬ ëª¨ë‹ˆí„°ë§ ë° ê°œì„ : ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ê³¼ ê°œì„ ì„ í†µí•´ ì„œë¹„ìŠ¤ì˜ AI Flywheel êµ¬ì¶• í˜‘ì—… ë°±ì—”ë“œ/í”„ë¡ íŠ¸ì—”ë“œ/DevOps ì—”ì§€ë‹ˆì–´, ë°ì´í„° ë¶„ì„ê°€, PM ë“± ë‹¤ì–‘í•œ ì „ë¬¸ ì¡°ì§ê³¼ ê¸´ë°€íˆ í˜‘ì—…í•˜ë©° ì‹¤ì œ ì‚¬ìš©ìì—ê²Œ ì„íŒ©íŠ¸ë¥¼ ì£¼ëŠ” AI ê²½í—˜ì„ ë§Œë“¤ì–´ê°‘ë‹ˆë‹¤. ì°¸ê³  ìë£Œ How AI Lab Works - Head of AI - Shurain ì¸í„°ë·° AI in Social Discovery(Blending Research and Production) ì—°êµ¬ ì„±ê³¼ ì—…ë¬´ ì„±ê³¼ ì¤‘ ì¼ë¶€ëŠ” ë…¼ë¬¸ ë˜ëŠ” ì˜¤í”ˆì†ŒìŠ¤ ì½”ë“œë¡œ ì™¸ë¶€ì— ê³µê°œí•˜ê¸°ë„ í•©ë‹ˆë‹¤. ì§€ê¸ˆê¹Œì§€ ì•½ 20ì—¬ ê±´ì˜ ëŒ€ì™¸ì  ì—°êµ¬ ì„±ê³¼ë¥¼ ê±°ë‘˜ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì£¼ìš” ì—°êµ¬ ì„±ê³¼ 2024ë…„ CUPID: 1:1 ì†Œì…œ ë””ìŠ¤ì»¤ë²„ë¦¬ í”Œë«í¼ì„ ìœ„í•œ ì‹¤ì‹œê°„ ì„¸ì…˜ ê¸°ë°˜ ìƒí˜¸ ì¶”ì²œ ì‹œìŠ¤í…œ (ICDM Workshop ë°œí‘œ) 2023ë…„ TiDAL: íš¨ìœ¨ì ì¸ í•™ìŠµ ê³¼ì •ì˜ ëª¨ë¸ í–‰ë™ì— ê¸°ë°˜í•œ ì•¡í‹°ë¸Œ ëŸ¬ë‹ ê¸°ë²• (ICCV 2023 ê²Œì¬) ëª¨ë”ë ˆì´ì…˜ í™˜ê²½ì—ì„œ ì—¬ëŸ¬ ë¶„ë¥˜ ê¸°ì¤€ì„ ë™ì‹œì— ë§Œì¡±í•˜ê¸° ìœ„í•œ ë¬¸í„±ê°’ì„ ì¡ëŠ” ì—°êµ¬ (WSDM 2023 ê²Œì¬) 2022ë…„ ëŒ€í™” ìƒì„±ì—ì„œì˜ ì˜ë¯¸ì  ë‹¤ì–‘ì„±ì„ ë†’ì´ëŠ” ì—°êµ¬ (EMNLP 2022 ê²Œì¬) ë ˆì´ë¸” ë…¸ì´ì¦ˆê°€ ì‹¬í•œ í™˜ê²½ì—ì„œ íš¨ê³¼ì ìœ¼ë¡œ í•™ìŠµí•˜ëŠ” ë°©ë²• (ECCV 2022 ê²Œì¬) íƒ€ê¹ƒ ìºë¦­í„°ì˜ ëª‡ê°€ì§€ ë°œí™”ë§Œì„ ì´ìš©í•˜ì—¬ íƒ€ê¹ƒ ìºë¦­í„°ë¥¼ ëª¨ë°©í•˜ëŠ” ì±—ë´‡ì—°êµ¬ (NAACL 2022 ê²Œì¬) ëŒ€í™” ìƒì„± ëª¨ë¸ì—ì„œ ì˜ˆì‹œë¥¼ í™œìš©í•˜ì—¬ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ì—°êµ¬ (ACL 2022 Workshop ë°œí‘œ) ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ì˜¤ë””ì˜¤ ë¶„ë¥˜ë¥¼ ìœ„í•œ distillation ê¸°ìˆ  ì—°êµ¬ (ICASSP ê²Œì¬) ì¸í”„ë¼ ë° í™˜ê²½ ë”¥ëŸ¬ë‹ í´ëŸ¬ìŠ¤í„° ì´ 160ëŒ€ A100 GPU 40ëŒ€ H100 GPU ë°ì´í„° íŒŒì´í”„ë¼ì¸ ë°ì´í„° ìˆ˜ì§‘, ì „ì²˜ë¦¬ë¥¼ í¬í•¨í•œ ìì²´ì ì¸ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ êµ¬ì¶• ë° ìš´ì˜ í˜‘ì—… í™˜ê²½ ë‹¤ì–‘í•œ software engineer(ë°±ì—”ë“œ/í”„ë¡ íŠ¸ì—”ë“œ/DevOps/MLSE)ì™€ í•¨ê»˜ ì‘ì—… ë‹´ë‹¹ ì—…ë¬´ Match Group AIì˜ ML EngineerëŠ” Tinder, Hinge ë“± ë§¤ì¹˜ê·¸ë£¹ì˜ ê¸€ë¡œë²Œ íŒ€ë“¤ê³¼ í˜‘ì—…í•˜ë©°, ìƒì„±í˜• AI, ì˜¨ë””ë°”ì´ìŠ¤ AI, ì¶”ì²œ ì‹œìŠ¤í…œ ë“± ë‹¤ì–‘í•œ AI ê¸°ìˆ ì„ í™œìš©í•´ ë°ì´íŒ… ë„ë©”ì¸ ì‚¬ìš©ì ê²½í—˜ì„ í˜ì‹ í•˜ëŠ” ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤. ì£¼ìš” ì—­í•  ì•„ì´ë””ì–´ êµ¬ìƒë¶€í„° í”„ë¡œí† íƒ€ì´í•‘, ì‹¤ì œ ì‚¬ìš©ìì—ê²Œ ë„ë‹¬í•˜ê¸°ê¹Œì§€ ì „ ê³¼ì •ì„ ì£¼ë„ì ìœ¼ë¡œ ì´ëŒê¸° í•„ìš”ì‹œ ì§ë¬´ ì—­í• ê³¼ ë„ë©”ì¸, ê¸°ìˆ  ìŠ¤íƒì˜ ê²½ê³„ë¥¼ ë„˜ë‚˜ë“¤ë©° ë¬¸ì œ í•´ê²° ë‹¤ë£¨ëŠ” ë¬¸ì œ ì˜ì—­ ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œë“¤ ì¤‘ ì¼ë¶€ì— ëŒ€í•œ ê²½í—˜ì´ ìˆê±°ë‚˜, ì•„ì§ ê²½í—˜í•´ë³´ì§€ ì•Šì€ ë¬¸ì œë¼ë„ ë¹ ë¥´ê²Œ í•™ìŠµí•˜ê³  ì£¼ë„ì ìœ¼ë¡œ ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” ë¶„ì„ ì°¾ê³  ìˆìŠµë‹ˆë‹¤: Multi-modal ë°ì´í„° í™œìš© ì œí’ˆì—ì„œ ë°œìƒí•˜ëŠ” multi-modal dataë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš© ê·¹ë„ë¡œ ë¶ˆê· í˜•í•˜ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ ë§ì€ ë°ì´í„° ì²˜ë¦¬ ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œ ìˆ˜ì§‘ëœ ë°ì´í„° ê°„ ì°¨ì´ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆëŠ” domain adaptation ê¸°ë²• Meta Learning ëª¨ë¸ ìš”êµ¬ì‚¬í•­ì´ ìì£¼ ë³€ê²½ë˜ê±°ë‚˜ ì´ˆê¸° ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•Šì€ ìƒí™©ì—ë„ ìœ ì—°í•˜ê²Œ ëŒ€ì‘í•  ìˆ˜ ìˆëŠ” meta learning ì ‘ê·¼ë²• ëª¨ë°”ì¼ ìµœì í™” ëª¨ë°”ì¼ í™˜ê²½ì—ì„œ ì§§ì€ ì‘ë‹µ ì‹œê°„(latency)ì„ ìœ ì§€í•˜ë©´ì„œë„ ë†’ì€ ì„±ëŠ¥ì„ ë‹¬ì„±í•  ìˆ˜ ìˆë„ë¡, ëª¨ë¸ì„ ê²½ëŸ‰í™”í•˜ê³  ìµœì í™”í•˜ëŠ” ë°©ë²• ëŒ€ê·œëª¨ ëª¨ë¸ í™œìš© ëŒ€ê·œëª¨ ì–¸ì–´ ëª¨ë¸(large language models)ì´ë‚˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸(multimodal models)ì„ í™œìš©í•´ ìƒˆë¡œìš´ ê¸°ëŠ¥ì„ ê°œë°œ íŠ¹ì • ê¸°ëŠ¥ì„ ìœ„í•´ íš¨ê³¼ì ìœ¼ë¡œ ëª¨ë¸ ì„±ëŠ¥ì„ ëŒì–´ì˜¬ë¦¬ëŠ” ë°©ë²• ì„œë¹„ìŠ¤ ìš´ì˜ ëŒ€ê·œëª¨ ëª¨ë¸ì„ ì‹¤ì œ ì„œë¹„ìŠ¤ í™˜ê²½ì—ì„œ ì•ˆì •ì ìœ¼ë¡œ ìš´ì˜í•˜ê¸° ìœ„í•œ ëª¨ë¸ë§, ìµœì í™” ë° ê²½ëŸ‰í™” ê¸°ë²• í¸í–¥ í•´ê²° ì„œë¹„ìŠ¤, ì‹œìŠ¤í…œ, ëª¨ë¸ì— ì˜í•´ ë°œìƒí•˜ëŠ” ë‹¤ì–‘í•œ ë°ì´í„° í¸í–¥ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²• Requirements í•„ìˆ˜ ìš”êµ¬ì‚¬í•­ AI/MLì— ëŒ€í•œ ê¸°ë³¸ ì§€ì‹ê³¼ ì ì–´ë„ í•œ ê°œ ì´ìƒì˜ íŠ¹ì • ë„ë©”ì¸ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì§€ì‹ì„ ê°–ì¶”ê³ , ê´€ë ¨ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„ Exploratory Data Analysis(EDA)ë¥¼ í†µí•´, ë°ì´í„°ì˜ í†µê³„ì  íŠ¹ì„±ê³¼ íŒ¨í„´ì„ ë°œê²¬í•˜ê³  ì´ë¥¼ ML ëª¨ë¸ì— ë°˜ì˜í•˜ì‹¤ ìˆ˜ ìˆëŠ” ë¶„ ê³µê°œëœ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì…‹ì„ ì´ìš©í•´ ëª¨ë¸ì˜ í…ŒìŠ¤íŠ¸ ì„±ëŠ¥ì„ ì˜¬ë¦¬ê¸° ìœ„í•´ ì—¬ëŸ¬ ê°€ì§€ ëª¨ë¸ë§ì„ í•´ë³¸ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„ êµ¬í˜„ì²´ê°€ ê³µê°œë˜ì§€ ì•Šì€ ë…¼ë¬¸ì„ ì½ê³ , ë¹ ë¥´ê³  ì •í™•í•˜ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ” êµ¬í˜„ ì—­ëŸ‰ì„ ê°–ì¶”ì‹  ë¶„ Tensorflow, PyTorch, JAX ë“± ì˜¤í”ˆì†ŒìŠ¤ í”„ë ˆì„ì›Œí¬ ë° ì „ë°˜ì ì¸ íŒŒì´ì¬ ê°œë°œì— ëŠ¥ìˆ™í•˜ì‹  ë¶„ ML ëª¨ë¸ í•™ìŠµ ë° ì„œë¹„ìŠ¤ ë°°í¬ì— í•„ìš”í•œ ì—”ì§€ë‹ˆì–´ë§ ì—­ëŸ‰ì„ ê°–ì¶”ì‹  ë¶„ AI ê¸°ìˆ ì˜ ì„œë¹„ìŠ¤í™”ì— ê´€ì‹¬ì´ ë§ìœ¼ì‹  ë¶„ í•™ìœ„/êµ­ì  ë¬´ê´€, ì˜ì–´ë¡œ ê¸°ì´ˆì ì¸ ì˜ì‚¬ì†Œí†µì´ ê°€ëŠ¥í•˜ê³ , í•œêµ­ì–´ë¡œ ì›í™œí•œ ì˜ì‚¬ì†Œí†µì´ ê°€ëŠ¥í•œ ë¶„ Preferred Qualifications ìš°ëŒ€ì‚¬í•­ AI/ML ë„ë©”ì¸ ì „ë°˜ì— ëŒ€í•œ ì´í•´ë¥¼ ê°–ì¶”ì‹  ë¶„ ML ê´€ë ¨ íƒ‘í‹°ì–´ í•™íšŒ ë° ì €ë„(NeurIPS, ICLR, ICML, CVPR, ICCV/ECCV, KDD, â€¦) ê²Œì¬ ì‹¤ì  í˜¹ì€ AI ê´€ë ¨ ëŒ€íšŒ ìˆ˜ìƒ ì‹¤ì ì´ ìˆìœ¼ì‹  ë¶„ ML ê´€ë ¨ ì˜¤í”ˆ ì†ŒìŠ¤ ê°œë°œì— ì°¸ì—¬í•´ ë³¸ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„ ML ì›Œí¬í”Œë¡œìš°ì˜ ìë™í™” ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„(ì˜ˆ: AutoML, hyperparameter optimization, ë°ì´í„° ë° í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë“±) í´ë¼ì´ì–¸íŠ¸(Android, iOS), ë°±ì—”ë“œë¥¼ í¬í•¨í•´ AI/ML ë¶„ì•¼ ì™¸ í”„ë¡œì íŠ¸ ê°œë°œ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„ ì‹¤ì œ ì„œë¹„ìŠ¤ì— AI ê¸°ìˆ ì„ í†µí•©í•˜ê³  ì£¼ìš” ì§€í‘œë¥¼ ìœ ì˜ë¯¸í•˜ê²Œ í–¥ìƒì‹œì¼œ ë³¸ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„ A/B í…ŒìŠ¤íŠ¸ ì‹¤í—˜ ê¸°íš ë° íƒ€ê²Ÿ KPI ì§€í‘œë¥¼ ì •ì˜í•˜ê³ , SQLê¸°ë°˜ ë°ì´í„° ë¶„ì„ì„ ì§„í–‰í•œ ê²½í—˜ì´ ìˆìœ¼ì‹  ë¶„ ì˜ì–´ë¡œ ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ì†Œí†µì´ ê°€ëŠ¥í•˜ì‹  ë¶„ Hiring Process ì±„ìš© ì •ë³´ ê³ ìš© í˜•íƒœ: ì •ê·œì§ ì§€ì› ì„œë¥˜: ììœ  ì–‘ì‹ì˜ ìƒì„¸ ê²½ë ¥ê¸°ë°˜ ì˜ë¬¸ì´ë ¥ì„œ(PDF) ì±„ìš© ì ˆì°¨ ì„œë¥˜ì „í˜• ì½”ë”©í…ŒìŠ¤íŠ¸, ê³¼ì œì „í˜• 1ì°¨ ë©´ì ‘ Recruiter Call 2ì°¨ ë©´ì ‘ ìµœì¢…í•©ê²© í•„ìš” ì‹œ ì „í˜•ì´ ë³€ê²½ ì§„í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì„œë¥˜ ì „í˜•ì˜ ê²½ìš° í•©ê²©ìì— í•œí•˜ì—¬ ê°œë³„ ì•ˆë‚´ë“œë¦´ ì˜ˆì •ì…ë‹ˆë‹¤. íŠ¹ë³„ ì•ˆë‚´ì‚¬í•­ ì´ í¬ì§€ì…˜ì€ ì „ë¬¸ì—°êµ¬ìš”ì› í˜„ì—­ í¸ì…/ì „ì§, ì „ë¬¸ì—°êµ¬ìš”ì› ë³´ì¶©ì—­ í¸ì…/ì „ì§ ì±„ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë³‘ì—­íŠ¹ë¡€ìš”ì›ì˜ ê²½ìš°, ë³‘ì—­íŠ¹ë¡€ ê´€ë ¨ë²•ì— ë”°ë¼ ë³µë¬´ê´€ë¦¬ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ì£¼ì˜ì‚¬í•­ ì œì¶œí•´ ì£¼ì‹  ë‚´ìš© ì¤‘ í—ˆìœ„ ì‚¬ì‹¤ì´ ìˆê±°ë‚˜ ê´€ë ¨ë²• ìƒ ê·¼ë¡œì œê³µì— ê²°ê²©ì‚¬ìœ ê°€ ìˆëŠ” ê²½ìš° ì±„ìš©ì´ ì·¨ì†Œë  ìˆ˜ ìˆìœ¼ë©°, í•„ìš”ì‹œ ì‚¬ì „ì— ì•ˆë‚´ëœ ì±„ìš© ì ˆì°¨ ì™¸ì—ë„ ì¶”ê°€ ì „í˜• ë° ì„œë¥˜ í™•ì¸ì´ ì§„í–‰ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµ­ê°€ë³´í›ˆëŒ€ìƒìëŠ” ê´€ê³„ ë²•ë ¹ì— ë”°ë¼ ìš°ëŒ€í•˜ì˜¤ë‹ˆ, í•´ë‹¹ë˜ì‹œëŠ” ë¶„ê»˜ì„œëŠ” ì§€ì› ì‹œ ê³ ì§€í•´ì£¼ì‹œê³  ì±„ìš© ì‹œ ì¦ë¹™ì„œë¥˜ë¥¼ ì œì¶œí•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. í•˜ì´í¼ì»¤ë„¥íŠ¸ê°€ ì±„ìš©í•˜ëŠ” í¬ì§€ì…˜ì— ì§€ì›í•˜ëŠ” ê²½ìš°, ê°œì¸ì •ë³´ ì²˜ë¦¬ì— ê´€í•˜ì—¬ì„œëŠ” ë³¸ ê°œì¸ì •ë³´ì²˜ë¦¬ë°©ì¹¨ì´ ì ìš©ë©ë‹ˆë‹¤: https://career.hyperconnect.com/privacy",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-04-í† ìŠ¤MLì—”ì§€ë‹ˆì–´-ê°„í¸ì§€ì›",
    "title": "Toss - ML Engineer ê°„í¸ì§€ì›",
    "date": "2025-08-04",
    "excerpt": "Toss ML Engineer ê°„í¸ì§€ì› ì ì–´ë³´ê¸°",
    "category": "Career",
    "tags": [
      "í† ìŠ¤ ì±„ìš©ê³µê³ "
    ],
    "content": "https://toss.im/builder/to/59b74ab2bee94d93b41025c8bd7afb49?utm_source=linkedin&utm_medium=display&utm_campaign=2507_ML ğŸ¤– ML ì—”ì§€ë‹ˆì–´ì˜ ë‹¤ìŒ ì»¤ë¦¬ì–´, ë¶€ë‹´ ì—†ì´ ì‹œì‘í•˜ì„¸ìš” ML ì—”ì§€ë‹ˆì–´ë¡œ 3ë…„ ì´ìƒì˜ ê²½ë ¥ì„ ê°€ì§€ê³  ìˆë‹¤ë©´, ì´ë ¥ì„œ ì—†ì´ ê°€ë³ê²Œ ì§€ì›í•´ ë³´ì„¸ìš”. ë³¸ ê°„í¸ì§€ì› ì´ë²¤íŠ¸ëŠ” ì§€ì› ì´ë ¥ì— ë‚¨ì§€ ì•Šì•„ìš”. 3ê°œì˜ ì§ˆë¬¸ì„ í†µí•´ ì—¬ëŸ¬ë¶„ì˜ ê²½í—˜ê³¼ ì˜ ë§ëŠ” í¬ì§€ì…˜ì„ ì°¾ì•„ë“œë¦´ê²Œìš”. ê¸°ìˆ ì ì¸ ê³ ë¯¼ì„ ì¦ê¸°ê³ , ë¬¸ì œë¥¼ í’€ì–´ì˜¨ ê²½í—˜ì´ ìˆë‹¤ë©´ ì¶©ë¶„í•´ìš”. ğŸ“† ì§€ì› ë§ˆê° ~8/6(ìˆ˜) ì˜¤í›„ 12ì‹œê¹Œì§€ ê²°ê³¼ëŠ” ë§ˆê°ì¼ ì´í›„ ìˆœì°¨ì ìœ¼ë¡œ ì•ˆë‚´í•´ ë“œë ¤ìš”. ğŸ—ºï¸ ì§„í–‰ ì ˆì°¨ ê°„í¸ ì§€ì› â†’ í•©ê²©í•  ê²½ìš° ì±„ìš©íŒ€ì˜ í¬ì§€ì…˜ ì œì•ˆ â†’ ì§ë¬´ ì¸í„°ë·° ê²½ë ¥ê³¼ ë°©í–¥ì— ë§ëŠ” í¬ì§€ì…˜ì„ í•¨ê»˜ ê²€í† í•´ ë“œë ¤ìš”. ğŸ” í† ìŠ¤ì˜ ML í¬ì§€ì…˜ì´ ê¶ê¸ˆí•˜ë‹¤ë©´, ë‹¤ìŒ ë§í¬ë¥¼ í†µí•´ ìì„¸íˆ ì‚´í´ë³´ì‹¤ ìˆ˜ ìˆì–´ìš”. JD ë°”ë¡œ ê°€ê¸° *ë¬¸ì˜: í† ìŠ¤ ì±„ìš©íŒ€ (recruit@toss.im) Q1. í† ìŠ¤ì— ì§€ì›í•˜ëŠ” ì´ìœ ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”. Q2. ë³¸ì¸ì´ ê°€ì¥ ê¹Šì´ ê´€ì—¬í–ˆë˜ í”„ë¡œì íŠ¸ëŠ” ë¬´ì—‡ì¸ê°€ìš”? í•´ë‹¹ í”„ë¡œì íŠ¸ì—ì„œ ë‹¤ë¤˜ë˜ ë¬¸ì œì™€ ë„ë©”ì¸ì„ ê°„ë‹¨íˆ ì†Œê°œí•´ì£¼ì„¸ìš”. Q3. í•´ë‹¹ í”„ë¡œì íŠ¸ì—ì„œ ë³¸ì¸ì´ ì •ì˜í•œ ë¬¸ì œì™€ ì„ íƒí•œ ì ‘ê·¼ ë°©ì‹ì— ëŒ€í•´ êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”. í”„ë¡œì íŠ¸ ê²°ê³¼ê°€ ì¡°ì§ì´ë‚˜ ì„œë¹„ìŠ¤ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆë‚˜ìš”? ê°€ëŠ¥í•˜ë‹¤ë©´ ì •ëŸ‰ì  ì„±ê³¼(AUC, CVR, latency ë“±)ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”. (e.g. ì–´ë–¤ ë°ì´í„°ë¥¼ ì–´ë–»ê²Œ í™œìš©í–ˆëŠ”ì§€, ì–´ë–¤ ëª¨ë¸ì´ë‚˜ ê¸°ë²•ì„ ì ìš©í–ˆëŠ”ì§€ ë“±) í•œ ë²ˆ ì œì¶œí•œ ë‚´ìš©ì€ ìˆ˜ì •í•  ìˆ˜ ì—†ìœ¼ë‹ˆ, ì œì¶œ ì „ ë§ˆì§€ë§‰ìœ¼ë¡œ í™•ì¸í•´ ì£¼ì„¸ìš”.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-04-ì§€ë§ˆì¼“-ë°ì´í„°ê³¼í•™ì",
    "title": "Gmarket ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ (Data Scientist)",
    "date": "2025-08-04",
    "excerpt": "Gmarket ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ (Data Scientist) ì ì–´ë³´ê¸°",
    "category": "Career",
    "tags": [
      "ì§€ë§ˆì¼“ ì±„ìš©ê³µê³ "
    ],
    "content": "íŒ€ ì†Œê°œ Foundation AIíŒ€ì€ ì§€ë§ˆì¼“ì˜ í•µì‹¬ ìì‚°ì¸ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, e-commerce ë¹„ì¦ˆë‹ˆìŠ¤ì˜ ë‹¤ì–‘í•œ ë¬¸ì œë¥¼ AI ê¸°ìˆ ë¡œ í•´ê²°í•˜ëŠ” ëª¨ë¸ ë° ì‹œìŠ¤í…œì„ ê°œë°œí•©ë‹ˆë‹¤. ë²”ìš© ëª¨ë¸ì´ ì•„ë‹Œ, ì‹¤ì œ ì»¤ë¨¸ìŠ¤ ë°ì´í„° ê¸°ë°˜ì˜ ìì²´ í•™ìŠµ Foundation ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³  ìˆìœ¼ë©°, ì´ë¥¼ ë‹¤ì–‘í•œ ë¹„ì¦ˆë‹ˆìŠ¤ ê³¼ì œì— ì ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ, ê³ ê° í–‰ë™ ì´ë ¥ ë°ì´í„°ë¥¼ í™œìš©í•œ ê°œì¸í™” íƒ€ê²ŸíŒ… ë° CRM ì¿ í° ìµœì í™” ëª¨ë¸ë§ì„ í†µí•´ ê³ ê° ê²½í—˜ì„ í–¥ìƒì‹œí‚¤ê³ , ë°ì´í„° ê¸°ë°˜ì˜ ë§ˆì¼€íŒ… ì „ëµ ê³ ë„í™”ì— ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ì—…ë¬´ â–  CRM ì¿ í° íƒ€ê²ŸíŒ… ìµœì í™”ë¥¼ ìœ„í•œ ê°œì¸í™” ëª¨ë¸ ê°œë°œ ê³ ê° ìƒí™©ê³¼ ë¬¸ë§¥ ê¸°ë°˜ì˜ ê´€ì‹¬ ìƒí’ˆ ì˜ˆì¸¡ ë° ì„¸ë¶„í™”(Contextual Segmentation) ì¿ í°/ë§ˆì¼€íŒ… ìº í˜ì¸ íš¨ìœ¨ ë¶„ì„ ë° CRM ì„±ê³¼ ìµœì í™”ë¥¼ ìœ„í•œ ë¶„ì„ ëª¨ë¸ ì„¤ê³„ â–  ì´ì»¤ë¨¸ìŠ¤ ë„ë©”ì¸ ë‚´ ë°ì´í„° ìœµí•© ë¶„ì„ ë° AI ëª¨ë¸ë§ ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì—ì„œ ë°œìƒí•˜ëŠ” ëŒ€ê·œëª¨ ë°ì´í„°ë¥¼ í†µí•©Â·ë¶„ì„í•˜ì—¬ í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ ê°œì„ ì„ ìœ„í•œ AI/ML ê¸°ë°˜ ëª¨ë¸ ê°œë°œ ë° ìš´ì˜ ìê²© ìš”ê±´ ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ë˜ëŠ” ìœ ì‚¬ ì§ë¬´ ê²½ë ¥ 3ë…„ ì´ìƒ 9ë…„ ì´í•˜ PyTorch, TensorFlow ë“± ì£¼ìš” ML í”„ë ˆì„ì›Œí¬ ì‚¬ìš© ê²½í—˜ì´ ìˆëŠ” ë¶„ Python ë“± ë°ì´í„° ë¶„ì„ ë° ML ëª¨ë¸ë§ ê´€ë ¨ ì–¸ì–´ì— ëŠ¥ìˆ™í•˜ì‹  ë¶„ NoSQL / SQL ê¸°ë°˜ ë°ì´í„° ì²˜ë¦¬ ë° ë¶„ì„ ì—­ëŸ‰ ë³´ìœ í•œ ë¶„ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ì ìš©ì„ ìœ„í•œ ê¸°ë³¸ì ì¸ í†µê³„ ë° ì•Œê³ ë¦¬ì¦˜ì„ ì´í•´í•˜ëŠ” ë¶„ (ìš°ëŒ€) E-commerce ë˜ëŠ” ìœ ì‚¬ í”Œë«í¼ ë„ë©”ì¸ì—ì„œ ë°ì´í„° ë¶„ì„ ë° ML ëª¨ë¸ë§ ì‹¤ë¬´ ê²½í—˜ì´ ìˆëŠ” ë¶„ (ìš°ëŒ€) Spark, Hive ë“± ë¶„ì‚° ì»´í“¨íŒ… í™˜ê²½ì—ì„œì˜ ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ê²½í—˜ì´ ìˆëŠ” ë¶„ (ìš°ëŒ€) ë°ì´í„° ë¶„ì„ ê´€ë ¨ ë¶„ì•¼ ì„ì‚¬ í•™ìœ„ ì´ìƒì„ ì†Œì§€í•˜ì‹  ë¶„ ê³µí†µ ìš”ê±´ í•´ì™¸ì—¬í–‰ ê²°ê²© ì‚¬ìœ ê°€ ì—†ëŠ” ë¶„ ë‚¨ì„±ì˜ ê²½ìš°, ë³‘ì—­í•„ ë˜ëŠ” ë©´ì œì êµ­ê°€ë³´í›ˆ ëŒ€ìƒìì™€ ì¥ì• ì¸ì€ ê´€ë ¨ ë²•ì— ë”°ë¼ ìš°ëŒ€í•©ë‹ˆë‹¤ êµ­ê°€ìœ ê³µì ê°€ì‚°ì  ë¶€ì—¬ë°›ê¸° ìœ„í•´ \"ì·¨ì—… ì§€ì› ëŒ€ìƒì ì¦ëª…ì„œ\" íšŒì‚¬ì— ì œì¶œ í•„ìš” ì§€ì› ë°©ë²• ë° ì „í˜• ì ˆì°¨ ëª¨ì§‘ê¸°ê°„: 2025ë…„ 08ì›” 4ì¼ 23ì‹œ 59ë¶„ ê¹Œì§€ ì§€ë§ˆì¼“ ì±„ìš©í™ˆí˜ì´ì§€ (https://careers.gmarket.com) í†µí•´ ì§€ì› ì „í˜• ì ˆì°¨: ì„œë¥˜ì „í˜• > 1ì°¨ ë©´ì ‘(ì½”ë”©í…ŒìŠ¤íŠ¸ í¬í•¨) > 2ì°¨ ë©´ì ‘ > ìµœì¢… í•©ê²© ë©´ì ‘ì „í˜•ì˜ ì„¸ë¶€ì ˆì°¨ ë° ì§„í–‰ë°©ì‹ì€ ìƒí™©ì— ë”°ë¼ ë³€ë™ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì€ í•©ê²© ì‹œ ê°œë³„ ì•ˆë‚´ ì˜ˆì •ì…ë‹ˆë‹¤. ë³¸ ì±„ìš©ì€ ìˆ˜ì‹œ ëª¨ì§‘ìœ¼ë¡œ ì í•©í•œ ë¶„ì´ ìˆìœ¼ì‹¤ ê²½ìš° ì¡°ê¸° ë§ˆê°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìµœì¢… ì œì¶œ í›„ì—ëŠ” ì§€ì›ì„œ ìˆ˜ì •ì´ ë¶ˆê°€í•˜ì˜¤ë‹ˆ ì´ ì  ìœ ì˜í•˜ì—¬ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤. ê¸°íƒ€ì‚¬í•­ ê¸°ë³¸ì •ë³´(ì„±ëª…, ì „í™”ë²ˆí˜¸, ì´ë©”ì¼, ì´ë ¥ì„œ, ìœ ê´€ ê²½ë ¥ ê¸°ê°„)ëŠ” í•„ìˆ˜ ì œì¶œ í•„ìš”, ì´ì™¸ ì •ë³´ëŠ” ì„ íƒí•˜ì‹œì–´ ì‘ì„± ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì‹œìŠ¤í…œìƒ ë°˜ë³µë˜ëŠ” ì˜¤ë¥˜ê°€ ìˆìœ¼ì‹¤ ê²½ìš°, í¬ë¡¬ìœ¼ë¡œ ì ‘ì† ë¶€íƒë“œë¦½ë‹ˆë‹¤. ì§€ì›ì„œ ë‚´ìš©ì´ ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ê±°ë‚˜, í—ˆìœ„ ì„œë¥˜ë¥¼ ì œì¶œí•˜ì‹  ê²½ìš° ì±„ìš©ì´ ì·¨ì†Œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°íƒ€ ë¬¸ì˜ ì‚¬í•­ì€ FAQ ì°¸ì¡° ë˜ëŠ” ì§€ì› í›„ ì•ˆë‚´í•´ ë“œë¦¬ëŠ” ë©”ì¼, ì¹´ì¹´ì˜¤í†¡ ë‚´ 'ë¬¸ì˜í•˜ê¸°' ì´ìš© ë¶€íƒë“œë¦½ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-meta-learning",
    "title": "meta-learningì´ë€?",
    "date": "2025-08-04",
    "excerpt": "meta-learningì˜ ê°œë…ê³¼ í™œìš©ë˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "meta-learning",
      "machine-learning",
      "statistics"
    ],
    "content": "ì°¸ê³ ìë£Œ Bot Penguin - Meta Learning ë…¼ë¬¸ì„¸ë¯¸ë‚˜ 007 - Meta-Learningì˜ ê°œë… ë° ì—°êµ¬ ë¶„ì•¼ (ê¹€ë„í˜• ì—°êµ¬ì›) meta-learningì´ë€? ê°œë…ì  ì •ì˜ --- â€œë°°ìš°ëŠ” ë°©ë²•ì„ ë°°ìš°ëŠ” ê²ƒâ€ (learning to learn) > meta-learning (ë©”íƒ€ ëŸ¬ë‹)ì€ ê¸°ê³„í•™ìŠµì—ì„œ ì ì  ë” ì¤‘ìš”í•œ ê°œë…ì´ ë˜ê³  ìˆìœ¼ë©°, íŠ¹íˆ few-shot learning, online adaptation, ê°œì¸í™” ëª¨ë¸ë§ ë“±ì—ì„œ ë§¤ìš° ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤. > Meta-learning, also known as \"learning how to learn,\" is a cutting-edge approach in machine learning that focuses on algorithms that learn from their experiences and adapt to new data more effectively ê°œë…ë§Œìœ¼ë¡œëŠ” í¬ê²Œ ì™€ë‹¿ì§„ ì•Šë„¤. <figure> <img src=\"/post/DataScience/meta_learning_concept_1.webp\" alt=\"Meta-learning ê°œë…ë„\" /> <figcaption>ê·¸ë¦¼ 1. Meta-learningì˜ ê¸°ë³¸ ê°œë…</figcaption> </figure> > Meta-learning enhances traditional machine learning methods by adding a meta-layer to optimize learning algorithms. ê³¼ê±°ì˜ í•™ìŠµ ê²½í—˜ (ë°ì´í„°ë‚˜ íŒŒë¼ë¯¸í„°)ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìƒˆë¡œìš´ ë¬¸ì œì— ëŒ€í•´ì„œ ë¹ ë¥´ê²Œ í•™ìŠµí•´ë‚˜ê°€ëŠ” ë°©ë²• <figure> <img src=\"/post/DataScience/A-taxonomy-of-meta-learning-Meta-learning-strategies-are-divided-into-two-main.png\" alt=\"Meta-learning strategies\" /> <figcaption>ê·¸ë¦¼ 2. Meta-learning strategies</figcaption> </figure> ê¸°ë³¸ ì•„ì´ë””ì–´ : ì¼ë°˜ì ì¸ ë¨¸ì‹ ëŸ¬ë‹ì—ì„œëŠ” í•˜ë‚˜ì˜ ì‘ì—…(task)ì— ëŒ€í•´ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë©”íƒ€ ëŸ¬ë‹ì€ ì—¬ëŸ¬ ê°œì˜ ì‘ì—…ë“¤ì„ ë³´ê³ , ìƒˆë¡œìš´ ì‘ì—…ì— ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆëŠ” í•™ìŠµ ì „ëµì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤. ì¼ë°˜ ë¨¸ì‹ ëŸ¬ë‹: í•™ìŠµ: í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ì—ì„œ íŒŒë¼ë¯¸í„°ë¥¼ ìµœì í™” í…ŒìŠ¤íŠ¸: ê°™ì€ ë„ë©”ì¸ì˜ unseen ë°ì´í„°ì— ì ìš© ë©”íƒ€ ëŸ¬ë‹: í•™ìŠµ: ì—¬ëŸ¬ taskë¥¼ ë³´ë©´ì„œ, ëª¨ë¸ì´ \"ì–´ë–»ê²Œ ë¹ ë¥´ê²Œ ì ì‘í•  ìˆ˜ ìˆì„ì§€\"ë¥¼ í•™ìŠµ í…ŒìŠ¤íŠ¸: ìƒˆë¡œìš´ taskì— ì†Œìˆ˜ì˜ ìƒ˜í”Œ(few-shot)ë§Œìœ¼ë¡œ ë¹ ë¥´ê²Œ ì ì‘ Meta-Classifiers and Regressors > Meta-classifiers play a central role in ensemble machine learning techniques. They act as a higher-level classifier that takes the predictions of multiple base-level classifiers to generate a final prediction. ì—¬ëŸ¬ ê°’ì„ í•˜ë‚˜ë¡œ í•©ì¹˜ëŠ” ì—­í• ì„ í•˜ëŠ” ëª¨ë¸. stacking ë°©ë²•ì—ì„œ ë§¨ ë§ˆì§€ë§‰ì— ì“°ì´ëŠ” ëª¨ë¸, ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ ë°”íƒ•ìœ¼ë¡œ ì˜ˆì¸¡ê°’ì„ ìƒì„±í•˜ëŠ” ëª¨ë¸. random-forestë„ ì–´ë–»ê²Œ ë³´ë©´ meta-classifierêµ¬ë‚˜. decision-tree ì˜ˆì¸¡ê°’ì„ votingí•´ì„œ ê²°ê³¼ë‚´ëŠ” ë°©ì‹ì´ë‹ˆê¹Œ.",
    "public": true
  },
  {
    "id": "Data Science/Clean Code Principles/github-notebook-preview",
    "title": "GitHubì—ì„œ Jupyter Notebook ë¦¬ë·°í•˜ê¸° - ReviewNB",
    "date": "2025-08-04",
    "excerpt": "GitHubì—ì„œ Jupyter Notebook ì½”ë“œ ë¦¬ë·°ë¥¼ ì‰½ê²Œ í•  ìˆ˜ ìˆëŠ” ReviewNB ë„êµ¬ ì†Œê°œ",
    "category": "Data Science",
    "tags": [
      "github",
      "jupyter-notebook",
      "code-review",
      "tools"
    ],
    "content": "GitHubì—ì„œ Jupyter Notebook ë¦¬ë·°í•˜ê¸° ë¬¸ì œ ìƒí™© ì£¼í”¼í„° ë…¸íŠ¸ë¶ì„ GitHubì—ì„œ ê´€ë¦¬í•  ë•Œ ê²ªëŠ” ì–´ë ¤ì›€: ê¸°ì¡´ ë°©ì‹ì˜ í•œê³„ JSON diff: ë…¸íŠ¸ë¶ íŒŒì¼ì´ JSON í˜•íƒœë¡œ í‘œì‹œë˜ì–´ ê°€ë…ì„± ë–¨ì–´ì§ ë¦¬ë·° ì–´ë ¤ì›€: ì‹¤ì œ ì½”ë“œì™€ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê¸° ì–´ë ¤ì›€ ê²€ì¦ ë³µì¡: ë¡œì»¬ í™˜ê²½ì´ë‚˜ Colabì— í¬íŒ…í•´ì„œ validation í•„ìš” ì‹¤ì œ ë¬¸ì œì  ``json { \"cells\": [ { \"cell_type\": \"code\", \"execution_count\": 1, \"metadata\": {}, \"outputs\": [], \"source\": [ \"import pandas as pd\\n\", \"df = pd.read_csv('data.csv')\" ] } ] } ` ì´ëŸ° ì‹ìœ¼ë¡œ JSONì´ ë³´ì´ë©´ ì‹¤ì œ ì½”ë“œë¥¼ ì´í•´í•˜ê¸° ì–´ë ¤ì›€! í•´ê²°ì±…: ReviewNB What is ReviewNB? > \"Say Goodbye to messy JSON diffs!\" ReviewNBëŠ” GitHubì—ì„œ Jupyter Notebook ì½”ë“œ ë¦¬ë·°ë¥¼ ì‹¤ì œ ë…¸íŠ¸ë¶ì´ ë Œë”ë§ëœ ìƒíƒœì—ì„œ ì§ê´€ì ìœ¼ë¡œ í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì£¼ëŠ” ë„êµ¬ì…ë‹ˆë‹¤. ì£¼ìš” íŠ¹ì§• ì§ê´€ì ì¸ ë¦¬ë·° âœ… ì‹¤ì œ ë…¸íŠ¸ë¶ í˜•íƒœë¡œ ë Œë”ë§ âœ… ì½”ë“œì™€ ê²°ê³¼ë¥¼ í•¨ê»˜ í™•ì¸ âœ… ì…€ ë‹¨ìœ„ë¡œ ë¦¬ë·° ê°€ëŠ¥ ë¬´ë£Œ ì‚¬ìš© âœ… Public ì €ì¥ì†Œ: ë¬´ì œí•œ ë¬´ë£Œ ì‚¬ìš© âœ… Private ì €ì¥ì†Œ: ì œí•œì  ë¬´ë£Œ ì‚¬ìš© í¸ë¦¬í•œ ê¸°ëŠ¥ âœ… Side-by-side diff í™•ì¸ âœ… ê³¼ê±° PR ê²€í†  ê°€ëŠ¥ âœ… ì‹¤ì‹œê°„ ì½”ë©˜íŠ¸ ì‘ì„± ì„¤ì¹˜ ë° ì‚¬ìš©ë²• ì„¤ì¹˜ ê³¼ì • ReviewNB ì›¹ì‚¬ì´íŠ¸ ë°©ë¬¸: https://www.reviewnb.com/ \"Install GitHub App\" ë²„íŠ¼ í´ë¦­ GitHub ë§ˆì¼“í”Œë ˆì´ìŠ¤ì—ì„œ Free ë²„ì „ ì„¤ì¹˜ ì €ì¥ì†Œ ê¶Œí•œ ì„¤ì • ì‚¬ìš© ë°©ë²• PR ë¦¬ë·°í•˜ê¸° ` GitHub PR í˜ì´ì§€ ë°©ë¬¸ ReviewNB íƒ­ í™•ì¸ ë…¸íŠ¸ë¶ íŒŒì¼ ì„ íƒ ë Œë”ë§ëœ ìƒíƒœì—ì„œ ë¦¬ë·° ` ì½”ë©˜íŠ¸ ì‘ì„± ì…€ ë‹¨ìœ„ë¡œ ì½”ë©˜íŠ¸ ì¶”ê°€ ê°€ëŠ¥ ì½”ë“œ ë¼ì¸ë³„ ì½”ë©˜íŠ¸ ì‘ì„± ë§ˆí¬ë‹¤ìš´ ì§€ì› ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ Before (ê¸°ì¡´ ë°©ì‹) ` JSON diffë§Œ ë³´ì„ ì‹¤ì œ ì½”ë“œ ë‚´ìš© íŒŒì•… ì–´ë ¤ì›€ ê²°ê³¼ í™•ì¸ ë¶ˆê°€ëŠ¥ ` After (ReviewNB ì‚¬ìš©) ` ì‹¤ì œ ë…¸íŠ¸ë¶ í˜•íƒœë¡œ í‘œì‹œ ì½”ë“œì™€ ê²°ê³¼ë¥¼ í•¨ê»˜ í™•ì¸ ì§ê´€ì ì¸ ë¦¬ë·° ê°€ëŠ¥ `` ì¥ì ê³¼ í™œìš© ì‚¬ë¡€ ì¥ì  ê°€ë…ì„± í–¥ìƒ: JSON ëŒ€ì‹  ì‹¤ì œ ë…¸íŠ¸ë¶ í˜•íƒœ íš¨ìœ¨ì„± ì¦ëŒ€: ë³„ë„ í™˜ê²½ êµ¬ì¶• ë¶ˆí•„ìš” í˜‘ì—… ê°œì„ : íŒ€ ë¦¬ë·° í”„ë¡œì„¸ìŠ¤ ê°„ì†Œí™” í’ˆì§ˆ í–¥ìƒ: ë” ì •í™•í•œ ì½”ë“œ ê²€í†  í™œìš© ì‚¬ë¡€ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ í”„ë¡œì íŠ¸ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì½”ë“œ ë¦¬ë·° ë°ì´í„° ì „ì²˜ë¦¬ ê³¼ì • ê²€í†  ì‹œê°í™” ê²°ê³¼ í™•ì¸ êµìœ¡ ë° íŠœí† ë¦¬ì–¼ ë…¸íŠ¸ë¶ íŠœí† ë¦¬ì–¼ ë¦¬ë·° êµìœ¡ ìë£Œ í’ˆì§ˆ ê´€ë¦¬ í•™ìŠµ ìë£Œ ê²€ì¦ ì—°êµ¬ í”„ë¡œì íŠ¸ ì—°êµ¬ ë…¸íŠ¸ë¶ ê³µìœ  ì‹¤í—˜ ê²°ê³¼ ê²€í†  ì¬í˜„ì„± í™•ì¸ ëŒ€ì•ˆ ë„êµ¬ë“¤ nbviewer GitHub ë…¸íŠ¸ë¶ ë Œë”ë§ ë‹¨ìˆœ ë·°ì–´ ê¸°ëŠ¥ Binder ì¸í„°ë™í‹°ë¸Œ ë…¸íŠ¸ë¶ ì‹¤í–‰ í™˜ê²½ ì„¤ì • ë³µì¡ Google Colab í´ë¼ìš°ë“œ ê¸°ë°˜ ì‹¤í–‰ GitHub ì—°ë™ ì œí•œ ê²°ë¡  ReviewNBëŠ” GitHubì—ì„œ Jupyter Notebookì„ ê´€ë¦¬í•˜ëŠ” íŒ€ì—ê²Œ í•„ìˆ˜ì ì¸ ë„êµ¬ì…ë‹ˆë‹¤. í•µì‹¬ ê°€ì¹˜ ê°„í¸í•¨: ì„¤ì¹˜ í›„ ì¦‰ì‹œ ì‚¬ìš© íš¨ìœ¨ì„±: JSON diff ë¬¸ì œ í•´ê²° ë¬´ë£Œ: Public ì €ì¥ì†Œ ë¬´ì œí•œ ì‚¬ìš© ì§ê´€ì„±: ì‹¤ì œ ë…¸íŠ¸ë¶ í˜•íƒœë¡œ ë¦¬ë·° ì¶”ì²œ ëŒ€ìƒ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ íŒ€ ë¨¸ì‹ ëŸ¬ë‹ í”„ë¡œì íŠ¸ ê´€ë¦¬ì êµìœ¡ ê¸°ê´€ ì—°êµ¬ í”„ë¡œì íŠ¸ íŒ€ ë§í¬: https://www.reviewnb.com/ --- ì´ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ë©´ GitHubì—ì„œ Jupyter Notebook ë¦¬ë·°ê°€ í›¨ì”¬ íš¨ìœ¨ì ì´ê³  ì§ê´€ì ì´ ë©ë‹ˆë‹¤!",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/lenze-size-recommendation-problem-definition",
    "title": "ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì¶”ì²œ ë¬¸ì œì—ì„œ ë¬¸ì œ ì •ì˜ì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2025-08-04",
    "excerpt": "ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì¶”ì²œì—ì„œ ì¸ê³¼ì¶”ë¡ ì„ ë„ì…í•  ë•Œ ë¬¸ì œ ì •ì˜ë¥¼ ì–´ë–»ê²Œ í•  ê²ƒì¸ê°€",
    "category": "Causal Inference",
    "tags": [
      "problem definition"
    ],
    "content": "ë Œì¦ˆ ì‚½ì…ìˆ ì—ì„œ YëŠ” vaultë¡œ ì •ì˜ê°€ ë¨ (ì§€ê¸ˆ í’€ë ¤ê³  í•˜ëŠ” ë¬¸ì œì—ì„œëŠ”). vaultëŠ” ë Œì¦ˆ ì‚½ì…ìˆ  í›„ì— ì¸¡ì •ë˜ëŠ” ê°’ìœ¼ë¡œ, ë Œì¦ˆ í›„ë©´ê³¼ ìˆ˜ì •ì²´ ì „ë©´ê¹Œì§€ì˜ ê±°ë¦¬ë¥¼ ì˜ë¯¸í•¨. ì—¬ê¸°ì„œ ê³ ë¯¼ì‚¬í•­ì€ $T=0$ì¸ ê²ƒì„ ì–´ë–»ê²Œ ì •ì˜í•´ì•¼í•˜ëŠ”ì§€. $T=0$ ì¸ ìƒí™©ì€ ë¬´ì—‡ì¼ê¹Œ? --- ìˆ˜ìˆ  ë°›ê¸° ì „ ìƒíƒœ ìˆ˜ìˆ (ì²˜ì¹˜) ë°›ê¸° ì „ ìƒíƒœë¼ê³  ì •ì˜í•˜ê²Œ ë˜ë©´ vaultê°’ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ì™œëƒí•˜ë©´ vaultì˜ ì •ì˜ ìì²´ê°€ ë Œì¦ˆ í›„ë©´ê³¼ ìˆ˜ì •ì²´ ì „ë©´ê¹Œì§€ì˜ ê±°ë¦¬ë¼ì„œ. ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê°’ì´ì§€ë§Œ ê³„ì‚°ì„ ìœ„í•´ 0ìœ¼ë¡œ ì •ì˜í•´ë†”ë„ ê´œì°®ì„ê¹Œ? $\\text{vault} = Y(T=0, X=x) = 0$ ê°€ì¥ ì‘ì€ í¬ê¸°ì˜ ë Œì¦ˆë¥¼ ë°›ì€ ìƒíƒœ í˜„ì¬ ì‚½ì…í•˜ëŠ” ë Œì¦ˆ í¬ê¸°ì—ëŠ” [12.1, 12.6, 13.2]ê°€ ìˆë‹¤. ì´ë•Œ ê°€ì¥ ì‘ì€ ë Œì¦ˆë¥¼ ì‚½ì…í•œ ìƒíƒœë¥¼ $T=0$ ì²˜ì¹˜ë¥¼ ë°›ì€ ìƒíƒœë¡œ ì •ì˜í•´ë„ ë ê¹Œ? $Y(T=0, X=x) = Y(T=12.1, X=x)$ ì¸ê³¼ì¶”ë¡ ì—ì„œ T=0 ì˜ ì •ì˜ê°€ ë­ì§€? > ì²˜ì¹˜(treatment)ê°€ ì ìš©ë˜ê¸° ì „ì˜ ì‹œì , ì¦‰ control ìƒíƒœ. T=0: ì²˜ì¹˜ê°€ ì‹œì‘ë˜ê¸° ì§ì „ì˜ ì‹œì  (baseline), ëŒ€ì¡°êµ°ì— í•´ë‹¹í•˜ëŠ” ê°’ T=1: ì²˜ì¹˜ê°€ ì ìš©ëœ í›„ì˜ ì‹œì  (post-treatment) ì˜ˆë¥¼ ë“¤ì–´: ì˜ì•½í’ˆ íš¨ê³¼ ì—°êµ¬: ì•½ì„ ë¨¹ê¸° ì „(T=0)ê³¼ ì•½ì„ ë¨¹ì€ í›„(T=1)ì˜ ê±´ê°• ìƒíƒœ ë¹„êµ êµìœ¡ í”„ë¡œê·¸ë¨ í‰ê°€: í”„ë¡œê·¸ë¨ ì°¸ì—¬ ì „(T=0)ê³¼ ì°¸ì—¬ í›„(T=1)ì˜ ì„±ì  ë¹„êµ ë§ˆì¼€íŒ… ìº í˜ì¸: ìº í˜ì¸ ì‹œì‘ ì „(T=0)ê³¼ ì‹œì‘ í›„(T=1)ì˜ ë§¤ì¶œ ë¹„êµ ëŒ€ì¡°êµ°(control group)ê³¼ ì²˜ì¹˜êµ°(treatment group)ì˜ ê°œë…ì²˜ëŸ¼ ì–´ë–¤ ì²˜ì¹˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ë ¤ê³  í•  ë•Œ ë¹„êµëŒ€ìƒì´ ë˜ëŠ” ê°’. ì´ ê°œë…ì— ë”°ë¥´ë©´ 2ë²ˆ ì •ì˜ëŠ” ë§ì´ ì•ˆëœë‹¤. í•˜ì§€ë§Œ ì´ë ‡ê²Œ ì •ì˜í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œ? > ì²˜ì¹˜íš¨ê³¼ëŠ” ê°€ì¥ ì‘ì€ ë Œì¦ˆ ëŒ€ë¹„ ë Œì¦ˆì˜ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ í˜¹ì€ ì˜í–¥ë„ ì²˜ì¹˜ë¥¼ ë°›ê¸° ì „ì— Yê°’ì´ ì—†ëŠ” ê²½ìš°ëŠ” ì–´ë–»ê²Œ í•˜ì§€? (ì •ì˜ê°€ ì•ˆë  ë–„) AIë‘ ideation í•´ë³´ë©´ì„œ ë‚˜ì˜¨ ê²ƒë“¤ ì •ë¦¬. ëŒ€ì•ˆì  ê²°ê³¼ ë³€ìˆ˜ ì •ì˜: vault ëŒ€ì‹  ë‹¤ë¥¸ ê²°ê³¼ ë³€ìˆ˜ ì‚¬ìš©: ìˆ˜ìˆ  ì „í›„ ëª¨ë‘ ì¸¡ì • ê°€ëŠ¥í•œ ë³€ìˆ˜ ì˜ˆ: ì‹œë ¥ ê°œì„ ë„, ì•ˆì•• ë³€í™”, í™˜ì ë§Œì¡±ë„ ë“± vaultì˜ ë³€í™”ìœ¨: ìˆ˜ìˆ  ì „í›„ vault ë³€í™”ëŸ‰ì„ ê²°ê³¼ ë³€ìˆ˜ë¡œ ì‚¬ìš© T=12.1mm ë Œì¦ˆë¥¼ ê¸°ì¤€(control)ìœ¼ë¡œ ì •ì˜ â€” ìƒëŒ€ ë¹„êµ: ì™„ì „ ë¬´ì²˜ì¹˜ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŒ -> ìµœì†Œ í¬ê¸° ë Œì¦ˆë¥¼ ê¸°ì¤€ì ìœ¼ë¡œ ì„¤ì •: $T=12.1$ì„ ê¸°ì¤€ìœ¼ë¡œ $CATE(T=t, X=x) = E[Y T=t, X=x] - E[Y T=12.1, X=x]$ ì´ëŠ” \"ìµœì†Œ í¬ê¸° ëŒ€ë¹„ ë‹¤ë¥¸ í¬ê¸°ì˜ íš¨ê³¼\"ë¥¼ ì¸¡ì • ë Œì¦ˆ í¬ê¸° ìì²´ê°€ treatment levelì¸ multi-valued treatment setting $T \\in {12.1, 12.6, 13.2}$ ê¸°ì¤€ì : $T_0 = 12.1$ ì´ì „ ë Œì¦ˆ ëŒ€ë¹„ ê°œì„ ë„: í™˜ìê°€ ì´ì „ì— ì‚¬ìš©í•˜ë˜ ë Œì¦ˆ ëŒ€ë¹„ ê°œì„  íš¨ê³¼ ì¥ì : ì¸ê³¼ì¶”ë¡ ì˜ í‹€ì—ì„œ ì •ì˜ ê°€ëŠ¥í•œ treatment comparison ë‹¨ì : ì ˆëŒ€ê°’ì´ ì•„ë‹Œ ìƒëŒ€ê°’ë§Œ ì¶”ì • ê°€ëŠ¥í•˜ë©°, baseline vaultê°€ \"ì¶©ë¶„íˆ ì‘ì€ê°€?\"ì— ëŒ€í•œ í•´ì„ì  ê³ ë¯¼ í•„ìš” Vaultë¥¼ 0ìœ¼ë¡œ ì •ì˜ â€” \"ì ˆëŒ€ íš¨ê³¼ë¡œ ë°”ê¾¸ê¸°\": vault = 0 ê°€ì •: ìˆ˜ìˆ  ì „ vaultë¥¼ 0ìœ¼ë¡œ ê°€ì • $CATE(T=t, X=x) = E[Y T=t, X=x] - 0 = E[Y T=t, X=x]$ ì´ëŠ” \"ì ˆëŒ€ì  vault íš¨ê³¼\"ë¥¼ ì¸¡ì • CATEê°€ ì•„ë‹Œ ATEì™€ ìœ ì‚¬í•œ êµ¬ì¡°ë¡œ í•´ì„ë¨ íŠ¹ì • ë Œì¦ˆë¥¼ ë„£ì—ˆì„ ë•Œ vaultê°’ì´ ì–¼ë§ˆë‚˜ ë‚˜ì˜¤ëŠ”ì§€ë¥¼ ì ˆëŒ€ì  ê´€ì ì—ì„œ í•´ì„ ğŸ§  ê²°ë¡  ë° ì œì•ˆ ì„ìƒ í˜„ì‹¤ + ì¸ê³¼ì¶”ë¡  ì´ë¡ ì„ ë™ì‹œì— ê³ ë ¤í•  ë•Œ, ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ì •ì˜ëŠ”: $T=12.1$mm ë Œì¦ˆë¥¼ ê¸°ì¤€ìœ¼ë¡œ ìƒëŒ€ì  íš¨ê³¼(CATE)ë¥¼ ì •ì˜í•˜ê³ , ì ˆëŒ€íš¨ê³¼(=vault ê°’ ìì²´)ëŠ” ìµœì í™” ëª©ì ì˜ íƒ€ê²Ÿìœ¼ë¡œ ì‚¬ìš© ì¸ê³¼ì¶”ë¡ : $CATE(T=t, X=x) = E[Y T=t, X=x] - E[Y T=12.1, X=x]$ ì¶”ì²œ ëª©ì : $\\arg\\max_t E[Y T=t, X=x]$ ì¸¡ì •í•˜ì§€ ì•Šì€ ê°’ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì€ ì–´ë–»ê²Œ í‰ê°€í•  ê²ƒì¸ê°€ --- <figure> <img src=\"/post/Causal_Inference/ë Œì¦ˆì¶”ì²œ_í‰ê°€ê³ ë¯¼.jpeg\" alt=\"how-to-evalutate-counterfactual\" /> <figcaption>ê·¸ë¦¼1. ì¸¡ì •í•˜ì§€ ì•Šì€ ê°’ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì€ ì–´ë–»ê²Œ í‰ê°€í•  ê²ƒì¸ê°€</figcaption> </figure> ë‚´ê°€ ìƒê°í•˜ëŠ” ë¬¸ì œëŠ” ê²°êµ­ ê·¸ë¦¼1ê³¼ ê°™ì€ ìƒí™©ì´ë‹¤. 3ê°€ì§€ì˜ ë Œì¦ˆ í¬ê¸° ì¤‘ ê³ ê°ì€ í•˜ë‚˜ì— ëŒ€í•´ì„œ ìˆ˜ìˆ ì„ ë°›ê²Œ ë˜ëŠ”ë°, ê·¸ëŸ¬ë©´ ë‚˜ë¨¸ì§€ 2ê°œì— ëŒ€í•œ ê²°ê³¼ê°’ì´ ì—†ëŠ” ìƒí™©ì—ì„œ, ì˜ˆì¸¡ëª¨ë¸ì˜ ì„±ëŠ¥ì€ ì–´ë–»ê²Œ íŒë‹¨í•  ìˆ˜ ìˆì„ê¹Œ? ì—¬ê¸°ì„œ ì‚¬ìš©ë˜ëŠ” ê°œë…ì´ ì–‘ìˆ˜ì„± (positivity). ì´ëŠ” $ 0 < P(T=t X=x) < 1$ ì„ ë§Œì¡±í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•¨. ì¦‰ ì–´ë–¤ ê³ ê°ì´ ê° ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì´ 0ë³´ë‹¤ í¬ë‹¤ëŠ” ê²ƒ. ë”°ë¼ì„œ AëŠ” 12.1ì„ ë°›ì•˜ì§€ë§Œ, Aë‘ ë¹„ìŠ·í•œ ë‹¤ë¥¸ ê³ ê°ì€ 12.6ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì„ í™•ì¸í•´ë³¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒ. ì²˜ì¹˜ ê°„ì˜ ë¶„í¬ê°€ ê²¹ì³ìˆìœ¼ë©´ ë°˜ì‚¬ì‹¤ì— ëŒ€í•´ì„œë„ ì–´ëŠì •ë„ ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤. > ì´ëŠ” \"ë¹„ìŠ·í•œ ê³ ê°ë“¤ì´ ë‹¤ì–‘í•œ ì²˜ì¹˜ë¥¼ ë°›ì•˜ëŠ”ê°€?\"ë¼ëŠ” ì§ˆë¬¸ê³¼ ì§ê²°ë¨. $P(T=t X=x)$ë¡œ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ ì •ì˜í•˜ëŠ” ë²• ì§ê´€ì ìœ¼ë¡œ ìƒê°í•´ë³´ë©´ P(T=12.1 X)ê°€ ë‚®ë‹¤ë©´ í•´ë‹¹ ê³ ê° ê°™ì€ ê²½ìš°ì—ëŠ” 12.1ì„ ë°›ì„ í™•ë¥ ì´ ë‚®ìœ¼ë‹ˆê¹Œ, 12.1ì— ëŒ€í•œ ì˜ˆì¸¡ê°’ì˜ ì‹ ë¢°ë„ê°€ ë‚®ì„ ê²ƒì´ë‹¤. ì´ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆì„ê¹Œ?",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/how-to-define-confidence-score",
    "title": "ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì¶”ì²œ ë¬¸ì œì—ì„œ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì¸ê°€",
    "date": "2025-08-04",
    "excerpt": "ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì¶”ì²œì—ì„œ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì¸ê°€",
    "category": "Causal Inference",
    "tags": [
      "confidence score"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì†Œë¬¸ë°ê°€ì‹­/08-03-ëª»ë¯¿ê² ì§€ë§Œíƒì¿ ë¥´ì¦ˆë´¤ì–´",
    "title": "ëª» ë¯¿ê² ì§€ë§Œ íƒí¬ë£¨ì¦ˆ ë´¤ì–´",
    "date": "2025-08-03",
    "excerpt": "ë¯¿ì§€ ëª»í• ë§Œí•œ ì†Œë¬¸ì´ë‚˜ ì†Œì‹ì„ ì „í•  ë•Œ ì“°ëŠ” í‘œí˜„ë“¤",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Believe it or not, I bumped into a Tom Cruise. Believe it or not : ë„ˆê°€ ë¯¿ê¸° í˜ë“¤ê² ì§€ë§Œ, ë¯¿ë“  ë§ë“ . Believe it or not, Joe and Mary are getting married. Believe it or not, Joe and Mary are engaged. engaged : ì•½í˜¼í•˜ë‹¤ divorce : ì´í˜¼í•˜ë‹¤ This may be a surprise : ë†€ë„ë§Œí•œ ì¼ì¼ìˆ˜ë„ ìˆëŠ”ë°, ì˜ˆìƒì¹˜ ëª»í–ˆê² ì§€ë§Œ This may be a surprise, but Joe is getting divorced. This may be a surprise, but Lenny passed bar exam. bar exam : ë³€í˜¸ì‚¬ ì‹œí—˜ You may not know this : ëª¨ë¥¼ìˆ˜ë„ ìˆëŠ”ë° You may not know this, but Drake is Canadian. ê°„ë‹¨í•œ ì‘ë¬¸ Believe it or not, I will have love metting(?ì†Œê°œíŒ…, ì˜ì–´ë¡œ ë­ì§€). Someone who is joined in same project suggest it to me. I like her style, maybe she has lovely smile I guess in the picture. You may not know this, my ì´ìƒí˜• is someone who has lovely smile and isn't short (more than 165cm). í”¼ë“œë°±ì´ë‘ ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë¬¸ Believe it or not, I'm going on a blind date this weekend! Someone from the same project suggested it to me. I really like her style, and from her picture, I can see she has a lovely smile. You might not know this about me, but my ideal type is someone with a beautiful smile and who isn't too short (preferably over 165cm). I'm actually quite nervous about it. I haven't been on a blind date in ages, and I'm not sure what to expect. But I'm trying to stay positive and just be myself. The person who set us up said we have similar interests and personalities, so hopefully we'll hit it off. I'm planning to meet her at a nice cafÃ© downtown - nothing too fancy, just somewhere comfortable where we can talk and get to know each other. Wish me luck! I'll let you know how it goes. í”¼ë“œë°± ì›ë³¸ ì‘ë¬¸ì˜ ë¬¸ì œì ë“¤: ë¬¸ë²• ì˜¤ë¥˜ë“¤: \"I will have love metting\" â†’ \"I'm going on a blind date\" \"Someone who is joined\" â†’ \"Someone who joined\" \"suggest it to me\" â†’ \"suggested it to me\" ì–´íœ˜ ì„ íƒ: \"love meeting\" â†’ \"blind date\" (ì†Œê°œíŒ…ì˜ ì •í™•í•œ í‘œí˜„) \"ì´ìƒí˜•\" â†’ \"ideal type\" ë˜ëŠ” \"type\" ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„: \"I guess in the picture\" â†’ \"from her picture, I can see\" \"isn't short\" â†’ \"isn't too short\" ê°œì„ ëœ ì ë“¤: ë” ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ êµ¬ì¡° ê°ì •ê³¼ ìƒí™© ì„¤ëª… ì¶”ê°€ (ê¸´ì¥ê°, ê³„íš ë“±) ëŒ€í™”ì²´ í†¤ ìœ ì§€ (ì¹œê·¼í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ëŠë‚Œ) êµ¬ì²´ì ì¸ ì„¸ë¶€ì‚¬í•­ (ì¹´í˜ì—ì„œ ë§Œë‚˜ê¸°, ë¹„ìŠ·í•œ ê´€ì‹¬ì‚¬ ë“±) í•™ìŠµ í¬ì¸íŠ¸: ì†Œê°œíŒ… = blind date \"hit it off\" = ì˜ ë§ë‹¤, ì‚¬ì´ê°€ ì¢‹ë‹¤ \"in ages\" = ì˜¤ë«ë™ì•ˆ \"set us up\" = ì†Œê°œí•´ì£¼ë‹¤",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/í•©ê²©ìê°€ë˜ëŠ”ëª¨ì˜í…ŒìŠ¤íŠ¸",
    "title": "í•©ê²©ìê°€ ë˜ëŠ” ëª¨ì˜ í…ŒìŠ¤íŠ¸ - ê·¸ë˜í”„",
    "date": "2025-08-03",
    "excerpt": "ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ê·¸ë˜í”„",
      "ì•Œê³ ë¦¬ì¦˜"
    ],
    "content": "--- title: \"í•©ê²©ìê°€ ë˜ëŠ” ëª¨ì˜ í…ŒìŠ¤íŠ¸ - ê·¸ë˜í”„\" date: \"2025-08-03\" category: \"Codility Study\" tags: [\"ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°\", \"Codility Study\", \"ê·¸ë˜í”„\", \"ì•Œê³ ë¦¬ì¦˜\"] excerpt: \"ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„\" --- # ë¬¸ì œ 42: ê²Œì„ ë§µ ìµœë‹¨ ê±°ë¦¬ https://school.programmers.co.kr/learn/courses/30/lessons/1844 5x5 í¬ê¸°ì˜ ê²Œì„ë§µì—ì„œ (1, 1) -> (5, 5)ê¹Œì§€ ê°€ëŠ” ìµœë‹¨ê±°ë¦¬ë¥¼ êµ¬í•˜ëŠ” ë¬¸ì œ ì œì•½ì¡°ê±´ mapsëŠ” n x m í¬ê¸°ì˜ ê²Œì„ ë§µì˜ ìƒíƒœê°€ ë“¤ì–´ ìˆëŠ” 2ì°¨ì› ë°°ì—´. 1 <= n, m <= 100 nê³¼ mì€ ê°™ê±°ë‚˜ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ mapsëŠ” 0ê³¼ 1ë¡œ êµ¬ì„±ë¨ 0 : ë²½ 1 : ë²½ì´ ì—†ëŠ” ìë¦¬ def solution(maps: list) -> int: # ê´€ë ¨ ê°’ë“¤ ì •ì˜ start_x, start_y = 0, 0 end_x, end_y = len(maps[0]) - 1, len(maps[1]) -1 distance_list = [] def bfs(x, y, distance): print(x, y, distance) if x == end_x and y == end_y: distance_list.append(distance) return if 0 < x + 1 < end_x and 0 < y < end_y and maps[x + 1][y] == 1: maps[x + 1][y] = 0 bfs(x + 1, y, distance + 1) elif 0 < x - 1 < end_x and 0 < y < end_y and maps[x -1][y] == 1: maps[x - 1][y] = 0 bfs(x - 1, y, distance + 1) elif 0 < x < end_x and 0 < y - 1 < end_y and maps[x][y - 1] == 1: maps[x][y - 1] = 0 bfs(x, y - 1, distance + 1) elif 0 < x < end_x and 0 < y + 1 < end_y and maps[x][y + 1] == 1: maps[x][y + 1] = 0 bfs(x, y + 1, distance + 1) bfs(start_x, start_y, 0) return distance_list solution_1 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,1],[0,0,0,0,1]]) print(solution_1) assert solution_1 == 11 solution_2 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,0],[0,0,0,0,1]]) print(solution_2) assert solution_2 == -1 from collections import deque def solution(maps: list) -> int: # 1. ì´ë™í•  ìˆ˜ ìˆëŠ” ë°©í–¥ ì •ì˜ move = [[-1, 0], [0, -1], [0, 1], [1, 0]] # 2. ë§µì˜ í¬ê¸° ì €ì¥ n = len(maps) m = len(maps[0]) # 3. ê±°ë¦¬ë¥¼ ì €ì¥í•˜ëŠ” ë°°ì—´ ì´ˆê¸°í™” dist = [[-1] * m for _ in range(n)] # 4. bfs def bfs(start): q = deque([start]) dist[start[0]][start[1]] = 1 while q: here = q.popleft() for direct in move: row, column = here[0] + direct[0], here[1] + direct[1] if row < 0 or row >= n or column < 0 or column >= m: continue if maps[row][column] == 0: continue if dist[row][column] == -1: q.append([row, column]) dist[row][column] = dist[here[0]][here[1]] + 1 return dist bfs([0, 0]) return dist[n - 1][m - 1] solution_1 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,1],[0,0,0,0,1]]) print(solution_1) assert solution_1 == 11 solution_2 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,0],[0,0,0,0,1]]) print(solution_2) assert solution_2 == -1",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/ëª¸í’€ê¸°ë¬¸ì œ",
    "title": "ë­„í’€ê¸° ë¬¸ì œ - ê·¸ë˜í”„",
    "date": "2025-08-03",
    "excerpt": "ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ê·¸ë˜í”„",
      "ì•Œê³ ë¦¬ì¦˜"
    ],
    "content": "--- title: \"ë­„í’€ê¸° ë¬¸ì œ - ê·¸ë˜í”„\" date: \"2025-08-03\" category: \"Codility Study\" tags: [\"ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°\", \"Codility Study\", \"ê·¸ë˜í”„\", \"ì•Œê³ ë¦¬ì¦˜\"] excerpt: \"ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„\" --- # ë¬¸ì œ 38 : ê¹Šì´ ìš°ì„  íƒìƒ‰ ìˆœíšŒ ì…ì¶œë ¥ì˜ ì˜ˆ graph : ì¸ì ‘ë¦¬ìŠ¤íŠ¸ / start / return : ë¦¬ìŠ¤íŠ¸ def solution(graph: list[list[str]], start: str) -> list[str]: def dfs(node): # ë°©ë¬¸í–ˆë˜ ê±°ë©´ pass if visited[node]: return # ë°©ë¬¸ì²˜ë¦¬ visited[node] = True visited_list.append(node) if node in graph_list: for adj_nodes in graph_list[node]: dfs(adj_nodes) graph_list = {} node_set = set() for start_node, end_node in graph: if start_node in graph_list: graph_list[start_node].append(end_node) else: graph_list[start_node] = [end_node] node_set.add(start_node) node_set.add(end_node) visited = {key: False for key in node_set} # ë…¸ë“œê°€ ìˆ«ìì¸ ê²½ìš°ì—ëŠ” [False] len(node) ì´ëŸ°ì‹ìœ¼ë¡œ ê°€ëŠ¥ visited_list = [] dfs(start) return visited_list # ë‹µì•ˆì§€ í’€ì´ from collections import defaultdict adj_list = defaultdict(list) visited = set() result = [] def dfs(node): visited.add(node) result.append(node) for adj_node in adj_list.get(node, []): if adj_node not in visited: dfs(adj_node) def solution(graph, start): for u, v in graph: adj_list[u].append(v) dfs(start) return result solution_1 = solution(graph=[['A', 'B'], ['B', 'C'], ['C', 'D'], ['D', 'E']], start='A') print(solution_1) assert solution_1 == [\"A\", \"B\", \"C\", \"D\", \"E\"] solution_2 = solution(graph=[['A', 'B'], ['A', 'C'], ['B', 'D'], ['B', 'E'], ['C', 'F'], ['E', 'F']], start='A') print(solution_2) assert solution_2 == [\"A\", \"B\", \"D\", \"E\", \"F\", \"C\"] ì‹œê°„ë³µì¡ë„ ê³„ì‚°í•˜ê¸° : ë…¸ë“œ ê°œìˆ˜ N, ê°„ì„  ê°œìˆ˜ Eë¼ê³  í•˜ë©´, ì¸ì ‘ë¦¬ìŠ¤íŠ¸ ìƒì„± : O(E) íƒìƒ‰ì‹œ ëª¨ë“  ë…¸ë“œ ë°©ë¬¸ : O(N) ë”°ë¼ì„œ O(N+E) # ë¬¸ì œ 39: ë„ˆë¹„ ìš°ì„  íƒìƒ‰ ìˆœíšŒ from queue import deque from collections import defaultdict def solution(graph: list, start:int) -> list: adj_list = defaultdict(list) visited = set() for start_node, end_node in graph: adj_list[start_node].append(end_node) visited_list = [] # í—·ê°ˆë¦´ ìˆ˜ ìˆëŠ” í¬ì¸íŠ¸ê°€, bfsëŠ” ë°”ë¡œ ë°©ë¬¸í• ê±°ë‹ˆê¹Œ ë„£ìœ¼ë©´ì„œ ë°©ë¬¸í–ˆë‹¤ëŠ” í‘œì‹œ í•´ì£¼ê¸° queue = deque() queue.append(start) visited.add(start) visited_list.append(start) while queue: next_node = queue.popleft() for adj_node in adj_list[next_node]: if adj_node not in visited: queue.append(adj_node) visited.add(adj_node) visited_list.append(adj_node) return visited_list solution_1 = solution(graph=[(1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (3, 7), (4, 8), (5, 8), (6, 9), (7, 9)], start=1) print(solution_1) assert solution_1 == [1, 2, 3, 4, 5, 6, 7, 8, 9] solution_2 = solution(graph=[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)], start=1) print(solution_2) assert solution_2 == [1, 2, 3, 4, 5, 0] ì‹œê°„ë³µì¡ë„ : adj_list ìƒì„±í•  ë•Œ : O(E) íƒìƒ‰í•  ë•Œ : O(N) # ë¬¸ì œ 40: ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜ ì‹œì‘ë…¸ë“œì—ì„œ ê° ë…¸ë“œê¹Œì§€ ìµœë‹¨ê±°ë¦¬ êµ¬í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ from collections import defaultdict def solution(start:int, numNodes:int, edges:list) -> list: adj_list = defaultdict(list) check_nodes = set() for start_node, end_node, weight in edges: adj_list[start_node].append((end_node, weight)) # 1. ìµœë‹¨ê±°ë¦¬ ê²°ê³¼ë¥¼ ë‹´ì„ ìë£Œêµ¬ì¡° INF = 1e20 result = [INF] len(adj_list) result[start] = 0 check_nodes.add(0) for end_node, weight in adj_list[start]: if result[end_node] > weight: result[end_node] = weight # 2. ê°€ì¥ ì‘ì€ ë…¸ë“œë¶€í„° greedyí•˜ê²Œ ê°±ì‹ í•˜ê¸° # ê°€ì¥ ì‘ì€ ê±°ë¦¬ì¸ ë…¸ë“œë¥¼ ì°¾ëŠ”ê²ƒë„ ì§œì¤˜ì•¼í•˜ëŠ”ê±´ê°€? min_node = None for i, tmp in arange(result): if i not in check_nodes # ìš°ì„  ìˆœìœ„ í (heap) ìë£Œêµ¬ì¡°ë¡œ ìµœë‹¨ ê±°ë¦¬ë¥¼ ê´€ë¦¬ import heapq from collections import defaultdict, deque from turtle import distance INF = 1e10 def solution(start:int, numNodes:int, edges:list) -> list: # 1. ê·¸ë˜í”„ ì´ˆê¸°í™” graph = defaultdict(list) for start_node, end_node, weight in edges: graph[start_node].append((end_node, weight)) # 2. ìµœë‹¨ê²½ë¡œ ê¸¸ì´ ë° ë°©ë¬¸ ê¸°ë¡ ì´ˆê¸°í™” distances = [INF] numNodes visited = [False] numNodes distances[start] = 0 # 3. ìš°ì„ ìˆœìœ„ í priority_queue = [(0, start)] # (ê±°ë¦¬, ë…¸ë“œ) while priority_queue: # 4. í˜„ì¬ ë…¸ë“œ ì°¾ê¸° current_distance, current_node = heapq.heappop(priority_queue) # 5. ì´ë¯¸ ë°©ë¬¸í•œ ë…¸ë“œëŠ” ë¬´ì‹œ if visited[current_node]: continue # 6. í˜„ì¬ ë…¸ë“œ ë°©ë¬¸ ì²˜ë¦¬ visited[current_node] = True # 7. ì¸ì ‘ ë…¸ë“œì— ëŒ€í•œ ê±°ë¦¬ ì—…ë°ì´íŠ¸ for adj_node, weight in graph[current_node]: new_distance = distances[current_node] + weight if new_distance < distances[adj_node]: distances[adj_node] = new_distance heapq.heappush(priority_queue, (new_distance, adj_node)) return distances solution_1 = solution(start=0, numNodes=3, edges=[[0, 1, 9], [0, 2, 3], [1, 0, 5], [2, 1, 1]]) print(solution_1) assert solution_1 == [0, 4, 3] solution_2 = solution(start=0, numNodes=4, edges=[[0, 1, 1], [1, 2, 5], [2, 3, 1]]) print(solution_2) assert solution_2 == [0, 1, 6, 7] ì‹œê°„ë³µì¡ë„ : ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ ìƒì„± : O(E) ìµœì†Œ ë¹„ìš© ì°¾ëŠ” ë¶€ë¶„ : O(VlogV) ìµœì¢… : O(E + VlogV) # ë²¨ë§Œ-í¬ë“œ ì•Œê³ ë¦¬ì¦˜ ìŒì˜ ê°€ì¤‘ì¹˜ê°€ ìˆì„ ë•Œ ìµœì†Œê±°ë¦¬ êµ¬í•˜ëŠ” ë°©ë²• from collections import defaultdict import dis INF = 1e15 def solution(num_vertices:int, edges:list, source: int) -> list int: # 1. adj list adj_list = defaultdict(list) for start_node, end_node, weight in edges: adj_list[start_node].append((end_node, weight)) # 2. distasnce ì´ˆê¸°í™” distance = [INF] num_vertices distance[source] = 0 # 3. ì •ì ì˜ ê°œìˆ˜ -1ë§Œí¼ ìµœì†Œ ë¹„ìš©ì„ ê°±ì‹  for _ in range(num_vertices - 1): for u in range(num_vertices): for v, weight in adj_list[u]: if distance[u] + weight < distance[v]: distance[v] = distance[u] + weight # 4. ìŒì˜ ìˆœí™˜ì´ ìˆëŠ”ì§€ í™•ì¸ for u in range(num_vertices): for v, weight in adj_list[u]: if distance[u] + weight < distance[v]: return [-1] return distance solution_1 = solution(num_vertices=5, edges=[[0, 1, 4], [0, 2, 3], [0, 4, -6], [1, 3, 5], [2, 1, 2], [3, 0, 7], [3, 2, 4], [4, 2, 2]], source=0) print(solution_1) assert solution_1 == [0, -2, -4, 3, -6] solution_2 = solution(num_vertices=4, edges=[[0, 1, 5], [0, 2, -1], [1, 2, 2], [2, 3, -2], [3, 0, 2], [3, 1, 6]], source=0) print(solution_2) assert solution_2 == [-1] # ìŒì˜ ìˆœí™˜ì´ ìˆì„ ë•Œ ì‹œê°„ë³µì¡ë„ = O(N E)",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ìë£Œêµ¬ì¡°/heapq",
    "title": "heapq ê°œë…ì •ë¦¬",
    "date": "2025-08-03",
    "excerpt": "heapqì— ëŒ€í•œ ì„¤ëª…ê³¼ ì–´ë–»ê²Œ ì‚¬ìš©ë˜ëŠ”ì§€ ì •ë¦¬",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ìë£Œêµ¬ì¡°"
    ],
    "content": "heapqì™€ ìš°ì„ ìˆœìœ„ í (Priority Queue) <figure> <img src=\"/post/CodilityStudy/heapq.png\" alt=\"heapq ê°œë…ë„\" /> <figcaption>heapq ê·¸ë¦¼</figcaption> </figure> heapqë€? Pythonì˜ ë‚´ì¥ ëª¨ë“ˆë¡œ, ìµœì†Œ í™(Min Heap)ì„ êµ¬í˜„í•˜ëŠ” ëª¨ë“ˆì…ë‹ˆë‹¤. ``python import heapq ë¹ˆ í™ ìƒì„± heap = [] ìš”ì†Œ ì¶”ê°€ heapq.heappush(heap, 5) heapq.heappush(heap, 2) heapq.heappush(heap, 8) heapq.heappush(heap, 1) print(heap) # [1, 2, 8, 5] - ìµœì†Ÿê°’ì´ ë£¨íŠ¸ì— ìœ„ì¹˜ ` ê¸°ë³¸ ë©”ì„œë“œë“¤ ìš”ì†Œ ì¶”ê°€ `python heapq.heappush(heap, item) # í™ì— ìš”ì†Œ ì¶”ê°€ ` ìµœì†Ÿê°’ ì œê±° ë° ë°˜í™˜ `python min_value = heapq.heappop(heap) # ìµœì†Ÿê°’ ì œê±°í•˜ê³  ë°˜í™˜ ` ë¦¬ìŠ¤íŠ¸ë¥¼ í™ìœ¼ë¡œ ë³€í™˜ `python numbers = [5, 2, 8, 1, 9] heapq.heapify(numbers) # ë¦¬ìŠ¤íŠ¸ë¥¼ í™ìœ¼ë¡œ ë³€í™˜ print(numbers) # [1, 2, 8, 5, 9] ` ìµœì†Ÿê°’ í™•ì¸ (ì œê±°í•˜ì§€ ì•ŠìŒ) `python min_value = heap[0] # í™ì˜ ë£¨íŠ¸ ë…¸ë“œ í™•ì¸ ` ìš°ì„ ìˆœìœ„ í êµ¬í˜„ ê¸°ë³¸ ìš°ì„ ìˆœìœ„ í `python import heapq class PriorityQueue: def __init__(self): self.heap = [] def push(self, item): heapq.heappush(self.heap, item) def pop(self): return heapq.heappop(self.heap) def peek(self): return self.heap[0] if self.heap else None def is_empty(self): return len(self.heap) == 0 ì‚¬ìš© ì˜ˆì‹œ pq = PriorityQueue() pq.push(5) pq.push(2) pq.push(8) pq.push(1) print(pq.pop()) # 1 (ê°€ì¥ ì‘ì€ ê°’) print(pq.pop()) # 2 ` íŠœí”Œì„ ì´ìš©í•œ ìš°ì„ ìˆœìœ„ í `python (ìš°ì„ ìˆœìœ„, ë°ì´í„°) í˜•íƒœë¡œ ì €ì¥ pq = [] heapq.heappush(pq, (3, \"task3\")) heapq.heappush(pq, (1, \"task1\")) heapq.heappush(pq, (2, \"task2\")) print(heapq.heappop(pq)) # (1, \"task1\") - ìš°ì„ ìˆœìœ„ê°€ ê°€ì¥ ë†’ì€ ê²ƒ ` ìµœëŒ€ í™ êµ¬í˜„ ë°©ë²• 1: ìŒìˆ˜ ë³€í™˜ `python ìµœëŒ€ í™ì„ ì›í•  ë•ŒëŠ” ìŒìˆ˜ë¡œ ë³€í™˜ numbers = [5, 2, 8, 1, 9] max_heap = [-x for x in numbers] heapq.heapify(max_heap) ìµœëŒ“ê°’ ì¶”ì¶œ max_value = -heapq.heappop(max_heap) print(max_value) # 9 ` ë°©ë²• 2: íŠœí”Œ ì‚¬ìš© `python (-ìš°ì„ ìˆœìœ„, ë°ì´í„°) í˜•íƒœ max_heap = [] heapq.heappush(max_heap, (-5, \"task5\")) heapq.heappush(max_heap, (-2, \"task2\")) heapq.heappush(max_heap, (-8, \"task8\")) priority, task = heapq.heappop(max_heap) print(f\"ìš°ì„ ìˆœìœ„: {-priority}, ì‘ì—…: {task}\") # ìš°ì„ ìˆœìœ„: 8, ì‘ì—…: task8 ` heapqë¥¼ ì‚¬ìš©í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ë“¤ 5.1 ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜ (Dijkstra's Algorithm) ëª©ì : ê°€ì¤‘ ê·¸ë˜í”„ì—ì„œ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸° `python import heapq def dijkstra(graph, start): distances = {node: float('inf') for node in graph} distances[start] = 0 pq = [(0, start)] while pq: current_dist, current_node = heapq.heappop(pq) if current_dist > distances[current_node]: continue for neighbor, weight in graph[current_node].items(): distance = current_dist + weight if distance < distances[neighbor]: distances[neighbor] = distance heapq.heappush(pq, (distance, neighbor)) return distances ` 5.2 í”„ë¦¼ ì•Œê³ ë¦¬ì¦˜ (Prim's Algorithm) ëª©ì : ìµœì†Œ ì‹ ì¥ íŠ¸ë¦¬(MST) ì°¾ê¸° `python def prim(graph, start): mst = [] visited = set() pq = [(0, start, None)] # (ê°€ì¤‘ì¹˜, ë…¸ë“œ, ë¶€ëª¨) while pq: weight, node, parent = heapq.heappop(pq) if node in visited: continue visited.add(node) if parent is not None: mst.append((parent, node, weight)) for neighbor, edge_weight in graph[node].items(): if neighbor not in visited: heapq.heappush(pq, (edge_weight, neighbor, node)) return mst ` 5.3 A* ì•Œê³ ë¦¬ì¦˜ ëª©ì : íœ´ë¦¬ìŠ¤í‹±ì„ ì‚¬ìš©í•œ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸° `python def a_star(graph, start, goal, heuristic): pq = [(0, start, [start])] # (f_score, node, path) g_score = {start: 0} f_score = {start: heuristic(start, goal)} while pq: current_f, current, path = heapq.heappop(pq) if current == goal: return path for neighbor, weight in graph[current].items(): tentative_g = g_score[current] + weight if neighbor not in g_score or tentative_g < g_score[neighbor]: g_score[neighbor] = tentative_g f_score[neighbor] = tentative_g + heuristic(neighbor, goal) heapq.heappush(pq, (f_score[neighbor], neighbor, path + [neighbor])) return None ` 5.4 í—ˆí”„ë§Œ ì½”ë”© (Huffman Coding) ëª©ì : ë°ì´í„° ì••ì¶•ì„ ìœ„í•œ ê°€ë³€ ê¸¸ì´ ì¸ì½”ë”© `python from collections import Counter def huffman_coding(text): # ë¹ˆë„ìˆ˜ ê³„ì‚° freq = Counter(text) # í™ì— ëª¨ë“  ë¬¸ì ì¶”ê°€ heap = [[freq[char], char] for char in freq] heapq.heapify(heap) # í—ˆí”„ë§Œ íŠ¸ë¦¬ êµ¬ì„± while len(heap) > 1: left = heapq.heappop(heap) right = heapq.heappop(heap) # ìƒˆë¡œìš´ ë…¸ë“œ ìƒì„± new_node = [left[0] + right[0], left, right] heapq.heappush(heap, new_node) return heap[0] # ë£¨íŠ¸ ë…¸ë“œ ` 5.5 Top K ë¬¸ì œ ëª©ì : ë°°ì—´ì—ì„œ ìƒìœ„ Kê°œì˜ ìš”ì†Œ ì°¾ê¸° `python def find_top_k(nums, k): # ìµœì†Œ í™ì„ ì‚¬ìš©í•´ì„œ ìƒìœ„ Kê°œ ìœ ì§€ heap = [] for num in nums: heapq.heappush(heap, num) if len(heap) > k: heapq.heappop(heap) # ê°€ì¥ ì‘ì€ ê°’ ì œê±° return sorted(heap, reverse=True) # ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ ì‚¬ìš© ì˜ˆì‹œ nums = [3, 2, 1, 5, 6, 4] print(find_top_k(nums, 2)) # [6, 5] - ìƒìœ„ 2ê°œ ` 5.6 ì¤‘ê°„ê°’ ì°¾ê¸° (Median) ëª©ì : ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì—ì„œ ì¤‘ê°„ê°’ ìœ ì§€ `python class MedianFinder: def __init__(self): self.max_heap = [] # ì™¼ìª½ ì ˆë°˜ (ìµœëŒ€ í™) self.min_heap = [] # ì˜¤ë¥¸ìª½ ì ˆë°˜ (ìµœì†Œ í™) def addNum(self, num): # ìµœëŒ€ í™ì— ì¶”ê°€ heapq.heappush(self.max_heap, -num) # ê· í˜• ë§ì¶”ê¸° if self.max_heap and self.min_heap and -self.max_heap[0] > self.min_heap[0]: val = -heapq.heappop(self.max_heap) heapq.heappush(self.min_heap, val) # í¬ê¸° ê· í˜• ë§ì¶”ê¸° if len(self.max_heap) > len(self.min_heap) + 1: val = -heapq.heappop(self.max_heap) heapq.heappush(self.min_heap, val) elif len(self.min_heap) > len(self.max_heap): val = heapq.heappop(self.min_heap) heapq.heappush(self.max_heap, -val) def findMedian(self): if len(self.max_heap) > len(self.min_heap): return -self.max_heap[0] return (-self.max_heap[0] + self.min_heap[0]) / 2 ` 5.7 ë³‘í•© Kê°œ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ ëª©ì : Kê°œì˜ ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•© `python def merge_k_sorted_lists(lists): pq = [] result = [] # ê° ë¦¬ìŠ¤íŠ¸ì˜ ì²« ë²ˆì§¸ ìš”ì†Œë¥¼ í™ì— ì¶”ê°€ for i, lst in enumerate(lists): if lst: heapq.heappush(pq, (lst[0], i, 0)) while pq: val, list_idx, element_idx = heapq.heappop(pq) result.append(val) # ë‹¤ìŒ ìš”ì†Œê°€ ìˆìœ¼ë©´ í™ì— ì¶”ê°€ if element_idx + 1 < len(lists[list_idx]): heapq.heappush(pq, (lists[list_idx][element_idx + 1], list_idx, element_idx + 1)) return result ` ì‹œê°„ë³µì¡ë„ ì‚½ì…: O(log n) ì‚­ì œ: O(log n) ìµœì†Ÿê°’ í™•ì¸: O(1) í™ ìƒì„±: O(n) ì½”ë”©í…ŒìŠ¤íŠ¸ íŒ ìì£¼ ì‚¬ìš©ë˜ëŠ” íŒ¨í„´ `python ìµœì†Œê°’/ìµœëŒ€ê°’ ìœ ì§€ min_heap = [] max_heap = [] ìš°ì„ ìˆœìœ„ì™€ í•¨ê»˜ ë°ì´í„° ì €ì¥ pq = [(priority, data)] ì—¬ëŸ¬ ì •ë³´ë¥¼ íŠœí”Œë¡œ ì €ì¥ pq = [(cost, node, path)] ìŒìˆ˜ ë³€í™˜ìœ¼ë¡œ ìµœëŒ€ í™ êµ¬í˜„ max_heap = [-x for x in numbers] ` ì£¼ì˜ì‚¬í•­ íŠœí”Œ ë¹„êµ: ì²« ë²ˆì§¸ ìš”ì†Œë¶€í„° ìˆœì„œëŒ€ë¡œ ë¹„êµ ê°™ì€ ìš°ì„ ìˆœìœ„: ë‘ ë²ˆì§¸ ìš”ì†Œë¡œ êµ¬ë¶„ ìµœëŒ€ í™: ìŒìˆ˜ ë³€í™˜ ë˜ëŠ” (-priority, data) í˜•íƒœ ì¤‘ë³µ ë°©ë¬¸: ë°©ë¬¸ ì²´í¬ í•„ìˆ˜ (ë‹¤ìµìŠ¤íŠ¸ë¼ ë“±) í•µì‹¬ í¬ì¸íŠ¸: heapq`ëŠ” ìµœì†Œ í™ì„ êµ¬í˜„ ìš°ì„ ìˆœìœ„ íì˜ ê¸°ë³¸ êµ¬í˜„ì²´ ê·¸ë˜í”„ ì•Œê³ ë¦¬ì¦˜ì—ì„œ í•„ìˆ˜ì  ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ì— ìœ ìš© ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ìë£Œêµ¬ì¡°",
    "public": true
  },
  {
    "id": "Self_Development/Singing/ì¥íš¨ì§„ê°•ì˜/3ê°•_ê¹Šê²Œ ë§ˆì‹œê³  ë±‰ê¸°ë¥¼ í†µí•œ í˜¸í¡ ìµíˆê¸°",
    "title": "3ê°• ê¹Šê²Œ ë§ˆì‹œê³  ë±‰ê¸°ë¥¼ í†µí•œ í˜¸í¡ ìµíˆê¸°",
    "date": "2025-08-02",
    "excerpt": "ê¹Šê²Œ ë§ˆì‹œê³  ë±‰ëŠ” ë²•ì„ í†µí•´ ë‚´ë³µê·¼ ëŠë‚Œ ì¡ëŠ” ë²•. ê´€ë ¨ ì—°ìŠµë²• ì •ë¦¬",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "ì¥íš¨ì§„ ê°•ì˜",
      "ì—°ìŠµë°©ë²•"
    ],
    "content": "ì ì§„ì  ê³¼ë¶€í•˜ ê³µê¸°ë¥¼ ë‹¤ ëºë‹¤ê³  ëŠë‚„ ë•Œ, ê·¸ ë–„ ë–  ëº¼ë–„ ë‚´ë³µê·¼ì´ ë” ë§ì´ ì“°ì„ ë˜‘ê°™ì€ í˜ìœ¼ë¡œ ëê¹Œì§€ ë±‰ëŠ” ëŠë‚Œ í—ˆë¦¬í´ê³ . ê°™ì€ í˜ìœ¼ë¡œ. > ë§ˆì§€ë§‰ì— ë§ˆì§€ë§‰ì— ë§ˆì§€ë§‰ê¹Œì§€ ë±‰ì–´ì£¼ê¸°",
    "public": true
  },
  {
    "id": "Self_Development/Singing/ì¥íš¨ì§„ê°•ì˜/2ê°•_ë‚´ë³µê·¼_ì»¨íŠ¸ë¡¤",
    "title": "2ê°• ë‚´ë³µê·¼ ì»¨íŠ¸ë¡¤",
    "date": "2025-08-02",
    "excerpt": "2ê°• ë‚´ë³µê·¼ ì»¨íŠ¸ë¡¤ , ì— ëŒ€í•œ ë‚´ìš©ì •ë¦¬",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "ì¥íš¨ì§„ ê°•ì˜"
    ],
    "content": "ìœ ë¡œì¸ê°• ìœ íŠ­ ë§í¬ í˜¸í¡ì„ ë‚´ë±‰ëŠ” ë²•. íŒ¨ì‹œë¸Œê°€ ë˜ì–´ì•¼ í•¨. íš¡ê²½ë§‰ì˜ ì—­í• . ë‚˜ì˜¨ ë°°ë¥¼ ë‹¤ì‹œ ë°°ë§Œ ì •ìœ„ì¹˜ë¡œ ëŒë ¤ë†“ëŠ” ëŠë‚Œ. \"ë°°ë§Œ ì›€ì§ì´ëŠ” ëŠë‚Œ\" \"í”¼ì§€ì»¬ì˜ ì¤‘ìš”ì„±\" ê°€ìŠ´ë„ ì•ˆì¨ì•¼ë¨. ë°°ë¡œë§Œ ì»¨íŠ¸ë¡¤í•˜ëŠ” ëŠë‚Œ.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/ì¥íš¨ì§„ê°•ì˜/1ê°•_ê°œë…ì •ë¦¬ ë° ë³¸ì§ˆ",
    "title": "1ê°• ê°œë…ì •ë¦¬ ë° ë³¸ì§ˆ",
    "date": "2025-08-02",
    "excerpt": "1ê°• ê°œë…ì •ë¦¬ ë° ë³¸ì§ˆ , ì— ëŒ€í•œ ë‚´ìš©ì •ë¦¬",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "ì¥íš¨ì§„ ê°•ì˜"
    ],
    "content": "ì „ì²´ì ì¸ ëª©ì°¨ì— ëŒ€í•œ ì„¤ëª… í˜¸í¡ì— ëŒ€í•œ ìì„¸í•œ ì„¤ëª… ë°œì„±ì˜ ì‹œì‘. ì†Œë¦¬ë¥¼ ë§Œë“œëŠ” ë²• ì„±ëŒ€ ì»¨íŠ¸ë¡¤ê³¼ ê´€ë ¨ëœ ë‚´ìš©ë“¤ ì •~~ë§ ë§ì€ ë‚´ìš©ì´ ìˆêµ¬ë‚˜. ê¾¸ì¤€íˆ í•˜ë‚˜ì”© ë“¤ì–´ë³´ìê³ . > ì‹œí‚¤ëŠ”ëŒ€ë¡œë§Œ í•˜ì„¸ìš”. ë¬»ì§€ë„ ë”°ì§€ì§€ë„ ë§ê³  ì˜ìƒì—ì„œ í•˜ë¼ëŠ”ëŒ€ë¡œ.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/ê·¸ë˜í”„_ìµœë‹¨ê²½ë¡œ_êµ¬í•˜ê¸°",
    "title": "ê·¸ë˜í”„ ìµœë‹¨ ê²½ë¡œ êµ¬í•˜ê¸°",
    "date": "2025-08-02",
    "excerpt": "ê·¸ë˜í”„ ìµœë‹¨ ê²½ë¡œ êµ¬í•˜ê¸°",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ê·¸ë˜í”„",
      "ì•Œê³ ë¦¬ì¦˜"
    ],
    "content": "> ìµœë‹¨ ê²½ë¡œ (shortest path)ëŠ” ê·¸ë˜í”„ì˜ ì¢…ë¥˜ì— ë”°ë¼ ê·¸ ì§„ì˜ê°€ ë‹¤ë¥´ê²Œ í•´ì„ë  ìˆ˜ë„ ìˆëŠ” ì£¼ì œì…ë‹ˆë‹¤. ê°€ì¤‘ì¹˜ì˜ ì—¬ë¶€. ì•Œê³ ë¦¬ì¦˜ ëª©ì  ì¥ë‹¨ì  ë° íŠ¹ì§• ì‹œê°„ ë³µì¡ë„ --------- ------ ---------------- ------------- ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜ ì¶œë°œ ë…¸ë“œë¡œë¶€í„° ë„ì°© ë…¸ë“œë“¤ê¹Œì§€ì˜ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸° ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” ê·¸ë˜í”„ì—ì„œ ìµœë‹¨ ê²½ë¡œë¥¼ êµ¬í•  ìˆ˜ ì—†ìŒ (ê·¸ë¦¬ë”” ë°©ì‹) O(V^2), ìš°ì„ ìˆœìœ„ íë¡œ ê°œì„ í•˜ë©´ O(E logV) ë²¨ë§Œ-í¬ë“œ ì•Œê³ ë¦¬ì¦˜ ì¶œë°œ ë…¸ë“œë¡œë¶€í„° ë„ì°© ë…¸ë“œë“¤ê¹Œì§€ì˜ ìµœë‹¨ ê²½ë¡œ ì°¾ê¸° ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” ê·¸ë˜í”„ì—ì„œ ìµœë‹¨ ê²½ë¡œë¥¼ êµ¬í•  ìˆ˜ ìˆê³ , ìŒì˜ ìˆœí™˜ë„ ê°ì§€í•  ìˆ˜ ìˆìŒ O(VE) ë‹¤ìµìŠ¤íŠ¸ë¼(dijkstra) ì•Œê³ ë¦¬ì¦˜ --- ëª¨ë“  ê°€ì¤‘ì¹˜ê°€ ì–‘ìˆ˜ì¼ ë•Œ. > ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜ì˜ ëª©ì ì€ ê·¸ë˜í”„ ë‚´ì—ì„œ íŠ¹ì • ë…¸ë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ê° ë…¸ë“œê¹Œì§€ ìµœë‹¨ ê²½ë¡œ ë˜ëŠ” ìµœë‹¨ ê²½ë¡œì˜ ê¸¸ì´ë¥¼ ì°¾ëŠ” ê²ƒì— ìˆìŠµë‹ˆë‹¤. > ë‹¤ìµìŠ¤íŠ¸ë¼ ì•Œê³ ë¦¬ì¦˜ì˜ í•µì‹¬ì€ í˜„ì¬ê¹Œì§€ ë°œê²¬í•œ ê°€ì¥ ì ì€ ë¹„ìš©ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆëŠ” ë…¸ë“œë¥¼ ê²½ìœ í•´ì„œ, ê° ë…¸ë“œê¹Œì§€ ìµœë‹¨ ê²½ë¡œë¥¼ ê°±ì‹ í•˜ëŠ” ì‘ì—…ì„ ë°˜ë³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹œì‘ ë…¸ë“œì—ì„œ ê° ë…¸ë“œê¹Œì§€ ê±°ë¦¬ë¥¼ ì´ˆê¸°í™” í˜„ì¬ ê°€ì¥ ì ì€ ë¹„ìš©ìœ¼ë¡œ ê°ˆ ìˆ˜ ìˆëŠ” ë…¸ë“œ ì„ íƒ (ê° ë…¸ë“œëŠ” í•œ ë²ˆì”©ë§Œ ì„ íƒ) 2ì—ì„œ ì„ íƒí•œ ë…¸ë“œì˜ ì£¼ë³€ ë…¸ë“œ ê±°ë¦¬ë¥¼ ê°±ì‹  ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ìˆœì„œ ì‹œì‘ ë…¸ë“œë¥¼ ì„¤ì •í•˜ê³  ì‹œì‘ ë…¸ë“œë¡œë¶€í„° íŠ¹ì • ë…¸ë“œê¹Œì§€ì˜ ìµœì†Œ ë¹„ìš©ì„ ì €ì¥í•  ê³µê°„ê³¼ ì§ì „ ë…¸ë“œë¥¼ ì €ì¥í•  ê³µê°„ì„ ë§ˆë ¨í•©ë‹ˆë‹¤. 1-1. ìµœì†Œ ë¹„ìš©ì„ ì €ì¥í•  ê³µê°„ì€ ëª¨ë‘ ë§¤ìš° í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. 1-2. ì‹œì‘ ë…¸ë“œì˜ ìµœì†Œë¹„ìš©ì€ 0, ì§ì „ ë…¸ë“œëŠ” ìì‹ ìœ¼ë¡œ í•©ë‹ˆë‹¤. í˜„ì¬ê¹Œì§€ êµ¬í•œ ê° ë…¸ë“œê¹Œì§€ ìµœì†Œ ë¹„ìš©ì„ ê¸°ì¤€ìœ¼ë¡œ ì§€ê¸ˆê¹Œì§€ ì„ íƒí•˜ì§€ ì•Šì€ ë…¸ë“œ ì¤‘ ìµœì†Œ ë¹„ìš©ì´ ê°€ì¥ ì ì€ ë…¸ë“œë¥¼ ì„ íƒí•©ë‹ˆë‹¤. 2-1. í•´ë‹¹ ë…¸ë“œë¥¼ ê±°ì³ì„œ ê° ë…¸ë“œê¹Œì§€ ê°€ëŠ” ìµœì†Œ ë¹„ìš©ê³¼ í˜„ì¬ê¹Œì§€ êµ¬í•œ ìµœì†Œ ë¹„ìš©ì„ ë¹„êµí•˜ì—¬ ì‘ì€ ê°’ì„ ê° ë…¸ë“œì˜ ìµœì†Œ ë¹„ìš©ìœ¼ë¡œ ê°±ì‹ í•©ë‹ˆë‹¤. 2-2. ì´ë•Œ ì§ì „ ë…¸ë“œë„ ê°™ì´ ê°±ì‹ í•©ë‹ˆë‹¤. ë…¸ë“œ ê°œìˆ˜ì—ì„œ 1ì„ ëº€ ë§Œí¼ ë°˜ë³µí•©ë‹ˆë‹¤. ë²¨ë§Œ-í¬ë“œ(bellman-ford) ì•Œê³ ë¦¬ì¦˜ --- > ë²¨ë§Œ-í¬ë“œ ì•Œê³ ë¦¬ì¦˜ì€ ë§¤ ë‹¨ê³„ë§ˆë‹¤ ëª¨ë“  ê°„ì„ ì˜ ê°€ì¤‘ì¹˜ë¥¼ ë‹¤ì‹œ í™•ì¸í•˜ì—¬ ìµœì†Œ ë¹„ìš©ì„ ê°±ì‹ í•˜ë¯€ë¡œ ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ëŠ” ê·¸ë˜í”„ì—ì„œë„ ìµœë‹¨ ê²½ë¡œë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•Œê³ ë¦¬ì¦˜ ë™ì‘ìˆœì„œ ì‹œì‘ë…¸ë“œë¥¼ ì„¤ì •í•œ ë‹¤ìŒ ì‹œì‘ ë…¸ë“œì˜ ìµœì†Œ ë¹„ìš©ì€ 0, ë‚˜ë¨¸ì§€ ë…¸ë“œëŠ” INFë¡œ ì´ˆê¸°í™”. ë…¸ë“œ ê°œìˆ˜ -1 ë§Œí¼ ë‹¤ìŒ ì—°ì‚°ì€ ë°˜ë³µ 2-1. ì‹œì‘ ë…¸ë“œì—ì„œ ê°ˆ ìˆ˜ ìˆëŠ” ê° ë…¸ë“œì— ëŒ€í•˜ì—¬ ì „ì²´ ë…¸ë“œ ê°ê°ì„ ê±°ì³ê°ˆ ë•Œ í˜„ì¬ê¹Œì§€ êµ¬í•œ ìµœì†Œ ë¹„ìš©ë³´ë‹¤ ë” ì ì€ ìµœì†Œ ë¹„ìš©ì´ ìˆëŠ”ì§€ í™•ì¸í•˜ì—¬ ê°±ì‹ . ê³¼ì •2ë¥¼ ë§ˆì§€ë§‰ìœ¼ë¡œ í•œë²ˆ ë” ìˆ˜í–‰í•˜ì—¬ ê°±ì‹ ë˜ëŠ” ìµœì†Œ ë¹„ìš©ì´ ìˆëŠ”ì§€ í™•ì¸. ìˆë‹¤ë©´ ìŒì˜ ìˆœí™˜ì´ ìˆìŒì„ ì˜ë¯¸. ì™œ ì •ì ê°œìˆ˜ -1ë§Œí¼ ë°˜ë³µí•˜ëŠ”ê°€? ë§¤ ì—°ì‚°ë§ˆë‹¤ ìµœë‹¨ê²½ë¡œê°€ 1ê°œì”© í™•ì •ë˜ë¯€ë¡œ! ë²¨ë§Œ-í¬ë“œ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì •ì  ê°œìˆ˜ - 1ë²ˆë§Œí¼ ë°˜ë³µí•˜ëŠ” ì´ìœ ëŠ” ìµœë‹¨ ê²½ë¡œì˜ íŠ¹ì„± ë•Œë¬¸ì…ë‹ˆë‹¤. í•µì‹¬ ì›ë¦¬ ìµœë‹¨ ê²½ë¡œëŠ” ìµœëŒ€ V-1ê°œì˜ ê°„ì„ ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤ nê°œì˜ ì •ì ì´ ìˆì„ ë•Œ, ìµœë‹¨ ê²½ë¡œëŠ” ìµœëŒ€ n-1ê°œì˜ ê°„ì„ ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤ ë§Œì•½ nê°œ ì´ìƒì˜ ê°„ì„ ì´ ìˆë‹¤ë©´, ê·¸ ê²½ë¡œì—ëŠ” ë°˜ë“œì‹œ ìˆœí™˜(cycle)ì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤ ì˜ˆì‹œë¡œ ì´í•´í•˜ê¸° `` ì •ì : A, B, C, D (4ê°œ) ê°€ëŠ¥í•œ ìµœë‹¨ ê²½ë¡œ: A â†’ B â†’ C â†’ D (3ê°œ ê°„ì„ , ìµœëŒ€) A â†’ B â†’ D (2ê°œ ê°„ì„ ) A â†’ D (1ê°œ ê°„ì„ ) ` ì™œ V-1ë²ˆì´ë©´ ì¶©ë¶„í•œê°€? 1ë²ˆì§¸ ë°˜ë³µ: ì¶œë°œì ì—ì„œ 1ê°œ ê°„ì„ ìœ¼ë¡œ ë„ë‹¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ì ì˜ ìµœë‹¨ ê±°ë¦¬ í™•ì • 2ë²ˆì§¸ ë°˜ë³µ: ì¶œë°œì ì—ì„œ 2ê°œ ê°„ì„ ìœ¼ë¡œ ë„ë‹¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ì ì˜ ìµœë‹¨ ê±°ë¦¬ í™•ì • ... (V-1)ë²ˆì§¸ ë°˜ë³µ: ì¶œë°œì ì—ì„œ (V-1)ê°œ ê°„ì„ ìœ¼ë¡œ ë„ë‹¬ ê°€ëŠ¥í•œ ëª¨ë“  ì •ì ì˜ ìµœë‹¨ ê±°ë¦¬ í™•ì • ì´í›„ ë” ë°˜ë³µí•´ë„ ìƒˆë¡œìš´ ìµœë‹¨ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì™œ í•œë²ˆ ë” ì—°ì‚°ì„ ë°˜ë³µí•˜ëŠ”ê°€? ìŒì˜ ìˆœí™˜ì„ ì°¾ê¸° ìœ„í•´! ë²¨ë§Œ-í¬ë“œ ì•Œê³ ë¦¬ì¦˜ì˜ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ í•œ ë²ˆ ë” ë°˜ë³µí•˜ëŠ” ì´ìœ ëŠ” ìŒì˜ ìˆœí™˜(negative cycle)ì„ ê°ì§€í•˜ê¸° ìœ„í•´ì„œì…ë‹ˆë‹¤. ìŒì˜ ìˆœí™˜ì´ë€? ` A â†’ B â†’ C â†’ A (ìˆœí™˜) ê°€ì¤‘ì¹˜: 2 â†’ 3 â†’ (-6) â†’ 2 ` ì´ ê²½ìš° Aì—ì„œ ì‹œì‘í•´ì„œ Aë¡œ ëŒì•„ì˜¬ ë•Œë§ˆë‹¤ ë¹„ìš©ì´ -1ì”© ê°ì†Œí•©ë‹ˆë‹¤. ì¦‰, ë¬´í•œíˆ ì‘ì•„ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°ì§€ ë°©ë²• V-1ë²ˆ ë°˜ë³µ: ì •ìƒì ì¸ ìµœë‹¨ ê²½ë¡œ ê³„ì‚° Vë²ˆì§¸ ë°˜ë³µ: í•œ ë²ˆ ë” ë°˜ë³µí•˜ì—¬ ê±°ë¦¬ ê°’ì´ ê°±ì‹ ë˜ëŠ”ì§€ í™•ì¸ ë§Œì•½ ê±°ë¦¬ ê°’ì´ ê°±ì‹ ëœë‹¤ë©´ â†’ ìŒì˜ ìˆœí™˜ì´ ì¡´ì¬ ë§Œì•½ ê±°ë¦¬ ê°’ì´ ê°±ì‹ ë˜ì§€ ì•ŠëŠ”ë‹¤ë©´ â†’ ìŒì˜ ìˆœí™˜ì´ ì—†ìŒ ì˜ˆì‹œ ` ì •ì : A, B, C ê°„ì„ : Aâ†’B(2), Bâ†’C(3), Câ†’A(-6) V-1ë²ˆ ë°˜ë³µ í›„: dist[A] = 0, dist[B] = 2, dist[C] = 5 Vë²ˆì§¸ ë°˜ë³µ ì‹œ: dist[A] = 5 + (-6) = -1 (ê°±ì‹ ë¨!) â†’ ìŒì˜ ìˆœí™˜ ì¡´ì¬! `` ì™œ Vë²ˆì§¸ì—ì„œ ê°ì§€ë˜ëŠ”ê°€? V-1ë²ˆ ë°˜ë³µìœ¼ë¡œ ëª¨ë“  ì •ìƒì ì¸ ìµœë‹¨ ê²½ë¡œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤ Vë²ˆì§¸ ë°˜ë³µì—ì„œ ê±°ë¦¬ ê°’ì´ ê°±ì‹ ëœë‹¤ë©´, ê·¸ê²ƒì€ Vê°œ ì´ìƒì˜ ê°„ì„ ì„ ì‚¬ìš©í•˜ëŠ” ê²½ë¡œì…ë‹ˆë‹¤ Vê°œ ì´ìƒì˜ ê°„ì„ ì„ ì‚¬ìš©í•˜ëŠ” ê²½ë¡œëŠ” ë°˜ë“œì‹œ ìˆœí™˜ì„ í¬í•¨í•©ë‹ˆë‹¤ ê·¸ ìˆœí™˜ì´ ìŒì˜ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ë‹¤ë©´, ê±°ë¦¬ ê°’ì´ ê³„ì† ê°ì†Œí•˜ê²Œ ë©ë‹ˆë‹¤",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/08-01-ìµœì•…ì˜ëŒ€ë¹„_ë¦¬ë·°",
    "title": "ìµœì•…ì˜ ëŒ€ë¹„ ë¦¬ë·° ë°ì´",
    "date": "2025-08-01",
    "excerpt": "ìµœì•…ì˜ ëŒ€ë¹„ ê´€ë ¨ í‘œí˜„ë“¤ ë¦¬ë·° ë°ì´",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë¦¬ë·°í•˜ëŠ” ê²ƒë“¤ What if I mess up? What if I fail the exam? What if it rains? What if the flight is delayed? What if I miss thre connecting flight? What if I miss my flight? What if I miss my train? What if you were in Seoul? What if you were in New York? What would you do if you were in Newyork? If it hadn't been for your help, I wouldn't have made it. Would you have moved to New York? Let me know if you're interested. I don't mind if we go clubbing. Even if it's inconvenient, I'll still wear a mask. No matter what happens, I'll always be there for you.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/í˜„ëŒ€ì˜¤í† ì—ë²„ì½”í…Œ",
    "title": "í˜„ëŒ€ì˜¤í† ì—ë²„ ì½”í…Œ ì¤€ë¹„í•˜ê¸°",
    "date": "2025-08-01",
    "excerpt": "í˜„ëŒ€ì˜¤í† ì—ë²„ ì½”í…Œ ì¤€ë¹„í•˜ê¸°",
    "category": "Codility Study",
    "tags": [
      "ì½”í…Œ ì¤€ë¹„"
    ],
    "content": "ì‹œí—˜ ê¸°ì¶œ ë¬¸ì œ ìœ í˜• ì°¸ê³ í•œ ë¸”ë¡œê·¸ 2025ë…„ ê¸°ì¶œ ë¬¸ì œ ìœ í˜• ë°±ì—”ë“œ ì´ 2ë¬¸ì œ DFS ë¬¸ì œ ê·¸ë˜í”„ê°€ ë‚˜ë‰˜ì–´ì ¸ ìˆëŠ” ê²½ìš°ì™€ ì‚¬ì´í´ì¸ ê²½ìš°ë¥¼ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë¬¸ì œ. DP ìœ í˜•ì˜ ë¬¸ì œ ëŠ˜ë³´ì½”ì¹˜ì˜ IT ì·¨ì—… ì½”í…Œ + ì„œë¥˜ë¡œ ë³´í†µ 1ì°¨ í‰ê°€ ì˜ˆì • ì•Œê³ ë¦¬ì¦˜ ìœ„ì£¼ì¸ë° ì‹œê°„ë³µì¡ë„ë¥¼ ì¤„ì—¬ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜ ì„ í˜¸ íˆ¬í¬ì¸í„° (ë°±ì¤€ 1253ë²ˆ) ì™„ì „íƒìƒ‰ìœ¼ë¡œ í’€ë©´ ì‹œê°„ ì´ˆê³¼ ì´ë¶„íƒìƒ‰ (ë°±ì¤€ 6236ë²ˆ) DP ê¸°ë³¸ë¬¸ì œ (ë°±ì¤€ 12865ë²ˆ), ë©”ëª¨ì´ì œì´ì…˜ (ë°±ì¤€ 9184ë²ˆ) ìš°ì„ ìˆœìœ„ í (ë°±ì¤€ 1766ë²ˆ) ëˆ„ì í•© (ë°±ì¤€ 2143ë²ˆ)",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/ê·¸ë˜í”„_íƒìƒ‰_ë…¸íŠ¸ë¶",
    "title": "ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„",
    "date": "2025-08-01",
    "excerpt": "ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ê·¸ë˜í”„",
      "ì•Œê³ ë¦¬ì¦˜"
    ],
    "content": "--- title: \"ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„\" date: \"2025-08-01\" category: \"Codility Study\" tags: [\"ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°\", \"Codility Study\", \"ê·¸ë˜í”„\", \"ì•Œê³ ë¦¬ì¦˜\"] excerpt: \"ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„\" --- # DFS êµ¬í˜„ !DFS ê·¸ë˜í”„ ê·¸ë¦¼ # ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ë¡œ í‘œì‹œëœ ê·¸ë˜í”„ graph = { 1: [4, 5], 2: [3], 3: [], 4: [2, 3], 5: [4] } ë°©ë¬¸ ì—¬ë¶€ visited = [False] (len(graph) + 1) def dfs(current_node): visited[current_node] = True print(current_node) for neighbor in graph[current_node]: if not visited[neighbor]: dfs(neighbor) dfs(1) # BFS êµ¬í˜„ from collections import deque ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ë¡œ í‘œì‹œëœ ê·¸ë˜í”„ graph = { 1: [4, 5], 2: [3], 3: [], 4: [2, 3], 5: [4] } def bfs(start_node): # ë°©ë¬¸ ì—¬ë¶€ visited = [False] (len(graph) + 1) queue = deque([start_node]) visited[start_node] = True while queue: node = queue.popleft() print(node) for adj_node in graph[node]: if not visited[adj_node]: queue.append(adj_node) visited[adj_node] = True bfs(1)",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/ê·¸ë˜í”„_íƒìƒ‰",
    "title": "ê·¸ë˜í”„ íƒìƒ‰ ê°œë… ì •ë¦¬",
    "date": "2025-08-01",
    "excerpt": "ê·¸ë˜í”„ íƒìƒ‰ ê°œë… ì •ë¦¬",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ê·¸ë˜í”„",
      "ì•Œê³ ë¦¬ì¦˜"
    ],
    "content": "êµ¬í˜„ ë…¸íŠ¸ë¶ ê·¸ë˜í”„ íƒìƒ‰ íŒŒì´ì¬ êµ¬í˜„ DFS / BFS ë” ì´ìƒ íƒìƒ‰í•  ë…¸ë“œê°€ ì—†ì„ ë•Œê¹Œì§€ ì¼ë‹¨ ê°€ë´…ë‹ˆë‹¤. ê·¸ëŸ¬ë‹¤ê°€ ë” ì´ìƒ íƒìƒ‰í•  ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìµœê·¼ì— ë°©ë¬¸í–ˆë˜ ë…¸ë“œë¡œ ë˜ëŒì•„ê°„ ë‹¤ìŒ ê°€ì§€ ì•Šì€ ë…¸ë“œë¥¼ ë°©ë¬¸í•©ë‹ˆë‹¤. í˜„ì¬ ìœ„ì¹˜ì—ì„œ ê°€ì¥ ê°€ê¹Œìš´ ë…¸ë“œë¶€í„° ëª¨ë‘ ë°©ë¬¸í•˜ê³  ë‹¤ìŒ ë…¸ë“œë¡œ ë„˜ì–´ê°‘ë‹ˆë‹¤. ê·¸ ë…¸ë“œì—ì„œ ë˜ ë‹¤ì‹œ ê°€ì¥ ê°€ê¹Œìš´ ë…¸ë“œë¶€í„° ëª¨ë‘ ë°©ë¬¸í•©ë‹ˆë‹¤. ìµœë‹¨ê²½ë¡œì°¾ê¸° ì¼ë•ŒëŠ” ë³´í†µ BFS, ë‚˜ë¨¸ì§€ëŠ” DFSì¸ ê²½ìš°ê°€ ë§ë‹¤. > DFSì™€ BFSê°€ ë°©ë¬¸ì²˜ë¦¬ ì‹œì ì´ ë‹¤ë¥¸ ì´ìœ  : > DFSëŠ” stackì—ì„œ popì„ í•  ë•Œ ë°©ë¬¸ì²˜ë¦¬ë¥¼ í•˜ì§€ë§Œ, BFSëŠ” queueì— pushí•  ë•Œ ë°©ë¬¸ì²˜ë¦¬ë¥¼ í•¨. > ì™œëƒí•˜ë©´ DFSëŠ” ë‚˜ì¤‘ì— í™•ì¸í•  ë…¸ë“œì´ê³ , BFSëŠ” ë°”ë¡œ í™•ì¸í•  ë…¸ë“œì´ê¸° ë•Œë¬¸ì—. DFS(Deep-First Search) : ê¹Šì´ ìš°ì„  íƒìƒ‰ > ìµœëŒ€ ê¹Šì´ ë…¸ë“œê¹Œì§€ ë°©ë¬¸í•œ ë‹¤ìŒì—ëŠ” ì´ì „ì— ë°©ë¬¸í•œ ë…¸ë“œë¥¼ ê±°ìŠ¬ëŸ¬ ì˜¬ë¼ê°€ë©° í•´ë‹¹ ë…¸ë“œì™€ ì—°ê²°ëœ ë…¸ë“œ ì¤‘ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë…¸ë“œë¡œ ë‹¤ì‹œ ìµœëŒ€ ê¹Šì´ê¹Œì§€ ì°¨ë¡€ëŒ€ë¡œ ë°©ë¬¸í•©ë‹ˆë‹¤. stack ìë£Œêµ¬ì¡° í™œìš© <figure> <img src=\"/post/CodilityStudy/stack.png\" alt=\"stack\" style=\"width: 100%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">ê·¸ë¦¼1. stack ìë£Œêµ¬ì¡° : FILO(First In Last Out) êµ¬ì¡°</figcaption> </figure> sudo code : ìŠ¤íƒì´ ë¹„ì—ˆëŠ”ì§€ í™•ì¸. ìŠ¤íƒì´ ë¹„ì—ˆë‹¤ë©´ -> ëª¨ë“  ë…¸ë“œ ë°©ë¬¸ ì™„ë£Œ -> ì¢…ë£Œ ìŠ¤íƒì—ì„œ ë…¸ë“œë¥¼ íŒ. íŒí•œ ë…¸ë“œëŠ” ìµœê·¼ì— ìŠ¤íƒì— í‘¸ì‰¬í•œ ë…¸ë“œ íŒí•œ ë…¸ë“œì˜ ë°©ë¬¸ ì—¬ë¶€ í™•ì¸ ì•„ì§ ë°©ë¬¸ ì•ˆí–ˆìœ¼ë©´ -> ë…¸ë“œë¥¼ ë°©ë¬¸ì²˜ë¦¬ ë°©ë¬¸í•œ ë…¸ë“œì™€ ì¸ì ‘í•œ ëª¨ë“  ë…¸ë“œë¥¼ í™•ì¸ ì•„ì§ ë°©ë¬¸í•˜ì§€ ì•Šì€ ë…¸ë“œë¥¼ ìŠ¤íƒì— í‘¸ì‰¬. ê³ ë ¤ì‚¬í•­ : íƒìƒ‰í•  ë…¸ë“œê°€ ì—†ì„ ë•Œê¹Œì§€ ê°„ì„ ì„ íƒ€ê³  ë‚´ë ¤ê°ˆ ìˆ˜ ìˆì–´ì•¼ í•¨ ê°€ì¥ ìµœê·¼ì— ë°©ë¬¸í•œ ë…¸ë“œë¥¼ ì•Œì•„ì•¼ í•¨ ì´ë¯¸ ë°©ë¬¸í•œ ë…¸ë“œì¸ì§€ í™•ì¸í•  ìˆ˜ ìˆì–´ì•¼ í•¨ > ê°€ì¥ ê¹Šì€ ë…¸ë“œê¹Œì§€ ë°©ë¬¸í•œ í›„ì— ë” ì´ìƒ ë°©ë¬¸í•  ë…¸ë“œê°€ ì—†ìœ¼ë©´ ìµœê·¼ ë°©ë¬¸í•œ ë…¸ë“œë¡œ ëŒì•„ì˜¨ ë‹¤ìŒ, í•´ë‹¹ ë…¸ë“œì—ì„œ ë°©ë¬¸í•  ë…¸ë“œê°€ ìˆëŠ”ì§€ í™•ì¸í•œë‹¤. back tracking : íƒìƒ‰í•˜ê³  ìˆëŠ” ë°©í–¥ì˜ ì—­ë°©í–¥ìœ¼ë¡œ ë˜ëŒì•„ê°€ëŠ” ë™ì‘ ìŠ¤íƒì€ ìµœê·¼ì— í‘¸ì‰¬í•œ ë…¸ë“œë¥¼ íŒ ì—°ì‚°ìœ¼ë¡œ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ íŠ¹ì • ë…¸ë“œë¥¼ ë°©ë¬¸í•˜ê¸° ì „ì— ìµœê·¼ ë°©ë¬¸ë…¸ë“œë¥¼ íŒ ì—°ì‚°ìœ¼ë¡œ ì‰½ê²Œ í™•ì¸í•  ìˆ˜ ìˆìŒ. ìŠ¤íƒì„ í™œìš©í•œ êµ¬í˜„ / ì¬ê·€ë¥¼ í™œìš©í•œ êµ¬í˜„ ìŠ¤íƒì„ í™œìš©í•œ ê¹Šì´ ìš°ì„  íƒìƒ‰ : <figure> <img src=\"/post/CodilityStudy/DFS_ê·¸ë ¤ë³´ê¸°.png\" alt=\"DFS\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">ê·¸ë¦¼2. DFS stack êµ¬í˜„</figcaption> </figure> ì¬ê·€ í•¨ìˆ˜ë¥¼ í™œìš©í•œ ê¹Šì´ ìš°ì„  íƒìƒ‰ : > ì¬ê·€ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•  ë•Œë§ˆë‹¤ í˜¸ì¶œí•œ í•¨ìˆ˜ëŠ” ì‹œìŠ¤í…œ ìŠ¤íƒì´ë¼ëŠ” ê³³ì— ìŒ“ì´ë¯€ë¡œ ê¹Šì´ ìš°ì„  íƒìƒ‰ì— í™œìš©í•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤. <figure> <img src=\"/post/CodilityStudy/DFS_ì¬ê·€.png\" alt=\"DFS\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">ê·¸ë¦¼3. DFS ì¬ê·€ êµ¬í˜„</figcaption> </figure> stackìœ¼ë¡œ êµ¬í˜„í•˜ëŠ” ê²ƒê³¼ ë…¼ë¦¬ëŠ” ê°™ìŒ. ìŠ¤íƒì— ìŒ“ì•„ì£¼ëŠ” ëŒ€ì‹  ì´ì›ƒë…¸ë“œì— ëŒ€í•œ ì¬ê·€í•¨ìˆ˜ë¥¼ ì‹¤í–‰ì‹œì¼œì£¼ëŠ” ë°©ì‹. BFS(Breadth-First Search) : ë„ˆë¹„ ìš°ì„  íƒìƒ‰ Queue ìë£Œêµ¬ì¡° í™œìš© <figure> <img src=\"/post/CodilityStudy/queue.webp\" alt=\"queue\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">ê·¸ë¦¼4. queue ìë£Œêµ¬ì¡° : FIFO(First In First Out) êµ¬ì¡°</figcaption> </figure> <figure> <img src=\"/post/CodilityStudy/BFS_í.png\" alt=\"BFS\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">ê·¸ë¦¼5. BFS í êµ¬í˜„</figcaption> </figure>",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/ê·¸ë˜í”„_ê°œë…ì •ë¦¬",
    "title": "ê·¸ë˜í”„ ê°œë…ì •ë¦¬",
    "date": "2025-08-01",
    "excerpt": "ê·¸ë˜í”„ ê°œë…ì •ë¦¬",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study",
      "ìë£Œêµ¬ì¡°",
      "ê·¸ë˜í”„"
    ],
    "content": "ê·¸ë˜í”„ë€? ì •ì˜ : > ê·¸ë˜í”„ëŠ” ë…¸ë“œ (vertex)ì™€ ê°„ì„  (edge)ì„ ì´ìš©í•œ ë¹„ì„ í˜• ë°ì´í„° êµ¬ì¡° ë°ì´í„°ì˜ ê´€ê³„ë¥¼ í‘œí˜„í•˜ëŠ”ë° ì‚¬ìš© ê·¸ë˜í”„ì˜ íŠ¹ì§•ê³¼ ì¢…ë¥˜ : ë°©í–¥ì„± ì¡´ì¬ ì—¬ë¶€ ë°©í–¥ ê·¸ë˜í”„ (directed graph) : ê°„ì„ ì— ë°©í–¥ì„±ì´ ìˆëŠ” ê·¸ë˜í”„ ë¬´ë°©í–¥ ê·¸ë˜í”„ (undirected graph) : ê°„ì„ ì— ë°©í–¥ì„±ì´ ì—†ëŠ” ê·¸ë˜í”„ ê°„ì„ ì˜ ê°€ì¤‘ì¹˜ ì¡´ì¬ ì—¬ë¶€ ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ (weighted graph) : ê°„ì„ ì— ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ê·¸ë˜í”„ ë¬´ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ (unweighted graph) : ê°„ì„ ì— ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê·¸ë˜í”„ ìˆœí™˜ ì—¬ë¶€ ìˆœí™˜ ê·¸ë˜í”„ (cyclic graph) : ê°„ì„ ì„ ë”°ë¼ ì´ë™í•˜ë©´ ì¶œë°œì ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ” ê·¸ë˜í”„ ë¹„ìˆœí™˜ ê·¸ë˜í”„ (acyclic graph) : ê°„ì„ ì„ ë”°ë¼ ì´ë™í•˜ë©´ ì¶œë°œì ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê·¸ë˜í”„ ê·¸ë˜í”„ êµ¬í˜„ : ê·¸ë˜í”„ì˜ í‘œí˜„ ë°©ë²• ì¸ì ‘ í–‰ë ¬ (adjacency matrix) : 2ì°¨ì› ë°°ì—´ë¡œ ê·¸ë˜í”„ì˜ ì—°ê²° ê´€ê³„ë¥¼ í‘œí˜„ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ (adjacency list) : ì—°ê²° ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ë˜í”„ì˜ ì—°ê²° ê´€ê³„ë¥¼ í‘œí˜„ > ì…ë ¥ê°’ì´ ì ê±°ë‚˜ íŠ¹ì • ì •ì  ì‚¬ì´ì˜ ê°„ì„  ì—¬ë¶€ë¥¼ ë¹ ë¥´ê²Œ íŒë‹¨í•´ì•¼ í•  ë•ŒëŠ” ì¸ì ‘ í–‰ë ¬ì„, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤. ì¸ì ‘í–‰ë ¬ ë…¸ë“œ ê°„ì˜ ê´€ê³„ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ [ì •ì (v), ê°€ì¤‘ì¹˜(w), ë‹¤ìŒ ë…¸ë“œ(next)] ë¡œ êµ¬ì„±ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ë˜í”„ë¥¼ í‘œí˜„. ë…¸ë“œì— ëŒ€í•œ ë°°ì—´ì´ ìˆê³ , ë…¸ë“œë³„ë¡œ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ë‹¤. ì¥ë‹¨ì  ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œê°„ë³µì¡ë„ ê¸°íƒ€ --- --- --- --- ì¸ì ‘ í–‰ë ¬ $O(N^2)$ $O(1)$ êµ¬í˜„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ $O(N+E)$ $O(N)$ ì¸ì ‘ í–‰ë ¬ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ì˜ ì¥ì  : ì •ë³´ë¥¼ í™•ì¸í•  ë–„ ì‹œê°„ ë³µì¡ë„ê°€ O(1)ë¡œ ì¢‹ìŒ ì¸ì ‘ í–‰ë ¬ì˜ ë‹¨ì  : ì¸ì ‘ í–‰ë ¬ë¡œ í¬ì†Œ ê·¸ë˜í”„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš° -> êµ‰ì¥íˆ ë¹„íš¨ìœ¨ì . ë©”ëª¨ë¦¬ ë‚­ë¹„. ë…¸ë“œë“¤ì˜ ê°’ì˜ ì°¨ì´ê°€ ë§¤ìš° í° ê·¸ë˜í”„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš° -> ê°€ì¥ í° ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¸ì ‘ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ì¡ì•„ì•¼ í•˜ë‹ˆê¹Œ. ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ (ì¸ì ‘ í–‰ë ¬ê³¼ ë°˜ëŒ€) ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ì˜ ì¥ì  : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ ë…¸ë“œë“¤ì˜ ê°’ì˜ ì°¨ì´ê°€ ë§¤ìš° í° ê·¸ë˜í”„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš° ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ì ìŒ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ì˜ ë‹¨ì  : ì •ë³´ë¥¼ í™•ì¸í•  ë–„ ì‹œê°„ ë³µì¡ë„ê°€ O(n)ìœ¼ë¡œ ë¹„íš¨ìœ¨ì .",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-31-ì¢€ë¶ˆí¸í•´ë„ë§ˆìŠ¤í¬ì“¸ê±°ì•¼",
    "title": "ì¢€ ë¶ˆí¸í•´ë„ ë§ˆìŠ¤í¬ ì“¸ ê±°ì•¼",
    "date": "2025-07-31",
    "excerpt": "ì¢€ ë¶ˆí¸í•´ë„ ë§ˆìŠ¤í¬ ì“¸ ê±°ì•¼",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ What if I catch the flu? Even if it's inconvinient, I will still wear a mask. Even if you mess up, we'll still believe in you. No matter what happens, I'll always believe in you. be there for you. ê²ë‚˜ ë¡œë§¨í‹±í•˜ê²Œ ëŠê»´ì§€. Failures are okay as long as youy learn from then. ê°„ë‹¨í•œ ì‘ë¬¸ Even if I'm tired, I'll practice English. No matter what happens, I'll always be there for you.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-30-ê³ ê¹ƒì§‘ì—ê°€ë„ë‚œìƒê´€ì—†ì–´",
    "title": "ê³ ê¹ƒì§‘ì— ê°€ë„ ë‚œìƒê´€ì—†ì–´",
    "date": "2025-07-30",
    "excerpt": "ê³ ê¹ƒì§‘ì— ê°€ë„ ë‚œìƒê´€ì—†ì–´",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Sharing is caring. ê´€ì‹¬ ìˆìœ¼ë©´ ì•Œë ¤ì¤˜ ê·¸ë ‡ê²Œ í•´ë„ ìƒê´€ì—†ì–´ Let me know. ì•Œë ¤ì¤˜. ì—°ë½í•´. Let me know if you're interested. ê´€ì‹¬ ìˆìœ¼ë©´ ì•Œë ¤ì¤˜. change your mind. hange out : ê°™ì´ ì‹œê°„ì„ ë³´ë‚´ë‹¤. I don't mind if we go clubbing. ê°„ë‹¨í•œ ì‘ë¬¸ I totally agree about the sentence \"Sharing is caring.\" I'm willing to share my insights or knowledge with my pioneers and friends. Today I learned the phares that is used in the conversation. Let me know if you wonder my codes. Then I will explain it. And my food type (ì‹ì„±) is so normal. I like almost foods. So I always say to my friends that I don't mind if we have chinese foods. í”¼ë“œë°± ë¬¸ë²• ì˜¤ë¥˜ \"pioneers\" â†’ \"peers\" (ë™ë£Œë¥¼ ì˜ë¯¸í•  ë•Œ) \"phares\" â†’ \"phrases\" (ì² ì ì˜¤ë¥˜) \"that is\" â†’ \"that are\" (phrasesëŠ” ë³µìˆ˜) \"wonder my codes\" â†’ \"wonder about my code\" (codeëŠ” ë³´í†µ ë‹¨ìˆ˜í˜• ì‚¬ìš©) \"almost foods\" â†’ \"almost all foods\" (ê±°ì˜ ëª¨ë“  ìŒì‹) \"chinese foods\" â†’ \"Chinese food\" (ìŒì‹ì€ ë³´í†µ ë‹¨ìˆ˜í˜•) í‘œí˜„ ê°œì„  \"I totally agree about\" â†’ \"I totally agree with\" \"I'm willing to share\" â†’ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ê°œì„  \"Let me know if you wonder\" â†’ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ê°œì„  ê°œì„ ëœ ë²„ì „ I totally agree with the saying \"Sharing is caring.\" I'm always happy to share my insights and knowledge with my peers and friends. Today I learned some useful phrases that are commonly used in conversations. If you're curious about my code, feel free to ask. I'd be happy to explain it to you. As for my food preferences, I'm quite flexible. I enjoy almost all types of food. That's why I always tell my friends that I don't mind going to Chinese restaurants.",
    "public": true
  },
  {
    "id": "Self_Development/Career/site-recommendation",
    "title": "ì·¨ì—… ì¤€ë¹„ ì‚¬ì´íŠ¸ ì¶”ì²œ",
    "date": "2025-07-30",
    "excerpt": "ì·¨ì—… ì¤€ë¹„ ì‚¬ì´íŠ¸ ì¶”ì²œ",
    "category": "Career",
    "tags": [
      "ì·¨ì—…"
    ],
    "content": "ì·¨ì—… ì¤€ë¹„ ì‚¬ì´íŠ¸ ì¶”ì²œ zighang offercent bzpp ë§í¬ë“œì¸ ì±„ìš©ê³µê³ ë‚˜ ì±„ìš©í•˜ëŠ” í”¼ë“œ ì°¸ê³ í•˜ê¸°",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-29-ë„ˆì•„ë‹ˆì—ˆìœ¼ë©´",
    "title": "ë„ˆ ì•„ë‹ˆì—ˆìœ¼ë©´",
    "date": "2025-07-29",
    "excerpt": "ë„ˆ ì•„ë‹ˆì—ˆìœ¼ë©´, ë„ˆë¼ë©´ ì–´ë–»ê²Œ í–ˆê² ì–´?",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ê±° ì •ë¦¬ match maker ë¬¸ë²•. í˜„ì¬ì™€ ë‹¤ë¥¸ ì–´ë–¤ ìƒí™©ì„ ê°€ì •í•  ë•ŒëŠ” ê³¼ê±°ì™„ë£Œë¥¼ ì‚¬ìš©. If it hadnt been for you, I wouldn't have made it. would've -> /wÊŠdÉ™v/ (woul-duv) wouldn't -> /wÊŠdnt/ (woul-dnt) wou-ov ë°œìŒ ê¸°í˜¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤! ğŸ“ ë°œìŒ ê°€ì´ë“œ would've: /wÊŠdÉ™v/ (woul-duv) wÊŠ = \"woul\" (woolê³¼ ë¹„ìŠ·) dÉ™v = \"duv\" (duh + v ì†Œë¦¬) wouldn't: /wÊŠdnt/ (woul-dnt) wÊŠ = \"woul\" (woolê³¼ ë¹„ìŠ·) dnt` = \"dnt\" (d + n + të¥¼ ë¹ ë¥´ê²Œ ì—°ê²°) ì‹¤ì œë¡œëŠ” \"would have\"ì™€ \"would not\"ì´ ì¶•ì•½ë˜ì–´ ë” ì§§ê³  ë¹ ë¥´ê²Œ ë°œìŒë˜ëŠ” í˜•íƒœì…ë‹ˆë‹¤. Would you have moved to New York?",
    "public": true
  },
  {
    "id": "Self_Development/Career/how-to-write-resume",
    "title": "ì´ë ¥ì„œ, ê²½ë ¥ê¸°ìˆ ì„œ ì‘ì„± ë°©ë²• ì •ë¦¬",
    "date": "2025-07-29",
    "excerpt": "ì´ë ¥ì„œ, ê²½ë ¥ê¸°ìˆ ì„œ ì ëŠ” ë°©ë²• ì •ë¦¬",
    "category": "Career",
    "tags": [
      "ì´ë ¥ì„œ"
    ],
    "content": "ì°¸ê³ ë§í¬: ìœ íŠœë¸Œ ì´í˜• - ê²½ë ¥ê¸°ìˆ ì„œ ì‘ì„±ë²• ì´ì •ë¦¬ ì´ì§ì´ ì²˜ìŒì´ë¼ë©´ ì´ê²ƒë¶€í„° ë³´ì„¸ìš” ì´ì§ë°”ì´ë¸” 137p ~ 187p ê²½ë ¥ê¸°ìˆ ì„œ ì‘ì„±ë°©ë²• : ìµœê·¼ê²½ë ¥ - ì§€ì›ì§ë¬´ì— ë§ê²Œ ë¨¼ì € ì œì¶œ ì§ë¬´ì™€ ë¬´ê´€í•œ ê²½í—˜ì— ëŒ€í•´ì„œëŠ” ê³¼ê°í•˜ê²Œ ì‚­ì œ ì—°ì°¨ë³„ 1ì¹¸ì”©ì€ í•„ìˆ˜ ê°€ê¸‰ì  10ê°œ ë¯¸ë§Œ ê²½ë ¥ê¸°ìˆ ì„œì˜ ë‚´ìš© : êµ¬ë¶„ ì‘ì„± ë°©ë²• --- --- ê¸°ê°„ í”„ë¡œì íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª… ì„±ê³¼ (1ì¤„ ì´ë‚´) How + Result -> KPI ì—­í•  (5ì¤„ ì´ë‚´) 1. ë¬¸ì œ í•´ê²° ê´€ì : í•µì‹¬ ë¬¸ì œ í•´ê²° ê³¼ì •ì—ì„œì˜ ì—­í• <br>2. ìƒì‚°ì„± í–¥ìƒ ê´€ì : ì—…ë¬´ íš¨ìœ¨ì„± ê°œì„ ì—ì„œì˜ ì—­í• <br>3. ì˜ì‚¬ì†Œí†µ ê´€ì : íŒ€ í˜‘ì—… ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—ì„œì˜ ì—­í•  ê¸°ìˆ  1. ë„êµ¬/íˆ´: ì‚¬ìš©í•œ ê¸°ìˆ  ìŠ¤íƒ<br>2. í”„ë¡œì„¸ìŠ¤ ì´í•´: ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì´í•´ë„ ë¬¼ê²½ë ¥ ì§„ë‹¨ ë¦¬ìŠ¤íŠ¸: ìˆ«ìë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì„±ê³¼ê°€ ì—†ë‹¤. ë‚´ ì§ë¬´ KPIê°€ ë¬´ì—‡ì¸ì§€ ëª¨ë¥¸ë‹¤ ì§„í–‰í•œ í”„ë¡œì íŠ¸/ì—…ë¬´ì˜ Before/Afterê°€ ëª…í™•í•˜ì§€ ì•Šë‹¤. ì„±ê³¼ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì—­í• ì„ ìˆ˜í–‰í–ˆëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ì—†ë‹¤. ì—°ì† ê·¼ë¬´ ê¸°ê°„ì´ 1ë…„ ë¯¸ë§Œì´ë‹¤. ìµœê·¼ 3ë…„ ë‚´ 2ê³³ ì´ìƒìœ¼ë¡œ ì´ì§í–ˆë‹¤. ë™ê¸°ë‚˜ ë™ì¼ ì—°ì°¨ì— ë¹„í•´ ìŠ¹ì§„ì´ ëŠ¦ë‹¤. ê²½ë ¥ ê¸°ìˆ ì„œ ì‘ì„± í”„ë¡œì„¸ìŠ¤ : ê²½ë ¥ ê¸°ìˆ ì„œë¥¼ ìœ„í•œ ì†Œì¬ ì¤€ë¹„í•˜ê¸° : ê²½í—˜ ë¦¬ìŠ¤íŠ¸ì—… ê²½í—˜ ë¦¬ìŠ¤íŠ¸ì—… í…œí”Œë¦¿ì„ í™œìš©í•˜ì—¬ ì£¼ìš” í™œë™ ì •ë¦¬í•˜ê¸° KPI ë§¤ì¹­í•˜ê¸° ê²½ë ¥ ê¸°ìˆ ì„œ ì‘ì„±í•˜ê¸° í”„ë¡œì íŠ¸ëª… ê¸°ê°„ ì„±ê³¼ ì—­í•  ê¸°ìˆ  > ê°„í˜¹ ê²½ë ¥ ê¸°ìˆ ì„œë¥¼ êµ¬ì²´ì ì´ê³  ê¸¸ê²Œ ì„œìˆ í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, ê²½ë ¥ ê¸°ìˆ ì„œì˜ ëª©ì  ìì²´ê°€ í•µì‹¬ì„ ê°„ê²°í•˜ê²Œ ë³´ê¸° ìœ„í•¨ì´ê¸° ë•Œë¬¸ì— > ìš°ë¦¬ëŠ” ë©´ì ‘ê´€ì´ ê¶ê¸ˆí•´ì•¼í•  ë‚´ìš©ë§Œ ì‘ì„±í•˜ì. ì˜ˆì‹œ ì§ë¬´ : MD í”„ë¡œì íŠ¸ëª…/ì—…ë¬´ëª… : ëª…ì ˆ ì‹œì¦Œ ë¼ì´ë¸Œ íŒë§¤ ë°©ì†¡ìœ¼ë¡œ ë§¤ì¶œ 154% ê²¬ì¸ ê¸°ê°„ : 2010ë…„ 9ì›” ~ 2020ë…„ 10ì›” (2ê°œì›”) ì„±ê³¼ : 5000ëª…ì˜ ê´€ì‹¬ê³ ê° CRM ë¶„ì„ì„ í†µí•œ ëª…ì ˆ íŒë§¤ ë°©ì†¡ìœ¼ë¡œ ë¼ì´ë¸Œ ë§¤ì¶œ 1ì–µ ë‹¬ì„± ì—­í•  : ê²½ìŸì‚¬ 100ê°œ ìƒí’ˆì¡°ì‚´ë¥´ ë°”íƒ•ìœ¼ë¡œ 3ë§Œì› ëŒ€ ì„¸íŠ¸ ìƒí’ˆì„ ì œì•ˆí•´ ê°ë‹¨ê°€ 50% ì¦ê°€ 5000ëª… ê´€ì‹¬ê³ ê° ëŒ€ìƒìœ¼ë¡œ CRM ë§ˆì¼€íŒ… ì§„í–‰ìœ¼ë¡œ ìœ ì… 300% ì¦ê°€ 100ê°œ ë²¤ë”ì‚¬ ë¶„ì„ì„ í†µí•´ ë²¤ë”ì‚¬ ì„ ì •ìœ¼ë¡œ ê¸°íš ë‹¨ê°€ 30% ì ˆê° ê¸°ìˆ  : Jira, Data Studio, Slack ë¼ì´ë¸Œ ì»¤ë¨¸ìŠ¤ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì´í•´ ì§ë¬´: í’ˆì§ˆê´€ë¦¬ í”„ë¡œì íŠ¸ëª…/ì—…ë¬´ëª… : ëŒ€ë§Œ ê³µì¥ ìƒì‚° ê°€ë™ ê¸°ê°„ : 2019.10 ~ 2020.10 (1ë…„) ì„±ê³¼ : ì‹ ê·œ í’ˆì§ˆê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ í†µí•´ ëŒ€ë§Œ ê³µì¥ ìƒì‚° ì œí’ˆ ë¶ˆëŸ‰ë¥  80% ê°ì†Œ ì—­í•  : ë¶ˆëŸ‰í’ˆ 100% ì „ìˆ˜ê²€ì‚¬ë¥¼ í†µí•´ ì™„ì œí’ˆ ê²°í•¨ ë¹„ìš© 30% ê°ì†Œ ë°˜ë³µë˜ëŠ” ì ‘ì°© ê²°í•¨ ì´ìŠˆ í”„ë¡œì„¸ìŠ¤ ê°œì„ ì„ í†µí•œ ê²€ì‚¬ ì‹œê°„ 10% ê°ì†Œ ìœ íš¨ê¸°ê°„ ë§Œë£Œëœ ì™„ì œí’ˆì„ ê²°í•¨ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì•…ì„± ì¬ê³  70% ì²˜ë¦¬ ë¶ˆëŸ‰í’ˆ ë°ì´í„° 100ê°œ ë¶„ì„ì„ í†µí•´ ìƒì‚° ë©”ë‰´ì–¼ ê°œë°œ ë° êµìœ¡ ë¶ˆëŸ‰ ë°œìƒ ì›ì¸ íŒŒì•…ì„ í†µí•´ ì‹ ê·œ ì¥ë¹„ êµ¬ë§¤ë¡œ ìƒì‚°ë¹„ 3% ì ˆê° ê¸°ìˆ  : QMS, ISO 9001 ëŒ€ë§Œ ë²•ë¥ ì— ëŒ€í•œ ì´í•´ ì§ë¬´: ì¬ë¬´íšŒê³„ í”„ë¡œì íŠ¸ëª…/ì—…ë¬´ëª… : ë¶„ê¸° ì¬ë¬´ ì‹¤ì‚¬ ê¸°íš ë° ë¶„ì„ ë³´ê³  ê¸°ê°„ : 2022ë…„ 8ì›” ~ 2022ë…„ 10ì›” (3ê°œì›”) ì„±ê³¼: ë¶„ê¸°ë³„ ì¬ê³ ì¡°ì‚¬ ì´ 10íšŒ ì‹¤ì‹œë¥¼ í†µí•´ ì¬ë¬´ ë°ì´í„° ì¼ì¹˜ìœ¨ 10%p ìƒìŠ¹ ì—­í•  í˜„ì¥ ì¬ê³ ì¡°ì‚¬ë¥¼ í†µí•´ ìë£Œ ì˜¤ì°¨ìœ¨ 0% ë‹¬ì„± ì‹¤ì‚¬ ë‚´ì—­ ë¶„ì„ì„ í†µí•´ ì„¸ë¬´ ì¦ë¹™ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ë³€ê²½ ë° ë©”ë‰´ì–¼ êµìœ¡ ì˜ëª» ê¸°ì¬ëœ ê³„ì • 70% ìˆ˜ì • ì¬ê³ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ ê°œì„ ì„ í†µí•´ ì¬ê³  ê´€ë¦¬ ë¹„ìš© 10% ì ˆê° ê¸°ìˆ  : ERP ìì‚° ìœ í˜•ë³„ ì¬ë¬´ê´€ë¦¬ ë²•ë¥ ì— ëŒ€í•œ ì´í•´ ê²½ë ¥ ê¸°ìˆ ì„œ íƒ¬í”Œë¦¿ ì‚¬ìš© ê°€ì´ë“œ ê¸°ê°„ : ë˜ê²Œ ì¤‘ìš”í•œ ë¶€ë¶„. ê¼­ ë²ˆê±°ë¡­ì§€ ì•Šê²Œ ëª‡ê°œì›” í•œê±´ì§€ë¥¼ ì ì–´ì£¼ì. ê¸°ê°„ : 2022.08 ~ 2022.10 (3ê°œì›”) ì„±ê³¼ : > ê²½ë ¥ê¸°ìˆ ì„œëŠ” ë‚˜ì˜ ì†Œì†ê³¼ ì§ë¬´ë¥¼ ì ëŠ” ì¹¸ì´ ì•„ë‹ˆë‹¤. > ë©´ì ‘ê´€ì€ ê·¼ê±°ë¥¼ í† ëŒ€ë¡œ ì§€ì›ìë“¤ì˜ í•©ë¶ˆì„ ê°€ë¥¸ë‹¤. > ê·¸ ê·¼ê±°ë¡œ ê²°ì •ì ì¸ ìš”ì†Œê°€ ë°”ë¡œ ì„±ê³¼ì´ë‹¤. > ì•ì—ì„œ ì•„ë§ˆì¶”ì–´ì™€ í”„ë¡œí˜ì…”ë„ì˜ ì°¨ì´ë¥¼ ë…¸ë ¥ì„ ê°•ì¡°í•˜ëŠ”ê°€ ê²°ê³¼ë¥¼ ê°•ì¡°í•˜ëŠ”ê°€ë¼ê³  ì„¤ëª…í–ˆë‹¤. > ê²½ë ¥ê¸°ìˆ ì„œì˜ ì„±ê³¼ë¥¼ ì‘ì„±í•˜ëŠ”ê²Œ ë°”ë¡œ ê²°ê³¼ë¥¼ ê°•ì¡°í•˜ëŠ” í–‰ìœ„ì´ë‹¤. > ì„±ê³¼ë¥¼ ë³´ê³  ì§€ì›ìë¥¼ ë½‘ê³  ì‹¶ì€ì§€ë¥¼ ê²°ì •í•œë‹¤ê³  í•´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤. ì„±ê³¼ëŠ” ê°„ê²°í•˜ê²Œ ì‘ì„±í• ìˆ˜ë¡ ì¢‹ë‹¤. ë°˜ë“œì‹œ í•œ ì¤„ë¡œ. How + Result. ì•„ì‰¬ìš´ ì˜ˆì‹œ ì‹ ê·œë¸Œëœë“œ ëŸ°ì¹­ 100ë§Œì› ë‹¬ì„± ì¢‹ì€ ì˜ˆì‹œ ë¸Œëœë“œ ì½œë¼ë³´ë ˆì´ì…˜ì„ í†µí•œ ì‹ ì¬í’ˆ ì¶œì‹œë¡œ ê°ë‹¨ê°€ 1.5ë°° ìƒìŠ¹ ë° ë§¤ì¶œ 300% ìƒìŠ¹ ìˆ˜ìš” ì—ì¸¡ ì˜¤ë¥˜ ë°œê²¬ ë° ë¡œì§ ë³€ê²½ í†µí•œ íŒë§¤ ìˆœìœ„ 100ìœ„ê¶Œ ë‚´ forecast accuracy 5% ìƒìŠ¹ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„. > ì§€ì›ìë“¤ì´ ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ëŠ” ë„ˆë¬´ ë§ì€ ë‚´ìš©ì„ ë‹´ìœ¼ë ¤ëŠ” ê²ƒì´ë‹¤. > ì„±ê³¼ëŠ” ìì„¸í•¨ë³´ë‹¤ í•µì‹¬ì„ ì˜ í‘œí˜„í–ˆëŠ”ê°€ê°€ ì¤‘ìš”í•˜ë‹¤. ì—­í•  : ì–´ë–»ê²Œ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶€ë¶„. > ì—­í• ì´ë¼ê³  ì ì—ˆì§€ë§Œ ì‚¬ì‹¤ì€ ì „ëµì´ë¼ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ ë” ì í•©í•˜ë‹¤. ë¬¸ì œë¥¼ í•´ê²°í•œ ì•¡ì…˜ ìƒì‚°ì„±ì„ ë†’ì¸ ì•¡ì…˜ ì˜ì‚¬ì†Œí†µì„ ì˜í•œ ì•¡ì…˜ > ì—­í• ì„ í›¨ì”¬ ì—£ì§€ìˆê²Œ ì‘ì„±í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. > ê° ì•¡ì…˜ë“¤ì— ìˆ˜ì¹˜í™”ë¥¼ ê°€ë¯¸í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ì‰¬ìš´ ì˜ˆì‹œ ì—­í•  : ì œí’ˆ ì½˜ì…‰íŠ¸ì™€ ìŠ¤í† ë¦¬ë³´ë“œ ë„ì¶œ, ì‹œì¥ë¶„ì„ -> ë‹¹ì—°íˆ í•´ì•¼í•˜ëŠ” ì¼ë“¤ì„ ì ê³  ìˆìŒ ì„±ê³¼ : ì‹ ì œí’ˆ ì¶œì‹œë¥¼ í†µí•´ ê³ ê°ë§Œì¡±ë„ 100% ìƒìŠ¹ ì—­í•  : í•´ì™¸ í”Œë«í¼ ì‹œì¥ ë§ˆì¼€íŒ… ë°©ì•ˆ ì¡°ì‚¬ ê²½ìŸì‚¬ ì½˜í…ì¸ ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•´ ì œí’ˆ ëœë”© í˜ì´ì§€ ë¦¬ë‰´ì–¼ ë° ê´‘ê³  ì†Œì¬ ê¸°íš ë§ˆì¼€íŒ… ì„±ê³¼ ë¶„ì„ -> ì‹ ì œí’ˆ ì¶œì‹œë¼ëŠ” ì•¡ì…˜ì´ ëª¨í˜¸í•˜ê¸° ë•Œë¬¸ì—, ì—­í• ì´ ì„±ê³¼ì™€ ì—°ê²°ë˜ì–´ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤. ê° ì—­í• ë„ ìˆ˜ì¹˜í™”ê°€ ë˜ì§€ ì•Šì•„ì„œ ë‹¹ì—°í•œ ì—…ë¬´ í™œë™ìœ¼ë¡œ í•´ì„ëœë‹¤. ì—­í• ì— ê¸°ìˆ ëœ ë‚´ìš©ì´ ì„±ê³¼ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ ìˆ˜ì¹˜ë¡œ ì„¤ëª…í•´ë³´ì. ì¢‹ì€ ì˜ˆì‹œ ì—­í•  : ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë¶„ì„ì„ í†µí•œ ìƒì‚°ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ ë¡œì§ì„ ì„¤ê²Œí•˜ì—¬ ë¶ˆëŸ‰í’ˆ ê²€ì¦ ìë™í™” ê¸°ëŠ¥ ê°œë°œ ë¶ˆëŸ‰í’ˆ ê²€ì¦ ìë™í™” ì‹œìŠ¤í…œ ê°œë°œì„ í†µí•´ ë¶ˆëŸ‰í’ˆ ê²€ì‚¬ ì‹œê°„ 300% ë‹¨ì¶• ì œí’ˆë³„, ê³ ê°ì‚¬ë³„ 100ê°€ì§€ ë¶ˆëŸ‰í’ˆ ì¼€ì´ìŠ¤ ë¶„ì„ì„ í†µí•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ ë¶ˆëŸ‰í’ˆ ê²€ì‚¬ ë¹„ìš© 30% ì ˆê° -> ë¬¸ì œë¥¼ í•´ê²°í•œ í•µì‹¬ ì•¡ì…˜ê³¼ ìƒì‚°ì„±ì„ ë†’ì¸ ì•¡ì…˜ì„ ìˆ«ìë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê²Œ ë‚˜íƒ€ëƒˆë‹¤. ì—­í•  : ì‹ ê·œ ìœ ì…ì 30%ê°€ ëœë”© í˜ì´ì§€ ì²´ë¥˜ì‹œê°„ì´ 10ì´ˆ ì´ë‚´ì¸ ì ì„ ë¶„ì„ í™ˆí˜ì´ì§€ ìœ ì…ì 5000ëª… ë°ì´í„° ë¶„ì„ìœ¼ë¡œ ëœë”© í˜ì´ì§€ ê°œì„ ì•ˆ ë„ì¶œ A/B í…ŒìŠ¤íŠ¸ë¡œ ê°œì„ ì•ˆ ë””ìì¸ ê²€ì¦ì„ í†µí•´ í´ë¦­ë¥  ë†’ì€ ë¬¸êµ¬, ë²„íŠ¼ ìœ„ì¹˜ íŒŒì•… ë³€ê²½ëœ ëœë”© í˜ì´ì§€ë¡œ ì—°ê³„ ë§¤ì¶œ 100% ì¦ê°€ -> íˆíŠ¸ë§µ ë¶„ì„œê¸°ë¼ëŠ” ë¶„ëª…í•œ ì•¡ì…˜ì´ ìˆê³  ì—­í•­ì— ê¸°ìˆ ëœ ë‚´ìš©ì´ ì—°ê²°ëœë‹¤. ì—­í• ì— ê¸°ìˆ ëœ ë‚´ìš©ì´ ì„±ê³¼ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ ìˆ«ìë¡œ ì˜ í‘œí˜„ë˜ì–´ ìˆë‹¤. > ì—­í• ì„ ì ì„ ë•Œ ê°€ì¥ í”¼í•´ì•¼ í•  ì¼ì€, ë‹¹ì—°íˆ í•´ì•¼ í•˜ëŠ” ì¼ë“¤ì„ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ë‹¤. > ëª©í‘œì™€ ìƒê´€ì—†ì´ ë°˜ë³µí•˜ëŠ” ì¼ë“¤ì€ ì ì„ í•„ìš”ê°€ ì—†ë‹¤. ê¸°ìˆ  : > ë‚´ê°€ ì§€ì›í•˜ëŠ” íšŒì‚¬ì˜ ì •ë³´ë¥¼ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒ í•„ìˆ˜ì ì¸ ê±´ ì•„ë‹˜ ê²½ë ¥ê¸°ìˆ ì„œ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ê´€, ì„±ê³¼, ì—­í• , íšë“ ê¸°ìˆ ì„ ì‘ì„±í•˜ì˜€ëŠ”ê°€? ì„±ê³¼ë¥¼ ì„œìˆ í˜•ì´ ì•„ë‹Œ ê°œì¡°ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ëŠ”ê°€? í•µì‹¬ì ì¸ ë‚´ìš© ì™¸ì— ë‹¤ë¥¸ ë‚´ìš©ì´ ë§ì§€ëŠ” ì•Šì€ê°€? í•´ì„ì´ í•„ìš” ì—†ëŠ” ê°ê´€ì ì´ê³  ì§ê´€ì ì¸ ìˆ«ì í˜¹ì€ ê²°ê³¼ë¬¼ë¡œ ì‘ì„±í•˜ì˜€ëŠ”ê°€? ì„±ê³¼ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë‚˜ì˜ ì—­í• ì„ 3-5ê°€ì§€ë¡œ ê¸°ì¬í–ˆëŠ”ê°€? ì§€ì›í•˜ëŠ” ì§ë¬´, ì‚°ì—…, íšŒì‚¬ì™€ ì¼ì¹˜ë„ê°€ ë–¨ì–´ì§€ëŠ” ê²½ë ¥ ë‚´ìš©ì„ ì œê±°í•˜ì˜€ëŠ”ê°€? ê²½ë ¥ ê¸°ìˆ ì„œì— ê¸°ìˆ í•œ ê²½ë ¥ìˆ˜ê°€ ì´ ê²½ë ¥ê¸°ê°„ / 2 ë³´ë‹¤ ë§ì€ê°€?",
    "public": true
  },
  {
    "id": "Self_Development/Career/Resumes/25-07-29-hyundai-autoever-ml-engineer-resume",
    "title": "[íƒˆë½!] Hyundai Autoever - ML Engineer Resume",
    "date": "2025-07-29",
    "excerpt": "í˜„ëŒ€ìë™ì°¨ ì´ë ¥ì„œ",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³ ",
      "ì´ë ¥ì„œ"
    ],
    "content": "ì±„ìš©ê³µê³ ì™€ ë¶„ì„ë‚´ìš© í•„ìš”í•œ ê²½í—˜ë§Œ ì‚´ë ¤ì„œ ì§§ê³  êµµê²Œ ì“°ê¸°. 2ì¥ì •ë„ ë¶„ëŸ‰ í•„ì‚´ê¸°? ìœ ì‚¬ê²½í—˜ ğŸ” ì´ëŸ° ë¶„ê³¼ í•¨ê»˜ í•˜ê³  ì‹¶ì–´ìš” -> ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ í”„ë¡œì íŠ¸ëª…: ì±—ë´‡ ì‹œìŠ¤í…œ ê°œë°œ ë° ì„œë¹„ìŠ¤ ìš´ì˜ ê¸°ê°„: 2024.11 ~ 2025.02 (4ê°œì›”) ì„±ê³¼: RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë° ìµœì í™”ë¡œ ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ëŒ€ì²´ ë° ì‚¬ìš©ì ë§Œì¡±ë„ 90% ì´ìƒ ë‹¬ì„± ì—­í• : ì¿¼ë¦¬ ë¶„í•´ ë° ì¬ì‘ì„±, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ 90% ì´ìƒ ë‹¬ì„± ì§ˆë¬¸ ë¶„ë¥˜ ëª¨ë¸ ë„ì…ìœ¼ë¡œ ë¶ˆì™„ì „ ì‘ë‹µë¥ ì„ ë‚®ì¶° ì‚¬ìš©ì ë¶ˆë§Œ 70% ê°ì†Œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§(Instruct, Role, Context ê¸°ë°˜) ìµœì í™”ë¡œ ìƒë‹´ í†¤ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„± ë° ë‹µë³€ ì •í™•ë„ 90% ì´ìƒ ë‹¬ì„± Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ìš´ì˜, ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ì‹œìŠ¤í…œ êµ¬í˜„ìœ¼ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ 40% í–¥ìƒ LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ ìë™ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ë¹„ìš© 80% ê°ì†Œ ì¹´ì¹´ì˜¤í†¡ ì±„ë„ ì—°ë™ì„ í†µí•œ ì‹¤ì œ ë³‘ì› ìƒë‹´ ì„œë¹„ìŠ¤ ìš´ì˜ìœ¼ë¡œ ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ì±—ë´‡ ëŒ€ì²´ ê¸°ìˆ : Python, FastAPI, Transformers, Qdrant, Docker, AWS RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ìš´ì˜, ì±—ë´‡ ì„œë¹„ìŠ¤ í’ˆì§ˆ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì´í•´ --- ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì í”„ë¡œì íŠ¸ëª…: OCR íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê°œë°œ ê¸°ê°„: 2024.08 ~ 2024.10 (3ê°œì›”) ì„±ê³¼: OCR íŒŒì´í”„ë¼ì¸ ìµœì í™”ë¡œ ì •í™•ë„ 99% ë‹¬ì„± ë° ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œ ìš´ì˜, ìë™í™”ëœ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ìë™í™” ì—­í• : ì´ë¯¸ì§€ ìœ„ì¹˜ ê³ ì • íŠ¹ì„±ì„ í™œìš©í•œ ì „ì²˜ë¦¬ ë° text recognition íŠ¹í™” ëª¨ë¸ ë„ì…, í›„ì²˜ë¦¬ ë¡œì§ êµ¬í˜„ìœ¼ë¡œ OCR ì •í™•ë„ 99% ë‹¬ì„± ì´ë¯¸ì§€ ì†¡ìˆ˜ì‹ ë¶€í„° OCR ì²˜ë¦¬, DB ì ì¬ê¹Œì§€ ê° ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•´ ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œ ë‹¬ì„± ë° ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨ 10ë°° ì¦ê°€ ëª¨ë¸ ì •í™•ë„ ë° ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìë™í™”ë¥¼ ìœ„í•´ í†µê³„ì  ë¶„í¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê°œë°œ ê³ ê°ë²ˆí˜¸ ë“± ì¼ê´€ì„± ê²€ì¦ ë¡œì§ ê°œë°œ ë° ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶• ìë™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê°œë°œë¡œ ê° ê°ì²´ë³„ ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìë™í™” ê¸°ìˆ : Python, Transformers, OpenCV, gRPC, FastAPI, Triton Server, Docker OCR ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ìš´ì˜, ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬ ë° ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•œ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì´í•´ --- ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì í”„ë¡œì íŠ¸ëª…: ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ ë³‘ë³€ íƒì§€ ëª¨ë¸ë§ ë° ì„œë¹„ìŠ¤ ê°œë°œ ê¸°ê°„: 2024.04 ~ 2024.07 (4ê°œì›”) ì„±ê³¼: Contrastive Learning ë° Multi-Task Learning ì ìš©ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ì¼ë°˜í™” ì„±ëŠ¥ 10% ì´ìƒ í–¥ìƒ ë° Dice Score 0.8 ë‹¬ì„± ì—­í• : U-Net ê¸°ë°˜ segmentation ëª¨ë¸ ì„¤ê³„ ë° ìµœì í™”ë¥¼ í†µí•´ ê¸°ì¡´ ëŒ€ë¹„ ì„±ëŠ¥ 10-15% í–¥ìƒ CLAHE ì „ì²˜ë¦¬ ê¸°ë²• ì ìš©ìœ¼ë¡œ ë°ì´í„° íŠ¹ì§• ëª…í™•í™”ì„ í†µí•´ ë¯¼ê°ë„ 10% í–¥ìƒ Contrastive Learningê³¼ Multi-Task Learningì„ í™œìš©í•´ ì œí•œëœ ë°ì´í„°(1,500ê°œ) í™˜ê²½ì—ì„œ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ 10%ì´ìƒ ê°œì„  Triton Serverë¥¼ í†µí•œ ê³ ì„±ëŠ¥ ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ì¶”ë¡  ì†ë„ 20% í–¥ìƒ ê¸°ìˆ : U-Net, OpenCV, CLAHE, PyTorch, Contrastive Learning, Multi-Task Learning, Triton Server, TensorRT, gRPC ì œí•œëœ ë°ì´í„° í™˜ê²½ì—ì„œì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ ë°©ë²•ë¡  ë° ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œì— ëŒ€í•œ ì´í•´ --- ì‚¬ì „ì§ˆë¬¸ (ì„ íƒ) ì´ë ¥ì„œ ë‚´ìš© ì™¸ì— ì¶”ê°€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê±°ë‚˜ ì–´í•„í•˜ê³ ì í•˜ëŠ” ë‚´ìš©ì´ ìˆìœ¼ì‹œë©´ ììœ ë¡­ê²Œ ì‘ì„± ë¶€íƒë“œë¦½ë‹ˆë‹¤. (ìµœëŒ€ 1,000ì) ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë°ì´í„°ê³¼í•™ìë¡œì„œ ì¼í•˜ë©´ì„œ ì±—ë´‡ ê°œë°œë¶€í„° ìˆ˜ìˆ  ê²½ê³¼ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œê¹Œì§€ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë©° ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ê³  ìš´ì˜í•˜ëŠ” ê²½í—˜ì„ í•˜ë©´ì„œ ì¦ê²ê²Œ ì¼ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ê³  ëª¨ë¸ì„ ë§Œë“¤ë©´ì„œ ê°€ì¥ ë§ì´ ì‹œê°„ì„ ì“°ëŠ” ë¶€ë¶„ì€ ì—­ì‹œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ì œí•˜ëŠ” ê³¼ì •ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ ë°ì´í„° í’ˆì§ˆì„ ê´€ë¦¬í•˜ëŠ” ì¼ì€ ê°€ì¥ ë§ì€ ë…¸ë ¥ì„ ìŸëŠ” ì¼ì…ë‹ˆë‹¤. ì´ìƒì¹˜ëŠ” ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì´ë©° ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ì§€ì™€ ë°ì´í„° ì•ˆì— ì¡´ì¬í•˜ëŠ” ë…¸ì´ì¦ˆëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•´ì•¼í•˜ëŠ”ì§€ ë“±ë“± ë§ì€ ê³ ë¯¼ê³¼ ì‹œí–‰ì°©ì˜¤ ëì— í•˜ë‚˜ì”© í•´ê²°í•´ ë‚˜ê°€ëŠ” ê³¼ì •ì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì œê°€ êµ¬ì¶•í•œ OCR íŒŒì´í”„ë¼ì¸ì„ í†µí•´ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ ê·¸ ì™¸ì— ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°ë“¤ì„ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì ì¬í•˜ê³  ì´ë¥¼ ì •ì œí•´ ë°ì´í„° ë§ˆíŠ¸ì™€ í”¼ì²˜ ìŠ¤í† ì–´ë¥¼ ê°œë°œí•˜ëŠ” ê²½í—˜ì„ í•˜ë©° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•œ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ëŠ” ë²•ì„ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ê°œë…ì„ ë°°ì›Œ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ë©° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì¼ì€ ê½¤ë‚˜ ì„¤ë ˆëŠ” ì¼ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. segmentation ëª¨ë¸ì„ ê°œë°œí•˜ë©´ì„œ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ contrast learningê³¼ multi-task learningì— ëŒ€í•œ ê°œë…ì„ ê³µë¶€í•˜ê³  ë…¼ë¬¸ë“¤ì„ ì°¸ê³ í•´ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ìµœì í™”í•˜ëŠ” ê²½í—˜ì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•´ì£¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ë©´ì„œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„ íƒ ë¬¸ì œì— ê°€ì§€ëŠ” í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡ ì— ëŒ€í•´ ê³µë¶€í–ˆê³ , ì´ë¥¼ ì„œë¹„ìŠ¤ì— ì ìš©í•œ ê²½í—˜ì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ë§‰íˆëŠ” ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ëŠ” ê²½í—˜ì€ ê½¤ë‚˜ ì¦ê²ê³  ì¬ë¯¸ìˆì—ˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ ì €ëŠ” ê¸¸ì§€ ì•Šì€ ê¸°ê°„ì´ì§€ë§Œ ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ë¥¼ ê²½í—˜í•˜ë©° ë§ì€ ë¬¸ì œë¥¼ ë§ˆì£¼í–ˆê³  ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì´í•´í•˜ê³  í™œìš©í•˜ë©´ì„œ ì´ë¥¼ í•´ê²°í•˜ëŠ” ê²½í—˜ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. í›„ê¸° ë–¨ë¦¬ëŠ” êµ¬ë§Œ... ì¼ë‹¨ ê¸°ëŒ€ëŠ” 0... ë–¨ì–´ì§€ë©´ ë–¨ì–´ì§€ëŠ”ê±°ì§€ ë­ ã….ã… ê²°ê³¼ -> íƒˆë½ 1ì°¨ì—ì„œ ê³„ì† ë–¨ì–´ì§€ëŠ”êµ¬ë§Œ.. ì™œ ë–¨ì–´ì§€ëŠ”ê±¸ê¹Œ? [Tech] Machine Learning Engineer - AI ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ì„œë¥˜ì „í˜• ê²°ê³¼ë¥¼ ì•ˆë‚´ë“œë¦½ë‹ˆë‹¤. ì•ˆë…•í•˜ì‹­ë‹ˆê¹Œ, í˜„ëŒ€ì˜¤í† ì—ë²„ì…ë‹ˆë‹¤. ë‹¹ì‚¬ ì±„ìš©ì— ê´€ì‹¬ì„ ê°€ì§€ê³  ê·€í•œ ì‹œê°„ì„ ë‚´ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦½ë‹ˆë‹¤. ê¹€ë¯¼ê·œë‹˜ê»˜ì„œëŠ” ì•„ì‰½ê²Œë„ ì´ë²ˆ ì „í˜•ì— ë¶ˆí•©ê²©í•˜ì…¨ìŠµë‹ˆë‹¤. ë‹¤ìŒë²ˆì— ë” ì¢‹ì€ ì¸ì—°ìœ¼ë¡œ ë§Œë‚˜ëµ™ê¸¸ í¬ë§í•˜ë©°, ì•ìœ¼ë¡œ í•˜ì‹œëŠ” ì¼ ëª¨ë‘ ê±´ìŠ¹í•˜ì‹œê¸¸ ë°”ëë‹ˆë‹¤. ê°ì‚¬í•©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-29-hyundai-autoever-ml-engineer",
    "title": "Hyundai Autoever - ML Engineer",
    "date": "2025-07-29",
    "excerpt": "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³  ë¶„ì„ê³¼ ì¤€ë¹„ê³¼ì •",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³ "
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´</strong></summary> ë§í¬ [Tech] Machine Learning Engineer - AI ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ğŸ“ƒ ëˆ„êµ¬ë‚˜ ë§ˆìŒ ì†ì— ì´ë ¥ì„œ í•œ ì¥ì€ ìˆìœ¼ë‹ˆê¹Œ, âš¡ 1ë¶„ ì»· ì§€ì›ìœ¼ë¡œ í˜„ëŒ€ì˜¤í† ì—ë²„ë¡œì˜ ì—¬ì •ì„ ì‹œì‘í•˜ì„¸ìš”. (ì´ë ¥ì„œ ììœ¨ì–‘ì‹) âœ… AI â€‹ë°ì´í„° â€‹ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ğŸš€ í•©ë¥˜í•˜ì‹¤ â€‹íŒ€ì„ â€‹ì†Œê°œí•´ìš” ì¸ê³µì§€ëŠ¥ê¸°ìˆ ì‹¤ì€ â€‹AI ì‹ ê¸°ìˆ ì„ ë°”íƒ•ìœ¼ë¡œ â€‹ì „ì‚¬ â€‹AI ì£¼ì œë¥¼ ë¦¬ë“œí•˜ë©° â€‹ê³µí†µ â€‹í™œìš© â€‹ê¸°ìˆ ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. â€‹ì—°êµ¬ê°œë°œì„ í†µí•´ â€‹'ì‚´ì•„ìˆëŠ” â€‹AI' ê¸°ìˆ ì„ â€‹ë§Œë“¤ì–´ ì‚¬ë‚´ â€‹ì£¼ìš” â€‹ì„œë¹„ìŠ¤ ë° ì œí’ˆì´ â€‹ë”ìš± ê²½ìŸë ¥ â€‹ìˆë„ë¡ ë§Œë“¤ì–´ ê°‘ë‹ˆë‹¤. ìŒì„±/ì–¸ì–´, ë¹„ì „/ì¸ì§€ ë“± í­ë„“ì€ AI ì£¼ì œ ì „ë°˜ì„ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, ê°œë°œ ê²°ê³¼ëŠ” ìš°ë¦¬ íšŒì‚¬ ë° ê·¸ë£¹ì˜ ì¤‘ìš” ì„œë¹„ìŠ¤ì— ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. AI ì‹ ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ì™€ AI ëª¨ë¸ ì—°êµ¬ê°œë°œ ì—­ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ ëª¨ë¹Œë¦¬í‹° ë° ìœ ê´€ ë¶„ì•¼ì˜ ì£¼ì¸ê³µì´ ë  ê°œë°œì/ì—°êµ¬ì ë¶„ë“¤ì„ ëª¨ì‹­ë‹ˆë‹¤. ğŸ’» í•©ë¥˜í•˜ì‹œë©´ í•¨ê»˜ í•  ì—…ë¬´ì˜ˆìš” AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ì„¤ê³„ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ / ë°ì´í„° ì¦ê°• AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤ ë°ì´í„° ê´€ë ¨ ìš´ì˜ ë° í’ˆì§ˆ ê´€ë¦¬ ğŸ” ì´ëŸ° ë¶„ê³¼ í•¨ê»˜ í•˜ê³  ì‹¶ì–´ìš” ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ NLP ê´€ë ¨ Task ë°ì´í„° êµ¬ì¶• ë° ì„¤ê³„ ê²½í—˜ Python ê°œë°œ ê²½ë ¥ 1ë…„ ì´ìƒ í˜¹ì€ ê·¸ì— ì¤€í•˜ëŠ” ì‹¤ë ¥ ğŸ” ì´ëŸ° ë¶„ì´ë¼ë©´ ë”ìš± ì¢‹ì•„ìš” ì „ì‚°ì–¸ì–´í•™/ìì—°ì–´ì²˜ë¦¬ ì „ê³µ ì„ì‚¬ ì´ìƒ LLM ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê²½í—˜ì LLM / GPT ê¸°ë°˜ ë°ì´í„° ì¦ê°• ê²½í—˜ì ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì âŒ› ì´ë ‡ê²Œ í•©ë¥˜í•´ìš” ì„œë¥˜ ì ‘ìˆ˜ â†’ ì„œë¥˜ ê²€í†  â†’ ì§ë¬´ì—­ëŸ‰í…ŒìŠ¤íŠ¸(ì½”ë”© ë˜ëŠ” ê³¼ì œí…ŒìŠ¤íŠ¸) ë° ì¸ì„±ê²€ì‚¬ â†’ 1ì°¨ë©´ì ‘ â†’ 2ì°¨ë©´ì ‘ â†’ ì²˜ìš°í˜‘ì˜ ë° ì±„ìš©ê²€ì§„ â†’ ìµœì¢… í•©ê²©ğŸ‰ ğŸ“ ë§Œë‚˜ê²Œ ë  ê·¼ë¬´ì§€ëŠ” ì—¬ê¸°ì˜ˆìš” ì„œìš¸ ê°•ë‚¨ ğŸ“£ ë™ë£Œì˜ í•œ ë§ˆë”” ì¸ê³µì§€ëŠ¥ê¸°ìˆ ì‹¤ì˜ ê¸°ë‘¥ Lì±…ì„ì…ë‹ˆë‹¤. ìš°ë¦¬ íŒ€ì€ ììœ¨ì„±ì´ íŠ¹ì§•ì´ì—ìš”. ì„œë¹„ìŠ¤ì—ì„œë¶€í„° ì œì¡°/ìƒì‚°ê¹Œì§€, ê¸°ìˆ ì´ ì ìš©ë˜ëŠ” ë¶„ì•¼ê°€ ë‹¤ì–‘í•˜ê³  ë˜ íŒ”ë¡œì—… í•´ì•¼ í•˜ëŠ” ì£¼ì œê°€ ê´‘ë²”ìœ„í•˜ë‹¤ ë³´ë‹ˆ ììœ¨ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ì¥ì„ ì§€ì›í•˜ê³  ìˆì–´ìš”. ì „ë°˜ì ì¸ ë¶„ì•¼ë¥¼ ì˜ ì•„ëŠ” ë©˜í† ë“¤ì´ ìˆì–´ ë§ì€ ë„ì›€ì„ ë°›ê³  ìˆê³ , ë˜ ê° ë¶„ì•¼ë³„ ì£¼ì œì— ë”°ë¼ í™œë°œí•œ ììœ¨ì  í•™ìŠµ ê·¸ë£¹ êµ¬ì„±ì„ í†µí•´ ì—…ë¬´ ì™¸ì— ì—­ëŸ‰ì  ì„±ì¥ì„ ê¾¸ì¤€íˆ í•˜ê³  ìˆë‹µë‹ˆë‹¤ :) ì—°êµ¬ê°œë°œ ê²°ê³¼ëŠ” ê·¸ë£¹ ì„œë¹„ìŠ¤ë‚˜ ì œí’ˆì— ë°˜ì˜ë˜ê¸°ë„ í•˜ê³ , ê´€ë ¨ í•™íšŒë‚˜ í–‰ì‚¬ ë“±ì— ì´ˆëŒ€ë¥¼ ë°›ì•„ ë°œí‘œí•˜ê¸°ë„ í•´ìš”. ì‚¬ë¬´ì‹¤ì€ ì‚¼ì„±ì—­ ì¸ê·¼ì— ìˆê³ , ë¡¯ë°íƒ€ì›Œê°€ ì¡°ë§ë˜ëŠ” ë·°ë„ í™˜ê²½ì  ì¥ì ì´ëë‹ˆë‹¤! í•¨ê»˜ ì‹ ê¸°ìˆ ì„ ê³µë¶€í•˜ê³  êµ¬ì²´ì  ê²°ê³¼ê°€ ìˆëŠ” ì‹¤ì§ˆì  ê¸°ìˆ  ì‘ìš© ê²½í—˜ì„ í†µí•´ ì»¤ë¦¬ì–´ì ìœ¼ë¡œ ì„±ì¥í•˜ê³  ì‹¶ì€ ë¶„ë“¤ì„ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ìš”! (íŒ€ ë¦¬ë”) ì¸ê³µì§€ëŠ¥ê¸°ìˆ ì‹¤ì€ ì‹ ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ì™€ í™œìš©ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì—´ì •ì„ ê°€ì§„ ì‚¬ëŒë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. AIë¥¼ ìœ„í•œ ì¸í”„ë¼, ë°ì´í„°, ë¶„ì„ ëª¨ë¸ë§ ë° ì•Œê³ ë¦¬ì¦˜, ê¸°ìˆ  ê¸°ë°˜ ê³µí†µ ì„œë¹„ìŠ¤ ê°œë°œê¹Œì§€ ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ ë‹¤ë£¨ë©°, ì´ëŸ° ë…¸ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ê·¸ë™ì•ˆ ì˜ë¯¸ ìˆê³  ê²°ê³¼ë“¤ì„ ë§Œë“¤ì–´ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ë¡œ ì£¼ìš” ê° ë¶„ì•¼ AI í”„ë ˆì„ì›Œí¬ë¥¼ ì´ìš©í•´ ê°œë°œì„ í•˜ë©° Python ë“±ì˜ ì–¸ì–´ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ íŒ€ì€ ë©€í‹°ëª¨ë‹¬ ê¸°ë°˜ì˜ ê¶ê·¹ì  AI ê°œë°œì„ ì§€í–¥í•˜ê³ , ì´ë¥¼ í†µí•´ íšŒì‚¬ì˜ ì„±ì¥ê³¼ ê° êµ¬ì„±ì›ì˜ ì»¤ë¦¬ì–´ ì„±ì¥ì„ í•¨ê»˜ ì´ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ì‹¬ì„ ê°€ì§€ê³  ì„±ê³µì˜ ì—¬ì •ì„ í•¨ê»˜í•´ ì£¼ì„¸ìš”! Join Us! ğŸ“Œ ì°¸ê³ í•´ ì£¼ì„¸ìš” ì±„ìš© ì‹œ ë§ˆê°ë˜ëŠ” ìƒì‹œ ì±„ìš© ê³µê³ ë¡œ ìš´ì˜ë˜ë©°, ì±„ìš© ì ˆì°¨ì™€ ì¼ì •ì€ ë³€ë™ë  ìˆ˜ ìˆì–´ìš”. ì‚¬íšŒì  ë°°ë ¤ ëŒ€ìƒì(ë³´í›ˆ ì·¨ì—…ì§€ì›ëŒ€ìƒì, ì¥ì• ì¸)ëŠ” ê´€ê³„ ë²•ë ¹ê³¼ ë‚´ê·œì— ë”°ë¼ ìš°ëŒ€í•´ìš”. ëª¨ì§‘ ë¶„ì•¼ ë° ë‹´ë‹¹ ì—…ë¬´ì— ë”°ë¼ ì˜ì–´ êµ¬ìˆ í‰ê°€, ë ˆí¼ëŸ°ìŠ¤ ì²´í¬, ë˜ëŠ” ê¸°íƒ€ ì „í˜•ì´ ì‹¤ì‹œë  ìˆ˜ ìˆì–´ìš”. ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ë¥¸ í¬ì§€ì…˜ì´ ë” ì í•©í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ê²½ìš° ì§€ì› ë¶„ì•¼ê°€ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ë°°ì¹˜ ë¶€ì„œ ë° ê·¼ë¬´ì§€ëŠ” íšŒì‚¬ ì‚¬ì •ì— ë”°ë¼ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ì•„ë˜ì˜ ê²½ìš°, í•©ê²©ì´ ì·¨ì†Œë˜ê±°ë‚˜ ì „í˜• ì§„í–‰ì—ì„œ ë¶ˆì´ìµì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì–´ìš”. ì§€ì›ì„œê°€ ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ê±°ë‚˜ ì¦ë¹™ì´ ë¶ˆê°€í•  ê²½ìš° í•´ì™¸ì—¬í–‰ ê²°ê²© ì‚¬ìœ ê°€ ìˆëŠ” ê²½ìš° (ë‚¨ì„±ì˜ ê²½ìš°, íšŒì‚¬ê°€ ì§€ì •í•œ ì…ì‚¬ì¼ê¹Œì§€ ë³‘ì—­ í•„ ë˜ëŠ” ë©´ì œ í•„ìš”) ìµœì¢… í•©ê²© í›„ íšŒì‚¬ê°€ ì§€ì •í•˜ëŠ” ì…ì‚¬ì¼ì— ì…ì‚¬ ë¶ˆê°€í•œ ê²½ìš° </details> ì´ë ¥ì„œ ì „ëµ í•„ìš”ê²½í—˜ ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ -> ì±—ë´‡ ê°œë°œí•˜ê³  ì„œë¹„ìŠ¤ ìš´ì˜í•´ë³¸ ê²½í—˜ì´ ìˆìŒ ì„ í˜¸ ê²½í—˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì NLP taskëŠ” ì•„ë‹ˆì§€ë§Œ segmentation ëª¨ë¸ ê°œë°œ ê²½í—˜ì´ë‘ --- ğŸ¯ í•„ìš” ê²½í—˜ (Must Have) ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ âœ… Chatbot í”„ë¡œì íŠ¸: RAG ê¸°ë°˜ ì§€ëŠ¥í˜• ì±—ë´‡ ì‹œìŠ¤í…œ ê°œë°œ ë° ì„œë¹„ìŠ¤ ìš´ì˜ âœ… ì‹¤ì œ ì„œë¹„ìŠ¤: ì¹´ì¹´ì˜¤í†¡ ì±„ë„ì„ í†µí•œ ë³‘ì› ìƒë‹´ ìë™í™” ì„œë¹„ìŠ¤ ğŸŒŸ ì„ í˜¸ ê²½í—˜ (Nice to Have) ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì âœ… Computer Vision: OCR Pipelineì—ì„œ TrOCR ëª¨ë¸ í™œìš© âœ… NLP: Chatbotì—ì„œ BGE-m3 ì„ë² ë”© ëª¨ë¸, GPT ê¸°ë°˜ ë‹µë³€ ìƒì„± âœ… ëª¨ë¸ ì„œë¹™: Triton Server, BentoMLì„ í†µí•œ í”„ë¡œë•ì…˜ ë°°í¬ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì âœ… ìë™í™”ëœ í’ˆì§ˆ í‰ê°€: LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ ìë™ í‰ê°€ ì‹œìŠ¤í…œ âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: Slack ê¸°ë°˜ ì—ëŸ¬ ì•Œë¦¼ ë° ì‘ì—… ìš”ì•½ âœ… í’ˆì§ˆ ì§€í‘œ ì •ì˜: ë‹µë³€ ì¶œì²˜ ì í•©ì„±, ì§ˆë¬¸-ë¬¸ì„œ ì—°ê´€ì„± í‰ê°€ âœ… ì„ê³„ì¹˜ ê¸°ë°˜ ê´€ë¦¬: í’ˆì§ˆ ì ìˆ˜ ì„ê³„ì¹˜ ì´í•˜ ì‹œ ìë™ ì•Œë¦¼ âœ… ë°ì´í„° ê²€ì¦ ë¡œì§: OCR ê²°ê³¼ ì •ì œ, ìˆ˜ê¸° ì…ë ¥ ë°ì´í„° ì™„ì „ì„± ê²€ì¦ ğŸ“‹ í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ì¹­ë„ ìš”êµ¬ì‚¬í•­ í”„ë¡œì íŠ¸ ê²½í—˜ ìˆ˜ì¤€ --------- --------- ---------- ì±—ë´‡ ì„œë¹„ìŠ¤ ê°œë°œ Chatbot â­â­â­â­â­ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ Chatbot + OCR â­â­â­â­â­ ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ OCR + Chatbot â­â­â­â­ Python ê°œë°œ ëª¨ë“  í”„ë¡œì íŠ¸ â­â­â­â­â­ ğŸ¯ ê°•ì¡° í¬ì¸íŠ¸ AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤: ì‹¤ì œ ë³‘ì› ìƒë‹´ ì±—ë´‡ ì„œë¹„ìŠ¤ ìš´ì˜ ê²½í—˜ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬: ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œìœ¼ë¡œ í’ˆì§ˆ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶• í”„ë¡œë•ì…˜ ë°°í¬: Docker, AWS, ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì—ì„œ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ ì„±ê³¼ ì§€í‘œ: ì‚¬ìš©ì ë§Œì¡±ë„ 90%+, ë‹µë³€ ì •í™•ë„ 95%+ ë‹¬ì„±",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-28-ë‚˜ë§ì¹˜ë©´ì–´ë–¡í•˜ì§€",
    "title": "ë‚˜ë§ì¹˜ë©´ ì–´ë–¡í•˜ì§€?",
    "date": "2025-07-28",
    "excerpt": "ë‚˜ë§ì¹˜ë©´ ì–´ë–¡í•˜ì§€?",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "--- ë°°ìš´ê±° ~í•˜ë©´ ì–´ì©Œì§€? ~ë©´ ì–´ë–»ê²Œ í• ê±°ì•¼. What if I mess up? What if the flight is delayed? What if + ê³¼ê±°í˜• -> í˜„ì¬ì™€ ì¼ì¹˜í•˜ê³  ìˆì§€ ì•Šì€ ìƒí™©ì„ ê°€ì • What if you were in Seoul? What if he weren't married? (í˜„ì¬ì™€ ë‹¤ë¥¸ ê°€ì •ì„ í•  ë•ŒëŠ” ë¬´ì¡°ê±´ wereì„ ì”€) What would you do if you were in Seoul? anxious ê°„ë‹¨í•œ ì‘ë¬¸ I think that what if questions are good to check value-thinking (ê°€ì¹˜ê´€). What if I got a lot of money? Will I work continue? It then, it means I love my job. What would I do If I were fluent in English? This imaginations make me more ëª…í™•í•˜ê²Œ. --- ìˆ˜ì •ëœ ë²„ì „ I think that \"what if\" questions are good for checking values and priorities. What if I got a lot of money? Would I continue working? If so, it means I love my job. What would I do if I were fluent in English? These imaginations help me think more clearly. --- í”¼ë“œë°± ë¬¸ë²• ìˆ˜ì •ì‚¬í•­: \"what if questions\" â†’ \"what if\" questions (ë”°ì˜´í‘œ ì¶”ê°€) \"check value-thinking\" â†’ \"check values and priorities\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"Will I work continue?\" â†’ \"Would I continue working?\" (ì¡°ê±´ë¬¸ì—ì„œëŠ” would ì‚¬ìš©) \"It then\" â†’ \"If so\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"What would I don\" â†’ \"What would I do\" (ì˜¤íƒ€ ìˆ˜ì •) \"This imaginations\" â†’ \"These imaginations\" (ë³µìˆ˜í˜•ì— ë§ëŠ” ì§€ì‹œì‚¬) \"make me more ëª…í™•í•˜ê²Œ\" â†’ \"help me think more clearly\" (ì˜ì–´ë¡œ ì™„ì „íˆ í‘œí˜„) í‘œí˜„ ê°œì„ ì‚¬í•­: \"value-thinking\" ëŒ€ì‹  \"values and priorities\" ì‚¬ìš© \"It then\" ëŒ€ì‹  \"If so\" ì‚¬ìš©í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ í‘œí˜„ ì „ì²´ì ì¸ í”¼ë“œë°±: ì¡°ê±´ë¬¸ì„ ì‚¬ìš©í•œ ê°€ì¹˜ê´€ íƒêµ¬ë¼ëŠ” ì¢‹ì€ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤! ë¬¸ë²•ì ìœ¼ë¡œ ëª‡ ê°€ì§€ ìˆ˜ì •ì´ í•„ìš”í–ˆì§€ë§Œ, í•µì‹¬ ë©”ì‹œì§€ëŠ” ì˜ ì „ë‹¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ \"What if\" ì§ˆë¬¸ì„ í†µí•´ ìì‹ ì˜ ì§„ì§œ ê°€ì¹˜ê´€ì„ íŒŒì•…í•˜ëŠ” ë°©ë²•ì€ ë§¤ìš° íš¨ê³¼ì ì¸ í•™ìŠµë²•ì…ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/íšŒì‚¬ì—ì„œí•˜ëŠ”ê³ ë¯¼_íš¨ìœ¨ì ì¸MLêµ¬ì¡°",
    "title": "íš¨ìœ¨ì ì¸ ML êµ¬ì¡° ì„¤ê³„ëŠ” ì–´ë–»ê²Œ í•˜ëŠ”ê±¸ê¹Œ?",
    "date": "2025-07-28",
    "excerpt": "íš¨ìœ¨ì ì¸ ML êµ¬ì¡° ì„¤ê³„ì— ëŒ€í•œ ê³ ë¯¼",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design"
    ],
    "content": "ì‚¬ìˆ˜ê°€ ì—†ëŠ” íšŒì‚¬ì—ì„œ ì¼ì„ í•˜ë‹¤ë³´ë‹ˆ ì‹¤ì œë¡œ ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•˜ë©´ì„œ ë§ë‹¥ëœ¨ë¦¬ëŠ” ë¬¸ì œë“¤ì„ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ê²Œ ì¢‹ì€ì§€ì— ëŒ€í•œ ê³ ë¯¼ì´ ëŠ˜ ìƒê¸´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Designing Machine Learning Systems ì±…ì„ ì½ê³  ê³µë¶€í•˜ë©´ì„œ ì ìš©í•´ë³´ë ¤ê³  í•œë‹¤. ì½ì„ ì±…ë“¤ Designing Machine Learning Systems Machine Learning System Design Interview AI Engineering Designing Data-Intensive Applications íšŒì‚¬ì˜ use caseë³„ë¡œ MLì„ ì–´ë–»ê²Œ ë°°í¬í•˜ê³  ìˆëŠ”ì§€. ì•„í‚¤í…ì²˜ ì–´ë–»ê²Œ ì„¤ê³„í• ì§€. ê³ ë¯¼í–ˆë˜ ë¶€ë¶„ í¬íŠ¸í´ë¦¬ì˜¤ë¡œë„ ë…¹ì—¬ë‚´ì.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch3_ë°ì´í„°ì—”ì§€ë‹ˆì–´ë§ê¸°ì´ˆ",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ",
    "date": "2025-07-28",
    "excerpt": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> í”„ë¡œë•ì…˜ìš© ML ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë ¤ë©´ ì¦ê°€í•˜ëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ì €ì¥, ê²€ìƒ‰ ë° ì²˜ë¦¬í•˜ëŠ” ë°©ë²™ã…‚ì„ ìˆ™ì§€í•´ì•¼ í•©ë‹ˆë‹¤. > ì‹œìŠ¤í…œ ê´€ì ì˜ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì„ ë” ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ ë§ˆí‹´ í´ë ˆí”„ë§Œì˜ \"ë°ì´í„° ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„\"ë¥¼ ì½ì–´ë³´ê¸° ë°”ëë‹ˆë‹¤. ë°ì´í„° ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„ ë°ì´í„° ì†ŒìŠ¤ --- > ML ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì˜¨ ë°ì´í„°ë¡œ ì‘ë™í•©ë‹ˆë‹¤. > ë°ì´í„°ë§ˆë‹¤ íŠ¹ì„±, ëª©ì , ì²˜ë¦¬ ë°©ë²•ì´ ë‹¤ë¥´ë©° ë°ì´í„° ì†ŒìŠ¤ë¥¼ íŒŒì•…í•˜ë©´ ë°ì´í„°ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. user input data system-generated data ë°ì´í„° í¬ë§· --- JSON CSV (í–‰ ìš°ì„ ) Parquet (ì—´ ìš°ì„ ) í–‰ ìš°ì„ ì¸ ë°ì´í„°ì— í–‰ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ë¹ ë¥´ê³ , ì—´ ìš°ì„  ë°ì´í„°ì— ì—´ë¡œ ì ‘ê·¼í•˜ë©´ ë¹ ë¥´ë‹¤. ë°ì´í„° ëª¨ë¸ --- ë°ì´í„°ê°€ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ ê´€ê³„í˜• ëª¨ë¸ : > ê´€ê³„í˜• ëª¨ë¸ì—ì„œëŠ” ë°ì´í„°ëŠ” ê´€ê³„ (relation)ë¡œ êµ¬ì„±ë˜ë©° ê° ê´€ê³„ëŠ” íŠœí”Œì˜ ì§‘í•©ì…ë‹ˆë‹¤. > ê´€ê³„ëŠ” ìˆœì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ê³„ì—ì„œ í–‰ì˜ ìˆœì„œë‚˜ ì—´ì˜ ìˆœì„œë¥¼ ì„ë”ë¼ë„ ì—¬ì „íˆ ë™ì¼í•œ ê´€ê³„ì…ë‹ˆë‹¤. ì •ê·œí™”í•˜ëŠ” í¸ì´ ì¢‹ë‹¤. ì •ê·œí™”ë¥¼ í†µí•´ ë°ì´í„° ì¤‘ë³µì„ ì¤„ì´ê³  ë°ì´í„° ë¬´ê²°ì„±ì„ í–¥ìƒ ë‹¤ë§Œ, ë°ì´í„°ê°€ ë¶„ì‚°ë˜ì–´ ê´€ë¦¬ë˜ê³  ì¡°ì¸ ë¹„ìš© ë“±ì´ ì¦ê°€. > ê´€ê³„í˜• ë°ì´í„° ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ë¼ê³  í•©ë‹ˆë‹¤. ì¿¼ë¦¬ ì–¸ì–´. SQL. > ì£¼ëª©í•´ì•¼ í•  ì ì€ SQLì´ ì„ ì–¸ì  ì–¸ì–´ë¼ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤. > ëª…ë ¹í˜• ì–¸ì–´ì¸ íŒŒì´ì¬ê³¼ ëŒ€ë¹„ë˜ì£ . ì„ ì–¸ì  ì–¸ì–´ (Declarative Language) -> ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ì–¸ì–´ (ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ìµœì í™” ìˆ˜í–‰) ëª…ë ¹í˜• ì–¸ì–´ (Imperative Language) -> ì–´ë–»ê²Œ ì›í•˜ëŠ” ê²ƒì„ ë‹¬ì„±í• ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ì–¸ì–´ ì„ ì–¸ì  ë°ì´í„° ì‹œìŠ¤í…œì—ì„œ ì„ ì–¸ì  ML ì‹œìŠ¤í…œìœ¼ë¡œ : > ì„ ì–¸ì  ë°ì´í„° ì‹œìŠ¤í…œì´ ì„±ê³µí•œ ë°ì„œ ì˜ê°ì„ ë°›ì€ ë§ì€ ì‚¬ëŒë“¤ì€ ì„ ì–¸ì  MLì„ ê¸°ëŒ€í•´ì™”ìŠµë‹ˆë‹¤. > ì„ ì–¸ì  ML ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©ìëŠ” í”¼ì²˜ì˜ ìŠ¤í‚¤ë§ˆì™€ ì‘ì—…ë§Œ ì„ ì–¸í•˜ë©´ ë©ë‹ˆë‹¤. > ê·¸ëŸ¬ë©´ ì‹œìŠ¤í…œì€ ì£¼ì–´ì§„ í”¼ì²˜ë¡œ í•´ë‹¹ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ”ë° ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì°¾ì•„ëƒ…ã„´ë””ã…. ALudwig, AutoML. ëª¨ë¸ ê°œë°œ ë¶€ë¶„ì€ ë§ì´ ì¶”ìƒí™”ë˜ëŠ” ì¤‘. ê·¸ë ‡ê²Œ ì–´ë ¤ìš´ ì‘ì—…ì´ ì•„ë‹ˆë‹¤. NoSQL : ìŠ¤ë¯¸ì¹´ ê´€ë¦¬ì— ëŒ€í•œ ë¶ˆë§Œ. > ë¹„ê´€ê³„í˜• ëª¨ë¸ì˜ ì£¼ìš” ìœ í˜• ë‘ ê°€ì§€ëŠ” ë¬¸ì„œ ëª¨ë¸ê³¼ ê·¸ë˜í”„ ëª¨ë¸ì…ë‹ˆë‹¤. ë¬¸ì„œ ëª¨ë¸ ê·¸ë˜í”„ ëª¨ë¸ > ë°ì´í„° ëª¨ë¸ì— ë”°ë¼ ìˆ˜í–‰í•˜ê¸° ì‰¬ìš´ ì¿¼ë¦¬ê°€ ìˆê³  ì–´ë ¤ìš´ ì¿¼ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. > ë”°ë¼ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•œ ë°ì´í„° ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ì£ . ì •í˜• ë°ì´í„° vs ë¹„ì •í˜• ë°ì´í„° ì •í˜• ë°ì´í„°ëŠ” ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤. > ì •í˜• ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì €ì¥ì†Œë¥¼ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¼ê³  í•˜ë©° ë¹„ì •í˜• ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì €ì¥ì†Œë¥¼ ë°ì´í„° ë ˆì´í¬ë¼ê³  í•©ë‹ˆë‹¤. > ë°ì´í„° ë ˆì´í¬ëŠ” ì¿ë‚˜ì ìœ¼ë¡œ ì²˜ë¦¬ ì „ ì›ì‹œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ”ë° ì‚¬ìš©í•˜ë©° ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë°ì´í„° ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ë° ì²˜ë¦¬ --- íŠ¸ëœì­ì…˜ ì²˜ë¦¬ì™€ ë¶„ì„ ì²˜ë¦¬ : transaction : ì˜¨ê°– ì¢…ë¥˜ì˜ ì‘ì—… ìƒì„±ë  ë•Œ ì‚½ì…ë˜ê³  ë³€ê²½ë  ë•Œ ì—…ë°ì´íŠ¸ ë˜ë©´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©´ ì‚­ì œë¨. OLTP (Online Transaction Processing) > íŠ¸ëœì­ì…˜ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì˜¨ë¼ì¸ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•˜ê³  ë‚®ì€ ë ˆì´í„´ì‹œì™€ ê³ ê°€ìš©ì„± ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ë„ë¡ ì„¤ê³„ëìŠµë‹ˆë‹¤. ACID (Atomicity, Consistency, Isolation, Durability) Atomicity(ì›ìì„±) : > íŠ¸ëœì­ì…˜ì˜ ëª¨ë“  ë‹¨ê³„ê°€ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œì„œ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. Consistency(ì¼ê´€ì„±) : > ë“¤ì–´ì˜¤ëŠ” ëª¨ë“  íŠ¸ëœì­ì…˜ì´ ë¯¸ë¦¬ ì •ì˜ëœ ê·œì¹™ì„ ë”°ë¼ì•¼ í•¨ì„ ë³´ì¥í•©ë‹ˆë‹¤. Isolation(ê²©ë¦¬ì„±) : > ë‘ íŠ¸ëœì­ì…˜ì´ ë§ˆì¹˜ ê²©ë¦¬ëœ ê²ƒì²˜ëŸ¼ ë™ì‹œì— ë°œìƒí•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. Durability(ì§€ì†ì„±) : > íŠ¸ëœì­ì…˜ì´ ì»¤ë°‹ëœ í›„ì—ëŠ” ì‹œìŠ¤í…œ ì¥ì• ê°€ ë°œìƒí•˜ë”ë¼ë„ ì»¤ë°‹ëœ ìƒíƒœë¥¼ ìœ ì§€í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. íŠ¸ëœì­ì…˜ ë°ì´í„°ë² ì´ìŠ¤ëŠ” í–‰ ìš°ì„ ì¼ ë•Œê°€ ë§ìŒ -> ë¶„ì„ì— ì ì ˆí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ OLAP (Online Analytical Processing) ë¶„ì„ì— ë” ì ì ˆí•œ database. ì—´ ì¡°íšŒì— ìµœì í™” ë¨. ìš”ì¦˜ì€ êµ¬ë¶„í•´ì„œ ì´ì•¼ê¸°ì•ˆí•¨. ë‘˜ ë‹¤ ì§€ì›í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ. ETL: Extract, Transform, Load : > ETLì€ ë°ì´í„°ë¥¼ ë²”ìš© ì²˜ë¦¬ ë° ì›í•˜ëŠ” ëª¨ì–‘ê³¼ í¬ë§·ìœ¼ë¡œ ì§‘ê³„í•¨ì„ ì˜ë¯¸ <figure> <img src=\"/post/DesigningMachineLaerningSystem/ETL.webp\" alt=\"ETL Process\" /> <figcaption> ì¶œì²˜: <a href=\"https://www.getdbt.com/blog/etl-vs-elt\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.getdbt.com/blog/etl-vs-elt</a> </figcaption> </figure> ELT: Extract, Load, Transform > ë°ì´í„°ë¥¼ ë¨¼ì € ìŠ¤í† ë¦¬ì§€ì— ì ì¬í•œ ë’¤ ë‚˜ì¤‘ì— ì²˜ë¦¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ <figure> <img src=\"/post/DesigningMachineLaerningSystem/ELT.webp\" alt=\"ELT Process\" /> <figcaption> ì¶œì²˜: <a href=\"https://www.getdbt.com/blog/etl-vs-elt\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.getdbt.com/blog/etl-vs-elt</a> </figcaption> </figure> ë°ì´í„°í”Œë¡œ ëª¨ë“œ --- > ë°ì´í„°ê°€ í•œ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ë‹¤ë¡œë”œ ë•Œ ë°ì´í„°ê°€ í•œ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ íë¥¸ë‹¤ê³  í•©ë‹ˆë‹¤. > ì¦‰, ë°ì´í„° í”Œë¡œê°€ ìƒê¹ë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•œ ë°ì´í„° ì „ë‹¬ ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ë°ì´í„° ì „ë‹¬ (REST, RPC APIì—ì„œ ì œê³µí•˜ëŠ” ìš”ì²­) ì‹¤ì‹œê°„ ì „ì†¡ì„ í†µí•œ ë°ì´í„° ì „ë‹¬ (ì•„íŒŒì¹˜ ì¹´í”„ì¹´, ì•„ë§ˆì¡´ í‚¤ë„¤ì‹œìŠ¤)",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch2_ë¨¸ì‹ ëŸ¬ë‹ì‹œìŠ¤í…œì„¤ê³„ì†Œê°œ",
    "title": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ì„¤ê³„ ì†Œê°œ",
    "date": "2025-07-28",
    "excerpt": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ì„¤ê³„ ì†Œê°œ",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "ML ì‹œìŠ¤í…œ ì„¤ê³„ëŠ” ì–´ë–»ê²Œ í•˜ëŠ”ê±¸ê¹Œ? MLOpsì— ì‹œìŠ¤í…œìœ¼ë¡œ ì ‘ê·¼ > ì‹œìŠ¤í…œì„ ë¹„ì¦ˆë‹ˆìŠ¤ìš©ìœ¼ë¡œ ê°œë°œí•œë‹¤ë©´ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë”°ë¼ ì—…ë¬´ë¥¼ ì¶”ì§„í•´ì•¼ í•˜ë©°, ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì€ ë‹¤ì‹œ ML ëª©ì ìœ¼ë¡œ ë°”ê¿”ì„œ ML ëª¨ë¸ ê°œë°œì˜ ì´ì •í‘œë¡œ ì‚¼ì•„ì•¼ í•©ë‹ˆë‹¤. ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ë¶€ë¶„. ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì„ ì´í•´í•˜ê¸° ì´ì— ë§ê²Œ ML ëª©ì ì„ ì„¸ìš°ëŠ” ê³¼ì •. ì‹ ë¢°ì„±, í™•ì¥ì„±, ìœ ì§€ë³´ìˆ˜ì„±, ì ì‘ì„± ë¹„ì¦ˆë‹ˆìŠ¤ì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª©ì  --- > ì‚¬ì‹¤ ê¸°ì—…ë“¤ì€ ëŒ€ë¶€ë¶„ ì´ëŸ° í™”ë ¤í•œ ML ì§€í‘œì— í¬ê²Œ ê´€ì‹¬ì´ ì—†ìŠµë‹ˆë‹¤. > ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” í•œ ëª¨ë¸ ì •í™•ë„ë¥¼ 94%ì—ì„œ 94.2%ë¡œ ë†’ì´ëŠ” ì¼ ë”°ìœ„ì— ì‹ ê²½ì“°ì§€ ì•Šì£ . > ë¹„ì¦ˆë‹ˆìŠ¤ ì¡°ì§ ë‚´ì—ì„œ ML í”„ë¡œì íŠ¸ê°€ ì„±ê³µí•˜ë ¤ë©´ ML ì‹œìŠ´í…œ ì„±ê³¼ë¥¼ ì „ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ì™€ ì—°ê²°í•´ì•¼ í•©ë‹ˆë‹¤. > ì‹ ê·œ ML ì‹œìŠ¤í…œì€ ê´‘ê³  ìˆ˜ìµ, ì›”ê°„ í™œì„± ì‚¬ìš©ì ìˆ˜ ë“± ì–´ë–¤ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ ì§€í‘œì— ì˜í–¥ì„ ë¯¸ì¹ ê¹Œìš”? > ë§ì€ íšŒì‚¬ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œë¥¼ ML ì§€í‘œì— ì—°ê²°í•˜ê¸° ìœ„í•´ ìì²´ ì§€í‘œë¥¼ ë§Œë“­ë‹ˆë‹¤. ë„·í”Œë¦­ìŠ¤ - ì¶”ì²œì‹œìŠ¤í…œ. ì±„íƒë¥  (take-rate) ì˜ìƒ ì¬ìƒ íšŸìˆ˜ / ì‚¬ìš©ìê°€ ë³¸ ì¶”ì²œ ì¸ë‚´ì¼ ê°œìˆ˜ ì¶”ì²œì‹œìŠ¤í…œì˜ ì±„íƒë¥ ì„ ë‹¤ë¥¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ, ì˜ˆì»¨ëŒ€ ì´ ìŠ¤íŠ¸ë¦¬ë° ì‹œê°„ì´ë‚˜ êµ¬ë… ì·¨ì†Œìœ¨ì˜ ë§¥ë½ì—ì„œë„ ê³ ë ¤. ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹œë„ë“¤. -> ë„·í”Œë¦­ìŠ¤ëŠ” ìƒˆë¡œìš´ í‰ê°€ì§€í‘œë¥¼ ê³„ì‚° ì§ì ‘ì ìœ¼ë¡œ íŒŒì•…í•˜ëŠ”ê±´ ê½¤ë‚˜ ì–´ë µê³  ë§ì€ ì‹¤í—˜ì´ í•„ìš”í•¨ ëŒ€ì•ˆìœ¼ë¡œ A/B testë¥¼ ì§„í–‰í•´ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ, ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ ê´€ì ì—ì„œë§Œ ë¶„ì„í•´ë³¼ ìˆ˜ë„ ìˆìŒ > ë§ì€ ê¸°ì—…ì—ì„œ 'AI ê¸°ë°˜'ì´ë¼ëŠ” ë¬¸êµ¬ê°€ ê³ ê° ìœ ì¹˜ì— ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸ì— MLì„ ì‚¬ìš©í•œë‹¤ê³  ì´ì•¼ê¸°í•©ë‹ˆë‹¤. > AIê°€ ì‹¤ì œë¡œ ìœ ìš©í•œì§€ëŠ” ìƒê´€ì—†ì´ ë§ì´ì£ . MLì´ ë§ˆë²•ê°™ì€ ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ëŠ” ìˆì§€ë§Œ, í•˜ë£¨ì•„ì¹¨ì— ë˜ëŠ”ê±´ ì ˆëŒ€ ì•„ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ìœ¼ë¡œ MLì´ ê°€ì ¸ì˜¬ ìˆ˜ìµì— ëŒ€í•´ì„œ í˜„ì‹¤ì ìœ¼ë¡œ ìƒê°í•´ì•¼ í•¨. > ML ê´€ë ¨í•œ íˆ¬ì ìˆ˜ìµì€ ë„ì… ì´í›„ ì„±ìˆ™ ë‹¨ê³„ì— ë‹¤ë‹¤ë¼ì„œì•¼ ëˆˆì— ë„ê¸° ì‹œì‘í•©ë‹ˆë‹¤. > MLì„ ë„ì…í•œ ê¸°ê°„ì´ ê¸¸ìˆ˜ë¡ íŒŒì´í”„ë¼ì¸ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ê°œë°œ ì£¼ê¸°ê°€ ë¹¨ë¼ì§€ë©° ì—”ì§€ë‹ˆì–´ë§ ì‹œê°„ê³¼ í´ë¼ìš°ë“œ ë¹„ìš©ì´ ì¤„ì–´ë“¤ì–´ ìˆ˜ìµì´ ë†’ì•„ì§€ê²Œ ë©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ìš”êµ¬ ì‚¬í•­ --- ML ì‹œìŠ¤í…œì´ ê°–ì¶°ì•¼ í•˜ëŠ” ìš”êµ¬ì‚¬í•­ ì‹ ë¢°ì„± : > ML ì‹œìŠ¤í…œì€ 'ì‹œìŠ¤í…œ ë™ì‘ì˜ ì˜¬ë°”ë¦„'ì„ íŒë‹¨í•˜ê¸° ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤ì œ ìƒí™©ì—ì„œëŠ” ì •ë‹µì´ ì—†ëŠ”ë°, í‹€ë ¸ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì–´ë–»ê²Œ íŒŒì•…í•  ìˆ˜ ìˆì„ê¹Œ? í™•ì¥ì„± : > ML ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. > ì‹œìŠ¤í…œ ë³µì¡ë„ê°€ ì¦ê°€í•˜ë“ , íŠ¸ë˜í”½ ì–‘ì´ ì¦ê°€í•˜ë“ , ML ëª¨ë¸ ê°œìˆ˜ê°€ ì¦ê°€í•˜ë“  ìƒê´€ì—†ì´ ê·œëª¨ ì¦ê°€ë¥¼ ì²˜ë¦¬í•  í•©ë¦¬ì ì¸ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. resource scaling. artfiact managment. ìœ ì§€ë³´ìˆ˜ì„± : > ì›Œí¬ë¡œë“œë¥¼ êµ¬ì¡°í™”í•˜ê³  ì¸í”„ë¼ë¥¼ ì„¤ì •í•˜ëŠ” ì¼ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì ì‘ì„± : > ì‹œìŠ¤í…œì€ ë³€í™”í•˜ëŠ” ë°ì´í„° ë¶„í¬ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ì‚¬í•­ì— ì ì‘í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. > ê·¸ëŸ¬ë ¤ë©´ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆì–´ì•¼ í•˜ë©° ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ì—…ë°ì´íŠ¸ê°€ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤. > ML ì‹œìŠ¤í…œì€ ë°˜ì€ ì½”ë“œì´ê³  ë°˜ì€ ë°ì´í„°ì…ë‹ˆë‹¤. > ë°ì´í„°ëŠ” ë¹ ë¥´ê²Œ ë³€í™”í•˜ë¯€ë¡œ ML ì‹œìŠ¤í…œ ë˜í•œ ìì²´ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì§„í™”í•  ìˆ˜ ìˆì–´ì•¼ í•˜ì£ . ë°˜ë³µ í”„ë¡œì„¸ìŠ¤ --- > ML ì‹œìŠ¤í…œ ê°œë°œì€ ë°˜ë³µì ì´ë©° ëŒ€ë¶€ë¶„ ëì´ ì—†ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. > ì‹œìŠ¤í…œì„ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ë©´ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë²”ìœ„ ì‚°ì • ë²”ìœ„ë¥¼ ì‚°ì •í•˜ê³  ëª©í‘œ, ëª©ì ê³¼ ì œì•½ì‚¬í•­ì„ ì„¤ì •í•˜ëŠ” ì¼ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ML ëª¨ë¸ ê°œë°œ ë°°í¬ > ML ì‹œìŠ¤í…œ ê°œë°œì€ ê¸€ì“°ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ê²°ì½” ì™„ë²½íˆ ì™„ì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. > í•˜ì§€ë§Œ ì–´ëŠ ì‹œì ì—ëŠ” ì‹œìŠ¤í…œì„ ì™¸ë¶€ì— ë‚´ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ê³¼ ì—°ì† í•™ìŠµ > ëª¨ë¸ì„ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•œ ë’¤ì—ëŠ” ì§€ì†ì ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë³€í™”í•˜ëŠ” í™˜ê²½ê³¼ ìš”êµ¬ ì‚¬í•­ì— ì ì‘í•˜ë„ë¡ ìœ ì§€ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ > ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ ê´€ì ì—ì„œ ë¶„ì„í•´ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œ êµ¬ì¡°í™”í•˜ê¸° --- > ê³ ê° ì§€ì› ì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” ì ì€ ë¬¸ì œì§€ë§Œ MLë¡œ ë°”ë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œëŠ” ì•„ë‹™ë‹ˆë‹¤. > ML ë¬¸ì œëŠ” ì…ë ¥ê³¼ ì¶œë ¥, í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´ëŒì–´ë‚˜ê°€ëŠ” ëª©ì  í•¨ìˆ˜ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ë¶„ë¥˜ vs íšŒê·€ ì´ì§„ë¶„ë¥˜ vs ë‹¤ì¤‘í´ë˜ìŠ¤ ë¶„ë¥˜ ë‹¤ì¤‘ í´ë˜ìŠ¤ vs ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ ë¬¸ì œë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²• : ë¬¸ì œ: ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ì—´ì–´ë³¼ ì•± ì˜ˆì¸¡í•˜ê¸° êµ¬ì¡°í™” ë°©ë²•: ë¶„ë¥˜ -> ì•±ì´ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ì¬í•™ìŠµì„ í•´ì•¼í•˜ëŠ” ì´ìŠˆ ë¬¸ì œ: ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ì—´ì–´ë³¼ ì•± ì˜ˆì¸¡í•˜ê¸° êµ¬ì¡°í™” ë°©ë²•: íšŒê·€ ì…ë ¥ìœ¼ë¡œ ì•±ê´€ë ¨ í”¼ì²˜ë¥¼ ë„£ì–´ì£¼ê³  ì—´ì–´ë³¼ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¡œ ì •ì˜í•˜ë©´, ë‹¤ì‹œ í•™ìŠµí•´ì•¼í•˜ëŠ” ì†Œìš”ê°€ ì¤„ì–´ë“¬ ì§€ì„± (mind) vs ë°ì´í„° (data) --- > ë°ì´í„°ë³´ë‹¤ ì§€ì„±ì´ ìœ„ìœ„ë¼ê³  ìƒê°í•˜ëŠ” ì§„ì˜ì—ëŠ” íŠœë§ìƒ ìˆ˜ìƒìì¸ ì£¼ë°ì•„ í„ ë°•ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤. > í„ì€ ì¸ê³¼ ê´€ê³„ ì¶”ë¡ ê³¼ ë² ì´ì¦ˆ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•œ ì—°êµ¬ë¡œ ì˜ ì•Œë ¤ì¡Œìœ¼ë©° ê·¸ì˜ ì €ì„œ ì¸ê³¼ì„±ì— ëŒ€í•´ì— ì‹¤ë¦° ì„œë¬¸ ì œëª©ì´ 'ë°ì´í„°ë³´ë‹¤ ìš°ìœ„ì¸ ì§€ì„±'ì…ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch1_ë¨¸ì‹ ëŸ¬ë‹ì‹œìŠ¤í…œê°œìš”",
    "title": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ê°œìš”",
    "date": "2025-07-28",
    "excerpt": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ê°œìš”",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> 'ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ'ì´ë¼ê³  í•˜ë©´ ë§ì€ ì‚¬ëŒì´ ë¡œì§€ìŠ¤í‹± íšŒê·€ë‚˜ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì‹ ê²½ë§ ë“± ML ì•Œê³ ë¦¬ì¦˜ë§Œ ë– ì˜¬ë¦½ë‹ˆë‹¤. > ê·¸ëŸ¬ë‚˜ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•Œê³ ë¦¬ì¦˜ì€ ML ì‹œìŠ¤í…œì˜ ì¼ë¶€ì¼ ë¿ì…ë‹ˆë‹¤. > ì‹œìŠ¤í…œì€ ML í”„ë¡œì íŠ¸ì˜ ì¶œë°œì ì´ ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ì‚¬í•­, ì‚¬ìš©ìì™€ ê°œë°œìê°€ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤, ë°ì´í„° ìŠ¤íƒ, ëª¨ë¸ì„ ê°œë°œ ë° ëª¨ë‹ˆí„°ë§í•˜ê³  ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ë¡œì§ì€ ë¬¼ë¡  í•´ë‹¹ ë¡œì§ì„ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ì¸í”„ë¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. MLì„ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ê²½ìš° (use case)ëŠ” ì–¸ì œì¸ê°€? ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê²ƒë“¤ì€? ì—°êµ¬ì™€ í”„ë¡œë•ì…˜ ìƒí™©ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ì„ ê°œë°œí•˜ëŠ”ê²Œ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì— ëŒ€í•œ ë‚´ìš©ë“¤. ì—°êµ¬ëŠ” ì—°êµ¬ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°ì— ëª©í‘œê°€ ìˆì§€ë§Œ, ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•  ë•ŒëŠ” ì´í•´ê´€ê³„ë¥¼ ë§Œì¡±í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì— ë§ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì´ ëª©í‘œ. > ML ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ìš”ì†Œë¡œ êµ¬ì„±ëœ ë³µì¡í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. > í”„ë¡œë•ì…˜ì—ì„œ ML ì‹œìŠ¤í…œì„ ì‘ì—…í•˜ëŠ” ë°ì´í„° ê³¼í•™ìì™€ ML ì—”ì§€ë‹ˆì–´ëŠ” ML ì•Œê³ ë¦¬ì¦˜ì—ë§Œ ì§‘ì¤‘í•˜ëŠ” ê±¸ë¡œëŠ” ì ˆëŒ€ ì¶©ë¶„ì¹˜ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¹¨ë‹¬ì„ ê²ë‹ˆë‹¤. > ì•Œê³ ë¦¬ì¦˜ ì™¸ì— ì‹œìŠ¤í…œì˜ ë‹¤ë¥¸ ì¸¡ë©´, ì˜ˆì»¨ëŒ€ ë°ì´í„° ìŠ¤íƒ, ë°°í¬, ëª¨ë‹ˆí„°ë§, ìœ ì§€ ê´€ë¦¬, ì¸í”„ë¼ì— ê´€í•´ ì•„ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‚´ê°€ ì„œë¹„ìŠ¤ë¥¼ êµ¬ì„±í•œ í›„ ì´ë¥¼ ì„¤ëª…í•  ë•Œë„ ë‚´ê°€ ë””ìì¸í•œ ì‹œìŠ¤í…œì— ëŒ€í•´ì„œ, ì´ëŸ° ë¶€ë¶„ì„ ê³ ë ¤í•´ ì„¤ëª…í•  í•„ìš”ê°€ ìˆê² ë‹¤. ì—°êµ¬ìš© ë¨¸ì‹ ëŸ¬ë‹ vs í”„ë¡œë•ì…˜ìš© ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬ìš© ML í”„ë¡œë•ì…˜ìš© ML --- --- --- ìš”êµ¬ì‚¬í•­ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìµœì ì˜ ëª¨ë¸ ì„±ëŠ¥ ë‹¬ì„±í•˜ê¸° ì´í•´ê´€ê³„ìë§ˆë‹¤ ë‹¤ë¦„ ê³„ì‚° ìš°ì„ ìˆœìœ„ ë¹ ë¥¸ í›ˆë ¨, ë†’ì€ ìŠ¤ë£¨í’‹ ë¹ ë¥¸ ì¶”ë¡ , ë‚®ì€ ë ˆì´í„´ì„¸ ë°ì´í„° ì •ì ì„ ëŠì„ì—†ì´ ë³€ë™í•¨ ê³µì •ì„± ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŒ ë°˜ë“œì‹œ ê³ ë ¤í•´ì•¼ í•¨ í•´ì„ ê°€ëŠ¥ì„± ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŒ ë°˜ë“œì‹œ ê³ ë ¤í•´ì•¼ í•¨ ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ìì™€ ìš”êµ¬ì‚¬í•­ : > ML ì‹œìŠ¤í…œì„ í”„ë¡œë•ì…˜ì— ì ìš©í•˜ëŠ” ê³¼ì •ì—ëŠ” ë§ì€ ì´í•´ê´€ê³„ìê°€ ì–½í˜€ ìˆìœ¼ë©° ê°ê° ìš”êµ¬ ì‚¬í•­ì´ ë‹¤ë¦…ë‹ˆë‹¤. > ìš”êµ¬ ì‚¬í•­ë“¤ì€ ì„œë¡œ ì¶©ëŒí•  ë•Œë„ ë§ìœ¼ë¯€ë¡œ ëª¨ë“  ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ” ML ëª¨ë¸ì„ ì„¤ê³„, ê°œë°œ, ì„ íƒí•˜ê¸°ëŠ” ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ëª©í‘œë¥¼ ì´í•´í•˜ê³  ëª©í‘œê°€ ì—¬ëŸ¬ê°œë¼ë©´ ìª¼ê°œì„œ ê°ê°ì˜ ëª¨ë¸ì„ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤. ì‚¬ìš©ì ëª©í‘œë¥¼ ë§Œì¡±í•˜ì§€ë§Œ ëª¨ë¸ì˜ ëª©í‘œê°€ ë‹¤ë¥¼ ìˆ˜ë„ ìˆê³ . > ML í”„ë¡œì íŠ¸ë¥¼ ê°œë°œí•  ë•Œ ML ì—”ì§€ë‹ˆì–´ëŠ” ëª¨ë“  ì´í•´ê´€ê³„ìì˜ ìš”êµ¬ ì‚¬í•­ì„ íŒŒì•…í•˜ê³  ê°ê°ì´ ì–¼ë§ˆë‚˜ ì—„ê²©í•œì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. > í”„ë¡œë•ì…˜ìš© MLì€ ì—°êµ¬ìš© MLê³¼ ìš”êµ¬ì‚¬í•­ì´ ë‹¤ë¦…ë‹ˆë‹¤. ê³„ì‚° ìš°ì„ ìˆœìœ„ : > ML ì‹œìŠ¤í…œì„ ë°°í¬í•œ ê²½í—˜ì´ ì—†ëŠ” ë¶„ë“¤ì€ ì‹œìŠ¤í…œì„ ì„¤ê³„í•  ë•Œ ì¢…ì¢… ëª¨ë¸ ê°œë°œì—ë§Œ ë„ˆë¬´ ì§‘ì¤‘í•˜ê³  ë°°í¬ì™€ ìœ ì§€ ê´€ë¦¬ë¥¼ ì†Œí™€íˆ í•˜ëŠ” ì‹¤ìˆ˜ë¥¼ í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” í•™ìŠµë‹¨ê³„ê°€ ë³‘ëª©ì´ì§€ë§Œ ë°°í¬ ë‹¨ê³„ì—ì„œëŠ” ì¶”ë¡ ì´ ë³‘ëª©ì´ë‹¤. > í•œê°€ì§€ ê²°ë¡ ì€ ì—°êµ¬ì—ì„œëŠ” ë†’ì€ ìŠ¤ë£¨í’‹ (throughput)ì„ ìš°ì„ ì‹œ í•˜ëŠ” ë°˜ë©´ì— í”„ë¡œë•ì…˜ì—ì„œëŠ” ë‚®ì€ ë ˆì´í„´ì‹œë¥¼ ìš°ì„ ì‹œí•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. > ë ˆì´í„´ì‹œëŠ” ì¿¼ë¦¬ë¥¼ ìˆ˜ì‹ í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì˜ë¯¸í•˜ë©° ìŠ¤ë£¨í’‹ì€ íŠ¹ì • ê¸°ê°„ ë‚´ì— ì²˜ë¦¬ëœ ì¿¼ë¦¬ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ìŠ¤ë£¨í’‹(Throughput): íŠ¹ì • ì‹œê°„ ë‚´ì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì˜ ì´ëŸ‰ ë ˆì´í„´ì‹œ(Latency): í•˜ë‚˜ì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ > ì‹¤í—˜ ë‹¨ê³„ì—ì„œ ëŒ€ëŸ‰ì˜ ì‹¤í—˜ì´ í•„ìš”í•¨. ``markdown ì—°êµ¬ ì‹œë‚˜ë¦¬ì˜¤: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: 1000ê°œ ì¡°í•© ì‹œë„ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‹¤í—˜: 50ê°€ì§€ êµ¬ì¡° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¦ê°• ì‹¤í—˜: 10ê°€ì§€ ë°©ë²• ë¹„êµ ëª©í‘œ: ê°€ëŠ¥í•œ ë§ì€ ì‹¤í—˜ì„ ë¹ ë¥´ê²Œ ì™„ë£Œ â†’ ìŠ¤ë£¨í’‹ì´ ì¤‘ìš” (ì‹œê°„ë‹¹ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‹¤í—˜ ìˆ˜) ` > í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ ì¤˜ì•¼í•¨. `markdown ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤: ì¶”ì²œ ì‹œìŠ¤í…œ: ì‚¬ìš©ìê°€ í˜ì´ì§€ ë¡œë“œ í›„ 100ms ë‚´ì— ì¶”ì²œ ì œê³µ ì±—ë´‡: ì‚¬ìš©ì ë©”ì‹œì§€ì— 2ì´ˆ ë‚´ì— ì‘ë‹µ ììœ¨ì£¼í–‰: ì„¼ì„œ ë°ì´í„°ë¥¼ 50ms ë‚´ì— ì²˜ë¦¬ ëª©í‘œ: ê°œë³„ ìš”ì²­ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬ â†’ ë ˆì´í„´ì‹œê°€ ì¤‘ìš” (ì‘ë‹µ ì‹œê°„) ì˜ˆì‹œ: ì´ì»¤ë¨¸ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œ ë ˆì´í„´ì‹œ 100ms â†’ ì „í™˜ìœ¨ 5% ë ˆì´í„´ì‹œ 500ms â†’ ì „í™˜ìœ¨ 3% (40% ê°ì†Œ) ë ˆì´í„´ì‹œ 1ì´ˆ â†’ ì „í™˜ìœ¨ 1% (80% ê°ì†Œ) ê²°ê³¼: 1ì´ˆ ì§€ì—° = ë§¤ì¶œ 80% ê°ì†Œ `` > ë ˆì´í„´ì‹œëŠ” ê°œë³„ ìˆ˜ì¹˜ê°€ ì•„ë‹Œ ë¶„í¬ì„ì„ ê¸°ì–µí•©ì‹œë‹¤. ë°ì´í„° : > í”„ë¡œë•ì…˜ ë‹¨ê³„ì˜ ë°ì´í„°ëŠ” í›¨ì”¬ ë” ë³µì¡í•©ë‹ˆë‹¤. ì¡ìŒì´ ë§ê³  ë¹„ì •í˜•ì¼ ìˆ˜ ìˆìœ¼ë©° ëŠì„ì—†ì´ ë³€í™”í•˜ì£ . > ê²Œë‹¤ê°€ ë°ì´í„°ê°€ í¸í–¥ëì„ ìˆ˜ ìˆëŠ”ë° ì–´ë–»ê²Œ í¸í–¥ëëŠ”ì§€ ëª¨ë¥¼ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. ì§„ì§œ raw dataë¶€í„° ë‹¤ë¤„ë³¸ ê²½í—˜ì´ ê½¤ ë§ì€ë°, ë‹¤ ì§„ì§œ ë„ˆë¬´ ë”ëŸ¬ì› ë‹¤ ã….ã….. ê³µì •ì„± : > ì—¬ëŸ¬ë¶„ í˜¹ì€ ì£¼ë³€ì— ìˆëŠ” ëˆ„êµ°ê°€ëŠ” ìì‹ ë„ ëª¨ë¥´ëŠ” ì‚¬ì´ì— í¸í–¥ëœ ìˆ˜í•™ ì•Œê³ ë¦¬ì¦˜ì˜ í¬ìƒìì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ì„ ê°€ëŠ¥ì„± : > ì‚°ì—…ê³„ ëŒ€ë¶€ë¶„ì˜ ML ìœ ìŠ¤ ì¼€ì´ìŠ¤ì—ì„œ í•´ì„ ê°€ëŠ¥ì„±ì€ ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ vs ì „í†µì ì¸ ì†Œí”„íŠ¸ì›¨ì–´ > ê¸°ì¡´ SWEì—ì„œëŠ” ì½”ë“œ í…ŒìŠ¤íŠ¸ ë° ë²„ì „ ê´€ë¦¬ì—ë§Œ ì§‘ì¤‘í•˜ë©´ ë˜ì§€ë§Œ MLì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ë˜í•œ í…ŒìŠ¤íŠ¸í•˜ê³  ë²„ì „ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. > í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì´ëŸ¬í•œ ëª¨ë¸ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë””ë²„ê¹…í•˜ëŠ” ì¼ ë˜í•œ ì‚¬ì†Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. > ML ëª¨ë¸ì´ ë³µì¡í•´ì§€ê³  ì‘ì—…ì— ëŒ€í•œ ê°€ì‹œì„±ì´ ë–¨ì–´ì§ì— ë”°ë¼ ë¬´ì—‡ì´ ì˜ëª»ëëŠ”ì§€ íŒŒì•…í•˜ê±°ë‚˜ ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë–„ ì‹ ì†í•˜ê²Œ ê²½ê³ ë¥¼ ë°›ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-27-ì„ íƒœì˜ìƒí™©ì—ì„œ_ReviewDay",
    "title": "ì„ íƒì˜ ìƒí™©ì—ì„œ ì“°ëŠ” í‘œí˜„ë“¤ Review Day",
    "date": "2025-07-27",
    "excerpt": "ì„ íƒì˜ ìƒí™©ì—ì„œ",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ One one hand One the other hand On one hand the note has really big screen. On the other the iPhone has a really good camera. On one hand, I havent had chinees food in a while. On the other hand, I'm craving fried chicken. Whether we eat Chinese food or fried chicken, we should order dilivery. The best thing about iPhone is its design. When it comes to online shopping, Amazon might be the best option. Although its a bit pricey, the bar has amazing view at night. Even though i`m not a big fan of animated films, I love Frozen. The dress is too expensive. It's pretty though. It's not about design. It's about performance. In which case, I'll probably go with the new Galaxy.",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/20250726_ì¢‹ì€ë°¤ì¢‹ì€ê¿ˆ",
    "title": "ì¢‹ì€ë°¤ ì¢‹ì€ê¿ˆ ì¸íŠ¸ë¡œ ì—°ìŠµ",
    "date": "2025-07-26",
    "excerpt": "ì¢‹ì€ë°¤ ì¢‹ì€ê¿ˆ ì¸íŠ¸ë¡œ ë¶€ë¶„ í•‘ê±°ìŠ¤íƒ€ì¼ ì•„ë¥´í˜ì§€ì˜¤ ì—°ìŠµ",
    "category": "Practice Log",
    "tags": [
      "ê¸°íƒ€ì—°ìŠµ",
      "Guitar Practice",
      "Arpeggio"
    ],
    "content": "@audioì¢‹ì€ë°¤ ì¢‹ì€ê¿ˆ ì¸íŠ¸ë¡œ ì—°ìŠµ ë­”ê°€ ë§›ì´ ì—†ë‹¤ ã…‹ã…‹ Fë‘ Fmê°€ ì™„ë²½í•˜ê²Œ ì•ˆëˆ„ë¦¼ íŠ¹íˆ 3ë²ˆì¤„ì´.",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/20250726_ê³ ë°±_ìŠ¤íŠ¸ë¡œí¬",
    "title": "ë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°± ìŠ¤íŠ¸ë¡œí¬ ì—°ìŠµ",
    "date": "2025-07-26",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [
      "ê¸°íƒ€ì—°ìŠµ",
      "Guitar Practice",
      "Strokes"
    ],
    "content": "@audioë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°± ìŠ¤íŠ¸ë¡œí¬ ì—°ìŠµ C#7ì´ ì˜ ì•ˆì¡í˜. íŠ¹íˆ G#m -> C#7ì´ ì˜ ì•ˆë¨. ê¸°íƒ€ í†¤ì€ ë‚«ë°°ë“œí•œ ë“¯? AC clean + front pickup + ì‹±ì‹±ì‹±",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/20250726_love",
    "title": "ì›¨ì´ë¸Œ íˆ¬ì–´ìŠ¤ ì‚¬ë‘ìœ¼ë¡œ ì¸íŠ¸ë¡œ ì»¤ë²„",
    "date": "2025-07-26",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [
      "ê¸°íƒ€ì—°ìŠµ",
      "Guitar Practice"
    ],
    "content": "@audioì›¨ì´ë¸Œ íˆ¬ì–´ìŠ¤ ì‚¬ë‘ìœ¼ë¡œ ì¸íŠ¸ë¡œ ì»¤ë²„ í™•ì‹¤íˆ ë…¹ìŒí•´ì„œ ë“¤ì–´ë³´ë‹ˆê¹Œ ëŠê¸°ëŠ” ë¶€ë¶„ì´ ë§ì´ ë“¤ë¦¼. ë°•ì ë‚˜ê°€ëŠ” ê²ƒë„ ë³´ì´ê³ .",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/vDB_code_notebook",
    "title": "vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤",
    "date": "2025-07-25",
    "excerpt": "vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤",
    "category": "Data Science",
    "tags": [
      "VectorDB",
      "Qdrant",
      "ChromaDB",
      "FAISS",
      "Milvus"
    ],
    "content": "--- title: \"vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤\" date: \"2025-07-25\" category: \"Data Science\" tags: [\"VectorDB\", \"Qdrant\", \"ChromaDB\", \"FAISS\", \"Milvus\"] excerpt: \"vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤\" --- # Qdrant íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ í•„ìš” pip install qdrant-client from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np Qdrant ì„œë²„ì— ì—°ê²° (ë¡œì»¬ ì„œë²„ ì˜ˆì‹œ) client = QdrantClient(host=\"localhost\", port=6333) ì»¬ë ‰ì…˜ ìƒì„± (ë²¡í„° ì°¨ì› 1536, cosine ê±°ë¦¬) client.recreate_collection( collection_name=\"my_collection\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) ë²¡í„° ë°ì´í„° ì‚½ì… vectors = np.random.rand(3, 1536).tolist() # 3ê°œ ë²¡í„°, 1536ì°¨ì› payloads = [ {\"doc_id\": \"A1\", \"text\": \"ë¬¸ì„œ1\"}, {\"doc_id\": \"A2\", \"text\": \"ë¬¸ì„œ2\"}, {\"doc_id\": \"A3\", \"text\": \"ë¬¸ì„œ3\"}, ] points = [ PointStruct(id=i, vector=vectors[i], payload=payloads[i]) for i in range(3) ] client.upsert(collection_name=\"my_collection\", points=points) ìœ ì‚¬ë„ ê²€ìƒ‰ (ì„ì˜ ì¿¼ë¦¬ ë²¡í„°) query_vector = np.random.rand(1536).tolist() results = client.search( collection_name=\"my_collection\", query_vector=query_vector, limit=2, ) for hit in results: print(f\"ID: {hit.id}, Score: {hit.score}, Payload: {hit.payload}\") from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np client = QdrantClient(host=\"localhost\", port=6333) íšŒì‚¬A ì»¬ë ‰ì…˜ ìƒì„± client.recreate_collection( collection_name=\"companyA_docs\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) íšŒì‚¬B ì»¬ë ‰ì…˜ ìƒì„± client.recreate_collection( collection_name=\"companyB_docs\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) íšŒì‚¬A ë°ì´í„° ì‚½ì… vectors_a = np.random.rand(2, 1536).tolist() payloads_a = [{\"doc_id\": \"A1\"}, {\"doc_id\": \"A2\"}] points_a = [PointStruct(id=i, vector=vectors_a[i], payload=payloads_a[i]) for i in range(2)] client.upsert(collection_name=\"companyA_docs\", points=points_a) íšŒì‚¬B ë°ì´í„° ì‚½ì… vectors_b = np.random.rand(2, 1536).tolist() payloads_b = [{\"doc_id\": \"B1\"}, {\"doc_id\": \"B2\"}] points_b = [PointStruct(id=i, vector=vectors_b[i], payload=payloads_b[i]) for i in range(2)] client.upsert(collection_name=\"companyB_docs\", points=points_b) íšŒì‚¬A ë°ì´í„°ë§Œ ê²€ìƒ‰ query_vector = np.random.rand(1536).tolist() results = client.search( collection_name=\"companyA_docs\", query_vector=query_vector, limit=1, ) for hit in results: print(f\"íšŒì‚¬A ê²°ê³¼: {hit.payload}\") íšŒì‚¬B ë°ì´í„°ë§Œ ê²€ìƒ‰ results = client.search( collection_name=\"companyB_docs\", query_vector=query_vector, limit=1, ) for hit in results: print(f\"íšŒì‚¬B ê²°ê³¼: {hit.payload}\") # Multi tenancy ë¥¼ ì˜ êµ¬í˜„í•˜ëŠ” ë²• # 1. ì»¬ë ‰ì…˜ ë„¤ì´ë° ì»¨ë²¤ì…˜ from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np client = QdrantClient(host=\"localhost\", port=6333) í…Œë„ŒíŠ¸ë³„ ì»¬ë ‰ì…˜ ë„¤ì´ë° íŒ¨í„´ def get_collection_name(tenant_id: str, data_type: str = \"docs\") -> str: \"\"\"í…Œë„ŒíŠ¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±\"\"\" return f\"{tenant_id}_{data_type}\" ì˜ˆì‹œ: íšŒì‚¬ë³„ ì»¬ë ‰ì…˜ ìƒì„± tenants = [\"company_a\", \"company_b\", \"company_c\"] data_types = [\"docs\", \"images\", \"products\"] for tenant in tenants: for data_type in data_types: collection_name = get_collection_name(tenant, data_type) client.recreate_collection( collection_name=collection_name, vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) print(f\"Created collection: {collection_name}\") # 2. ë©€í‹° í…Œë„Œì‹œ ë§¤ë‹ˆì € í´ë˜ìŠ¤ class QdrantMultiTenantManager: def __init__(self, client: QdrantClient): self.client = client self.tenant_configs = { \"company_a\": { \"collections\": [\"docs\", \"images\"], \"vector_size\": 1536, \"distance\": Distance.COSINE }, \"company_b\": { \"collections\": [\"docs\", \"products\"], \"vector_size\": 1536, \"distance\": Distance.COSINE } } def get_collection_name(self, tenant_id: str, collection_type: str) -> str: \"\"\"í…Œë„ŒíŠ¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±\"\"\" return f\"{tenant_id}_{collection_type}\" def ensure_collection_exists(self, tenant_id: str, collection_type: str): \"\"\"ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±\"\"\" if tenant_id not in self.tenant_configs: raise ValueError(f\"Unknown tenant: {tenant_id}\") collection_name = self.get_collection_name(tenant_id, collection_type) config = self.tenant_configs[tenant_id] # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ collections = self.client.get_collections() collection_names = [c.name for c in collections.collections] if collection_name not in collection_names: self.client.create_collection( collection_name=collection_name, vectors_config=VectorParams( size=config[\"vector_size\"], distance=config[\"distance\"] ), ) print(f\"Created collection: {collection_name}\") def upsert_for_tenant(self, tenant_id: str, collection_type: str, vectors, payloads, ids): \"\"\"í…Œë„ŒíŠ¸ë³„ ë°ì´í„° ì‚½ì…\"\"\" self.ensure_collection_exists(tenant_id, collection_type) collection_name = self.get_collection_name(tenant_id, collection_type) points = [ PointStruct(id=id_, vector=vector, payload=payload) for id_, vector, payload in zip(ids, vectors, payloads) ] return self.client.upsert( collection_name=collection_name, points=points ) def search_for_tenant(self, tenant_id: str, collection_type: str, query_vector, limit=10, kwargs): \"\"\"í…Œë„ŒíŠ¸ë³„ ê²€ìƒ‰\"\"\" collection_name = self.get_collection_name(tenant_id, collection_type) return self.client.search( collection_name=collection_name, query_vector=query_vector, limit=limit, kwargs ) def get_tenant_collections(self, tenant_id: str): \"\"\"í…Œë„ŒíŠ¸ì˜ ëª¨ë“  ì»¬ë ‰ì…˜ ì¡°íšŒ\"\"\" collections = self.client.get_collections() tenant_prefix = f\"{tenant_id}_\" return [ c.name for c in collections.collections if c.name.startswith(tenant_prefix) ] def delete_tenant_data(self, tenant_id: str): \"\"\"í…Œë„ŒíŠ¸ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ\"\"\" collections = self.get_tenant_collections(tenant_id) for collection_name in collections: self.client.delete_collection(collection_name) print(f\"Deleted collection: {collection_name}\") # 3. ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” manager = QdrantMultiTenantManager(client) íšŒì‚¬Aì— ë¬¸ì„œ ë°ì´í„° ì‚½ì… vectors_a = np.random.rand(3, 1536).tolist() payloads_a = [ {\"doc_id\": \"A1\", \"title\": \"íšŒì‚¬A ë¬¸ì„œ1\", \"tenant\": \"company_a\"}, {\"doc_id\": \"A2\", \"title\": \"íšŒì‚¬A ë¬¸ì„œ2\", \"tenant\": \"company_a\"}, {\"doc_id\": \"A3\", \"title\": \"íšŒì‚¬A ë¬¸ì„œ3\", \"tenant\": \"company_a\"} ] ids_a = [\"A1\", \"A2\", \"A3\"] manager.upsert_for_tenant(\"company_a\", \"docs\", vectors_a, payloads_a, ids_a) íšŒì‚¬Bì— ì œí’ˆ ë°ì´í„° ì‚½ì… vectors_b = np.random.rand(2, 1536).tolist() payloads_b = [ {\"product_id\": \"B1\", \"name\": \"íšŒì‚¬B ì œí’ˆ1\", \"tenant\": \"company_b\"}, {\"product_id\": \"B2\", \"name\": \"íšŒì‚¬B ì œí’ˆ2\", \"tenant\": \"company_b\"} ] ids_b = [\"B1\", \"B2\"] manager.upsert_for_tenant(\"company_b\", \"products\", vectors_b, payloads_b, ids_b) í…Œë„ŒíŠ¸ë³„ ê²€ìƒ‰ query_vector = np.random.rand(1536).tolist() íšŒì‚¬A ë¬¸ì„œ ê²€ìƒ‰ results_a = manager.search_for_tenant(\"company_a\", \"docs\", query_vector) print(\"íšŒì‚¬A ê²€ìƒ‰ ê²°ê³¼:\", [hit.payload for hit in results_a]) íšŒì‚¬B ì œí’ˆ ê²€ìƒ‰ results_b = manager.search_for_tenant(\"company_b\", \"products\", query_vector) print(\"íšŒì‚¬B ê²€ìƒ‰ ê²°ê³¼:\", [hit.payload for hit in results_b]) # ë°±ì—… ë³µêµ¬ import json import os class TenantBackup: def __init__(self, manager: QdrantMultiTenantManager): self.manager = manager def backup_tenant(self, tenant_id: str, backup_path: str): \"\"\"í…Œë„ŒíŠ¸ë³„ ë°±ì—…\"\"\" collections = self.manager.get_tenant_collections(tenant_id) for collection_name in collections: # ì»¬ë ‰ì…˜ ë°ì´í„° ë‚´ë³´ë‚´ê¸° points = self.manager.client.scroll( collection_name=collection_name, limit=10000 # ì ì ˆí•œ ë°°ì¹˜ í¬ê¸° )[0] # ë°±ì—… íŒŒì¼ì— ì €ì¥ backup_file = f\"{backup_path}/{collection_name}_backup.json\" with open(backup_file, 'w') as f: json.dump(points, f, indent=2) print(f\"Backed up {collection_name} to {backup_file}\") def restore_tenant(self, tenant_id: str, backup_path: str): \"\"\"í…Œë„ŒíŠ¸ë³„ ë³µêµ¬\"\"\" collections = self.manager.get_tenant_collections(tenant_id) for collection_name in collections: backup_file = f\"{backup_path}/{collection_name}_backup.json\" if os.path.exists(backup_file): with open(backup_file, 'r') as f: points_data = json.load(f) # ë°ì´í„° ë³µêµ¬ self.manager.client.upsert( collection_name=collection_name, points=points_data ) print(f\"Restored {collection_name} from {backup_file}\")",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/compare-vDB-libraries",
    "title": "VectorDBì— ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ë¹„êµ",
    "date": "2025-07-25",
    "excerpt": "ì±—ë´‡ êµ¬í˜„í•˜ë©´ì„œ RAG ë¶€ë¶„ì— ì‚¬ìš©í•  ë²¡í„°í™”ëœ ë¬¸ì„œë“¤ì„ ê´€ë¦¬í•  DBë“¤ì„ ë¹„êµ",
    "category": "Data Science",
    "tags": [
      "vector-database",
      "chatbot",
      "RAG"
    ],
    "content": "Qdrant vs ChromaDB vs FAISS vs Milvus DB ì£¼ìš” íŠ¹ì§• ì¥ì  ë‹¨ì  ------------ ---------------------------------------- -------------------------------------------------------------- -------------------------------------------------------------- Qdrant Rust ê¸°ë°˜, í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤, API ë‹¤ì–‘ ë¹ ë¥¸ ì¿¼ë¦¬, í™•ì¥ì„±, ë©”íƒ€ë°ì´í„° í•„í„°ë§, í´ë¼ìš°ë“œ ì§€ì› í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ í•„ìš”, ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥ì€ ë¯¸í¡ ChromaDB Python ê¸°ë°˜, ë‚´ì¥í˜•, LangChain ì—°ë™ ì‰¬ìš´ ì„¤ì¹˜, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, íŒŒì´ì¬ ì¹œí™”ì , ì‹¤ì‹œê°„ ê²€ìƒ‰ ëŒ€ê·œëª¨ ë°ì´í„°/ë¶„ì‚° ë¯¸ì§€ì›, ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì´ìŠˆ, ë¬¸ì„œí™” ë¶€ì¡± FAISS Facebook ê°œë°œ, ë¼ì´ë¸ŒëŸ¬ë¦¬, GPU ì§€ì› ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤, ëŒ€ê·œëª¨ ë²¡í„° ì²˜ë¦¬, ì˜¤í”ˆì†ŒìŠ¤ DB ì•„ë‹˜(ì €ì¥/ìš´ì˜ ì§ì ‘ êµ¬í˜„ í•„ìš”), í™•ì¥ì„±/ìš´ì˜ ìë™í™” ë¯¸í¡ Milvus ë¶„ì‚°í˜•, í´ë¼ìš°ë“œ/ì˜¨í”„ë ˆë¯¸ìŠ¤, MLOps ì—°ë™ ëŒ€ê·œëª¨ ë¶„ì‚°ì²˜ë¦¬, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤, RBAC ë“± ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ ë³µì¡í•œ ë°°í¬(ì¿ ë²„ë„¤í‹°ìŠ¤ ë“±), ë¦¬ì†ŒìŠ¤ ìš”êµ¬ ë†’ìŒ, ëŸ¬ë‹ì»¤ë¸Œ ìˆìŒ --- Qdrant íŠ¹ì§•: Rustë¡œ ê°œë°œëœ ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° DB. í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤(ë²¡í„°+ì†ì„±), ë‹¤ì–‘í•œ ê±°ë¦¬ ì¸¡ë„(ìœ í´ë¦¬ë“œ, ì½”ì‚¬ì¸ ë“±), REST/gRPC API, í´ë¼ìš°ë“œ ë° ì˜¨í”„ë ˆë¯¸ìŠ¤ ì§€ì›. ì¥ì : ë¹ ë¥¸ ì¿¼ë¦¬ ì„±ëŠ¥(HNSW, PQ ë“± ê³ ê¸‰ ì¸ë±ìŠ¤) ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§(ì†ì„±+ë²¡í„° ë™ì‹œ ê²€ìƒ‰) í´ëŸ¬ìŠ¤í„° í™•ì¥ì„±, í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ìš°ë“œ ì§€ì› ë¬¸ì„œí™” ë° ì»¤ë®¤ë‹ˆí‹° í™œë°œ ë‹¨ì : í´ëŸ¬ìŠ¤í„° ê´€ë¦¬/ìš´ì˜ ë³µì¡ì„± ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥(ì˜ˆ: ìë™ í´ëŸ¬ìŠ¤í„°ë§, ì¿¼ë¦¬ ë¦¬ë°¸ëŸ°ì‹± ë“±) ë¯¸í¡ ì¶”ì²œ ì‚¬ìš©ì²˜: RAG, ì¶”ì²œì‹œìŠ¤í…œ, ëŒ€ê·œëª¨ ê²€ìƒ‰, ì†ì„±+ë²¡í„° ë™ì‹œ í•„í„°ë§ì´ í•„ìš”í•œ ì„œë¹„ìŠ¤ ChromaDB íŠ¹ì§•: Python ê¸°ë°˜ ë‚´ì¥í˜• ë²¡í„° DB. LangChain ë“± LLM/RAG íŒŒì´í”„ë¼ì¸ì— ìµœì í™”. ë‚´ì¥ ì„ë² ë”© ëª¨ë¸, SQLite/ClickHouse ìŠ¤í† ë¦¬ì§€ ì§€ì›. ì¥ì : ì„¤ì¹˜/ì‚¬ìš©ì´ ë§¤ìš° ê°„ë‹¨, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ íŒŒì´ì¬ ì¹œí™”ì (ML/AI ì›Œí¬í”Œë¡œìš°ì™€ ì—°ë™ ìš©ì´) ì‹¤ì‹œê°„ ê²€ìƒ‰, ë™ì  ë©”íƒ€ë°ì´í„° í•„í„°ë§ LangChain, OpenAI API ë“±ê³¼ ì—°ë™ ì‰¬ì›€ ë‹¨ì : ëŒ€ê·œëª¨ ë°ì´í„°/ë¶„ì‚° ë¯¸ì§€ì›(ì‹±ê¸€ë…¸ë“œ í•œê³„) ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„œë¹„ìŠ¤ í¬ë˜ì‹œ ì´ìŠˆ(2024ë…„ ê¸°ì¤€) ë¬¸ì„œí™”/ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ ë¶€ì¡± ì¶”ì²œ ì‚¬ìš©ì²˜: ì†Œê·œëª¨ RAG, LLM ì‹¤í—˜, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, íŒŒì´ì¬ ê¸°ë°˜ í”„ë¡œì íŠ¸ FAISS íŠ¹ì§•: Facebook AIê°€ ê°œë°œí•œ ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬. ë‹¤ì–‘í•œ ì¸ë±ìŠ¤(HNSW, IVF, PQ ë“±), GPU ì§€ì›, ëŒ€ê·œëª¨ ë²¡í„° ì²˜ë¦¬ì— ìµœì í™”. ì¥ì : ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„(íŠ¹íˆ GPU í™œìš© ì‹œ) ë‹¤ì–‘í•œ ì¸ë±ìŠ¤/ê±°ë¦¬ ì¸¡ë„ ì§€ì› ëŒ€ê·œëª¨ ë²¡í„° ë°ì´í„° ì²˜ë¦¬ì— ê°•ì  ì˜¤í”ˆì†ŒìŠ¤, ì»¤ë®¤ë‹ˆí‹° í™œë°œ ë‹¨ì : DBê°€ ì•„ë‹ˆë¼ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì €ì¥/ìš´ì˜ ì§ì ‘ êµ¬í˜„ í•„ìš”) ë¶„ì‚°/í™•ì¥ì„±, ìš´ì˜ ìë™í™” ë¯¸í¡ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ë“± DBì  ê¸°ëŠ¥ ë¶€ì¡± ì¶”ì²œ ì‚¬ìš©ì²˜: ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì§ì ‘ êµ¬ì¶•, ML/AI ì‹¤í—˜, GPU ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ Milvus íŠ¹ì§•: ë¶„ì‚°í˜• ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° DB. í´ë¼ìš°ë“œ/ì˜¨í”„ë ˆë¯¸ìŠ¤, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤(HNSW, IVF ë“±), RBAC, MLOps ì—°ë™, ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”. ì¥ì : ëŒ€ê·œëª¨ ë¶„ì‚°ì²˜ë¦¬(ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ë²¡í„°) ë‹¤ì–‘í•œ ì¸ë±ìŠ¤, GPU ê°€ì† ì§€ì› ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥(RBAC, ê°ì‚¬ë¡œê·¸, GDPR ë“±) MLOps, Grafana/Prometheus ë“±ê³¼ ì—°ë™ ë‹¨ì : ë³µì¡í•œ ë°°í¬(ì¿ ë²„ë„¤í‹°ìŠ¤ ë“± ì¸í”„ë¼ í•„ìš”) ë†’ì€ ë¦¬ì†ŒìŠ¤ ìš”êµ¬, ëŸ¬ë‹ì»¤ë¸Œ ìˆìŒ ì†Œê·œëª¨/ë‹¨ì¼ ë…¸ë“œì—ëŠ” ê³¼í•œ ì„ íƒì¼ ìˆ˜ ìˆìŒ ì¶”ì²œ ì‚¬ìš©ì²˜: ëŒ€ê·œëª¨ RAG, ì—”í„°í”„ë¼ì´ì¦ˆ ê²€ìƒ‰, ë³´ì•ˆ/ê°ì‚¬/í™•ì¥ì„± ì¤‘ìš”í•œ ì„œë¹„ìŠ¤ --- ì—¬ëŸ¬ íšŒì‚¬/ë¬¸ì„œ ê·¸ë£¹ êµ¬ë¶„ ê´€ë¦¬: ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤/íŒŒí‹°ì…˜ ë¹„êµ DB ë°ì´í„° ë¶„ë¦¬ ì§€ì› ë¶„ë¦¬ ë°©ì‹(ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë“±) ë©€í‹°í…Œë„Œì‹œ/ê¶Œí•œê´€ë¦¬ ì¶”ì²œë„(íšŒì‚¬ë³„ êµ¬ë¶„) ------------ ------------------ ------------------------------- -------------------- -------------------- Qdrant O ì»¬ë ‰ì…˜(collection) O â˜…â˜…â˜…â˜…â˜† ChromaDB O ì»¬ë ‰ì…˜(collection) X â˜…â˜…â˜†â˜†â˜† FAISS X(ì§ì ‘êµ¬í˜„) ì—†ìŒ(ì½”ë“œë¡œ ì¸ë±ìŠ¤ ë¶„ë¦¬) X â˜…â˜†â˜†â˜†â˜† Milvus O ì»¬ë ‰ì…˜/íŒŒí‹°ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ O â˜…â˜…â˜…â˜…â˜… --- ì‹¤ë¬´/ìš´ì˜ ê´€ì ì—ì„œì˜ ì¶”ì²œ Qdrant: ì»¬ë ‰ì…˜ ë‹¨ìœ„ë¡œ íšŒì‚¬A, íšŒì‚¬B ë“± ì™„ì „íˆ ë¶„ë¦¬ëœ ë°ì´í„° ê´€ë¦¬ ê°€ëŠ¥. API/ê¶Œí•œê´€ë¦¬ë„ ìš°ìˆ˜. ìš´ì˜/í™•ì¥ì„± ëª¨ë‘ ê°•ì . Milvus: ì»¬ë ‰ì…˜, íŒŒí‹°ì…˜, ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë“± ë‹¤ì–‘í•œ ë¶„ë¦¬ ë°©ì‹ ì œê³µ. ëŒ€ê·œëª¨ ë©€í‹°í…Œë„Œì‹œ, RBAC ë“± ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì— ìµœì . ChromaDB: ì»¬ë ‰ì…˜ìœ¼ë¡œ êµ¬ë¶„ì€ ê°€ëŠ¥í•˜ë‚˜, ì‹±ê¸€ë…¸ë“œ/ë‚´ì¥í˜•ì´ë¼ ëŒ€ê·œëª¨ ìš´ì˜ì—ëŠ” í•œê³„. ì‹¤í—˜/ì†Œê·œëª¨ì— ì í•©. FAISS: DBê°€ ì•„ë‹ˆë¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¼ ë°ì´í„° ë¶„ë¦¬/ê¶Œí•œê´€ë¦¬ ì§ì ‘ êµ¬í˜„ í•„ìš”. ìš´ì˜/í™•ì¥ì„± ëª©ì ì—ëŠ” ë¹„ì¶”ì²œ. ê²°ë¡ : íšŒì‚¬ë³„, í”„ë¡œì íŠ¸ë³„ ë“± ì—¬ëŸ¬ ê·¸ë£¹ì˜ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë²¡í„° DBì—ì„œ ì•ˆì „í•˜ê²Œ êµ¬ë¶„/ìš´ì˜í•˜ë ¤ë©´ Qdrant ë˜ëŠ” Milvusê°€ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤. (ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤/ê¶Œí•œê´€ë¦¬ ë“± ì§€ì›) ChromaDBëŠ” ì†Œê·œëª¨/ì‹¤í—˜ì—, FAISSëŠ” ì§ì ‘ êµ¬í˜„ì´ í•„ìš”í•˜ë¯€ë¡œ ëŒ€ê·œëª¨ ìš´ì˜ì—ëŠ” ë¶€ì í•©í•©ë‹ˆë‹¤. ë©€í‹°í…Œë„Œì‹œë€? ë©€í‹°í…Œë„Œì‹œ(Multi-tenancy)ëŠ” í•˜ë‚˜ì˜ ë°ì´í„°ë² ì´ìŠ¤(í˜¹ì€ ì‹œìŠ¤í…œ) ì•ˆì—ì„œ ì—¬ëŸ¬ ì‚¬ìš©ì ê·¸ë£¹(íšŒì‚¬, ì¡°ì§, í”„ë¡œì íŠ¸ ë“±)ì´ ì„œë¡œì˜ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  ì•ˆì „í•˜ê²Œ ë¶„ë¦¬ëœ ê³µê°„ì—ì„œ ë°ì´í„°ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. í…Œë„ŒíŠ¸(tenant): í•˜ë‚˜ì˜ ê³ ê°, íšŒì‚¬, ì¡°ì§, í”„ë¡œì íŠ¸ ë“± ë…ë¦½ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê´€ë¦¬í•´ì•¼ í•˜ëŠ” ë‹¨ìœ„ ë©€í‹°í…Œë„Œì‹œ: ì—¬ëŸ¬ í…Œë„ŒíŠ¸(íšŒì‚¬A, íšŒì‚¬B, â€¦)ê°€ í•œ DB ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê³µìœ í•˜ì§€ë§Œ, ê° í…Œë„ŒíŠ¸ì˜ ë°ì´í„°ëŠ” ì™„ì „íˆ ë¶„ë¦¬ë˜ì–´ ìˆê³ , ì„œë¡œì˜ ë°ì´í„°ì— ì ‘ê·¼í•˜ê±°ë‚˜ ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ìŒ ë²¡í„° DBì—ì„œì˜ ë©€í‹°í…Œë„Œì‹œ ì˜ˆì‹œ íšŒì‚¬Aì™€ íšŒì‚¬Bê°€ ê°™ì€ Qdrant/Milvus ì„œë²„ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, íšŒì‚¬AëŠ” ìì‹ ì˜ ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤/íŒŒí‹°ì…˜ì—ë§Œ ì ‘ê·¼ ê°€ëŠ¥ íšŒì‚¬Bë„ ë§ˆì°¬ê°€ì§€ë¡œ ìì‹ ì˜ ë°ì´í„°ë§Œ ì ‘ê·¼ ê°€ëŠ¥ ì„œë¡œì˜ ë°ì´í„°ëŠ” ì ˆëŒ€ ë³´ê±°ë‚˜ ìˆ˜ì •í•  ìˆ˜ ì—†ìŒ ê¶Œí•œ ê´€ë¦¬(RBAC), API ì¸ì¦ ë“±ìœ¼ë¡œ ë³´ì•ˆë„ ë³´ì¥ ì™œ ì¤‘ìš”í• ê¹Œ? ìš´ì˜ë¹„ ì ˆê°: íšŒì‚¬ë§ˆë‹¤ DBë¥¼ ë”°ë¡œ ë„ìš°ì§€ ì•Šì•„ë„ ë¨ ë³´ì•ˆ/ê²©ë¦¬: ë°ì´í„°ê°€ ì„ì´ì§€ ì•Šê³ , ì‹¤ìˆ˜ë¡œë„ ì ‘ê·¼ ë¶ˆê°€ í™•ì¥ì„±: í…Œë„ŒíŠ¸ê°€ ëŠ˜ì–´ë‚˜ë„ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì—ì„œ ê´€ë¦¬ ê°€ëŠ¥ ì •ë¦¬: ë©€í‹°í…Œë„Œì‹œëŠ” â€œì—¬ëŸ¬ íšŒì‚¬/ì¡°ì§ì´ í•˜ë‚˜ì˜ DBë¥¼ ì“°ë”ë¼ë„, ê°ìì˜ ë°ì´í„°ê°€ ì™„ì „íˆ ë¶„ë¦¬Â·ê²©ë¦¬ë˜ì–´ ì•ˆì „í•˜ê²Œ ê´€ë¦¬ë˜ëŠ” êµ¬ì¡°â€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. (SaaS ì„œë¹„ìŠ¤ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…!) ë¬´ë£Œ ì„œë¹„ìŠ¤/ì˜¤í”ˆì†ŒìŠ¤ ì—¬ë¶€ DB ì˜¤í”ˆì†ŒìŠ¤ ìì²´ ì„œë²„ ë¬´ë£Œ ê³µì‹ í´ë¼ìš°ë“œ ë¬´ë£Œ í”Œëœ ë¹„ê³  ------------ ---------- --------------- ------------------------ --------------------------- Qdrant O O O (Qdrant Cloud Free) ì˜¤í”ˆì†ŒìŠ¤, í´ë¼ìš°ë“œ ë¬´ë£Œ í‹°ì–´(ì œí•œ ìˆìŒ) ChromaDB O O X ì˜¤í”ˆì†ŒìŠ¤, ìì²´ ì„œë²„ë§Œ ì§€ì› FAISS O O X ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬, í´ë¼ìš°ë“œ ì—†ìŒ Milvus O O O (Zilliz Cloud Free) ì˜¤í”ˆì†ŒìŠ¤, Zilliz Cloud ë¬´ë£Œ í‹°ì–´(ì œí•œ ìˆìŒ) --- ìƒì„¸ ì„¤ëª… Qdrant ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ)ë¡œ ì§ì ‘ ì„œë²„ ìš´ì˜ ê°€ëŠ¥ ê³µì‹ Qdrant Cloudì—ì„œ ë¬´ë£Œ í”Œëœ(ì œí•œì  ìš©ëŸ‰/ì¿¼ë¦¬) ì œê³µ ChromaDB ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ)ë¡œ ì§ì ‘ ì„œë²„ ìš´ì˜ë§Œ ê°€ëŠ¥ ê³µì‹ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤/ë¬´ë£Œ í”Œëœ ì—†ìŒ FAISS ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ) ë¼ì´ë¸ŒëŸ¬ë¦¬ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì—†ìŒ(ì§ì ‘ ì„œë²„/ì½”ë“œë¡œë§Œ ì‚¬ìš©) Milvus ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ)ë¡œ ì§ì ‘ ì„œë²„ ìš´ì˜ ê°€ëŠ¥ ê³µì‹ Zilliz Cloudì—ì„œ ë¬´ë£Œ í”Œëœ(ì œí•œì  ìš©ëŸ‰/ì¿¼ë¦¬) ì œê³µ --- ì •ë¦¬: 4ê°œ ëª¨ë‘ ì˜¤í”ˆì†ŒìŠ¤ë¼ ì§ì ‘ ì„œë²„ë¥¼ ë„ìš°ë©´ ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥ Qdrant, MilvusëŠ” ê³µì‹ í´ë¼ìš°ë“œì—ì„œ ë¬´ë£Œ í”Œëœë„ ì œê³µ ChromaDB, FAISSëŠ” í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ê°€ ì—†ê³ , ì§ì ‘ ì„¤ì¹˜/ìš´ì˜ë§Œ ê°€ëŠ¥ --- Qdrant vs ChromaDB ì»¬ë ‰ì…˜ì˜ ì°¨ì´ í•­ëª© Qdrant ì»¬ë ‰ì…˜ ChromaDB ì»¬ë ‰ì…˜ --------------------- ------------------------------------- ---------------------------------- ë¶„ë¦¬ ìˆ˜ì¤€ ì™„ì „ ë¶„ë¦¬(ë¬¼ë¦¬ì /ë…¼ë¦¬ì ) ë…¼ë¦¬ì  ë¶„ë¦¬(ë‚´ì¥ DB ë‚´ í…Œì´ë¸”) í™•ì¥ì„± ë¶„ì‚°/í´ëŸ¬ìŠ¤í„° ì§€ì›, ëŒ€ê·œëª¨ ìš´ì˜ ê°€ëŠ¥ ì‹±ê¸€ë…¸ë“œ, ëŒ€ê·œëª¨ ìš´ì˜ í•œê³„ ë³´ì•ˆ/ê¶Œí•œê´€ë¦¬ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê¶Œí•œ/RBAC ì§€ì› ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê¶Œí•œê´€ë¦¬ ë¯¸í¡ ìš´ì˜ ì‹ ë¢°ì„± ì¥ì•  ê²©ë¦¬, ë°ì´í„° ì†ìƒ ìµœì†Œí™” ì „ì²´ DB ì¥ì•  ì‹œ ëª¨ë“  ì»¬ë ‰ì…˜ ì˜í–¥ API/ê´€ë¦¬ë„êµ¬ REST/gRPC ë“± ê³µì‹ API, ê´€ë¦¬ë„êµ¬ í’ë¶€ íŒŒì´ì¬ API ìœ„ì£¼, ê´€ë¦¬ë„êµ¬ ì œí•œì  ë°±ì—…/ë³µêµ¬ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ë°±ì—…/ë³µêµ¬ ì§€ì› ì „ì²´ DB ë‹¨ìœ„ ë°±ì—…/ë³µêµ¬ ë©€í‹°í…Œë„Œì‹œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë©€í‹°í…Œë„Œì‹œ êµ¬í˜„ ìš©ì´ ì‹¤í—˜/ì†Œê·œëª¨ ë©€í‹°í…Œë„Œì‹œì— ì í•© ìƒì„¸ ì„¤ëª… Qdrant ì»¬ë ‰ì…˜ì´ ì™„ì „íˆ ë¶„ë¦¬ëœ ë°ì´í„°ë² ì´ìŠ¤ì²˜ëŸ¼ ë™ì‘í•©ë‹ˆë‹¤. ê° ì»¬ë ‰ì…˜ì€ ë³„ë„ì˜ ì¸ë±ìŠ¤, ì„¤ì •, ê¶Œí•œ, ë°±ì—…/ë³µêµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤, ë©€í‹°í…Œë„Œì‹œ, ë³´ì•ˆì´ ì¤‘ìš”í•œ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤. ì¥ì• ê°€ ë‚˜ë„ í•œ ì»¬ë ‰ì…˜ë§Œ ì˜í–¥ë°›ê³ , ë‚˜ë¨¸ì§€ëŠ” ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤. ChromaDB ì»¬ë ‰ì…˜ì€ ë…¼ë¦¬ì  ë¶„ë¦¬(í…Œì´ë¸” ìˆ˜ì¤€)ë¡œ, í•˜ë‚˜ì˜ ë‚´ì¥ DB íŒŒì¼ì— ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì‹±ê¸€ë…¸ë“œ ê¸°ë°˜ì´ë¼ ëŒ€ê·œëª¨ ìš´ì˜, ì¥ì•  ê²©ë¦¬, ê¶Œí•œ ë¶„ë¦¬ì—ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì‹¤í—˜, ì†Œê·œëª¨ ì„œë¹„ìŠ¤, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…ì—ëŠ” ì¶©ë¶„íˆ ìœ ìš©í•©ë‹ˆë‹¤. Qdrant vs Milvus ë¹„êµ í•­ëª© Qdrant Milvus --------------------- ---------------------------------------------- --------------------------------------------- ê°œë°œ ì–¸ì–´/ì—”ì§„ Rust ê¸°ë°˜, ê²½ëŸ‰/ê³ ì„±ëŠ¥ C++/Go ê¸°ë°˜, ëŒ€ê·œëª¨ ë¶„ì‚° ì•„í‚¤í…ì²˜ ì•„í‚¤í…ì²˜ ì‹±ê¸€ë…¸ë“œ/í´ëŸ¬ìŠ¤í„° ëª¨ë‘ ì§€ì›, ê²½ëŸ‰í™” ë¶„ì‚°í˜•(í´ëŸ¬ìŠ¤í„° ì „ìš©), ëŒ€ê·œëª¨ í™•ì¥ì„± ì„¤ì¹˜/ìš´ì˜ ì„¤ì¹˜/ìš´ì˜ ê°„ë‹¨, ë¡œì»¬/í´ë¼ìš°ë“œ ëª¨ë‘ ìš©ì´ ì¿ ë²„ë„¤í‹°ìŠ¤ ë“± ì¸í”„ë¼ í•„ìš”, ë³µì¡í•œ ë°°í¬ í™•ì¥ì„± ì¤‘~ëŒ€ê·œëª¨ê¹Œì§€ ë¬´ë‚œ, í´ëŸ¬ìŠ¤í„° ì§€ì› ì´ˆëŒ€ê·œëª¨(ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ë²¡í„°) ë¶„ì‚°ì²˜ë¦¬ ìµœì í™” API/ì–¸ì–´ ì§€ì› REST/gRPC, Python ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì› REST/gRPC, Python/Java/Go ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì› ì¸ë±ìŠ¤/ê²€ìƒ‰ HNSW, PQ ë“± ê³ ì„±ëŠ¥ ì¸ë±ìŠ¤, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ HNSW, IVF, DiskANN ë“± ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„°/í•„í„° ë²¡í„°+ì†ì„± ë™ì‹œ í•„í„°ë§(í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°•ì ) ë²¡í„°+ì†ì„± ë™ì‹œ í•„í„°ë§, ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì› ê¶Œí•œ/ë³´ì•ˆ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê¶Œí•œ, RBAC(í´ë¼ìš°ë“œ) RBAC, ê°ì‚¬ë¡œê·¸, GDPR ë“± ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ ë©€í‹°í…Œë„Œì‹œ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ì™„ì „ ë¶„ë¦¬, ë©€í‹°í…Œë„Œì‹œ ìš©ì´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤/ì»¬ë ‰ì…˜/íŒŒí‹°ì…˜ ë“± ë¶„ë¦¬, ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ Qdrant Cloud(ë¬´ë£Œ/ìœ ë£Œ í”Œëœ) Zilliz Cloud(ë¬´ë£Œ/ìœ ë£Œ í”Œëœ) ì»¤ë®¤ë‹ˆí‹°/ë¬¸ì„œ í™œë°œ, ë¬¸ì„œ/ê°€ì´ë“œ í’ë¶€ í™œë°œ, ì—”í„°í”„ë¼ì´ì¦ˆ ë¬¸ì„œ/ì‚¬ë¡€ í’ë¶€ ì¶”ì²œ ì‚¬ìš©ì²˜ RAG, ì¶”ì²œì‹œìŠ¤í…œ, ì¤‘~ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤, ë¹ ë¥¸ êµ¬ì¶• ì´ˆëŒ€ê·œëª¨ ê²€ìƒ‰, ì—”í„°í”„ë¼ì´ì¦ˆ, ë³´ì•ˆ/ê°ì‚¬ í•„ìˆ˜ í™˜ê²½ ì‹¤ë¬´ì  ì„ íƒ í¬ì¸íŠ¸ Qdrant ì„¤ì¹˜/ìš´ì˜ì´ ì‰½ê³ , ì¤‘~ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ì— ì í•© ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, RAG, ì¶”ì²œì‹œìŠ¤í…œ, ì†ì„±+ë²¡í„° ë™ì‹œ í•„í„°ë§ì´ ì¤‘ìš”í•œ ì„œë¹„ìŠ¤ í´ë¼ìš°ë“œ/ì˜¨í”„ë ˆë¯¸ìŠ¤ ëª¨ë‘ ìœ ì—°í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥ ëŒ€ê·œëª¨ ë¶„ì‚°ì´ ê¼­ í•„ìš”í•˜ì§€ ì•Šë‹¤ë©´ Qdrantê°€ ë” ê°„í¸ Milvus ì´ˆëŒ€ê·œëª¨(ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ë²¡í„°) ë°ì´í„°, ë¶„ì‚°ì²˜ë¦¬, ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë³´ì•ˆ/ê°ì‚¬/ê¶Œí•œì´ í•„ìš”í•œ í™˜ê²½ ì¿ ë²„ë„¤í‹°ìŠ¤ ë“± ì¸í”„ë¼ êµ¬ì¶•ì´ ê°€ëŠ¥í•˜ê³ , ë³µì¡í•œ ìš´ì˜ì„ ê°ìˆ˜í•  ìˆ˜ ìˆì„ ë•Œ MLOps, ëŒ€ê·œëª¨ AI/ê²€ìƒ‰ í”Œë«í¼, ë³´ì•ˆ/ê°ì‚¬/ê·œì œ ì¤€ìˆ˜ê°€ ì¤‘ìš”í•œ ê¸°ì—… í™˜ê²½ ê²°ë¡ : ë¹ ë¥¸ êµ¬ì¶•, ì¤‘~ëŒ€ê·œëª¨, ìš´ì˜ í¸ì˜ì„± â†’ Qdrant ì¶”ì²œ ì´ˆëŒ€ê·œëª¨, ì—”í„°í”„ë¼ì´ì¦ˆ, ë³´ì•ˆ/ê°ì‚¬/ë¶„ì‚° í•„ìˆ˜ â†’ Milvus ì¶”ì²œ ë‘˜ ë‹¤ ë¬´ë£Œ/í´ë¼ìš°ë“œ í”Œëœì´ ìˆìœ¼ë‹ˆ, ì‹¤ì œ PoC(íŒŒì¼ëŸ¿)ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ê²°ì •í•˜ëŠ” ê²ƒë„ ì¢‹ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ì½”ë“œë“¤ compare-vDB-libraries",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-24-ë¬¼ê±´ì—ëŒ€í•œë¶„ì„",
    "title": "ë¬¼ê±´ì— ëŒ€í•œ ë¶„ì„",
    "date": "2025-07-24",
    "excerpt": "ë¬¼ê±´ì— ëŒ€í•œ ë¶„ì„",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ It's not about performance. It's about design. It's not about customizabiltiy. It's about compatibility. In which case, I'll probably go with the new Galaxy. go with : ì„ íƒí•˜ë‹¤",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/how-can-evaluate-casual-models",
    "title": "how to evaluate CATE estimation model",
    "date": "2025-07-24",
    "excerpt": "CATE estimation model í‰ê°€ ë°©ë²•ì— ëŒ€í•´ ì •ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "ì¸ê³¼ì¶”ë¡ ",
      "metrics"
    ],
    "content": "ì°¸ê³ í•œ ìë£Œ Causal Inference for The Brave and True > When Prediction Fails chapter, they generate data on both $Y_{0i}$ and $Y_{1i}$ so that they can check if their model is correctly capturing the treatment effect $Y_{1i} - Y_{0i}$. > Thatâ€™s fine for academic purposes, but in the real world, we donâ€™t have that luxury. CATE estimation modelì€ ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆì„ê¹Œìš”? í•™ë¬¸ì ìœ¼ë¡œëŠ” ì„ì˜ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•´ì„œ ì‹¤ì œë¡œ ì´ë¥¼ ì˜ ì¶”ì •í•˜ëŠ”ì§€ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. > The quality of your model will have to be grounded on something more concrete than a beautiful theory. > Unfortunately, it isnâ€™t obvious at all how we achieve anything like a train-test paradigm in the case of causal inference. > Thatâ€™s because causal inference is interested in estimating an unobservable quantity, $\\frac{\\delta y}{\\delta t}$ í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” CATEë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ì—†ìœ¼ë‹ˆ, ë‹¤ë¥¸ ë°©ë²•ì„ ì¨ì•¼í•©ë‹ˆë‹¤. íšŒì‚¬ì— ì¸ê³¼ì¶”ë¡ ì„ ì ìš©í•˜ë©´ì„œ ê°€ì¥ ë§ì´ ê³ ë¯¼í—€ë˜ ë¶€ë¶„ì´ê¸°ë„ í•©ë‹ˆë‹¤. ë‚´ê°€ ë§Œë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì–´ë–»ê²Œ ì„¤ëª…í•˜ê³  ì„¤ë“í•  ìˆ˜ ìˆì„ê¹Œìš”? ë¨¸ì‹ ëŸ¬ë‹ì€ train-test paradigmì„ í†µí•´ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆê³ , ì´ëŠ” ê½¤ë‚˜ ì§ê´€ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¦ëª…í•´ì£¼ê³  ìˆìŠµë‹ˆë‹¤. CATE estimation modelì—ë„ ì´ëŸ¬í•œ ë°©ë²•ì´ ì¡´ì¬í• ê¹Œìš”? > Is not a definitive one, but it works in practice and it has that concreteness, which I hope will approach causal inference from a train-test paradigm similar to the one we have with machine learning. > The trick is to use aggregate measurements of sensitivity. > Even if you canâ€™t estimate sensitivity individually, you can do it for a group and that is what we will leverage here. ê°œê°œì¸ì˜ CATEëŠ” ì•Œ ìˆ˜ ì—†ì§€ë§Œ ê·¸ë£¹ì˜ ATEëŠ” ì•Œ ìˆ˜ ìˆìœ¼ë‹ˆ ì´ë¥¼ í™œìš©í•´ì„œ CATEë¥¼ í‰ê°€í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Sensitivity by Model Band --- > A good causal model should help us find which customers will respond better and worse to a proposed treatment. > They should be able to separate units into how elastic or sensitive they are to the treatment. CATE estimationì˜ ëª©ì ì€ ì²˜ì¹˜ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ ê°œì¸ë³„ë¡œ ì¶”ì •í•˜ì—¬, ê°œì¸í™”ëœ ì²˜ì¹˜ë¥¼ í•˜ëŠ”ë°ì— ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê°œì¸ë³„ë¡œ ë¯¼ê°ë„ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. > If that is the goal, it would be very useful if we could somehow order units from more sensitive to less sensitive. CATE estimationì„ í†µí•´ ê°œì¸ë³„ ë¯¼ê°ë„ë¥¼ ê³„ì‚°í•˜ê³  ì´ë¥¼ í™œìš©í•´ ì •ë ¬ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ë ¬ì„ ì–¼ë§ˆë‚˜ ì˜í•˜ëŠ”ì§€ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•´ë³¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œìš”? > Sadly, we canâ€™t evaluate that ordering on a unit level. > But, what if we donâ€™t need to? > What if, instead, we evaluate groups defined by the ordering? > If our treatment is randomly distributed (and here is where randomness enters), estimating sensitivity for a group of units is easy. > All we need is to compare the outcome between the treated and untreated. <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/cate_metric_using_ate.png\" alt=\"cate_metric_using_ate\" /> <figcaption> ê·¸ë¦¼1. CATE í‰ê°€ ë°©ë²•ì— ëŒ€í•œ ì§ê´€ì  ì„¤ëª… (low: ë¹„ì²˜ì¹˜ëŒ€ìƒ, high: ì²˜ì¹˜ëŒ€ìƒ-ê°€ê²©ì„ ë†’ì˜€ì„ ë•Œ)</figcaption> </figure> ê·¸ë¦¼1ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì¶”ì •ëœ cateë¡œ ì •ë ¬í–ˆì„ ë•Œ, ì •ë ¬ëœ ê²ƒë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ  ATEë¥¼ êµ¬í•˜ë©´ ê·¸ ì°¨ì´ê°€ ë¶„ëª…í•  ê²ƒì´ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§¨ ì˜¤ë¥¸ìª½ì— ìˆëŠ”, ëœë¤í•˜ê²Œ cateë¥¼ ë°°ì •í•œ ê²½ìš° ATEê°€ ì™„ì „í•˜ê²Œ ê°™ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ model-2ì˜ ê²½ìš° ATEê°€ ë¶„ëª…í•˜ê²Œ ì°¨ì´ê°€ ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > Just by looking at these plots, you can get a feeling of which model is better. > The more ordered the sensitivities look like and the more different they are between bands, the better. > Here, model 2 is probably better than model 1, which is probably better than the random model. Cumulative Gain Curve ê³„ì†í•´ì„œ ê°€ì ¸ê°€ê³  ìˆëŠ” ì»¨ì…‰ì€, CATE estimationì€ ê°œì¸ë³„ë¡œ cateë¥¼ êµ¬ë¶„ë˜ê²Œ ì¶”ì •í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ê³ , ì´ë¥¼ ì‚¬ìš©í•´ ì •ë ¬í•œ í›„ ê·¸ë£¹ì„ ë‚˜ëˆ  ATEë¥¼ êµ¬í•˜ë©´ ì°¨ì´ê°€ ë‚˜ì•¼í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/cumulative_gain_curve.png\" alt=\"cumulative_gain_curve\" /> <figcaption> ê·¸ë¦¼2. Cumulative gain curve</figcaption> </figure> ê·¸ë¦¼2ëŠ” ê·¸ë¦¼1ì˜ ê·¸ë£¹ë³„ ATEê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ í›„ì—, ëˆ„ì ë˜ëŠ” ATEë¥¼ ì‚´í´ë³¸ ê²ƒì…ë‹ˆë‹¤. ë¹„êµëŒ€ìƒì¸ ëœë¤í•œ ê²½ìš°ë¥¼ ë³´ë©´, ATE ëˆ„ì  ë¶„í¬ê°€ ì§ì„ ì˜ ëª¨ìŠµì„ ë³´ì´ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ì •ë ¬ì´ ì˜ëœ ê²½ìš°ë¼ë©´ ì´ ëœë¤í•œ ëª¨ë¸ì´ ê·¸ë¦¬ëŠ” ì§ì„ ë³´ë‹¤ ë” ë†’ê²Œ íœ˜ì–´ì§„ ì§ì„ ì„ ê·¸ë¦¬ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > Once we have the theoretic random curve, we can use it as a benchmark and compare our other models against it. > All curves will start and end at the same point. > However, the better the model at ordering sensitivity, the more the curve will diverge from the random line in the points between zero and one. ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-23-ë­ë­í•˜ê¸´í•´ë„",
    "title": "ë­ë­í•˜ê¸´ í•´ë„",
    "date": "2025-07-23",
    "excerpt": "ë­ë­í•˜ê¸´ í•´ë„ ì“°ëŠ” í‘œí˜„ë“¤",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Although, its a bit pricey, the bar has an amazing view at night. Although there is a long wait, it has the best bubble tea in town. Even though, Im not as big fan of the movie, I still enjoyed it. I like chocolate, even though I dont like sweet. I love burgers. Its not healthy though. The coat is too expensive. but its pretty though. Fair enough : ì¸ì • ê°„ë‹¨í•œ ì‘ë¬¸ Although seoul is confused, full of people, its a great place to live. Many í¸ì˜ì‹œì„¤ in here. Even though Im not a big fan of anime, I love ê·€ë©¸ì˜ ì¹¼ë‚ . I really love burgers. Its not healthy thought. ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë¬¸ & í”¼ë“œë°± Although Seoul is confusing and crowded, it's a great place to live. â†’ í”¼ë“œë°±: \"confused\" ëŒ€ì‹  \"confusing and crowded\"ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •. \"Many í¸ì˜ì‹œì„¤ in here.\"ëŠ” \"There are many convenient facilities here.\"ë¡œ ë°”ê¾¸ë©´ ì¢‹ì•„ìš”. Even though I'm not a big fan of anime, I love Demon Slayer. â†’ í”¼ë“œë°±: \"ê·€ë©¸ì˜ ì¹¼ë‚ \"ì€ ì˜ì–´ ì œëª© \"Demon Slayer\"ë¡œ ì“°ë©´ ìì—°ìŠ¤ëŸ¬ì›Œìš”. I really love burgers. It's not healthy though. â†’ í”¼ë“œë°±: \"thought\"ëŠ” ì˜¤íƒ€, \"though\"ê°€ ë§ì•„ìš”.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/law-of-total-expectaion",
    "title": "Law of total expectation",
    "date": "2025-07-23",
    "excerpt": "ì „ì²´ê¸°ëŒ€ë²•ì¹™ì— ëŒ€í•´ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "math"
    ],
    "content": "ì „ì²´ê¸°ëŒ€ë²•ì¹™, ì§„ì§œ ìƒí™œ ë²„ì „ > í•µì‹¬ ì•„ì´ë””ì–´ > â€œì „ì²´ í‰ê·  = â€˜ê° ìƒí™©ë³„ í‰ê· â€™ë“¤ì„ í™•ë¥ ë¡œ ê°€ì¤‘í•œ í‰ê· â€ --- ë§ˆíŠ¸ ê³„ì‚°ëŒ€ ì´ì•¼ê¸°ë¡œ ì§ê´€ ì¡ê¸° ë§ˆíŠ¸ì—ëŠ” \\\\ë¹ ë¥¸ ê³„ì‚°ëŒ€(ì…€í”„)\\\\ì™€ ì¼ë°˜ ê³„ì‚°ëŒ€(ì§ì›) ë‘ ì¤„ì´ ìˆë‹¤. ì…€í”„ ê³„ì‚°ëŒ€ë¥¼ ì„ íƒí•  í™•ë¥  70 %, ì¼ë°˜ ê³„ì‚°ëŒ€ëŠ” 30 %. ì…€í”„ì—ì„œ ê±¸ë¦¬ëŠ” í‰ê·  ì‹œê°„ì€ 2ë¶„, ì¼ë°˜ ê³„ì‚°ëŒ€ëŠ” 6ë¶„. â€œì „ì²´ ì¤„ ì„œê¸° í‰ê·  ì‹œê°„â€ì€? ì¡°ê±´ë¶€ í‰ê· : ì…€í”„ = 2ë¶„, ì¼ë°˜ = 6ë¶„ ê°€ì¤‘ í‰ê· : $2 \\times 0.7 + 6 \\times 0.3 = 3.2$ ë¶„ 3.2 ë¶„ì´ Law of Total Expectation ê²°ê³¼ë‹¤. ([Wikipedia][1]) > í•œ ì¤„ ìš”ì•½ â€• â€œìƒí™©ë³„ í‰ê· ì„ êµ¬í•œ ë’¤, ê° ìƒí™©ì´ ì¼ì–´ë‚  í™•ë¥ ë¡œ ë‹¤ì‹œ í•œ ë²ˆ í‰ê· ì„ ë‚´ë©´ ì „ì²´ í‰ê· ì´ ëœë‹¤.â€ --- ê³µì‹ì„ ëœ¯ì–´ë³´ë©´ $$ \\mathbb{E}[X] = \\mathbb{E}\\big[\\;\\mathbb{E}[X \\mid Y]\\;\\big] $$ $X$: ì•Œê³  ì‹¶ì€ ëŒ€ìƒ(ì¤„ ì„œëŠ” ì‹œê°„). $Y$: ìƒí™©ì„ ë‚˜ëˆ„ëŠ” ì—´ì‡ (ì„ íƒí•œ ê³„ì‚°ëŒ€). $\\mathbb{E}[X\\mid Y]$: â€œìƒí™© Y ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í‰ê· â€ â€” ì…€í”„ë©´ 2ë¶„, ì¼ë°˜ì´ë©´ 6ë¶„. $\\mathbb{E}[\\cdot]$: ê·¸ ê°’ë“¤ì„ ë‹¤ì‹œ í‰ê·  â€” í™•ë¥  0.7, 0.3ìœ¼ë¡œ ê°€ì¤‘. ([Wikipedia][1]) --- â€˜3ë‹¨ ìš”ë¦¬ë²•â€™ìœ¼ë¡œ ê¸°ì–µí•˜ê¸° ë‹¨ê³„ í•´ì•¼ í•  ì¼ ê²°ê³¼ --------- -------------------------- --------- â‘  ìƒí™© ë‚˜ëˆ„ê¸° ë¬¸ì œë¥¼ ì‰½ê²Œ ìª¼ê°¤ ë³€ìˆ˜Â·ì‚¬ê±´ $Y$ ê³ ë¥´ê¸° â€˜ì…€í”„ / ì¼ë°˜â€™ â‘¡ ìƒí™©ë³„ ê¸°ëŒ€ê°’ $\\mathbb{E}[X\\mid Y=y]$ ê³„ì‚° 2ë¶„, 6ë¶„ â‘¢ ê°€ì¤‘ í‰ê·  â‘¡ì— $P(Y=y)$ ê³±í•´ ë‹¤ ë”í•˜ê¸° 3.2ë¶„ > ì–´ë””ì— ì“°ë‚˜? > > ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ êµ¬ë§¤ì•¡ ì‚°ì • > - ë³´í—˜ë£Œ ì˜ˆìƒ(ë‚˜ì´ëŒ€Â·ì§€ì—­ë³„) > - A/B í…ŒìŠ¤íŠ¸ ë‘ ê·¸ë£¹ ê²°ê³¼ í•©ì¹˜ê¸° --- íŠ¸ë¦¬ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë ¤ ë³´ê¸° `` ì „ì²´ ê³ ê° / \\ (70%) (30%) ì…€í”„ ê³„ì‚°ëŒ€ ì¼ë°˜ ê³„ì‚°ëŒ€ í‰ê·  2ë¶„ í‰ê·  6ë¶„ `` ì•„ë˜ì¸µ í‰ê· (2, 6)ì„ ìœ„ì¸µ í™•ë¥ (0.7, 0.3)ë¡œ ëŒì–´ì˜¬ë ¤ ìœ„ì¸µ í‰ê· (3.2)ì„ ë§Œë“ ë‹¤. ê·¸ë˜ì„œ ì´ ë²•ì¹™ì„ \\\\Tower Rule(íƒ‘ ê·œì¹™)\\\\ì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤. ([Mathematics Stack Exchange][2]) --- â€˜ì¡°ê±´ì„ í•œ ë²ˆ ë” ì£¼ì–´ë„ OKâ€™ $$ \\mathbb{E}\\!\\big[\\;\\mathbb{E}[X \\mid Y,Z]\\;\\big]=\\mathbb{E}[X] $$ ì¸µì„ ì—¬ëŸ¬ ì¸µ ìŒ“ì•„ë„, ë§¨ ê¼­ëŒ€ê¸°ë¡œ ì˜¬ë¼ê°€ë©´ ì–¸ì œë‚˜ $X$ì˜ ì›ë˜ í‰ê· . ì •ë³´ë¥¼ ì¡°ê¸ˆ ì•Œê³ (í•œ ì¸µ ë‚´ë ¤ê°€ê³ ) ë‹¤ì‹œ ëª¨ë¥¸ ì²™(í‰ê· )í•´ë„, ê²°êµ­ ì œìë¦¬ë‹¤. ([The Book of Statistical Proofs][3]) --- í•œëˆˆì— ëë‚´ëŠ” ê¸°ì–µ ì¥ì¹˜ > â€œì¡°ê±´ë¶€ í‰ê· ì€ ê³§ â€˜ë¶€ë¶„ í‰ê· í‘œâ€™, > ê·¸ í‘œë¥¼ ë‹¤ì‹œ í‰ê· ë‚´ë©´ ì „ì²´ í‰ê· í‘œ.â€ --- ìì£¼ ë¬»ëŠ” Q\\&A ì§ˆë¬¸ ë‹µë³€ ------------------- ----------------------------------------------------------------- ì™œ êµ³ì´ ì´ë ‡ê²Œ ëŒë ¤ ê³„ì‚°? ë³µì¡í•œ ë¬¸ì œë¥¼ ì‘ì€ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ ê³„ì‚°ì´ í›¨ì”¬ ì‰½ë‹¤. í™•ë¥  ëŒ€ì‹  ë¹„ìœ¨Â·ê°€ì¤‘ì¹˜ ì¨ë„ ë˜ë‚˜? ì´í•©ì´ 1ì´ë©´ ì–´ë–¤ ê°€ì¤‘ì¹˜ë“  ë¬´ë°©. ì—°ì†í˜• ë³€ìˆ˜ë„ ë˜ë‚˜? ì‚¬ê±´ í•© ëŒ€ì‹  ì ë¶„ìœ¼ë¡œ ë°”ë€” ë¿ ë˜‘ê°™ì´ ì„±ë¦½í•œë‹¤. ([Everything Explained Today][4]) --- í•„ìš”í•˜ë©´ ì „ì²´ë¶„ì‚°ë²•ì¹™(Var ë²„ì „)ì´ë‚˜ ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ì—ì„œì˜ í™œìš© ì˜ˆë„ ì´ì–´ì„œ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤. [1]: https://en.wikipedia.org/wiki/Law_of_total_expectation?utm_source=chatgpt.com \"Law of total expectation\" [2]: https://math.stackexchange.com/questions/41536/intuitive-explanation-of-the-tower-property-of-conditional-expectation?utm_source=chatgpt.com \"Intuitive explanation of the tower property of conditional expectation\" [3]: https://statproofbook.github.io/P/mean-tot.html?utm_source=chatgpt.com \"Law of total expectation The Book of Statistical Proofs\" [4]: https://everything.explained.today/Law_of_total_expectation/?utm_source=chatgpt.com \"Law of total expectation explained\"",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-r-learner",
    "title": "R-learnerë€?",
    "date": "2025-07-23",
    "excerpt": "R-learnerì˜ ê°œë…ì— ëŒ€í•´ ì •ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "R-learner",
      "ì¸ê³¼ì¶”ë¡ "
    ],
    "content": "R-learnerëŠ” CATE estimationì— ì‚¬ìš©ë˜ëŠ” meta-learner ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•´ í•˜ë‚˜ì”© ì •ë¦¬í—€ìŠµë‹ˆë‹¤. CATE estimationì´ë€? --- ì¸ê³¼ì¶”ë¡  (Causal Inference)ì˜ ëª©ì ì€ ì²˜ì¹˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ”ë°ì— ìˆìŠµë‹ˆë‹¤. ì²˜ì¹˜ê°€ ê²°ê³¼ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì´ë¥¼ í†µí•´ ë” ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 1.1 ATEë€? > ATE(Average Treatment Effect)ëŠ” ì „ì²´ ì§‘ë‹¨ì—ì„œ ì²˜ì¹˜(ì˜ˆ: ì‹ ì•½, ì •ì±… ë“±)ê°€ ë¯¸ì¹˜ëŠ” í‰ê· ì ì¸ ì¸ê³¼ íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. $$ \\text{ATE} = \\mathbb{E}[Y^{(t)} - Y^{(0)}] $$ ì—¬ê¸°ì„œ $$Y^{(t)}$$ì€ ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼ (Potential Outcome)ì´ê³ , $$Y^{(0)}$$ì€ ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼ì…ë‹ˆë‹¤. ì¦‰ ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ í‰ê· ì ì¸ ì¸ê³¼íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 1.2 CATEë€? > CATE(Conditional Average Treatment Effect)ëŠ” íŠ¹ì • ì¡°ê±´ì—ì„œ ì²˜ì¹˜ê°€ ë¯¸ì¹˜ëŠ” í‰ê· ì ì¸ ì¸ê³¼ íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. $$ \\text{CATE} = \\tau(x) = \\mathbb{E}[Y^{(t)} - Y^{(0)} X = x] $$ ì—¬ê¸°ì„œ $$X$$ëŠ” ì¡°ê±´ì„ ì˜ë¯¸í•˜ê³ , $$Y^{(t)}$$ëŠ” ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼, $$Y^{(0)}$$ëŠ” ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ATEë¥¼ í†µí•´ ì²˜ì¹˜ì˜ í‰ê· ì ì¸ ì¸ê³¼íš¨ê³¼(ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥)ì„ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ê°œê°œì¸ë³„ë¡œ ì¸ê³¼íš¨ê³¼ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì€ ì§ê´€ì ì¸ë°ìš”, ì´ëŸ¬í•œ ê°œë³„ ê°œì²´ì˜ ì²˜ì¹˜íš¨ê³¼ë¥¼ ITEë¼ê³  í•©ë‹ˆë‹¤. > ITE (Individual Treatment Effect) > - ì •ì˜: ê°œë³„ ê°œì²´ì˜ ì²˜ì¹˜ íš¨ê³¼ > - ìˆ˜ì‹: $$\\text{ITE}_i = Y_i^{(1)} - Y_i^{(0)}$$ > - ì˜ë¯¸: íŠ¹ì • ê°œì¸ì´ ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì™€ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²°ê³¼ ì°¨ì´ > - íŠ¹ì§•: ì‹¤ì œë¡œëŠ” ê´€ì°° ë¶ˆê°€ëŠ¥ (fundamental problem of causal inference) ê·¸ë¦¬ê³  ì´ë¥¼ ì¶”ì •í•˜ëŠ” ê°’ì´ CATE ì…ë‹ˆë‹¤. > Estimating heteronenous treatment effects is fundamental in causal inference and provides insights into various fields. > > -> Towards R-learner with Continuous Treatment ê°œì¸ë³„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ì¼ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. í•™ìƒë³„ êµìœ¡íš¨ê³¼ë¥¼ ì¶”ì •í•´ ê°œì¸í™”ëœ êµìœ¡ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ê±°ë‚˜, í™˜ìë³„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•´ ê°œì¸í™”ëœ ì²˜ì¹˜ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CATE estimationì— ì‚¬ìš©ë˜ëŠ” meta-learnerë“¤ --- ë©”íƒ€ëŸ¬ë„ˆ (meta-learner)ëŠ” ê¸°ì¡´ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•´ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì…ë‹ˆë‹¤. cate estimationì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” meta-learnerì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ T-learner, X-learner, S-learnerê°€ ìˆìŠµë‹ˆë‹¤. 2.1 T-learner ë²”ì£¼í˜• ì²˜ì¹˜ë¥¼ ë‹¤ë£° ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë©”íƒ€ëŸ¬ë„ˆì…ë‹ˆë‹¤. $$ \\hat{\\mu}_0(x) = \\mathbb{E}[Y T=0, X = x] \\\\ \\hat{\\mu}_1(x) = \\mathbb{E}[Y T=1, X = x] \\\\ \\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x) $$ <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/T-learner.png\" alt=\"T-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> ê·¸ë¦¼ 1 : T-learner êµ¬ì¡°<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">ì¶œì²˜: Causal Inference for The Brave and True</a> </figcaption> </figure> ê·¸ë¦¼1ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼, ì²˜ì¹˜ë³„ë¡œ ê°ê°ì˜ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ê³  ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ë¹„êµí•´ CATEë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ``python m0 = LGBMRegressor(max_depth=2, min_child_samples=60) m1 = LGBMRegressor(max_depth=2, min_child_samples=60) m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y]) estimate the CATE t_learner_cate_train = m1.predict(train[X]) - m0.predict(train[X]) t_learner_cate_test = test.assign(cate=m1.predict(test[X]) - m0.predict(test[X])) ` 2.2 S-learner (aka, the Go-Horse Learner) SëŸ¬ë„ˆëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ì‹ìœ¼ë¡œ, ë‹¨ì¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì •í•©ë‹ˆë‹¤. $$ \\hat{\\mu}(x) = E[Y X = x] \\\\ \\hat{\\tau}(x) = \\hat{\\mu}(x, T=1) - \\hat{\\mu}(x, T=0) $$ <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/S-learner.png\" alt=\"S-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> ê·¸ë¦¼ 2 : S-learner êµ¬ì¡°<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">ì¶œì²˜: Causal Inference for The Brave and True</a> </figcaption> </figure> ê·¸ë¦¼2ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼ ëª¨ë“  ì²˜ì¹˜ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê³ , ì´ ëª¨ë¸ì„ í†µí•´ CATEë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. `python s_learner = LGBMRegressor(max_depth=3, min_child_samples=30) s_learner.fit(train[X+[T]], train[y]); s_learner_cate_train = (s_learner.predict(train[X].assign({T: 1})) - s_learner.predict(train[X].assign({T: 0}))) s_learner_cate_test = test.assign( cate=(s_learner.predict(test[X].assign({T: 1})) - # predict under treatment s_learner.predict(test[X].assign({T: 0}))) # predict under control ) ` ì´ ëª¨ë¸ì€ CATE estimationì— ì‚¬ìš©ë˜ëŠ” ë©”íƒ€ëŸ¬ë„ˆ ì¤‘ì— ê°€ì¥ ì‰½ê²Œ ì ìš©í•´ë³¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ì§€ë§Œ, í¸í–¥ì— ê°€ì¥ ì·¨ì•½í•œ ëª¨ë¸ì´ê¸°ë„ í•©ë‹ˆë‹¤. 2.3 X-learner <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/X-learner.png\" alt=\"X-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> ê·¸ë¦¼ 3 : X-learner êµ¬ì¡°<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">ì¶œì²˜: Causal Inference for The Brave and True</a> </figcaption> </figure> X-learnerëŠ” 2ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 1ë‹¨ê³„ì—ì„œëŠ” T-learnerì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì²˜ì¹˜ë³„ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ  ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. $$ \\hat{M}_0(X) \\approx \\mathbb{E}[Y T=0, X] \\\\ \\hat{M}_1(X) \\approx \\mathbb{E}[Y T=1, X] $$ 2ë‹¨ê³„ì—ì„œëŠ” 1ë‹¨ê³„ì—ì„œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ ì¶”ì •ëœ CATEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. $$ \\hat{\\tau}(X, T=0) = \\hat{M}_0(X) - Y_{T=0} \\\\ \\hat{\\tau}(X, T=1) = \\hat{M}_1(X) - Y_{T=1} $$ ê·¸ í›„ì— ì´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. $$ \\hat{M}_{\\tau 0}(X) = E[\\hat\\tau(X) T = 0] \\\\ \\hat{M}_{\\tau 1}(X) = E[\\hat\\tau(X) T = 1] $$ ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë³´ì™„í•´ì£¼ê¸° ìœ„í•´ì„œ propensity scoreë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•œ $\\hat{\\tau}(X, T=0)$ ì„ ì¶”ì •í•  ë•Œ $Y_1$ ì´ ì—†ê¸° ë•Œë¬¸ì— $\\hat{M}_1$ì„ ì‚¬ìš©í•´ì„œ ì´ë¥¼ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. ì´ë•Œ Xê°€ 0ì¼ í™•ë¥ ì´ ë†’ë‹¤ë©´ $\\hat{M}_1$ì˜ ì˜ˆì¸¡ê°’ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ê²ƒì´ê³ , ì´ë¡œ ì¸í•´ $\\hat{\\tau}(X, T=0)$ì˜ ì¶”ì •ê°’ë„ ì‹ ë¢°ê°€ ë–¨ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ \\hat{\\tau}(X, T=0)$ì˜ ì¶”ì •ê°’ì˜ ì‹ ë¢°ë„ëŠ” ì˜¬ë¼ê°ˆ ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§ê´€ì„ ê°€ì§€ê³  propensity scoreë¥¼ ì‚¬ìš©í•´ì„œ ë³´ì™„í•©ë‹ˆë‹¤. $$ \\hat{\\tau}(X) = \\hat{M}_{\\tau 0}(X)\\hat e(x) - \\hat{M}_{\\tau 1}(X)(1-\\hat e(x)) $$ `python from sklearn.linear_model import LogisticRegression np.random.seed(123) first stage models m0 = LGBMRegressor(max_depth=2, min_child_samples=30) m1 = LGBMRegressor(max_depth=2, min_child_samples=30) propensity score model g = LogisticRegression(solver=\"lbfgs\", penalty='none') m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y]) g.fit(train[X], train[T]); ` `python d_train = np.where(train[T]==0, m1.predict(train[X]) - train[y], train[y] - m0.predict(train[X])) second stage mx0 = LGBMRegressor(max_depth=2, min_child_samples=30) mx1 = LGBMRegressor(max_depth=2, min_child_samples=30) mx0.fit(train.query(f\"{T}==0\")[X], d_train[train[T]==0]) mx1.fit(train.query(f\"{T}==1\")[X], d_train[train[T]==1]); ` `python def ps_predict(df, t): return g.predict_proba(df[X])[:, t] x_cate_train = (ps_predict(train,1)mx0.predict(train[X]) + ps_predict(train,0)mx1.predict(train[X])) x_cate_test = test.assign(cate=(ps_predict(test,1)mx0.predict(test[X]) + ps_predict(test,0)mx1.predict(test[X]))) `` R-learnerë€? ë‹¤ë¥¸ meta-learnerë“¤ê³¼ ë¹„êµ --- R-learnerëŠ” FWL-theoremì„ meta-learner í˜•íƒœë¡œ êµ¬í˜„í•œ ëª¨ë¸ì…ë‹ˆë‹¤. FWL theoremì´ë€? ì¡°ê¸ˆ ë” ì—„ë°€í•˜ê²Œ ì´ì•¼ê¸°í•˜ë©´ double/debiased machine learningì´ë¼ê³  ë§í•  ìˆ˜ ìˆê³ , ì—¬ê¸°ì— propensity scoreë¥¼ í•¨ê»˜ ì‚¬ìš©í•´ í¸í–¥ì„ ë³´ì •í•˜ëŠ” ëª¨ë¸ì´ R-learnerì…ë‹ˆë‹¤. í¸í–¥ì„ nuisance functionì„ í†µí•´ ì”ì°¨í™”í•˜ì—¬ ì œê±°í•˜ê³ , CATEë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ì´ë•Œ nuisance functionë“¤ì€ ê¸°ì¡´ì˜ ì˜ˆì¸¡ ëª¨ë¸ì„ í†µí•´ ì‰½ê²Œ êµ¬í˜„ë˜ë©°, CATE estimationì— ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ì¤‘íšŒê·€ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ ë¹„ì„ í˜•ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. R-learnerëŠ” ë‹¤ë¥¸ meta-learnerë“¤ê³¼ ë‹¤ë¥´ê²Œ ì´ ëª¨ë¸ì€ CATEë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ CATEë¥¼ ë” ì˜ ì¶”ì •í•œë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. > CATE estimationì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì—ëŠ” 2ê°€ì§€ì •ë„ê°€ ìˆëŠ”ë°, > í•˜ë‚˜ëŠ” ì„ì˜ë¡œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ ì‹¤ì œë¡œ ì˜ ì¶”ì •í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒê³¼ > ë‹¤ë¥¸ í•˜ë‚˜ëŠ” cateë¡œ ì •ë ¬í•´ ê·¸ë£¹ë³„ë¡œ ATEë¥¼ êµ¬í•´ ë¹„êµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. -> how to evaluate CATE estimation model ë˜í•œ íšŒê·€ëª¨ë¸ì„ í†µí•œ ì”ì°¨í™”ì™€ propensity scoreë¥¼ í†µí•œ ì”ì°¨í™”ë¥¼ ì§„í–‰í•´ doubly robust í•˜ë‹¤ëŠ” ì¥ì ë„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. Double robustnessë€? --- í¸í–¥ì„ ì œê±°í•˜ê¸° ìœ„í•œ ë°©ë²•ì—ëŠ” íšŒê·€ë¥¼ í†µí•œ ì”ì°¨í™”ì™€ propensity scoreë¥¼ í†µí•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. íšŒê·€ë¥¼ í†µí•œ ì”ì°¨í™”ëŠ” FWL-theoremì— ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. propensity scoreë¥¼ í†µí•œ ì”ì°¨í™”ì˜ ì»¨ì…‰ì€ Xë¥¼ í†µì œí•  ìˆ˜ ì—†ë‹¤ë©´ e(X)ë¥¼ í†µì œí•´ì„œ, Xë¥¼ í†µì œí•œ ê²ƒê³¼ ê°™ì€ ìƒíƒœë¥¼ ë§Œë“¤ìëŠ” ê²ƒì…ë‹ˆë‹¤. ì°¸ê³ ìë£Œ PropensityScoreNotebook.ipynb Causal Inference for The Brave and True - Propensity Score R-leanerëŠ” ì´ 2ê°€ì§€ ëª¨ë‘ë¥¼ ì‚¬ìš©í•´ í¸í–¥ì„ ì—†ì• ê³  ìˆê³ , ì´ë¥¼ ìœ„í•œ 2ê°œì˜ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì˜ ì„±ëŠ¥ë§Œ ë³´ì¥ë˜ë©´ ë‚˜ë¨¸ì§€ í•˜ë‚˜ì˜ ì„±ëŠ¥ì´ ë¶€ì¡±í•˜ë”ë¼ë„ CATE estimationì˜ ì„±ëŠ¥ì´ ë³´ì¥ëœë‹¤ê³  í•©ë‹ˆë‹¤. 4.1 ATEì—ì„œ doubly robust estimation $$ \\hat{\\text{ATE}} = \\frac{1}{N} \\sum (\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)} + \\hat \\mu_1(X_i)) - \\frac{1}{N} \\sum (\\frac{(1-T_i)(Y_i - \\hat \\mu_0(X_i))}{1 - \\hat P(X_i)} + \\hat \\mu_0(X_i)) \\tag{1} $$ 1ë²ˆ ìˆ˜ì‹ì—ì„œ ì•ì— ë¶€ë¶„ì€ $\\hat E[Y X, T=1]$ ì˜ ì¶”ì •ê°’ì´ê³ , ë’¤ì— ë¶€ë¶„ì€ $\\hat E[Y X, T=0]$ ì˜ ì¶”ì •ê°’ì…ë‹ˆë‹¤. $\\hat E[Y X, T=1]$ì— ëŒ€í•´ì„œ doubly robustê°€ ì„±ë¦½í•˜ëŠ”ì§€ ì‚´í´ë³´ê³  ì´ë¥¼ ê¸°ë°˜í•´ì„œ $\\hat E[Y X, T=0]$ì— ëŒ€í•´ì„œëŠ” ìœ ì¶”í•´ë³´ë¡ í•˜ê² ìŠµë‹ˆë‹¤. $$ \\hat E[Y_1] = \\frac{1}{N} \\sum (\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)} + \\hat \\mu_1(X_i)) \\tag{2} $$ $\\hat \\mu_1(X_i)$ ê°€ ì •í™•í•˜ê³  propensity scoreê°€ ë¶€ì •í™•í•  ë•Œ. ì‹ 2ì—ì„œ $\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)}$ ë¶€ë¶„ì´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°€ ë³´ì¥ë©ë‹ˆë‹¤. $$ \\begin{align} \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\frac{T_i \\hat{\\mu}_1(X_i)}{\\hat{P}(X_i)} + \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\left( \\frac{T_i}{\\hat{P}(X_i)} - 1 \\right) \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\left( \\frac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)} \\right) \\hat{\\mu}_1(X_i) \\right) \\end{align} \\tag{3} $$ propensity scoreê°€ ì •í™•í•˜ê³  $\\hat \\mu_1(X_i)$ ê°€ ë¶€ì •í™•í•  ë•Œ. ì‹ 3ì—ì„œ $\\frac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)}$ ë¶€ë¶„ì´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°€ ë³´ì¥ë©ë‹ˆë‹¤. 4.2 R-learnerì—ì„œ doubly robustê°€ ë³´ì¥ë˜ëŠ” ë°©ì‹ Quasi-Oracle Estimation of Heterogeneous Treatment Effectsì—ì„œ ì´ë¥¼ ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. paper link paper review ê¸°ë³¸ì ì¸ ì»¨ì…‰ì€ 2ê°œì˜ nuisance functionì´ ì´ìƒì ì¼ ë•Œì˜ error boundì™€ 2ê°œ ì¤‘ 1ê°œë§Œ ì´ìƒì ì¼ ë•Œì˜ error boundê°€ ê°™ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. R-lossê°€ ì •ì˜ë˜ëŠ” ë°©ì‹ --- Generalized R-lossëŠ” Towards R-learner with Continuous Treatment ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. paper link paper review $$ L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right] \\tag{generalized R-loss} $$ ì´ëŠ” ë‹¤ìŒ ìˆ˜ì‹ë“¤ì—ì„œ ìœ ë„ë©ë‹ˆë‹¤. ì´ ìˆ˜ì‹ë“¤ì€ ì¸ê³¼ì¶”ë¡ ì˜ ê¸°ë³¸ ê°€ì •ì¸ unconfoundednessì™€ stable unit treatment value assumptionì„ ê°€ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. full conditional outcome mean model $$ \\mu(x, t) = \\mathbb{E}[Y X = x, T = t] $$ conditional outcome mean $$ \\mu(x) = \\mathbb{E}[Y X = x] $$ generalized propensity score $$ \\varpi(x) = \\mathbb{E}[T X = x] $$ ìˆ˜ì‹ ì „ê°œ $$ Y_i^{(T_i)} = \\mu(X_i, T_i) + \\epsilon_i = \\mu(X_i, 0) + \\tau(X_i, T_i) + \\epsilon_i \\\\ m(X_i) = E(Y_i^{(T_i)} X=X_i) = \\mu(X_i, 0) + E_{\\varpi}\\{\\tau(X, T) X=X_i\\} + \\epsilon_i \\\\ Y_i^{(T_i)} - m(X_i) = \\tau(X_i, T_i) - E_{\\varpi}\\{\\tau(X, T) X=X_i\\} + \\epsilon_i \\\\ $$ ë§ˆì§€ë§‰ ì‹ì„ $\\epsilon_i$ ì— ëŒ€í•´ì„œ ì •ë¦¬í•˜ë©´ loss functionì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ $h(X, t)$ëŠ” $tau(X, T)$ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 5.1 Binary treatmentì— ëŒ€í•œ loss function > Quasi-Oracle Estimation of Heterogeneous Treatment Effectsì—ì„œ ì •ì˜í•œ binary treatmentì— ëŒ€í•œ loss functionì€ generalized R-lossì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ì…ë‹ˆë‹¤. $$ L_b(h) =E[Y - m(X) - {T-e(X)}h(X, 1)]^2 \\tag{binary treatment} $$ Tê°€ 0 ë˜ëŠ” 1ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì´ê¸° ë–„ë¬¸ì— Të¥¼ í’€ì–´ì„œ ì“¸ ìˆ˜ ìˆê³ , $h(x, 0) = 0$ì´ë¼ëŠ” ê°€ì •ì„ loss functionì— ë…¹ì—¬ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/my-fail-experience-graduate-project",
    "title": "ì¡¸ì—… ê³¼ì œë¡œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ",
    "date": "2025-07-23",
    "excerpt": "ì¡¸ì—… ê³¼ì œë¡œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ",
    "category": "Causal Inference",
    "tags": [
      "ì¸ê³¼ì¶”ë¡ ",
      "í”„ë¡œì íŠ¸"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-22-ì„ íƒì˜ìƒí™©",
    "title": "ì„ íƒì˜ ìƒí™©",
    "date": "2025-07-22",
    "excerpt": "ì„ íƒì˜ ìƒí™©ì—ì„œ ì“°ëŠ” í‘œí˜„ë“¤",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ best-thing ~ì˜ ê°€ì¥ ì¢‹ì€ ì ì€ ~ì´ë‹¤. The best thing about iPhone is its camera. best-thing -> í•œë²ˆì— ì¼ê¸° aBOUT when it comes to ~ì— ëŒ€í•´ì„œëŠ”, ~ê°€ ê°€ì¥ ì¢‹ì„ ê²ƒì´ë‹¤. When it comes to online shopping, Amazon might be the best option. ê°„ë‹¨í•œ ì‘ë¬¸ When it comes to smart-watch, Galaxy Fit is the best option. The best thing about Galaxy Fit is price. It is ver cheap, but have good design and good quality. --- ë” ìì—°ìŠ¤ëŸ¬ìš´ ë²„ì „ & í”¼ë“œë°± ìì—°ìŠ¤ëŸ¬ìš´ ë²„ì „ When it comes to smartwatches, the Galaxy Fit is the best option. The best thing about the Galaxy Fit is its price. It is very cheap, but it also has a good design and high quality. í”¼ë“œë°± When it comes to smart-watch, Galaxy Fit is the best option. ìˆ˜ì •: When it comes to smartwatches, the Galaxy Fit is the best option. í”¼ë“œë°±: \"smart-watch\"ëŠ” ë³´í†µ ë³µìˆ˜í˜• \"smartwatches\"ë¡œ ì“°ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ê³ , \"the\"ë¥¼ ë¶™ì—¬ íŠ¹ì • ì œí’ˆì„ ì§€ì¹­í•˜ëŠ” ê²ƒì´ ë” ëª…í™•í•©ë‹ˆë‹¤. The best thing about Galaxy Fit is price. ìˆ˜ì •: The best thing about the Galaxy Fit is its price. í”¼ë“œë°±: \"its price\"ë¡œ ì†Œìœ ê²©ì„ ì¨ì„œ ë” ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"the\"ë¥¼ ë¶™ì—¬ ì œí’ˆì„ ëª…í™•íˆ ì§€ì¹­í•˜ì„¸ìš”. It is ver cheap, but have good design and good quality. ìˆ˜ì •: It is very cheap, but it also has a good design and high quality. í”¼ë“œë°±: \"ver\"ëŠ” ì˜¤íƒ€ë¡œ \"very\"ê°€ ë§ìŠµë‹ˆë‹¤. \"have\" ëŒ€ì‹  \"has\"ë¥¼ ì¨ì•¼ í•˜ê³ , \"good design and good quality\" ëŒ€ì‹  \"a good design and high quality\"ë¡œ ë” ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"also\"ë¥¼ ì¶”ê°€í•˜ë©´ ë¬¸ì¥ì´ ë” ë¶€ë“œëŸ½ê²Œ ì—°ê²°ë©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/bias-variance",
    "title": "Bias-Variance Tradeoff",
    "date": "2025-07-22",
    "excerpt": "Bias-Variance Tradeoffì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…",
    "category": "Data Science",
    "tags": [
      "bias-variance",
      "machine-learning",
      "statistics"
    ],
    "content": "Bias-Variance Tradeoff ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ê°€ Bias-Variance Tradeoffì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ì™€ ì¼ë°˜í™” ì„±ëŠ¥ ì‚¬ì´ì˜ ê· í˜•ì„ ì„¤ëª…í•˜ëŠ” í•µì‹¬ ê°œë…ì…ë‹ˆë‹¤. ê¸°ë³¸ ê°œë… Bias (í¸í–¥) > í•™ìŠµëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ í‰ê· ê³¼ ì‹¤ì œ ê°’ ê°„ì˜ ì°¨ì´ ì •ì˜: ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„° ìƒì„± ê³¼ì •ì„ ì–¼ë§ˆë‚˜ ì˜ ê·¼ì‚¬í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ ë†’ì€ Bias: ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•¨ ë‚®ì€ Bias: ëª¨ë¸ì´ ë³µì¡í•´ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ í•™ìŠµí•  ìˆ˜ ìˆìŒ Variance (ë¶„ì‚°) > ì˜ˆì¸¡ê°’ì´ ì–¼ë§ˆë‚˜ í©ì–´ì¡ŒëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ ì •ì˜: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì˜ ì‘ì€ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ ë†’ì€ Variance: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ ë‚®ì€ Variance: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ëœ ë¯¼ê°í•¨ ìˆ˜í•™ì  í‘œí˜„ ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ë¶„í•´ $$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$ ì—¬ê¸°ì„œ: $y$: ì‹¤ì œ ê°’ $\\hat{f}(x)$: ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ $\\text{Bias}^2 = E[(\\hat{f}(x) - f(x))^2]$: í¸í–¥ì˜ ì œê³± $\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$: ë¶„ì‚° $\\text{Irreducible Error}$: ë°ì´í„° ìì²´ì˜ ë…¸ì´ì¦ˆë¡œ ì¸í•œ ì˜¤ì°¨ í¸í–¥ê³¼ ë¶„ì‚°ì˜ ê³„ì‚° í¸í–¥ê³¼ ë¶„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤: $$\\text{Bias} = E[\\hat{f}(x)] - f(x)$$ $$\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$ Bias-Variance Tradeoff í•µì‹¬ ì•„ì´ë””ì–´ ëª¨ë¸ì´ ë‹¨ìˆœí• ìˆ˜ë¡: ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° ëª¨ë¸ì´ ë³µì¡í• ìˆ˜ë¡: ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° ì‹œê°ì  ì˜ˆì‹œ <figure style=\"text-align: center;\"> <img src=\"/post/bias_variance/bias_variance_1.png\" alt=\"Bias-Variance Tradeoff\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto;\"/> <figcaption>ê·¸ë¦¼ 1: Bias-Variance ê°œë… ì„¤ëª… ê·¸ë¦¼</figcaption> </figure> ê·¸ë¦¼1ì€ í¸í–¥ê³¼ ë¶„ì‚°ì˜ ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. í¸í–¥ì€ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì–¼ë§ˆë‚˜ ì˜ í•™ìŠµí–ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í¸í–¥ì´ ë‚®ìœ¼ë©´ íŒ¨í„´ì„ ì˜ í•™ìŠµí•´ íŒŒë€ ì› ì•ˆì— ì•ˆì— ì˜ˆì¸¡ê°’ì´ ëª¨ì—¬ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„ì‚°ì€ ì˜ˆì¸¡ê°’ì´ ì–¼ë§ˆë‚˜ í©ì–´ì ¸ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ í•œ ê³³ì— ë­‰ì³ìˆê³ , ë¶„ì‚°ì´ ë†’ìœ¼ë©´ ë„“ê²Œ í´ì ¸ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure style=\"text-align: center;\"> <img src=\"/post/bias_variance/bias_variance_2.png\" alt=\"Bias-Variance Tradeoff\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto;\"/> <figcaption>ê·¸ë¦¼ 2: ì‹¤ì œ ì˜ˆì¸¡ì—ì„œì˜ Bias-Variance Tradeoff - ë‹¨ìˆœí•œ ëª¨ë¸(ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚°) vs ë³µì¡í•œ ëª¨ë¸(ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚°)</figcaption> </figure> ê·¸ë¦¼2ëŠ” ì‹¤ì œ ì˜ˆì¸¡ì—ì„œì˜ Bias-Variance Tradeoffë¥¼ ì„¤ëª…í•˜ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. í•™ìŠµí•˜ëŠ” ë°ì´í„°ì…‹ ì•ˆì—ì„œ íŒ¨í„´ì„ ì˜ ì°¾ìœ¼ë©´ (í¸í–¥ì´ ë‚®ì„ìˆ˜ë¡) ëª¨ë¸ì€ ë³µì¡í•´ì ¸ì•¼ í•˜ê³ , ì´ë•Œ ì˜ˆì¸¡ ê°’ì´ ë„“ê²Œ í¼ì ¸ìˆê²Œ ë©ë‹ˆë‹¤ (ë¶„ì‚°ì´ ë†’ì•„ì§) ë°˜ëŒ€ë¡œ í•™ìŠµí•˜ëŠ” ë°ì´í„°ì…‹ ì•ˆì—ì„œ íŒ¨í„´ì„ ì˜ ì°¾ì§€ ëª»í•˜ë©´ (í¸í–¥ì´ ë†’ì„ìˆ˜ë¡) ëª¨ë¸ì€ ë‹¨ìˆœí•´ì ¸ì•¼ í•˜ê³ , ì´ë•Œ ì˜ˆì¸¡ ê°’ì´ í•œ ê³³ì— ëª¨ì—¬ìˆê²Œ ë©ë‹ˆë‹¤ (ë¶„ì‚°ì´ ë‚®ì•„ì§) `` ë‹¨ìˆœí•œ ëª¨ë¸ (ì˜ˆ: ì„ í˜• íšŒê·€) â”œâ”€â”€ ë†’ì€ Bias: ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ë†“ì¹¨ â””â”€â”€ ë‚®ì€ Variance: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì•ˆì •ì  ë³µì¡í•œ ëª¨ë¸ (ì˜ˆ: ê³ ì°¨ ë‹¤í•­ì‹, ë”¥ëŸ¬ë‹) â”œâ”€â”€ ë‚®ì€ Bias: ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ì˜ í•™ìŠµ â””â”€â”€ ë†’ì€ Variance: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë¶ˆì•ˆì • `` ì‹¤ì œ ì˜ˆì‹œ ì„ í˜• íšŒê·€ vs ë‹¤í•­ íšŒê·€ ì„ í˜• íšŒê·€ (1ì°¨ ë‹¤í•­ì‹) ë‚®ì€ ë³µì¡ë„ â†’ ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° ë°ì´í„°ê°€ ë¹„ì„ í˜• íŒ¨í„´ì„ ê°€ì§€ê³  ìˆë‹¤ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ ê³ ì°¨ ë‹¤í•­ íšŒê·€ (ì˜ˆ: 10ì°¨ ë‹¤í•­ì‹) ë†’ì€ ë³µì¡ë„ â†’ ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì™„ë²½í•˜ê²Œ ì í•©í•˜ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°ì—ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§ ëª¨ë¸ ì„ íƒì—ì„œì˜ ì ìš© Underfitting vs Overfitting Underfitting (ê³¼ì†Œì í•©) ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•¨ í•´ê²°ì±…: ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ Overfitting (ê³¼ì í•©) ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§ í•´ê²°ì±…: ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ, ì •ê·œí™”, ë” ë§ì€ ë°ì´í„° ìµœì  ëª¨ë¸ ì„ íƒ êµì°¨ ê²€ì¦ (Cross-Validation) í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€ í¸í–¥ê³¼ ë¶„ì‚°ì˜ ê· í˜•ì ì„ ì°¾ëŠ” ë° ë„ì›€ ì •ê·œí™” (Regularization) Ridge Regression (L2 ì •ê·œí™”): ë¶„ì‚°ì„ ì¤„ì—¬ ê³¼ì í•© ë°©ì§€ Lasso Regression (L1 ì •ê·œí™”): íŠ¹ì„± ì„ íƒì„ í†µí•œ ëª¨ë¸ ë‹¨ìˆœí™” ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­ ë°ì´í„° í¬ê¸°ì˜ ì˜í–¥ ì‘ì€ ë°ì´í„°ì…‹: ë¶„ì‚°ì´ ë†’ì•„ ê³¼ì í•© ìœ„í—˜ ì¦ê°€ í° ë°ì´í„°ì…‹: í¸í–¥ì´ ë” ì¤‘ìš”í•œ ê³ ë ¤ì‚¬í•­ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ì ˆí•œ íŠ¹ì„± ì„ íƒê³¼ ë³€í™˜ì´ í¸í–¥-ë¶„ì‚° ê· í˜•ì— í° ì˜í–¥ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ íŠ¹ì„± ìƒì„±ì´ ì¤‘ìš” ê²°ë¡  Bias-Variance TradeoffëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„ì˜ í•µì‹¬ ì›ì¹™ì…ë‹ˆë‹¤. ìµœì ì˜ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë°ì´í„°ì™€ ë¬¸ì œì— ëŒ€í•´ í¸í–¥ê³¼ ë¶„ì‚°ì´ ì ì ˆíˆ ê· í˜•ì„ ì´ë£¨ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ”: ë°ì´í„°ì˜ íŠ¹ì„± ì´í•´: ë°ì´í„°ì˜ ë³µì¡ë„ì™€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ íŒŒì•… ëª¨ë¸ ë³µì¡ë„ ì¡°ì ˆ: ë¬¸ì œì— ì í•©í•œ ëª¨ë¸ ì„ íƒ ì •ê·œí™” ê¸°ë²• í™œìš©: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë²• ì ìš© êµì°¨ ê²€ì¦: ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€ ì´ëŸ¬í•œ ì›ì¹™ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ë¬´ì—ì„œ ë” ë‚˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/parametric-vs-non-parametric",
    "title": "Parametric vs Non-Parametric, GBDTëŠ” ì™œ non-parametricì¼ê¹Œ?",
    "date": "2025-07-22",
    "excerpt": "Parametricê³¼ Non-Parametricì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…",
    "category": "Data Science",
    "tags": [
      "parametric",
      "non-parametric",
      "machine-learning",
      "statistics"
    ],
    "content": "Parametricì´ë€? ì˜ì–´ í•œêµ­ì–´ ì„¤ëª… ------------------------- ----------- -------------------------------- parametric model ëª¨ìˆ˜ì  ëª¨ë¸ ê³ ì •ëœ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°(ëª¨ìˆ˜)ë¥¼ ê°–ëŠ” ëª¨ë¸ parametric assumption ëª¨ìˆ˜ì  ê°€ì • ë°ì´í„°ê°€ íŠ¹ì •í•œ ë¶„í¬ í˜•íƒœ(ì˜ˆ: ì •ê·œë¶„í¬)ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì • non-parametric model ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ë‚˜ í•¨ìˆ˜ í˜•íƒœë¥¼ ê°€ì •í•˜ì§€ ì•ŠëŠ” ìœ ì—°í•œ ëª¨ë¸ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ê·¼ì‚¬í•´ë‚´ëŠ”ê²Œ Machine Learningì˜ í•µì‹¬ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ë°ì´í„°ê°€ íŠ¹ì •í•œ í•¨ìˆ˜ í˜•íƒœë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ëª¨ë¸ì˜ êµ¬ì¡°(ì˜ˆ: ì„ í˜• íšŒê·€ì²˜ëŸ¼)ê°€ ê³ ì •ë˜ì–´ ìˆëŠ” ê²½ìš°ë¥¼ ëª¨ìˆ˜ì (parametric) ëª¨ë¸ì´ë¼ê³  í•©ë‹ˆë‹¤. Parametric vs Non-Parametric êµ¬ë¶„ Parametric Non-parametric ------------------- -------------- ------------------------ ëª¨ë¸ êµ¬ì¡° ì‚¬ì „ì— ê³ ì •ë¨ ìœ ì—°í•˜ê³  ë°ì´í„° ê¸°ë°˜ ëª¨ìˆ˜(parameter) ìˆ˜ ê³ ì • ë°ì´í„°ê°€ ë§ì•„ì§€ë©´ ì¦ê°€í•¨ ë³µì¡ë„ ì œì–´ íŒŒë¼ë¯¸í„° ì¡°ì • ëª¨ë¸ ìì²´ì˜ êµ¬ì¡° ë³€í™”ë¡œ ì¡°ì • ì˜ˆì‹œ ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€ GBDT, KNN, ëœë¤í¬ë ˆìŠ¤íŠ¸, ì»¤ë„ íšŒê·€ ë°˜ëŒ€ë¡œ Non-parametric ëª¨ë¸ì€ ëª¨ë¸ì˜ í˜•íƒœë‚˜ êµ¬ì¡°ë¥¼ ì‚¬ì „ì— ê°€ì •í•˜ì§€ ì•Šê³ , ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìœ ì—°í•˜ê²Œ ë§Œë“¤ì–´ë‚˜ê°€ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì™œ GBDTëŠ” non-parametricì¼ê¹Œ? ì´ìœ  ì„¤ëª… --------- --------------------------------- íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€ ë°ì´í„°ê°€ ëŠ˜ë©´ íŠ¸ë¦¬ ê°œìˆ˜ë‚˜ ê¹Šì´ê°€ ì»¤ì ¸ì„œ êµ¬ì¡° ìì²´ê°€ ë³€í™”í•¨ êµ¬ì¡° ìœ ì—°ì„± íŠ¹ì • í•¨ìˆ˜ í˜•íƒœë¥¼ ê°€ì •í•˜ì§€ ì•ŠìŒ (ì˜ˆ: ì„ í˜•ì„±) ììœ ë„ ê³¼ì í•© ìœ„í—˜ë„ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ë³µì¡í•œ í•¨ìˆ˜ë„ í•™ìŠµ ê°€ëŠ¥ í•™ìŠµ ëŒ€ìƒ ë°ì´í„°ê°€ ê°€ë¥´ì³ì£¼ëŠ” ë°©ì‹ì— ë”°ë¼ í•¨ìˆ˜ êµ¬ì¡° ìì²´ê°€ ê²°ì •ë¨ GBDTëŠ” ì”ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ ê²°ì • íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ëˆ„ì í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ì§€ë§Œ, íŠ¸ë¦¬ì˜ ê°œìˆ˜, ê¹Šì´, ë…¸ë“œ ìˆ˜, ë¶„í•  ê·œì¹™ ë“±ì€ ë°ì´í„° ì–‘ê³¼ ë³µì¡ë„ì— ë”°ë¼ ê³„ì† ëŠ˜ì–´ë‚˜ê±°ë‚˜ ë°”ë€” ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ì ì—ì„œ GBDTëŠ” non-parametric ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/Double-Debiased-Machine-Learning-for-Treatment-and-Structural-Parameters",
    "title": "[Paper Review] Double Debiased Machine Learning for Treatment and Structural Parameters",
    "date": "2025-07-22",
    "excerpt": "Double Debiased Machine Learning for Treatment and Structural Parameters",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-21-ê³ ë¯¼í• ë•Œ",
    "title": "ê³ ë¯¼í•˜ëŠ” í‘œí˜„ë“¤",
    "date": "2025-07-21",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "On one hand, the Note has a really big screen. On one hand, I havent had Korean food in a while On one hand, taking a cab would be really convenient On the other hand, the new iPhone has a really good camera. On the other hand, Im craving fried chicken. On the other hand, I can walk there in 15 minute. Whether I buy the Note or the new iPhone, I should buy a phone tommorow.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-21-ë‚¨ì² ë‹˜-clabi-ì»¤í”¼ì±—",
    "title": "Clabi coffee chat with ë‚¨ì² ",
    "date": "2025-07-21",
    "excerpt": "Clabi ì±„ìš©ê³µê³  ê´€ë ¨í•´ì„œ ë‚¨ì² ë‹˜ê³¼ ì»¤í”¼ì±— ì§„í–‰",
    "category": "Career",
    "tags": [
      "Coffee Chat",
      "Clabi"
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì»¤í”¼ì±— ìƒì„¸ì •ë³´</strong></summary> í´ë¼ë¹„ ê¸°ìˆ ì—°êµ¬ì†Œ AI ê°œë°œì ì±„ìš© (RAG, LLM, NLP, ì¸ê³µì§€ëŠ¥) ëª¨ì§‘ë¶€ë¬¸ ë° ìƒì„¸ë‚´ìš© ê³µí†µ ìê²©ìš”ê±´ í•™ë ¥: ë¬´ê´€ ì±„ìš© í”„ë¡œì„¸ìŠ¤ ì„œë¥˜ ì§€ì› > ê¸°ìˆ  ë©´ì ‘(íŒ€ì¥) > 2ì°¨ ë©´ì ‘ (ì—°êµ¬ì†Œì¥ë‹˜) > 3ì°¨ ë©´ì ‘ (ëŒ€í‘œë‹˜) AIOps AI ê°œë°œì ëª¨ì§‘ (ê¸°ìˆ ì—°êµ¬ì†Œ 0ëª…) ì£¼ìš”ì—…ë¬´ LLM ë° RAG ì—°êµ¬ë¥¼ í†µí•œ AI ì„œë¹„ìŠ¤ ê³ ë„í™” ë° ì‹ ê·œ ê¸°ëŠ¥ ê°œë°œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í™œìš©í•œ ì±—ë´‡ ë° AI ëª¨ë¸ ê°œë°œ AI ëª¨ë¸ ì„±ëŠ¥ íŒŒì¸íŠœë‹ ë° ìµœì í™” RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ê´€ë ¨ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ ì „ì²˜ë¦¬ ë° ê´€ë¦¬ ê·¼ë¬´ì¡°ê±´ ê·¼ë¬´í˜•íƒœ: ì •ê·œì§(ìˆ˜ìŠµê¸°ê°„ 3ê°œì›”) ê·¼ë¬´ì¼ì‹œ: ì£¼ 5ì¼ (ì›”-ê¸ˆ) 10:00~17:00 ê·¼ë¬´ì§€ì—­: (05854) ì„œìš¸ ì†¡íŒŒêµ¬ ë²•ì›ë¡œ 114 ì— ìŠ¤í…Œì´íŠ¸ Bë™ 1108í˜¸(ë¬¸ì •ë™) ì„œìš¸ 8í˜¸ì„  ë¬¸ì •ì—­ì—ì„œ 200m ì´ë‚´ ì§€ì›ë°©ë²• ì´ë ¥ì„œ ë° í¬íŠ¸í´ë¦¬ì˜¤ ì œì¶œ ì§€ì› ë° ë¬¸ì˜ : DM ë˜ëŠ” ì´ë©”ì¼(ncher@clabi.co.kr) </details> ì»¤í”¼ì±— ì§„í–‰ ë°©ì‹ ì•„ì´í ì—ì„œ ê³µë¶€í•  ë•Œ í¼ì‹¤ë‹˜ì´ì—ˆë˜ ë‚¨ì² ë‹˜ê³¼ ì±„ìš©ê´€ë ¨ ì»¤í”¼ì±— ì§„í–‰. ì˜¨ë¼ì¸ìœ¼ë¡œ 1ì‹œê°„ì •ë„ ì§„í–‰í•¨. ê°„ë‹¨í•œ ê·¼í™© ì´ì•¼ê¸°ì™€ ë‚´ê°€ ì§„í–‰í–ˆë˜ í”„ë¡œì íŠ¸ì— ëŒ€í•´ì„œ ì„¤ëª…ë“œë¦¬ê³  íšŒì‚¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì  ì§ˆë¬¸. ìœ ì‚¬ ê²½í—˜ (í•„ì‚´ê¸°) ì„¤ëª… íšŒì‚¬ì—ì„œ í•˜ëŠ” ì¼ê³¼ ë‚´ê°€ ì§„í–‰í–ˆë˜ ì¼ì˜ ìœ ì‚¬ì„±ì„ ì„¤ëª…. ìœ ì‚¬ ê²½í—˜ì´ ìˆê³  íšŒì‚¬ì—ì„œ í•  ì¼ê³¼ ë¹„ìŠ·í•œ ê³ ë¯¼ë“¤ì„ í–ˆì—ˆë‹¤. ì±—ë´‡ í”„ë¡œì íŠ¸ Clabiì˜ ì£¼ìš” ì—…ë¬´ê°€ RAG SIì¸ë° ì‹¤ì œë¡œ RAGë¥¼ í†µí•´ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ í•´ê²°í–ˆë˜ ê²½í—˜ì´ ìˆìœ¼ë‹ˆê¹Œ ì´ë¥¼ ê°•ì¡°. -> ê¸ì •ì ì¸ ë°˜ì‘. ì•„ì‰¬ìš´ ì ì€ ì§„í–‰í–ˆë˜ ì¼ì˜ ë‚œì´ë„ê°€ ê·¸ë ‡ê²Œ ë†’ì§€ ì•Šì•˜ë‹¤ëŠ” ì . ë§Œì•½ ê¸°ìˆ ë©´ì ‘ì„ ì¤€ë¹„í•œë‹¤ë©´ ì´ ë¶€ë¶„ì„ ë¯¸ë¦¬ ê³µë¶€í•˜ê³  ê°€ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ë‚´ê°€ ê¶ê¸ˆí–ˆë˜ ë¶€ë¶„ RAG SI ì—…ë¬´ëŠ” ì§„ì§œ ì „í˜€ ì•ˆëŒë¦¼. ê·¸ë˜ì„œ ì´ ë¶€ë¶„ì„ ë²—ì–´ë‚˜ì„œ í™•ì¥í•˜ë ¤ëŠ” ë¶€ë¶„ì´ ìˆëŠ”ì§€ -> í˜„ì¬ ìˆ˜ìµìœ¼ë¡œ ìƒì„±í˜• AI, SasS ë¶€ë¶„ìœ¼ë¡œ í™•ì¥í•˜ê³  ìˆë‹¤ê³  í•¨ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬. ë¹…ë°ì´í„° í”Œë«í¼. ìŠ¤ë§ˆíŠ¸íŒœ ë¶€ë¶„ì€ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ -> ì•„ë§ˆ ì™¸ë¶€ì—…ì²´ê°€ ì°¸ì—¬í•˜ëŠ” ê²ƒ ê°™ë‹¤. ì¸í„°ë·°í•˜ê³  ëŠë‚€ ì ì€ ë‚´ê°€ ì›í•˜ëŠ” ë°©í–¥ì˜ ì»¤ë¦¬ì–´ëŠ” ì•„ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ë°ì´í„°ë¥¼ ì‚´í´ë³´ê³  ë¶„ì„í•˜ë©´ì„œ ëª¨ë¸ì„ ë§Œë“œëŠ”, ë°ì‹¸ì ì¸ ëŠë‚Œì˜ ì»¤ë¦¬ì–´ê°€ ë‚˜ëŠ” ë” ì¢‹ì€ ê²ƒ ê°™ë‹¤. ê·¸ë¦¬ê³  ë‹¨ìˆœíˆ SIì„± ì¼ë“¤ì´ ì•„ë‹Œ, ì¢€ ë” ë†’ì€ ìˆ˜ì¤€ì˜ ë‚œì´ë„ë¥¼ ìš”êµ¬í•˜ëŠ” ì¼ì„ í•˜ê³  ì‹¶ë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-20-ì¡°ì–¸í•˜ëŠ”í‘œí˜„Review",
    "title": "Review day",
    "date": "2025-07-20",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "Review day"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ê±° Try to ~~ Try to cut down on drinking. Try not to skip breakfast. Make sure to ~ Make sure not to eat too much junk foods. Im worried about ~ Im worried about my love life. Im little worried about my bills. Be on the lookout for Be on the lookout for pickpockets. Be careful with ~ Be careful with your phone. Be careful not to burn yourself. Be careful not to catch a cold. Be careful not to cut yourself. By the way By the way, I was the blind date? Well the things is Well the thing is Im not really attracted to her. ê°„ë‹¨í•œ ì‘ë¬¸ (ì˜¤ëŠ˜ ë°°ìš´ê±° ì‚¬ìš©ë„ í•´ë³´ë©´ì„œ ì‘ë¬¸í•˜ëŠ” ì—°ìŠµ) Time is so fast. ì •ì‹ ì„ ì°¨ë ¤ë³´ë©´ ì‹œê°„ì€ í•œì°¸ ì§€ë‚˜ìˆë‹¤. Acheving something like, get a good english, playing guitar well, ì¢‹ì€ íšŒì‚¬ì— ì·¨ì§í•˜ëŠ” ê²ƒ ,... are needs many time in the days. If you cant focus on today looking farway and beding nervous, I never acheive somethings that needs days. You should know that today is so precious. You can do everthing, but you couldn`t do nothing if you don't move. So, make sure to focus on the day you are living. ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë¬¸ Time flies so fast. Before you know it, a lot of time has passed. Achieving things like getting good at English, playing guitar well, or getting a job at a good company requires many days of effort. If you can't focus on today and keep looking far ahead while getting nervous, you'll never achieve things that take time. You should know that today is precious. You can do anything, but you won't accomplish anything if you don't take action. So, make sure to focus on the day you're living. í”¼ë“œë°± ë¬¸ë²• ë° í‘œí˜„ ìˆ˜ì •ì‚¬í•­ \"Time is so fast\" â†’ \"Time flies so fast\" \"Time is fast\"ëŠ” ë¬¸ë²•ì ìœ¼ë¡œ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. \"Time flies\"ê°€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì…ë‹ˆë‹¤. \"Acheving\" â†’ \"Achieving\" ì² ì ì˜¤ë¥˜ì…ë‹ˆë‹¤. \"get a good english\" â†’ \"getting good at English\" \"English\"ëŠ” ëŒ€ë¬¸ìë¡œ ì‹œì‘í•´ì•¼ í•˜ë©°, \"good at English\"ê°€ ì˜¬ë°”ë¥¸ í‘œí˜„ì…ë‹ˆë‹¤. \"are needs many time\" â†’ \"requires many days of effort\" ë¬¸ë²•ì ìœ¼ë¡œ ë§ì§€ ì•ŠëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. \"require\"ë¥¼ ì‚¬ìš©í•˜ê³  \"time\" ëŒ€ì‹  \"days of effort\"ê°€ ë” ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. \"If you cant focus\" â†’ \"If you can't focus\" \"can't\"ì˜ ì¶•ì•½í˜•ì—ì„œ apostropheê°€ ë¹ ì¡ŒìŠµë‹ˆë‹¤. \"looking farway\" â†’ \"looking far ahead\" \"farway\"ëŠ” ì˜¬ë°”ë¥¸ ë‹¨ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤. \"far ahead\"ê°€ ë§ìŠµë‹ˆë‹¤. \"beding nervous\" â†’ \"getting nervous\" \"beding\"ì€ \"being\"ì˜ ì˜¤íƒ€ë¡œ ë³´ì´ì§€ë§Œ, ë§¥ë½ìƒ \"getting nervous\"ê°€ ë” ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. \"I never acheive\" â†’ \"you'll never achieve\" ì£¼ì–´ë¥¼ ì¼ê´€ì„± ìˆê²Œ \"you\"ë¡œ í†µì¼í•˜ê³ , ë¯¸ë˜ ì‹œì œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. \"somethings\" â†’ \"things\" \"something\"ì€ ì…€ ìˆ˜ ì—†ëŠ” ëª…ì‚¬ì´ë¯€ë¡œ ë³µìˆ˜í˜•ì´ ì—†ìŠµë‹ˆë‹¤. \"You can do everthing\" â†’ \"You can do anything\" \"everthing\"ì€ \"everything\"ì˜ ì˜¤íƒ€ì…ë‹ˆë‹¤. \"couldn't do nothing\" â†’ \"won't accomplish anything\" ì´ì¤‘ ë¶€ì •ì„ í”¼í•˜ê³  ë” ëª…í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. \"if you don't move\" â†’ \"if you don't take action\" \"move\"ë³´ë‹¤ \"take action\"ì´ ë” êµ¬ì²´ì ì´ê³  ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ë°°ìš´ í‘œí˜„ í™œìš©ë„ âœ… \"Make sure to\" í‘œí˜„ì„ ì˜ í™œìš©í–ˆìŠµë‹ˆë‹¤! \"Make sure to focus on the day you're living.\" ì¶”ê°€ ì œì•ˆì‚¬í•­ ë” ë‹¤ì–‘í•œ í‘œí˜„ ì‚¬ìš©: \"Try to\", \"I'm worried about\", \"Be careful with\" ë“±ì˜ í‘œí˜„ë„ í™œìš©í•´ë³´ì„¸ìš”. ë¬¸ì¥ ì—°ê²°: \"By the way\", \"Well, the thing is\" ê°™ì€ ì—°ê²°ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ê°€ ë©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì˜ˆì‹œ: ì¶”ìƒì ì¸ ë‚´ìš©ë³´ë‹¤ êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì„¸ìš”.",
    "public": true
  },
  {
    "id": "Self_Development/Book/ë§Œë“¤ì–´ì§„ ì‹  (The God Delusion)/ëŒ€ë‹¨íˆì¢…êµì ì¸ë¶ˆì‹ ì",
    "title": "ëŒ€ë‹¨íˆ ì¢…êµì ì¸ ë¶ˆì‹ ì",
    "date": "2025-07-20",
    "excerpt": "ë‚˜ëŠ” ì¸ê²©ì‹ ì„ ìƒìƒí•˜ë ¤ëŠ” ì‹œë„ëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤. ì‹ ì€ ìš°ë¦¬ì˜ ë¶ˆì¶©ë¶„í•œ ê°ê°ìœ¼ë¡œ ì„¸ê³„ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨ìœ¼ë¡œì¨ ê²½ì™¸ì‹¬ì„ í’ˆê²Œ í•˜ëŠ” ì •ë„ë©´ ì¶©ë¶„í•˜ë‹¤. <ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸>",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "ë§Œë“¤ì–´ì§„ ì‹ "
    ],
    "content": "ê³¼í•™ìë“¤ì´ ë§í•˜ëŠ” ì‹ ì€ ë­˜ê¹Œ? > ë‚˜ëŠ” ì¸ê²©ì‹ ì„ ìƒìƒí•˜ë ¤ëŠ” ì‹œë„ëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤. ì‹ ì€ ìš°ë¦¬ì˜ ë¶ˆì¶©ë¶„í•œ ê°ê°ìœ¼ë¡œ ì„¸ê³„ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨ìœ¼ë¡œì¨ ê²½ì™¸ì‹¬ì„ í’ˆê²Œ í•˜ëŠ” ì •ë„ë©´ ì¶©ë¶„í•˜ë‹¤. > > <ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸> ê³¼í•™ìë“¤ì´ ë§í•˜ëŠ” ì‹ ì€ ë¬´ì—‡ì¼ê¹Œ? ê°™ì€ ì‹ ì´ë¼ëŠ” ë‹¨ì–´ë¡œ í‘œí˜„ë˜ê³  ìˆì§€ë§Œ ì´ë¥¼ êµ¬ë¶„í•´ì„œ ì´í•´í•  í•„ìš”ê°€ ìˆì„ ê²ƒ ê°™ë‹¤. ì•„ì´ìŠˆíƒ€ì¸ì€ ì‹ ì„ ë§í•˜ë©° ìš°ì£¼ì˜ í•©ë¦¬ì„±ê³¼ ì§ˆì„œì— ëŒ€í•œ ê²½ì™¸ì‹¬ì„ ë“¤ì–´ë‚´ê³  ìˆë‹¤. ì˜ˆìˆ˜ë‚˜ ë¶€ì²˜ì™€ ê°™ì€ ì‹ ì„ ì˜ë¯¸í•˜ëŠ”ê²Œ ì•„ë‹Œ. > í›¨ì”¬ ë” ë¶ˆí–‰í•œ í˜¼ë€ì€ ì•„ì¸ìŠˆíƒ€ì¸ì‹ì˜ ì¢…êµì™€ ì´ˆìì—°ì ì¸ ì¢…êµë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•¨ìœ¼ë¡œì¨ ë¹šì–´ì§„ë‹¤. ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸(Albert Einstein)ì€ ì¢…ì¢… ì‹ ì´ë¼ëŠ” ëª…ì¹­ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨(ê·¸ëŸ° ë¬´ì‹ ë¡ ì ê³¼í•™ìê°€ ê·¸ë§Œì€ ì•„ë‹ˆë‹¤), ê·¸ëŸ° ìœ ëª…í•œ ê³¼í•™ìê°€ ìì‹ ë“¤ì˜ í¸ì´ê¸°ë¥¼ ë„ˆë¬´ë‚˜ ë°”ë¼ëŠ” ì´ˆìì—°ì£¼ì˜ìë“¤ì˜ ì˜¤í•´ë¥¼ ìì´ˆí•˜ê³¤ í–ˆë‹¤. > > <ë§Œë“¤ì–´ì§„ ì‹ >, ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ - ë°€ë¦¬ì˜ ì„œì¬ ê³¼í•™ìë“¤ì´ ì‹ ì„ ì´ì•¼ê¸°í•œë‹¤ê³  ì´ë“¤ì´ ì‹ ì„ ë¯¿ëŠ”ë‹¤ê³  ìƒê°í•˜ë©´ ì•ˆë  ë“¯ í•˜ë‹¤. > ìš°ë¦¬ ì‹œëŒ€ì˜ ìœ„ëŒ€í•œ ê³¼í•™ìë“¤ì´ ì¢…êµì ì¸ ë§ì„ í•˜ëŠ” ë“¯ì´ ë³´ì—¬ë„ ê·¸ë“¤ì˜ ì‹ ë…ì„ ë” ê¹Šì´ íŒŒê³  ë“¤ì–´ê°€ë©´ ëŒ€ê°œ ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì´ ë“œëŸ¬ë‚œë‹¤. > > <ë§Œë“¤ì–´ì§„ ì‹ >, ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ - ë°€ë¦¬ì˜ ì„œì¬ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ëª°ë¼ë„ ê³¼í•™ìë“¤ ì¤‘ ì‹ ì„ ë¯¿ëŠ” ì‚¬ëŒë“¤ì€ ì™œ ìˆëŠ”ê±¸ê¹Œ? ê³ ë“±í•™êµë•Œ ì¹œêµ¬ë¥¼ ë”°ë¼ êµíšŒë¥¼ 1~2ë…„ë„ ë‹¤ë…”ë˜ ì ì´ ìˆë‹¤. ê·¸ë•Œë„ ì‹ ì´ë¼ëŠ” ì¡´ì¬ë¥¼ ì–´ë–»ê²Œ ë¯¿ì„ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ ì˜ë¬¸ì´ ë“¤ì–´ ì£¼ìœ„ì— ë¬¼ì–´ë´¤ì„ ë•Œ, ë¯¿ìŒì„ ë¯¿ëŠ”ë‹¤ëŠ” ì´ì•¼ê¸°ë¥¼ ë“¤ì—ˆë‹¤. ì¦‰ ì–´ë–¤ ê·¼ê±°ê°€ ìˆì–´ì„œ ë¯¿ëŠ”ë‹¤ê¸° ë³´ë‹¤ëŠ” ë¯¿ìŒ ìì²´ë¥¼ ë¯¿ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ë¯¿ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì‚¬ëŒì—ê²ŒëŠ” ë¯¿ìŒì´ ë¶€ì¡±í•˜ë‹¤, ì„±ê²½ì„ ë” ê³µë¶€í•˜ë‹¤ë³´ë©´ ë¯¿ê²Œ ëœë‹¤ëŠ” ì´ì•¼ê¸°ë¥¼ í•œë‹¤. ë‚˜ëŠ” ì´ëŸ° ë¯¿ìŒì´ë¼ëŠ”ê²Œ êµ‰ì¥íˆ ìœ„í—˜í•˜ë‹¤ê³  ìƒê°í•œë‹¤. ë„·í”Œë¦­ìŠ¤ ì‹œë¦¬ì¦ˆì˜ \"ë‚˜ëŠ” ì‹ ì´ë‹¤\"ì—ì„œ ì†Œê°œë˜ëŠ” JMS ê°™ì€ ì‚¬ì´ë¹„ ì¢…êµë“¤ì˜ ë…¼ë¦¬ì™€ êµíšŒê°€ ë‹¤ë¥¼ê²Œ ë­”ê°€? ë¯¿ìŒì„ ìš°ì„ ì‹œ í•˜ëŠ” ì‚¶ì˜ íƒœë„ê°€ ë‚˜ëŠ” ë„ì €íˆ ë‚©ë“ì´ ì•ˆê°„ë‹¤. ê³¼í•™ì„ í•˜ëŠ” ì‚¬ëŒë“¤ ì¤‘ì— ì‹ ì„ ë¯¿ëŠ” ì‚¬ëŒë“¤ì´ ë§ì€ë°, ì´ë“¤ë„ ì¢…êµì— ìˆì–´ì„œëŠ” ë¯¿ìŒì„ ìš°ì„ ì‹œ í•˜ê¸° ë•Œë¬¸ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤. ì¢…êµì— ìˆì–´ì„œëŠ” ë…¼ë¦¬ì ì¸ ì´ì•¼ê¸°ê°€ í•„ìš”í•˜ì§€ ì•Šì€. [ì´ê¸°ì  ìœ ì „ì ì €ì] ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤í•œí…Œ ê¹Œì¸ ì° í’€ì–´ë“œë¦¼ : ì¢…êµëŠ” ì—†ì–´ì ¸ì•¼ í•œë‹¤ / ìµœì¬ì²œì˜ ì•„ë§ˆì¡´ ìœ íŠœë¸Œ ë§í¬ ìµœì¬ì²œ êµìˆ˜ê°€ ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ì™€ì˜ ì¸í„°ë·° ê¸°íšŒë¥¼ ë§Œë“¤ì–´ì„œ ë§Œë“¤ì–´ì§„ ì‹ ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆë˜ ì´ì•¼ê¸°. ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ê°€ ì´ì•¼ê¸°í•˜ëŠ” ëª¨íƒœì‹ ì•™. ì´ ì˜ìƒì— ìˆëŠ” ëŒ“ê¸€ ì¤‘ ê¸°ì–µì— ë‚¨ëŠ” ê²ƒë“¤ > 'ê°œì¸ì´ ë§ìƒì— ë¹ ì§€ë©´ ë¯¸ì¹œ ê²ƒì´ì§€ë§Œ ì§‘ë‹¨ì´ ë§ìƒì— ë¹ ì§€ë©´ ì¢…êµì´ë‹¤. > > '-ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ > 'ì¢…êµëŠ” ë¶€ì¡±ì˜ ìƒì¡´ì— ë„ì›€ì„ ì¤€ëŒœ. > > '- ì—ë“œì›Œë“œ ìœŒìŠ¨ > ìˆ˜ ë°±ëª…ì˜ ì‚¬ëŒì´ ëª‡ ë‹¬ë™ì•ˆ ë¯¿ìœ¼ë©´ fake newsì´ì§€ë§Œ ìˆ˜ ì²œë§Œëª…ì˜ ì‚¬ëŒë“¤ì´ ìˆ˜ ì²œë…„ ë™ì•ˆ ë¯¿ìœ¼ë©´ ì¢…êµì´ë‹¤. > > - ìœ ë°œ í•˜ë¼ë¦¬ ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ : ì°½ì¡°ë¡ ì„ ë°˜ë°•í•˜ëŠ” íŒ©íŠ¸ , ì¶©ì½”ì˜ ì² í•™ ìœ íŠœë¸Œ ë§í¬ ìœ ì „ìë“¤ ê°„ì˜ ìœ ì‚¬ì„± íŒ¨í„´. ì°½ì¡°ë¡  vs ì§„í™”ë¡ . ì‹ ì„ ë¯¿ëŠ” ì‚¬ëŒì—ê²ŒëŠ” ë…¼ìŸì„ í•  ìˆ˜ ì—†ë‹¤. ê³¼í•™ì ì¸ ê·¼ê±°ë¥¼ ë¬´ì‹œí•œì±„ ì„±ê²½ì„ ë¯¿ìœ¼ë‹ˆê¹Œ. ê³¼í•™ìì™€ ì‹ ì— ëŒ€í•œ ì •ë¦¬ (GPT ì‹œí‚´) ì™œ í•˜ë‚˜ì˜ â€˜ê³¼í•™ì ê´€ì â€™ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ê°€ í˜„ëŒ€ ì—°êµ¬ìë“¤ì€ ê°œì¸ì  ì‹ Â·ë¹„ê°œì¸ì (ë²”-)ì‹ Â·\\\\ë¬´ì‹ (ë¶ˆê°€ì§€ë¡  í¬í•¨)\\\\ì˜ í­ë„“ì€ ìŠ¤í™íŠ¸ëŸ¼ì„ ì´ë£¨ë©°, í†µê³„ì¡°ì°¨ ì–‘ê·¹í™”ë¥¼ ë³´ì—¬ ì¤€ë‹¤. ë¯¸êµ­ ê³¼í•™ì 2,000 ì—¬ ëª…ì„ ì¡°ì‚¬í•œ Pew Research Center ë³´ê³ ì„œì— ë”°ë¥´ë©´ \\\\ê³¼í•™ìì˜ 51 %\\\\ê°€ ì–´ë–¤ í˜•íƒœë¡œë“  ì‹ ì  ì¡´ì¬ë¥¼ ì¸ì •í•˜ì§€ë§Œ, \\\\41 %\\\\ëŠ” â€œì‹ ì´ë‚˜ ì˜ì  í˜ì´ ì—†ë‹¤â€ê³  ì‘ë‹µí–ˆë‹¤. ì¼ë°˜ ëŒ€ì¤‘ê³¼ ë¹„êµí•˜ë©´ ë¬´ì‹ ë¡  ë¹„ìœ¨ì´ 10 ë°°ì— ê°€ê¹ë‹¤.([Pew Research Center][1]) --- â€œê³¼í•™ë„ ì‹ ì„ ë¯¿ëŠ”ë‹¤â€â€•ìœ ì‹ ë¡ Â·ê¸°ë…êµì  ì…ì¥ ëŒ€í‘œ ì¸ë¬¼ í•µì‹¬ ìƒê° ì£¼ìš” ì €ìˆ Â·ë°œì–¸ ------------------------ ------------------------------ ------------------------------------------------------ í”„ëœì‹œìŠ¤ ì½œë¦°ìŠ¤ (ìœ ì „í•™ì) â€œì§„í™”ëŠ” í•˜ë‚˜ë‹˜ì˜ ì°½ì¡° ë„êµ¬â€â€•â€˜ìœ ì‹ ë¡ ì  ì§„í™”â€™ The Language of God (2006)([ìœ„í‚¤ë°±ê³¼][2]) ì¡°ë¥´ì£¼ ë¥´ë©”íŠ¸ë¥´ (ë¹…ë±… ì œì•ˆìÂ·ì‚¬ì œ) â€œê³¼í•™ì€ ì–´ë–»ê²Œë¥¼, ì‹ ì•™ì€ ì™œë¥¼ ë‹¤ë£¬ë‹¤â€ NYT ì¸í„°ë·°(1933)([The Society of Catholic Scientists][3]) ê·¸ë“¤ì€ ê³¼í•™ ë°©ë²•ë¡ ì„ ì¤€ìˆ˜í•˜ë©´ì„œë„, ì°½ì¡° ëª©ì Â·ë„ë•ë²• ë“± â€˜ê¶ê·¹ì  ì˜ë¯¸â€™ëŠ” ê³¼í•™ ë°–(ì¢…êµ)ì˜ ë²”ì£¼ë¼ê³  ë³¸ë‹¤. --- â€œìš°ì£¼ê°€ ê³§ ì‹ â€â€•ë²”ì‹ ë¡ Â·ìš°ì£¼ì  ê²½ì™¸ ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸: â€œë‚˜ëŠ” ê°œì¸ì  ì‹ ì„ ë¯¿ì§€ ì•ŠëŠ”ë‹¤â€ë©´ì„œë„ â€œìš°ì£¼ì˜ ì‹ ë¹„ì— ëŒ€í•œ ê²½ì™¸ë¥¼ â€˜ì¢…êµì  ê°ì •â€™ì´ë¼ ë¶€ë¥¼ ìˆ˜ ìˆë‹¤â€([Learn Religions][4]) ì¹¼ ì„¸ì´ê±´: â€œìš°ë¦¬ëŠ” \\\\ë³„ì˜ ë¨¼ì§€(star-stuff)\\\\ë¡œ ì´ë£¨ì–´ì ¸, ìš°ì£¼ê°€ ìŠ¤ìŠ¤ë¡œë¥¼ ì¸ì‹í•˜ëŠ” ë°©ì‹â€ì´ë¼ í‘œí˜„í–ˆë‹¤.([Goodreads][5]) ì´ íë¦„ì€ ë²”ì‹ ë¡ Â·ë²”ì¬ì‹ ë¡ (Spinozaì  ì‹ )ìœ¼ë¡œ ë¶„ë¥˜ë˜ë©°, ìì—°ë²•ì¹™ = ì‹ ì  ì´ì„±ê³¼ ë™ë“±ì‹œí•œë‹¤. --- â€œì‹ ì´ ì„¤ ìë¦¬ëŠ” ì—†ë‹¤â€â€•ë¬´ì‹ ë¡ Â·ìì—°ì£¼ì˜ ì¸ë¬¼ ì£¼ëœ ë…¼ì§€ ------------ --------------------------------------------------------------- ìŠ¤í‹°ë¸ í˜¸í‚¹ â€œì‹œê°„ì´ â€˜ë¹…ë±… ì´ì „â€™ì— ì¡´ì¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì°½ì¡°ì£¼ê°€ ê°œì…í•  â€˜ìˆœê°„â€™ë„ ì—†ì—ˆë‹¤â€([Live Science][6]) ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ â€œê°œì¸ì  ì‹ ì€ â€˜ì°©ê°(delusion)â€™ì´ë©°, ì§„í™”ê°€ ì„¤ê³„ì˜ í™˜ìƒì„ ì„¤ëª…í•œë‹¤â€([ìœ„í‚¤ë°±ê³¼][7]) ë¡œëŸ°ìŠ¤ í¬ë¼ìš°ìŠ¤ â€œì–‘ìì¥ê³¼ ì¤‘ë ¥ì´ë©´ â€˜ë¬´(ç„¡)â€™ì—ì„œë„ ìš°ì£¼ê°€ ìì—° ë°œìƒí•  ìˆ˜ ìˆë‹¤â€([ìœ„í‚¤ë°±ê³¼][8]) ì´ë“¤ì€ ì¡´ì¬ë¡ ì  ìì—°ì£¼ì˜ë¥¼ ì±„íƒí•˜ë©°, ì´ˆì›”ì  ì„¤ëª…ì€ ê³¼ì‰ê°€ì„¤ë¡œ ë³¸ë‹¤. --- â€œíŒë‹¨ ìœ ë³´â€â€•ë¶ˆê°€ì§€ë¡ Â·ê³¼í•™ì  íšŒì˜ ì²œì²´ë¬¼ë¦¬í•™ì ë‹ ë“œê·¸ë˜ìŠ¤ íƒ€ì´ìŠ¨ì²˜ëŸ¼ â€œì¦ê±° ì—†ëŠ” ì£¼ì¥ì—” í™•ì‹ í•˜ì§€ ì•ŠëŠ”ë‹¤â€ëŠ” ê²½í—˜ë¡ ì  ë¶ˆê°€ì§€ë¡ ë„ í™•ì‚° ì¤‘ì´ë‹¤.([A-Z Quotes][9]) --- ë‹¤ë¦¬ë¥¼ ë†“ìœ¼ë ¤ëŠ” ì‹œë„: NOMA ì›ì¹™ ê³ (æ•…) ìŠ¤í‹°ë¸ ì œì´ êµ´ë“œì˜ ë¹„ì¤‘ì²© êµêµ¬(NOMA) ëª¨ë¸ì€ â€œê³¼í•™ì€ ì‚¬ì‹¤, ì¢…êµëŠ” ê°€ì¹˜Â·ì˜ë¯¸â€ë¼ëŠ” ì˜ì—­ ë¶„ë¦¬ë¡œ ì¶©ëŒì„ ì™„í™”í•˜ë ¤ í•œë‹¤.([ìœ„í‚¤ë°±ê³¼][10]) --- ì´ë¡ ì  ë°°ê²½ â‘  â€” ë°©ë²•ë¡ ì  ìì—°ì£¼ì˜(MN) ê³¼í•™ì€ ì‹¤í—˜ ê°€ëŠ¥í•œ ìì—° ì›ì¸ë§Œì„ íƒêµ¬í•´ì•¼ í•œë‹¤ëŠ” ê·œë²”ì´ë‹¤. ì´ˆìì—° ì›ì¸ì€ ê²€ì¦ ë¶ˆê°€í•˜ë¯€ë¡œ ì—°êµ¬ ì„¤ê³„ì—ì„œ ë°°ì œëœë‹¤. ì´ ì ì—ì„œ MNì€ â€œê³¼í•™ì˜ ì„±ê³µ ê²½í—˜â€ì´ ë‚³ì€ ì‹¤ìš©ì  ì „ëµìœ¼ë¡œ í•´ì„ëœë‹¤.([ìŠ¤í”„ë§ê±°ë§í¬][11]) > \\\\ì¡´ì¬ë¡ ì  ìì—°ì£¼ì˜(ON)\\\\ì™€ ë‹¬ë¦¬, MNì€ â€œì‹  ì¡´ì¬ ì—¬ë¶€ì— ëŒ€í•´ ì¹¨ë¬µâ€í•˜ì§€ë§Œ, ë¬´ì‹ ë¡ ìëŠ” ì¢…ì¢… MN â†’ ONì„ ì—°ì—­í•œë‹¤. ë…¼ìŸì€ ë°”ë¡œ ì—¬ê¸°ì„œ ë°œìƒí•œë‹¤. --- ì´ë¡ ì  ë°°ê²½ â‘¡ â€” ë¯¸ì„¸ ì¡°ì •Â·ë‹¤ì¤‘ìš°ì£¼Â·ì„¤ê³„ ë…¼ì¦ ë¬¼ë¦¬ ìƒìˆ˜ë“¤ì´ ìƒëª… íƒ„ìƒì— â€œì ˆë¬˜í•˜ê²Œâ€ ë§ì¶°ì ¸ ìˆë‹¤ëŠ” \\\\ë¯¸ì„¸ ì¡°ì •(fine-tuning)\\\\ì€ ë‘ ê°ˆë˜ í•´ì„ì„ ë‚³ì•˜ë‹¤. ì‹ ì  ì„¤ê³„ â€” ì½œë¦°ìŠ¤Â·í´ ë°ì´ë¹„ìŠ¤ ë“±ì€ â€œìš°ì—°ì´ë¼ ë³´ê¸°ì—” í™•ë¥ ì´ ê·¹ë¯¸â€ë¼ê³  ë³¸ë‹¤. ë‹¤ì¤‘ìš°ì£¼ / ì„ íƒ íš¨ê³¼ â€” í˜¸í‚¹Â·í¬ë¼ìš°ìŠ¤ëŠ” â€œê´€ì¸¡ ê°€ëŠ¥í•œ ìš°ì£¼ë§Œ ìƒëª… ì¹œí™”ì ì¼ ë¿â€ì´ë¼ ë°˜ë°•í•œë‹¤.([ìœ„í‚¤ë°±ê³¼][12]) --- ì—­ì‚¬ì  ë§¥ë½ í•œëˆˆì— ë³´ê¸° ì‹œëŒ€ ê³¼í•™ í˜ì‹  ì‹  ê°œë… ë³€í™” ---------- --------- -------------------------- 17C ë‰´í„´ ê¸°ê³„ì  ìš°ì£¼ê´€ â€˜ì‹œê³„ê³µ(Deus ex machina)â€™ 20C ì´ˆ ë¹…ë±…(ë¥´ë©”íŠ¸ë¥´) â€œì°½ì¡° = ì‹œì‘?â€ ë…¼ìŸ ì¬ì í™” 21C ì–‘ìì¤‘ë ¥Â·ë‹¤ì¤‘ìš°ì£¼ ë¬´ì‹ ë¡ Â·ë¶ˆê°€ì§€ë¡  ê°•í™” vs. ë¯¸ì„¸ì¡°ì • ë…¼ì¦ ê°•í™” --- ì¢…í•© Â· ì‹œì‚¬ì  ë‹¤ì–‘ì„±: ê³¼í•™ì  ì—…ì ê³¼ ì‹ ì•™ ìœ ë¬´ëŠ” ì§ê²°ë˜ì§€ ì•ŠëŠ”ë‹¤. ëŒ€í™” ëª¨ë¸: ê°ˆë“±Â·ë…ë¦½Â·ëŒ€í™”Â·ìœµí•© â€” ë„¤ ê°€ì§€ í‹€ ê°€ìš´ë° ì–´ëŠ ìª½ì„ íƒí•˜ëŠëƒì— ë”°ë¼ ì…ì¥ë„ ë¶„í™”ëœë‹¤. ì² í•™ì˜ ì—­í• : ê³¼í•™ì´ â€œì–´ë–»ê²Œâ€ë¥¼ ì„¤ëª…í• ìˆ˜ë¡, â€œì™œâ€ì— ëŒ€í•œ ì§ˆë¬¸ì€ ì² í•™Â·ì‹ í•™Â·ìœ¤ë¦¬ë¡œ ì´ë™í•œë‹¤. > ê²°ë¡ : â€œê³¼í•™ìë“¤ì€ ì‹ ì„ ë¯¿ëŠ”ê°€?â€ë¼ëŠ” ë‹¨ì¼ ì§ˆë¬¸ ëŒ€ì‹ , \\\\â€œì–´ë–¤ â€˜ì‹ â€™ ê°œë…ì„, ì–´ë–¤ ë°©ë²•ë¡ ìœ¼ë¡œ ë…¼ì˜í•˜ëŠ”ê°€?â€\\\\*ë¥¼ ë¬¼ì„ ë•Œ ë¹„ë¡œì†Œ ë…¼ìŸì˜ ê²°ì´ ì„ ëª…í•´ì§„ë‹¤. ì´ ë‹¤ì¸µì  êµ¬ë„ë¥¼ ì—¼ë‘ì— ë‘ë©´, ê³¼í•™ê³¼ ì¢…êµì˜ ê´€ê³„ëŠ” â€˜ì ëŒ€â€™ë³´ë‹¤ëŠ” ì°½ì¡°ì  ê¸´ì¥ê³¼ ìƒí˜¸ ë³´ì™„ì˜ ì¥ìœ¼ë¡œ ì¬í•´ì„ë  ìˆ˜ ìˆë‹¤. [1]: https://www.pewresearch.org/religion/2009/11/05/scientists-and-belief/ \"Scientists and Belief Pew Research Center\" [2]: https://en.wikipedia.org/wiki/The_Language_of_God \"The Language of God - Wikipedia\" [3]: https://catholicscientists.org/articles/lemaitre-follows-two-paths-to-truth-new-york-times-1933-interview-with-lemaitre/ \"\\\"LemaÃ®tre Follows Two Paths to Truth\\\" (1933 New York Times Interview with Fr. LemaÃ®tre, Father of the Big Bang Theory) - The Society of Catholic Scientists\" [4]: https://www.learnreligions.com/albert-einstein-quotations-249858 \"Albert Einstein Quotes on Science, God, and Religion\" [5]: https://www.goodreads.com/quotes/484665-the-cosmos-is-within-us-we-are-made-of-star-stuff?utm_source=chatgpt.com \"Quote by Carl Sagan: â€œThe cosmos is within us. We are made of star-st...â€ - Goodreads\" [6]: https://www.livescience.com/63854-stephen-hawking-says-no-god.html \"Stephen Hawking's Final Book Says There's 'No Possibility' of God in Our Universe Live Science\" [7]: https://en.wikipedia.org/wiki/The_God_Delusion \"The God Delusion - Wikipedia\" [8]: https://en.wikipedia.org/wiki/A_Universe_from_Nothing \"A Universe from Nothing - Wikipedia\" [9]: https://www.azquotes.com/author/14904-Neil_deGrasse_Tyson \"TOP 25 QUOTES BY NEIL DEGRASSE TYSON (of 764) A-Z Quotes\" [10]: https://en.wikipedia.org/wiki/Non-overlapping_magisteria \"Non-overlapping magisteria - Wikipedia\" [11]: https://link.springer.com/article/10.1007/s10670-024-00790-y?utm_source=chatgpt.com \"Methodological Naturalism, Analyzed Erkenntnis - Springer\" [12]: https://en.wikipedia.org/wiki/Fine-tuned_universe \"Fine-tuned universe - Wikipedia\"",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Towards-Optimal Doubly-Robust-Estimation-of-Heterogeneous-Causal-Effects",
    "title": "[Paper Review] Towards Optimal Doubly-Robust Estimation of Heterogeneous Causal Effects",
    "date": "2025-07-20",
    "excerpt": "Doubly-Robust Estimation of Heterogeneous Causal Effects ë…¼ë¬¸ ë¦¬ë·°",
    "category": "Causal Inference",
    "tags": [
      "paper review"
    ],
    "content": "paper link Abstact",
    "public": true
  },
  {
    "id": "Self_Development/Current Affairs/ì¤‘êµ­ë¬¸í™”ëŒ€í˜ëª…ê³¼í™ìœ„ë³‘",
    "title": "ì¤‘êµ­ ë¬¸í™”ëŒ€í˜ëª…ê³¼ í™ìœ„ë³‘",
    "date": "2025-07-17",
    "excerpt": "ë¬¸í™”ëŒ€í˜ëª…ê³¼ í™ìœ„ë³‘ì— ëŒ€í•œ ë‚´ìš©ë“¤",
    "category": "Current Affairs",
    "tags": [
      "ì¤‘êµ­",
      "ë¬¸í™”ëŒ€í˜ëª…",
      "í™ìœ„ë³‘"
    ],
    "content": "ì°¸ê³  ìë£Œ ë¦¬ë·°ì—‰ì´ - ì‚¼ì²´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ê±´ ë¬¸í™”ëŒ€í˜ëª… ë¬¸í™”ëŒ€í˜ëª…ì´ë€? ë¬¸í™”ëŒ€í˜ëª…(æ–‡åŒ–å¤§é©å‘½, Cultural Revolution)ì€ ì •ì‹ ëª…ì¹­ì´ \"ë¬´ì‚°ê³„ê¸‰ ë¬¸í™”ëŒ€í˜ëª…\"(ç„¡ç”¢éšç´šæ–‡åŒ–å¤§é©å‘½, Great Proletarian Cultural Revolution)ìœ¼ë¡œ, 1966ë…„ë¶€í„° 1976ë…„ê¹Œì§€ 10ë…„ê°„ ì¤‘êµ­ì—ì„œ ì¼ì–´ë‚œ ì‚¬íšŒì •ì¹˜ì  ìš´ë™ì´ë‹¤. ë°œìƒ ë°°ê²½ ëŒ€ì•½ì§„ìš´ë™ì˜ ì‹¤íŒ¨ (1958-1962) ë§ˆì˜¤ì©Œë‘¥ì´ ì£¼ë„í•œ ê²½ì œê°œë°œ ì •ì±…ì¸ ëŒ€ì•½ì§„ìš´ë™ì´ ì°¸í˜¹í•˜ê²Œ ì‹¤íŒ¨ 2ì²œë§Œ ëª… ì´ìƒì´ ê¸°ê·¼ìœ¼ë¡œ ì‚¬ë§ ë§ˆì˜¤ì©Œë‘¥ì˜ ê¶Œë ¥ê³¼ ì˜í–¥ë ¥ì´ í¬ê²Œ ì•½í™”ë¨ ì‹¤ìš©ì£¼ì˜ ë…¸ì„ ì˜ ë“±ì¥ ë¥˜ìƒ¤ì˜¤ì¹˜(åŠ‰å°‘å¥‡), ì €ìš°ì–¸ë¼ì´(å‘¨æ©ä¾†), ë©ìƒ¤ì˜¤í•‘(é„§å°å¹³) ë“±ì´ ì‹¤ìš©ì  ê²½ì œ ì •ì±… ë„ì… ê°œì¸ ë†ì§€ í—ˆìš© ë“± ì‹œì¥ê²½ì œ ìš”ì†Œ ë„ì…ìœ¼ë¡œ ê²½ì œ íšŒë³µ (1962-1965) ë§ˆì˜¤ì©Œë‘¥ì€ ì´ëŸ¬í•œ ì •ì±…ì´ ê³µì‚°ì£¼ì˜ ì›ì¹™ì— ìœ„ë°°ëœë‹¤ê³  íŒë‹¨ ë¬¸í™”ëŒ€í˜ëª…ì˜ ëª©í‘œ ë§ˆì˜¤ì©Œë‘¥ì´ ë¬¸í™”ëŒ€í˜ëª…ì„ ì¼ìœ¼í‚¨ ì£¼ìš” ëª©ì : ê¶Œë ¥ íšŒë³µ: ëŒ€ì•½ì§„ìš´ë™ ì‹¤íŒ¨ë¡œ ìƒì€ ì •ì¹˜ì  ì˜í–¥ë ¥ ë˜ì°¾ê¸° ì •ì  ì œê±°: ë¥˜ìƒ¤ì˜¤ì¹˜, ë©ìƒ¤ì˜¤í•‘ ë“± ì‹¤ìš©ì£¼ì˜ ë…¸ì„  ì§€ë„ìë“¤ ì¶•ì¶œ í˜ëª… ì •ì‹  ë¶€í™œ: ê´€ë£Œí™”ë˜ê³  ìˆëŠ” ì¤‘êµ­ ê³µì‚°ë‹¹ê³¼ ì‚¬íšŒì— í˜ëª… ì •ì‹  ì£¼ì… \"4êµ¬\"(å››èˆŠ) íƒ€íŒŒ: êµ¬ì‚¬ìƒ(èˆŠæ€æƒ³) - ì˜› ì‚¬ìƒ êµ¬ë¬¸í™”(èˆŠæ–‡åŒ–) - ì˜› ë¬¸í™” êµ¬í’ì†(èˆŠé¢¨ä¿—) - ì˜› í’ìŠµ êµ¬ìŠµê´€(èˆŠç¿’æ…£) - ì˜› ìŠµê´€ ë¬¸í™”ëŒ€í˜ëª…ì˜ ì „ê°œ ê³¼ì • 1ë‹¨ê³„: ë°œë°œê³¼ í˜¼ë€ (1966-1968) 1966ë…„ 5ì›”: ì‹œì‘ 5.16 í†µì§€: ì¤‘êµ­ ê³µì‚°ë‹¹ ì¤‘ì•™ìœ„ì›íšŒê°€ ë¬¸í™”ëŒ€í˜ëª… ì‹œì‘ì„ ê³µì‹ ì„ ì–¸ ë¶€ë¥´ì£¼ì•„ ë¶„ìë“¤ì´ ë‹¹, ì •ë¶€, êµ°ëŒ€, ë¬¸í™”ê³„ì— ì¹¨íˆ¬í–ˆë‹¤ê³  ì£¼ì¥ í™ìœ„ë³‘(ç´…è¡›å…µ) ì¡°ì§ ì£¼ë¡œ í•™ìƒë“¤ë¡œ êµ¬ì„±ëœ ì¤€êµ°ì‚¬ì¡°ì§ ë§ˆì˜¤ì©Œë‘¥ì–´ë¡(ëª¨íƒë™ì–´ë¡, \"ë¹¨ê°„ì±…\")ì„ ë“¤ê³  ë‹¤ë‹ˆë©° ìˆ­ë°° \"í˜ëª… ë¬´ì£„, ì¡°ë°˜ ìœ ë¦¬\"(é©å‘½ç„¡ç½ª, é€ åæœ‰ç†) - \"í˜ëª…ì€ ì£„ê°€ ì—†ê³ , ë°˜ë€ì€ ì´ì¹˜ê°€ ìˆë‹¤\" 20~30ëŒ€ í•™ìƒë“¤ì´ ì£¼ë¥¼ ì´ë¤˜ê³  ì´ë•Œì— ì—„ì²­ë‚œ ê¶Œë ¥ì„ ê°€ì§€ê³  ìˆì—ˆë‹¤ê³  í•œë‹¤. ì‚¬ë‹¨ì¥ë³´ë‹¤ ë” ë†’ì€ ê¶Œë ¥. ì‚¬ì†Œí•œ ì´ìœ ë¡œ ì‚¬ëŒë“¤ì„ ë¬´ìë¹„í•˜ê²Œ í­í–‰í•˜ê³  ì£½ì˜€ê¸° ë•Œë¬¸ì— ì‚¬ëŒë“¤ì´ í–‰ë™ì„ ì—„ì²­ ì¡°ì‹¬í–ˆì—ˆë‹¤ê³  í•œë‹¤. í­ë ¥ê³¼ íŒŒê´´ ì§€ì‹ì¸, êµì‚¬, ì •ë¶€ ê´€ë¦¬, ë¬¸í™”ì¬ ë“±ì— ëŒ€í•œ ë¬´ì°¨ë³„ ê³µê²© ê³µìë¬˜, ì²œë‹¨, ëª…ì‹­ì‚¼ë¦‰ ë“± ë¬¸í™”ìœ ì‚° íŒŒê´´ í•™êµì™€ ëŒ€í•™ íì‡„ 2ë‹¨ê³„: ê¶Œë ¥ íˆ¬ìŸê³¼ êµ° ê°œì… (1967-1969) 1967ë…„: \"ì „ë©´ì  ë‚´ì „\" í™ìœ„ë³‘ íŒŒë²Œ ê°„ ë¬´ë ¥ ì¶©ëŒ ê° ì§€ì—­ì—ì„œ \"ê¶Œë ¥ íƒˆì·¨\" ìš´ë™ ì „ê°œ ê´‘ì €ìš°, ë² ì´ì§• ë“±ì—ì„œ ëŒ€ê·œëª¨ í­ë ¥ ì‚¬íƒœ ì¸ë¯¼í•´ë°©êµ°ì˜ ê°œì… ë¦°ë±Œì˜¤(æ—å½ª)ê°€ ì´ë„ëŠ” ì¸ë¯¼í•´ë°©êµ°ì´ ì§ˆì„œ íšŒë³µì— ë‚˜ì„¬ í™ìœ„ë³‘ë“¤ì„ ë†ì´Œìœ¼ë¡œ ì¶”ë°©í•˜ëŠ” \"ìƒì‚°í•˜í–¥\"(ä¸Šå±±ä¸‹é„‰) ìš´ë™ ì‹œì‘ 3ë‹¨ê³„: ì•ˆì •í™”ì™€ ë‚´ë¶€ ê¶Œë ¥ íˆ¬ìŸ (1969-1976) ë¦°ë±Œì˜¤ ì‚¬ê±´ (1971) ë§ˆì˜¤ì©Œë‘¥ì˜ í›„ê³„ìë¡œ ì§€ëª…ëœ ë¦°ë±Œì˜¤ê°€ ì •ì¹˜ì ìœ¼ë¡œ ëª°ë½ ë§ˆì˜¤ ì•”ì‚´ ì‹œë„ í›„ ì†Œë ¨ìœ¼ë¡œ ë§ëª… ì‹œë„ ì¤‘ ëª½ê³¨ì—ì„œ ë¹„í–‰ê¸° ì¶”ë½ì‚¬ ë§ì€ ì¤‘êµ­ì¸ë“¤ì´ ê³µì‚°ë‹¹ ì§€ë„ë¶€ì— ëŒ€í•œ ì‹ ë¢° ìƒì‹¤ 4ì¸ë°©(å››äººå¹«)ì˜ ë“±ì¥ ì¥ì¹­(æ±Ÿé’, ë§ˆì˜¤ì©Œë‘¥ì˜ ë¶€ì¸), ì¥ì¶˜ì°¨ì˜¤(å¼µæ˜¥æ©‹), ì•¼ì˜¤ì›ìœ„ì•ˆ(å§šæ–‡å…ƒ), ì™•í›™ì›(ç‹æ´ªæ–‡) ë¬¸í™”ëŒ€í˜ëª… í›„ê¸°ì˜ ì‹¤ì§ˆì  ê¶Œë ¥ìë“¤ ê·¹ì¢Œ ë…¸ì„  ì¶”ì§„ ë¬¸í™”ëŒ€í˜ëª…ì˜ ì£¼ìš” ì •ì±… ìƒì‚°í•˜í–¥ ìš´ë™(ä¸Šå±±ä¸‹é„‰é‹å‹•) \"ì‚°ê³¼ ì‹œê³¨ë¡œ ê°€ë¼\"ëŠ” ì˜ë¯¸ ë„ì‹œ ì²­ë…„ 1ì²œë§Œ ëª… ì´ìƒì„ ë†ì´Œìœ¼ë¡œ ê°•ì œ ì´ì£¼ ë…¸ë™ì„ í†µí•œ \"ì¬êµìœ¡\" ëª…ëª© ì‹¤ì œë¡œëŠ” ë„ì‹œ ê³¼ë°€í™” í•´ê²°ê³¼ í™ìœ„ë³‘ í•´ì²´ ëª©ì  êµìœ¡ ì œë„ íŒŒê´´ ëŒ€í•™ ì…ì‹œ ì‹œí—˜ íì§€ (1966-1973) \"í˜ëª…ì  ìˆœìˆ˜ì„±\"ì´ í•™ë ¥ë³´ë‹¤ ì¤‘ìš”ì‹œë¨ ì§€ì‹ì¸ê³¼ ê³¼í•™ìë“¤ì´ \"êµ¬ì§€ì‹ë¶„ì\"ë¡œ íƒ„ì••ë°›ìŒ ë¬¸í™” ì˜ˆìˆ  í†µì œ ì„œêµ¬ ë¬¸í•™, ìŒì•…, ì—°ê·¹ ê¸ˆì§€ ì…°ìµìŠ¤í”¼ì–´ ì‘í’ˆ ë“± ì„œì–‘ ê³ ì „ ê¸ˆì§€ ì˜¤ì§ í˜ëª… ì£¼ì œë§Œ í—ˆìš© ë¬¸í™”ëŒ€í˜ëª…ì˜ ì¢…ë£Œ ë§ˆì˜¤ì©Œë‘¥ì˜ ì£½ìŒ (1976ë…„ 9ì›” 9ì¼) 27ë…„ê°„ ì¤‘êµ­ì„ í†µì¹˜í•œ ë§ˆì˜¤ì©Œë‘¥ ì‚¬ë§ ì „êµ­ì ìœ¼ë¡œ ì• ë„ ë¶„ìœ„ê¸° 4ì¸ë°© ì²´í¬ (1976ë…„ 10ì›” 6ì¼) í™”ê¶ˆí‘(è¯åœ‹é‹’)ì´ 4ì¸ë°©ì„ ì²´í¬ ë¬¸í™”ëŒ€í˜ëª…ì˜ ê³µì‹ì  ì¢…ë£Œë¡œ ê°„ì£¼ 4ì¸ë°©ì„ \"ë°˜í˜ëª… ë²”ì£„\" í˜ì˜ë¡œ ê¸°ì†Œ ë¬¸í™”ëŒ€í˜ëª…ì˜ ê²°ê³¼ì™€ ì˜í–¥ ëª‡ì²œë…„ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ìœ ì‚°ì„ ë‹¤ ë¶€ì‰ˆë‹¤. ê³µìë‚˜ ì œê°ˆëŸ‰ê³¼ ê°™ì€ ì„ ì¡°ë“¤ì˜ ë¬´ë¤ì„ íŒŒí—¤ì¹˜ê³  ì‹œì‹ ì„ ë¶ˆíƒœì› ë‹¤. ì¸ëª… í”¼í•´ ì‚¬ë§ì: ì•½ 150ë§Œ ëª…ì—ì„œ 200ë§Œ ëª… (ì¶”ì •ì¹˜ëŠ” í•™ìë§ˆë‹¤ ë‹¤ë¦„) ë°•í•´ë°›ì€ ì¸ì›: ìˆ˜ì²œë§Œ ëª… ê´‘ì‹œì„±ì—ì„œëŠ” ì‹ì¸ í–‰ìœ„ë¥¼ í¬í•¨í•œ í•™ì‚´ ë°œìƒ ê²½ì œì  í”¼í•´ 1968ë…„ ê³µì—… ìƒì‚°ëŸ‰ì´ 1966ë…„ ëŒ€ë¹„ 12% ê°ì†Œ ì „ë°˜ì ì¸ ê²½ì œ ë°œì „ ì§€ì—° ì‚¬íšŒì  íŒŒê´´ ì „í†µ ë¬¸í™”ì™€ ì—­ì‚¬ ìœ ë¬¼ ëŒ€ëŸ‰ íŒŒê´´ ê°€ì¡±, ì¹œêµ¬ ê´€ê³„ íŒŒê´´ êµìœ¡ ì‹œìŠ¤í…œ ë¶•ê´´ë¡œ \"ìƒì–´ë²„ë¦° ì„¸ëŒ€\" ë“±ì¥ ì •ì¹˜ì  ì˜í–¥ ì¤‘êµ­ ê³µì‚°ë‹¹ì˜ ê¶Œìœ„ì™€ ì‹ ë¢°ë„ í¬ê²Œ ì‹¤ì¶” 1978ë…„ ë©ìƒ¤ì˜¤í•‘ì˜ ê°œí˜ê°œë°© ì •ì±…ìœ¼ë¡œ ì´ì–´ì§ ë¬¸í™”ëŒ€í˜ëª…ì— ëŒ€í•œ í‰ê°€ ì¤‘êµ­ ê³µì‚°ë‹¹ì˜ ê³µì‹ ì…ì¥ (1981ë…„) > \"ë¬¸í™”ëŒ€í˜ëª…ì€ ì¤‘í™”ì¸ë¯¼ê³µí™”êµ­ ê±´êµ­ ì´í›„ ë‹¹, êµ­ê°€, ì¸ë¯¼ì´ ê²ªì€ ê°€ì¥ ì‹¬ê°í•œ ì¢Œì ˆì´ì ê°€ì¥ ë¬´ê±°ìš´ ì†ì‹¤ì„ ì´ˆë˜í•œ ì‚¬ê±´ì´ë‹¤.\" í˜„ì¬ ì¤‘êµ­ì—ì„œì˜ ì·¨ê¸‰ \"10ë…„ ë™ë€\"(åå¹´å‹•äº‚) ë˜ëŠ” \"10ë…„ ì¬ë‚œ\"(åå¹´æµ©åŠ«)ìœ¼ë¡œ ë¶ˆë¦¼ ê³µê°œì  ë…¼ì˜ëŠ” ì—¬ì „íˆ ì œí•œì  ë§ˆì˜¤ì©Œë‘¥ì˜ ëª…ì„±ì„ ë³´í˜¸í•˜ê¸° ìœ„í•´ 4ì¸ë°©ì—ê²Œ ì£¼ë¡œ ì±…ì„ ì „ê°€ ë¬¸í™”ëŒ€í˜ëª…ì´ ë‚¨ê¸´ êµí›ˆ ê¶Œë ¥ ì§‘ì¤‘ì˜ ìœ„í—˜ì„±: ê°œì¸ ë…ì¬ì˜ íí•´ ëŒ€ì¤‘ ë™ì›ì˜ í•œê³„: ì¡°ì§í™”ë˜ì§€ ì•Šì€ ëŒ€ì¤‘ ìš´ë™ì˜ ìœ„í—˜ ì´ë°ì˜¬ë¡œê¸°ì˜ ë§¹ëª©ì„±: í˜„ì‹¤ì„ ë¬´ì‹œí•œ êµì¡°ì  ì‚¬ê³ ì˜ ë¬¸ì œ ì§€ì‹ê³¼ ë¬¸í™”ì˜ ì¤‘ìš”ì„±: ë°˜ì§€ì„±ì£¼ì˜ì˜ ì°¸í˜¹í•œ ê²°ê³¼ ë¬¸í™”ëŒ€í˜ëª…ì€ 20ì„¸ê¸° ì¤‘êµ­ ì—­ì‚¬ìƒ ê°€ì¥ ë¹„ê·¹ì ì¸ ì‚¬ê±´ ì¤‘ í•˜ë‚˜ë¡œ, ì¤‘êµ­ ì‚¬íšŒì— ê¹Šì€ ìƒì²˜ë¥¼ ë‚¨ê²¼ìœ¼ë©° ê·¸ ì˜í–¥ì€ ì˜¤ëŠ˜ë‚ ê¹Œì§€ë„ ì§€ì†ë˜ê³  ìˆë‹¤. ì‚¼ì²´ì™€ì˜ ì—°ê²° ì‚¼ì²´ë€? ì‚¼ì²´(ä¸‰é«”, The Three-Body Problem)ëŠ” ì¤‘êµ­ì˜ SF ì†Œì„¤ê°€ ë¥˜ì¸ ì‹ (åŠ‰æ…ˆæ¬£)ì´ 2006ë…„ì— ë°œí‘œí•œ ê³¼í•™ì†Œì„¤ë¡œ, ã€Šì§€êµ¬ì™•ì‚¬ã€‹(åœ°çƒå¾€äº‹, Remembrance of Earth's Past) ì‚¼ë¶€ì‘ì˜ ì²« ë²ˆì§¸ ì‘í’ˆì´ë‹¤. ì†Œì„¤ì˜ ì„±ì·¨ 2015ë…„ íœ´ê³ ìƒ ìˆ˜ìƒ: ì¤‘êµ­ ì†Œì„¤ ìµœì´ˆë¡œ ì„¸ê³„ ìµœê³  ê¶Œìœ„ì˜ SF ë¬¸í•™ìƒ ìˆ˜ìƒ ê¸€ë¡œë²Œ ë² ìŠ¤íŠ¸ì…€ëŸ¬: 2014ë…„ ì¼„ ë¥˜(Ken Liu)ì˜ ì˜ì–´ ë²ˆì—­ì„ í†µí•´ ì „ ì„¸ê³„ì  ì¸ê¸° íšë“ \"ì¤‘êµ­ì˜ ìŠ¤íƒ€ì›Œì¦ˆ\"ë¡œ ë¶ˆë¦¬ë©° ì¤‘êµ­ SF ë¬¸í•™ì˜ ê¸€ë¡œë²Œ ì§„ì¶œ ìƒì§• ì‚¼ì²´ì˜ ì¤„ê±°ë¦¬ ê¸°ë³¸ ì„¤ì • ì†Œì„¤ì€ ì§€êµ¬ ê·¼ì²˜ì— ìˆëŠ” ì‚¼ì„±ê³„(ä¸‰æ˜Ÿç³», Three-Body System)ì— ì‚´ê³  ìˆëŠ” ì™¸ê³„ ë¬¸ëª… ì‚¼ì²´ì¸(ä¸‰é«”äºº, Trisolarians)ê³¼ ì§€êµ¬ì˜ ì ‘ì´‰ì„ ë‹¤ë£¬ë‹¤. ì‚¼ì²´ì¸ë“¤ì€ ì„¸ ê°œì˜ íƒœì–‘ì´ ë¶ˆê·œì¹™í•˜ê²Œ ì›€ì§ì´ëŠ” í˜¼ëˆìŠ¤ëŸ¬ìš´ í™˜ê²½ì—ì„œ ì‚´ê³  ìˆì–´ ë©¸ì¢… ìœ„ê¸°ì— ì²˜í•´ ìˆë‹¤. ì£¼ìš” ì¸ë¬¼ ì˜ˆì›ì œ(è‘‰æ–‡æ½”, Ye Wenjie): ë¬¸í™”ëŒ€í˜ëª… ì‹œê¸° ë¬¼ë¦¬í•™ìì¸ ì•„ë²„ì§€ë¥¼ í™ìœ„ë³‘ì—ê²Œ ìƒì€ ì—¬ì„± ì¸ë¥˜ì— ëŒ€í•œ ì ˆë§ìœ¼ë¡œ ì™¸ê³„ ë¬¸ëª…ê³¼ì˜ ì ‘ì´‰ì„ ì‹œë„ ì´ì•¼ê¸°ì˜ í•µì‹¬ ì¸ë¬¼ì´ì ë¹„ê·¹ì  ì˜ì›… ì˜ìƒí™” ì‘í’ˆë“¤ ì¤‘êµ­íŒ ë“œë¼ë§ˆ (2023) ì œì‘ì‚¬: í…ì„¼íŠ¸(Tencent) íŠ¹ì§•: ì›ì‘ì— ì¶©ì‹¤í•˜ë˜ ë¬¸í™”ëŒ€í˜ëª… ì¥ë©´ì€ ê²€ì—´ë¡œ ì¸í•´ ì‚­ì œ í‰ê°€: ì¤‘êµ­ ë‚´ì—ì„œ ë†’ì€ ì‹œì²­ë¥  ê¸°ë¡ ë„·í”Œë¦­ìŠ¤ ë“œë¼ë§ˆ: \"3 Body Problem\" (2024) ì œì‘ì§„: ë°ì´ë¹„ë“œ ë² ë‹ˆì˜¤í”„(David Benioff) D.B. ì™€ì´ìŠ¤(D.B. Weiss) - ã€Šì™•ì¢Œì˜ ê²Œì„ã€‹ ì œì‘ì§„ ì•Œë ‰ì‚°ë” ìš°(Alexander Woo) ì œì‘ë¹„: 1ì–µ 6ì²œë§Œ ë‹¬ëŸ¬ (ë„·í”Œë¦­ìŠ¤ ì‚¬ìƒ ìµœê³ ì•¡ ë“œë¼ë§ˆ ì¤‘ í•˜ë‚˜) ì£¼ìš” ë³€í™”: ë°°ê²½ì„ ì¤‘êµ­ì—ì„œ ì˜êµ­ ëŸ°ë˜ìœ¼ë¡œ ì´ì „ ìºë¦­í„°ë“¤ì˜ ì¸ì¢… ë‹¤ì–‘í™” \"ì˜¥ìŠ¤í¼ë“œ íŒŒì´ë¸Œ\"ë¼ëŠ” ìƒˆë¡œìš´ ì¸ë¬¼ ê´€ê³„ ì°½ì¡° ë¬¸í™”ëŒ€í˜ëª…ê³¼ ì‚¼ì²´ì˜ ì—°ê²° ì†Œì„¤ ì† ë¬¸í™”ëŒ€í˜ëª… íŠ¸ë¼ìš°ë§ˆì˜ ê·¼ì›: ì˜ˆì›ì œê°€ í™ìœ„ë³‘ë“¤ì—ê²Œ ì•„ë²„ì§€ë¥¼ ìƒëŠ” ì¥ë©´ìœ¼ë¡œ ì‹œì‘ ì¸ë¥˜ì— ëŒ€í•œ ì ˆë§: ë¬¸í™”ëŒ€í˜ëª…ì˜ í­ë ¥ì„±ì´ ì˜ˆì›ì œë¡œ í•˜ì—¬ê¸ˆ ì¸ë¥˜ ì „ì²´ì— ëŒ€í•œ ì‹ ë¢°ë¥¼ ìƒê²Œ ë§Œë“¦ ì™¸ê³„ ë¬¸ëª…ê³¼ì˜ ì ‘ì´‰ ë™ê¸°: ì¸ë¥˜ë³´ë‹¤ ë‚˜ì€ ë¬¸ëª…ì— ëŒ€í•œ ê°ˆë§ ì—­ì‚¬ì  ì˜ë¯¸ ê³¼í•™ì˜ íƒ„ì••: ë¬¸í™”ëŒ€í˜ëª… ì‹œê¸° ì§€ì‹ì¸ê³¼ ê³¼í•™ìë“¤ì— ëŒ€í•œ ë°•í•´ ì´ë°ì˜¬ë¡œê¸° vs ì§„ë¦¬: ê³¼í•™ì  ì§„ì‹¤ë³´ë‹¤ ì •ì¹˜ì  ì˜¬ë°”ë¦„ì„ ìš°ì„ ì‹œí•œ ê²°ê³¼ ì§‘ë‹¨ ê´‘ê¸°: í™ìœ„ë³‘ì˜ í­ë ¥ì„±ì´ ë³´ì—¬ì£¼ëŠ” êµ°ì¤‘ ì‹¬ë¦¬ì˜ ìœ„í—˜ì„± ì‚¼ì²´ê°€ ì œê¸°í•˜ëŠ” ì² í•™ì  ì§ˆë¬¸ë“¤ ë¬¸ëª…ì˜ ì¶©ëŒ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ëª…ì´ ë§Œë‚¬ì„ ë•Œì˜ í•„ì—°ì  ê°ˆë“± ìƒì¡´ì„ ìœ„í•œ ë„ë•ì  ì„ íƒì˜ ë”œë ˆë§ˆ ê³¼í•™ê³¼ ì •ì¹˜ ê³¼í•™ì˜ ì •ì¹˜ì  ì¤‘ë¦½ì„±ì— ëŒ€í•œ ì˜ë¬¸ ì§€ì‹ì¸ì˜ ì‚¬íšŒì  ì±…ì„ ì¸ë¥˜ì˜ ë¯¸ë˜ ê¸°ìˆ  ë°œì „ê³¼ ë„ë•ì  ì§„ë³´ì˜ ë¶ˆì¼ì¹˜ ë¬¸ëª…ì˜ ì§€ì†ê°€ëŠ¥ì„±ì— ëŒ€í•œ ê·¼ë³¸ì  ì˜ë¬¸ ê¸€ë¡œë²Œ ìˆ˜ìš©ê³¼ ë…¼ë€ ê¸ì •ì  í‰ê°€ ê³¼í•™ì  ìƒìƒë ¥: ì •êµí•œ ë¬¼ë¦¬í•™ ì´ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì„¤ì • ì² í•™ì  ê¹Šì´: ë¬¸ëª…, ê³¼í•™, ë„ë•ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì„±ì°° ì¤‘êµ­ SFì˜ ì„¸ê³„í™”: ì„œêµ¬ ì¤‘ì‹¬ì˜ SF ì¥ë¥´ì— ë™ì–‘ì  ê´€ì  ë„ì… ë¹„íŒê³¼ ë…¼ë€ ë„·í”Œë¦­ìŠ¤ ë²„ì „ì— ëŒ€í•œ ì¤‘êµ­ ë‚´ ë°˜ë°œ: ë¬¸í™”ëŒ€í˜ëª… ì¥ë©´ì´ ì¤‘êµ­ì„ ë¹„í•˜í•œë‹¤ëŠ” ë¹„íŒ ì¤‘êµ­ì  ìš”ì†Œì˜ í¬ì„ì— ëŒ€í•œ ë¶ˆë§Œ ì •ì¹˜ì  í•´ì„: ì¤‘êµ­ êµ­ê°€ì£¼ì˜ìë“¤ì˜ \"ì„œêµ¬ ë¬¸í™” íŒ¨ê¶Œì£¼ì˜\" ë¹„íŒ ë¬¸í™”ëŒ€í˜ëª… ë¬˜ì‚¬ì— ëŒ€í•œ ê²€ì—´ ìš°ë ¤ ì‚¼ì²´ê°€ ë³´ì—¬ì£¼ëŠ” ë¬¸í™”ëŒ€í˜ëª…ì˜ ì˜í–¥ ê°œì¸ì  ì°¨ì› íŠ¸ë¼ìš°ë§ˆì˜ ì„¸ëŒ€ ì „ìŠ¹: ì˜ˆì›ì œë¥¼ í†µí•´ ë³¸ ê°œì¸ì  ìƒì²˜ ì‹ ë¢°ì˜ ë¶•ê´´: ì¸ê°„ê³¼ ì‚¬íšŒì— ëŒ€í•œ ê·¼ë³¸ì  ë¶ˆì‹  ì‚¬íšŒì  ì°¨ì› ì§€ì‹ ì²´ê³„ì˜ íŒŒê´´: ê³¼í•™ê³¼ í•™ë¬¸ì— ëŒ€í•œ íƒ„ì••ì˜ ê²°ê³¼ ë„ë•ì  í˜¼ë€: ì„ ì•…ì˜ ê¸°ì¤€ì´ ë¬´ë„ˆì§„ ì‚¬íšŒì˜ ëª¨ìŠµ ë¬¸ëª…ì‚¬ì  ì°¨ì› ì§„ë³´ì— ëŒ€í•œ íšŒì˜: ì¸ë¥˜ ë¬¸ëª…ì˜ ë°œì „ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì˜ë¬¸ ì™¸ë¶€ êµ¬ì›ì— ëŒ€í•œ í™˜ìƒ: ë‚´ë¶€ ê°œí˜ì˜ í•œê³„ì™€ ì™¸ë¶€ ê°œì… ê¸°ëŒ€ ì‚¼ì²´ëŠ” ë‹¨ìˆœí•œ SF ì†Œì„¤ì„ ë„˜ì–´ì„œ, ë¬¸í™”ëŒ€í˜ëª…ì´ë¼ëŠ” ì—­ì‚¬ì  ë¹„ê·¹ì´ ì–´ë–»ê²Œ ê°œì¸ê³¼ ë¬¸ëª… ì „ì²´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ìš°í™”ì  ì‘í’ˆì´ë‹¤. ì˜ˆì›ì œì˜ ì„ íƒì€ ê°œì¸ì  íŠ¸ë¼ìš°ë§ˆê°€ ì–´ë–»ê²Œ ì¸ë¥˜ ì „ì²´ì˜ ìš´ëª…ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê·¹ì ìœ¼ë¡œ ë³´ì—¬ì£¼ë©°, ë¬¸í™”ëŒ€í˜ëª…ì˜ ìƒì²˜ê°€ ì—¬ì „íˆ ì¤‘êµ­ ì‚¬íšŒì™€ ë¬¸í™”ì— ê¹Šì´ ê°ì¸ë˜ì–´ ìˆìŒì„ ì‹œì‚¬í•œë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-ate",
    "title": "What is ATE?",
    "date": "2025-07-17",
    "excerpt": "What is ATE?",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "ATEë€? --- Average Treatment Effect (ATE)ëŠ” ëª¨ë“  ì‚¬ëŒë“¤ì´ ë°›ì€ ì²˜ì¹˜ì˜ í‰ê·  íš¨ê³¼ë¥¼ ë§í•œë‹¤. ì¦‰, ì²˜ì¹˜ë°›ì€ ê²°ê³¼ì™€ ë°›ì§€ ì•Šì€ ê²°ê³¼ì˜ ì°¨ì´ì˜ í‰ê· ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤. ATE ì¶”ì • --- $$ \\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) \\tag{1} $$ ì •ì˜ì— ë”°ë¼ ATEëŠ” (1)ì²˜ëŸ¼ ì •ì˜ëœë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ëª¨ë“  ì‚¬ëŒë“¤ì´ ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šê¸° ë•Œë¬¸ì— (1)ì„ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/double-robustness-explained",
    "title": "Double Robustness: ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì™„ì „ ì •ë³µ",
    "date": "2025-07-17",
    "excerpt": "Double Robustnessì˜ ì´ë¡ ì  ê¸°ë°˜ë¶€í„° ì‹¤ìš©ì  ì‘ìš©ê¹Œì§€ ì™„ì „ í•´ë¶€",
    "category": "Causal Inference",
    "tags": [
      "Double Robustness",
      "Causal Inference",
      "Theory",
      "R-learner"
    ],
    "content": "Double Robustness: ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì™„ì „ ì •ë³µ Double Robustness(ì´ì¤‘ ê°•ê±´ì„±)ëŠ” í˜„ëŒ€ ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì¤‘ í•˜ë‚˜ë¡œ, R-learnerì™€ ê°™ì€ ìµœì‹  ë°©ë²•ë¡ ì˜ ì´ë¡ ì  ê¸°ë°˜ì´ ë©ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Double Robustnessì˜ ëª¨ë“  ì¸¡ë©´ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. Double Robustnessë€ ë¬´ì—‡ì¸ê°€? 1.1 ì§ê´€ì  ì´í•´ Double RobustnessëŠ” \"ë‘ ê°€ì§€ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ í¸í–¥ ì—†ëŠ” ì¶”ì •ì´ ê°€ëŠ¥í•œ ì„±ì§ˆ\"ì…ë‹ˆë‹¤. ê°„ë‹¨í•œ ë¹„ìœ  ìë¬¼ì‡  ë‘ ê°œ: ë¬¸ì„ ì—´ë ¤ë©´ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì—´ë©´ ë¨ ë°±ì—… ì‹œìŠ¤í…œ: ì£¼ ì‹œìŠ¤í…œì´ ì‹¤íŒ¨í•´ë„ ë³´ì¡° ì‹œìŠ¤í…œì´ ì‘ë™ ì´ì¤‘ ë³´í—˜: í•˜ë‚˜ê°€ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ í•˜ë‚˜ê°€ ë³´ì¥ 1.2 ì¸ê³¼ì¶”ë¡ ì—ì„œì˜ ì˜ë¯¸ ì¸ê³¼ì¶”ë¡ ì—ì„œ ìš°ë¦¬ê°€ ì¶”ì •í•´ì•¼ í•˜ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ìš”ì†Œ: Outcome Model (ê²°ê³¼ ëª¨ë¸): $\\mu(x, w) = \\mathbb{E}[Y X = x, W = w]$ Propensity Score Model (ì²˜ì¹˜ í™•ë¥  ëª¨ë¸): $e(x) = \\text{Pr}(W = 1 X = x)$ Double Robustness: ì´ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ ì²˜ì¹˜íš¨ê³¼ë¥¼ í¸í–¥ ì—†ì´ ì¶”ì •í•  ìˆ˜ ìˆìŒ! ìˆ˜í•™ì  ì´ë¡ : í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 2.1 Potential Outcomes Framework ê¸°ë³¸ ì„¤ì •: ì ì¬ê²°ê³¼: $Y_i(0), Y_i(1)$ (ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œ/ë°›ì•˜ì„ ë•Œì˜ ê²°ê³¼) ê´€ì°°ê²°ê³¼: $Y_i = Y_i(W_i)$ ê°œë³„ ì²˜ì¹˜íš¨ê³¼: $\\tau_i = Y_i(1) - Y_i(0)$ í‰ê·  ì²˜ì¹˜íš¨ê³¼: $\\tau = \\mathbb{E}[Y(1) - Y(0)]$ ê·¼ë³¸ì  ë¬¸ì œ: ê°™ì€ ê°œì¸ì— ëŒ€í•´ $Y_i(0)$ê³¼ $Y_i(1)$ì„ ë™ì‹œì— ê´€ì°°í•  ìˆ˜ ì—†ìŒ! 2.2 Unconfoundedness Assumption $$\\{Y(0), Y(1)\\} \\perp \\!\\!\\! \\perp W \\mid X$$ ì˜ë¯¸: ê³µë³€ëŸ‰ $X$ë¥¼ í†µì œí•˜ë©´ ì²˜ì¹˜ í• ë‹¹ì´ ë¬´ì‘ìœ„ì™€ ê°™ìŒ ì´ ê°€ì • í•˜ì—ì„œ ë‹¤ìŒì´ ì„±ë¦½: $$\\mathbb{E}[Y(w) X] = \\mathbb{E}[Y X, W = w] = \\mu(X, w)$$ 2.3 Double Robustnessì˜ ìˆ˜í•™ì  í‘œí˜„ í•µì‹¬ í•­ë“±ì‹ Average Treatment Effect (ATE)ì— ëŒ€í•œ doubly robust ì¶”ì •ëŸ‰: $$\\hat{\\tau}_{DR} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{W_i Y_i}{\\hat{e}(X_i)} - \\frac{(1-W_i) Y_i}{1-\\hat{e}(X_i)} + \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1-\\hat{e}(X_i))} \\left\\{ \\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0) \\right\\} \\right]$$ ë” ê°„ë‹¨í•œ í˜•íƒœ AIPW (Augmented Inverse Propensity Weighting) ì¶”ì •ëŸ‰: $$\\hat{\\tau}_{AIPW} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0) + \\frac{W_i}{\\hat{e}(X_i)} \\{Y_i - \\hat{\\mu}(X_i, 1)\\} - \\frac{1-W_i}{1-\\hat{e}(X_i)} \\{Y_i - \\hat{\\mu}(X_i, 0)\\} \\right]$$ 2.4 ì™œ \"Double Robust\"ì¸ê°€? í•µì‹¬ ì •ë¦¬ Theorem: ë‹¤ìŒ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë§Œ ë§Œì¡±í•˜ë©´ $\\mathbb{E}[\\hat{\\tau}_{AIPW}] = \\tau$: Outcome modelì´ ì •í™•: $\\hat{\\mu}(x, w) = \\mu^(x, w)$ Propensity modelì´ ì •í™•: $\\hat{e}(x) = e^(x)$ ì¦ëª…ì˜ í•µì‹¬ ì•„ì´ë””ì–´ AIPW ì¶”ì •ëŸ‰ì„ ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$\\hat{\\tau}_{AIPW} = \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\{\\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0)\\}}_{\\text{Outcome-based}} + \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{W_i}{\\hat{e}(X_i)} - \\frac{1-W_i}{1-\\hat{e}(X_i)}\\right] \\{Y_i - \\hat{\\mu}(X_i, W_i)\\}}_{\\text{IPW correction}}$$ ì¼€ì´ìŠ¤ 1: Outcome modelì´ ì •í™•í•œ ê²½ìš° ì²« ë²ˆì§¸ í•­ì´ ì •í™•í•œ $\\tau$ ì œê³µ ë‘ ë²ˆì§¸ í•­ì˜ ê¸°ëŒ“ê°’ì´ 0 (ì”ì°¨ì˜ ê°€ì¤‘í‰ê· ) ì¼€ì´ìŠ¤ 2: Propensity modelì´ ì •í™•í•œ ê²½ìš° ë‘ ë²ˆì§¸ í•­ì´ IPW ì¶”ì •ëŸ‰ìœ¼ë¡œ ì •í™•í•œ $\\tau$ ì œê³µ ì²« ë²ˆì§¸ í•­ì˜ ì˜¤ì°¨ê°€ ë‘ ë²ˆì§¸ í•­ì— ì˜í•´ ìƒì‡„ ì§ê´€ì  ì´í•´: ì™œ ì‘ë™í•˜ëŠ”ê°€? 3.1 Visual Intuition `` ì§„ì‹¤í•œ ì²˜ì¹˜íš¨ê³¼: Ï„ = 5 ì‹œë‚˜ë¦¬ì˜¤ 1: Outcome Model ì •í™•, Propensity Model ë¶€ì •í™• â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Outcome Model â”‚ â”‚ Propensity Modelâ”‚ â”‚ âœ… Ï„Ì‚ = 5.0 â”‚ + â”‚ âŒ í¸í–¥ ìˆìŒ â”‚ = Ï„Ì‚ = 5.0 âœ… â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ì‹œë‚˜ë¦¬ì˜¤ 2: Outcome Model ë¶€ì •í™•, Propensity Model ì •í™• â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Outcome Model â”‚ â”‚ Propensity Modelâ”‚ â”‚ âŒ í¸í–¥ ìˆìŒ â”‚ + â”‚ âœ… ì •í™•í•¨ â”‚ = Ï„Ì‚ = 5.0 âœ… â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ì‹œë‚˜ë¦¬ì˜¤ 3: ë‘˜ ë‹¤ ì •í™• â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Outcome Model â”‚ â”‚ Propensity Modelâ”‚ â”‚ âœ… Ï„Ì‚ = 5.0 â”‚ + â”‚ âœ… ì •í™•í•¨ â”‚ = Ï„Ì‚ = 5.0 âœ… â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ` 3.2 ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ (Compensation Mechanism) Outcome Modelì´ ì •í™•í•œ ê²½ìš° `python Outcome modelì´ ì™„ë²½í•˜ë©´ mu_hat(x, 1) - mu_hat(x, 0) â‰ˆ Ï„ IPW correction termì€ E[W/e_hat(X) - (1-W)/(1-e_hat(X))] {Y - mu_hat(X,W)} â‰ˆ 0 (ì”ì°¨ì˜ ê°€ì¤‘í‰ê· ì€ 0ì— ê°€ê¹Œì›€) ` Propensity Modelì´ ì •í™•í•œ ê²½ìš° `python Propensity modelì´ ì™„ë²½í•˜ë©´ IPWê°€ ì •í™•í•œ ì¶”ì • ì œê³µ IPW_term = E[WY/e(X) - (1-W)Y/(1-e(X))] = Ï„ Outcome modelì˜ ì˜¤ì°¨ëŠ” IPW correctionì— ì˜í•´ ìƒì‡„ë¨ ` 3.3 ì‹¤ì œ ë°ì´í„° ì˜ˆì‹œ ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • `python n = 1000 X = np.random.normal(0, 1, (n, 2)) e_true = expit(X[:, 0]) # true propensity W = np.random.binomial(1, e_true) True outcome model: complex nonlinear Y_0 = X[:, 0]*2 + X[:, 1] + noise Y_1 = Y_0 + 2 + X[:, 0] # treatment effect = 2 + X[:, 0] Y = W Y_1 + (1 - W) Y_0 true_ATE = 2 + np.mean(X[:, 0]) # â‰ˆ 2.0 ` ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²°ê³¼ `python Scenario 1: Good outcome model, bad propensity model mu_hat_good = fit_complex_model(Y, X, W) # RÂ² = 0.95 e_hat_bad = 0.5 # constant (wrong!) AIPW_1 = compute_AIPW(Y, W, X, mu_hat_good, e_hat_bad) Result: 2.03 (ê±°ì˜ ì •í™•! âœ…) Scenario 2: Bad outcome model, good propensity model mu_hat_bad = fit_linear_model(Y, X, W) # RÂ² = 0.3 (underfit) e_hat_good = fit_logistic_model(W, X) # very accurate AIPW_2 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_good) Result: 1.97 (ì—¬ì „íˆ ì •í™•! âœ…) Scenario 3: Both models bad AIPW_3 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_bad) Result: 1.2 (í¸í–¥ë¨ âŒ) ` Double Robustnessì˜ ì¥ì ê³¼ í•œê³„ 4.1 í•µì‹¬ ì¥ì  ê²¬ê³ ì„± (Robustness) ëª¨ë¸ ì˜¤ì§€ì •ì— ëŒ€í•œ ë³´í—˜: í•˜ë‚˜ê°€ í‹€ë ¤ë„ ì•ˆì „ ì‹¤ë¬´ì  ì•ˆì •ì„±: ì™„ë²½í•œ ëª¨ë¸ë§ì´ ì–´ë ¤ìš´ í˜„ì‹¤ì—ì„œ ìœ ìš© íš¨ìœ¨ì„± (Efficiency) ë‘˜ ë‹¤ ì •í™•í•˜ë©´ ìµœê³  íš¨ìœ¨ì„±: ìµœì†Œ ë¶„ì‚° ë‹¬ì„± Semiparametric efficiency bound ë‹¬ì„± ìœ ì—°ì„± (Flexibility) ì„œë¡œ ë‹¤ë¥¸ ë°©ë²• ì¡°í•©: ê° ëª¨ë¸ì— ìµœì í™”ëœ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ ê¸°ê³„í•™ìŠµ ë°©ë²• í™œìš©: ë³µì¡í•œ ëª¨ë¸ë„ ì‚¬ìš© ê°€ëŠ¥ 4.2 í•œê³„ì  ë‘˜ ë‹¤ í‹€ë¦¬ë©´ í¸í–¥ `python ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ if outcome_model_wrong and propensity_model_wrong: bias = f(error_outcome, error_propensity) # í¸í–¥ ë°œìƒ ` ë¶„ì‚° ì¦ê°€ ê°€ëŠ¥ì„± IPW termì˜ ê·¹ë‹¨ê°’: propensity scoreê°€ 0 ë˜ëŠ” 1ì— ê°€ê¹Œìš°ë©´ ë¶„ì‚° ê¸‰ì¦ Overlap ì¡°ê±´ í•„ìš”: $0 < e(x) < 1$ for all $x$ ì¶”ì • ë³µì¡ì„± ë‘ ëª¨ë¸ ëª¨ë‘ ì¶”ì •: ê³„ì‚° ë¶€ë‹´ ì¦ê°€ êµì°¨ê²€ì¦ ë³µì¡ì„±: ê° ëª¨ë¸ì˜ íŠœë‹ì´ ë³µì¡ R-learnerì™€ì˜ ì—°ê²° 5.1 Robinson's Transformationê³¼ Double Robustness R-learnerëŠ” Robinson's transformationì„ ì‚¬ìš©í•©ë‹ˆë‹¤: $$Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i$$ ì´ë¥¼ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´: $$Y_i = m^(X_i) + \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i$$ Double Robust Structure R-loss í•¨ìˆ˜: $$\\hat{L}_n\\{\\tau(\\cdot)\\} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\left\\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\right\\} - \\left\\{W_i - \\hat{e}^{(-q(i))}(X_i)\\right\\} \\tau(X_i) \\right]^2$$ í•µì‹¬: ì´ ì†ì‹¤í•¨ìˆ˜ë„ double robust ì„±ì§ˆì„ ê°€ì§‘ë‹ˆë‹¤! $\\hat{m}$ì´ ì •í™•í•˜ë©´ ì²« ë²ˆì§¸ í•­ì´ ì •í™•í•œ ì”ì°¨ ì œê³µ $\\hat{e}$ê°€ ì •í™•í•˜ë©´ ë‘ ë²ˆì§¸ í•­ì´ ì •í™•í•œ ê°€ì¤‘ì¹˜ ì œê³µ 5.2 ì™œ R-learnerê°€ Quasi-Oracleì¸ê°€? `python Oracle knows m(x) and e(x) oracle_loss = E[(Y - m(X) - {W - e(X)}Ï„(X))Â²] R-learner uses estimates r_loss = E[(Y - mÌ‚(X) - {W - Ãª(X)}Ï„Ì‚(X))Â²] Double robustness ensures: if rate(mÌ‚ - m) rate(Ãª - e) â†’ 0 faster than oracle_rate: rate(R-learner) â‰ˆ oracle_rate # ğŸ¯ ` ì‹¤ìš©ì  êµ¬í˜„ ê°€ì´ë“œ 6.1 Python Implementation `python import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict def doubly_robust_ate(Y, W, X, outcome_model=None, propensity_model=None): \"\"\" Doubly Robust ATE ì¶”ì • Parameters: ----------- Y : array-like, shape (n,) Outcome variable W : array-like, shape (n,) Treatment indicator (0 or 1) X : array-like, shape (n, p) Covariates outcome_model : sklearn estimator Outcome regression model propensity_model : sklearn estimator Propensity score model Returns: -------- ate_estimate : float Doubly robust ATE estimate \"\"\" n = len(Y) # Default models if outcome_model is None: outcome_model = RandomForestRegressor(n_estimators=100, random_state=42) if propensity_model is None: propensity_model = LogisticRegression(random_state=42) # Cross-fitting for outcome model mu_1_hat = np.zeros(n) mu_0_hat = np.zeros(n) # Estimate E[Y X,W=1] idx_1 = W == 1 if np.sum(idx_1) > 0: mu_1_hat[idx_1] = cross_val_predict( outcome_model, X[idx_1], Y[idx_1], cv=5 ) outcome_model.fit(X[idx_1], Y[idx_1]) mu_1_hat[~idx_1] = outcome_model.predict(X[~idx_1]) # Estimate E[Y X,W=0] idx_0 = W == 0 if np.sum(idx_0) > 0: mu_0_hat[idx_0] = cross_val_predict( outcome_model, X[idx_0], Y[idx_0], cv=5 ) outcome_model.fit(X[idx_0], Y[idx_0]) mu_0_hat[~idx_0] = outcome_model.predict(X[~idx_0]) # Cross-fitting for propensity score e_hat = cross_val_predict( propensity_model, X, W, cv=5, method='predict_proba' )[:, 1] # Clip propensity scores to avoid extreme weights e_hat = np.clip(e_hat, 0.01, 0.99) # AIPW estimator aipw_components = ( (mu_1_hat - mu_0_hat) + # Outcome-based estimate W (Y - mu_1_hat) / e_hat - # IPW correction for treated (1 - W) (Y - mu_0_hat) / (1 - e_hat) # IPW correction for control ) ate_estimate = np.mean(aipw_components) # Standard error (using influence function) influence_func = aipw_components - ate_estimate se_estimate = np.std(influence_func) / np.sqrt(n) return { 'ate': ate_estimate, 'se': se_estimate, 'ci_lower': ate_estimate - 1.96 se_estimate, 'ci_upper': ate_estimate + 1.96 se_estimate } Usage example def simulate_data(n=1000): \"\"\"Generate simulation data\"\"\" X = np.random.normal(0, 1, (n, 3)) # True propensity score (logistic) e_true = 1 / (1 + np.exp(-(X[:, 0] + 0.5 X[:, 1]))) W = np.random.binomial(1, e_true) # True outcome model (nonlinear) Y_0 = X[:, 0]*2 + X[:, 1] + 0.5 X[:, 2] + np.random.normal(0, 0.5, n) Y_1 = Y_0 + 2 + X[:, 0] # Heterogeneous treatment effect Y = W Y_1 + (1 - W) Y_0 return Y, W, X, 2.0 # True ATE â‰ˆ 2.0 Run example Y, W, X, true_ate = simulate_data() result = doubly_robust_ate(Y, W, X) print(f\"True ATE: {true_ate:.3f}\") print(f\"DR Estimate: {result['ate']:.3f}\") print(f\"95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\") ` 6.2 Best Practices ëª¨ë¸ ì„ íƒ ê°€ì´ë“œë¼ì¸ `python Outcome Model High flexibility for complex relationships outcome_models = { 'linear': LinearRegression(), 'rf': RandomForestRegressor(n_estimators=200), 'xgb': XGBRegressor(), 'nn': MLPRegressor(hidden_layer_sizes=(100, 50)) } Propensity Model Focus on overlap and calibration propensity_models = { 'logistic': LogisticRegression(C=1.0), 'rf': RandomForestClassifier(n_estimators=200), 'calibrated': CalibratedClassifierCV(LogisticRegression()) } ` ì§„ë‹¨ ë„êµ¬ `python def diagnose_overlap(e_hat, W): \"\"\"Check overlap assumption\"\"\" print(f\"Propensity score range: [{e_hat.min():.3f}, {e_hat.max():.3f}]\") print(f\"Extreme scores (< 0.1 or > 0.9): {np.mean((e_hat < 0.1) (e_hat > 0.9)):.1%}\") # Plot distributions import matplotlib.pyplot as plt plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.hist(e_hat[W==0], alpha=0.7, label='Control', bins=30) plt.hist(e_hat[W==1], alpha=0.7, label='Treated', bins=30) plt.xlabel('Propensity Score') plt.ylabel('Frequency') plt.legend() plt.title('Overlap Check') plt.subplot(1, 2, 2) plt.boxplot([e_hat[W==0], e_hat[W==1]], labels=['Control', 'Treated']) plt.ylabel('Propensity Score') plt.title('Distribution by Treatment') plt.tight_layout() plt.show() def check_balance(X, W, e_hat): \"\"\"Check covariate balance after weighting\"\"\" weights_1 = W / e_hat weights_0 = (1 - W) / (1 - e_hat) for j in range(X.shape[1]): mean_1 = np.average(X[W==1, j], weights=weights_1[W==1]) mean_0 = np.average(X[W==0, j], weights=weights_0[W==0]) print(f\"Variable {j}: Weighted difference = {mean_1 - mean_0:.4f}\") ` ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í™•ì¥ 7.1 Machine Learningê³¼ì˜ ê²°í•© Targeted Maximum Likelihood Estimation (TMLE) One-step correction: ì´ˆê¸° ì¶”ì •ê°’ì„ ì—…ë°ì´íŠ¸ Cross-validation: ìµœì  ëª¨ë¸ ì„ íƒ Causal Random Forests Honest splitting: í¸í–¥ ì—†ëŠ” ì¶”ì • Local centering: Double robustness ë³´ì¥ Neural Network Approaches Representation learning: ê³µí†µ íŠ¹ì„± í•™ìŠµ Domain adaptation: Treatment/control domain ì •ë ¬ 7.2 ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ í™•ì¥ Debiased Machine Learning (DML) `python Neyman orthogonality + Cross-fitting def debiased_ml_ate(Y, W, X): # Step 1: Estimate nuisance functions theta_0 = estimate_initial(Y, W, X) # Step 2: Compute orthogonal score psi = compute_orthogonal_score(Y, W, X, theta_0) # Step 3: Solve orthogonal equation theta_final = solve_orthogonal_equation(psi) return theta_final ` 7.3 ì—°ì† ì²˜ì¹˜ì™€ ë‹¤ì¤‘ ì²˜ì¹˜ Continuous Treatment `python GPS (Generalized Propensity Score) + Outcome Model def continuous_dr(Y, T, X): # Estimate treatment density: f(T X) gps_hat = estimate_gps(T, X) # Estimate dose-response: E[Y T,X] outcome_hat = estimate_outcome(Y, T, X) # Double robust estimand dr_estimate = compute_dr_continuous(Y, T, X, gps_hat, outcome_hat) return dr_estimate `` ê²°ë¡ : Double Robustnessì˜ ì˜ì˜ 8.1 ì´ë¡ ì  ê¸°ì—¬ í¸í–¥ ì—†ëŠ” ì¶”ì •ì˜ ë³´ì¥: ëª¨ë¸ ì˜¤ì§€ì •ì—ë„ robust íš¨ìœ¨ì„±: ìµœì  ì¡°ê±´ì—ì„œ semiparametric efficiency bound ë‹¬ì„± ìœ ì—°ì„±: ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ê³¼ ê²°í•© ê°€ëŠ¥ 8.2 ì‹¤ìš©ì  ê°€ì¹˜ í˜„ì‹¤ì  í•´ê²°ì±…: ì™„ë²½í•œ ëª¨ë¸ë§ì´ ì–´ë ¤ìš´ í˜„ì‹¤ì—ì„œ ì•ˆì „ë§ ì œê³µ ë°©ë²•ë¡  ë°œì „ì˜ ê¸°ë°˜: R-learner, TMLE, DML ë“±ì˜ ì´ë¡ ì  í† ëŒ€ ì‚°ì—… ì‘ìš©: A/B í…ŒìŠ¤íŠ¸, ê°œì¸í™”, ì •ì±… í‰ê°€ ë“±ì—ì„œ ë„ë¦¬ í™œìš© 8.3 ë¯¸ë˜ ì „ë§ Double RobustnessëŠ” ì•ìœ¼ë¡œë„ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡  ë°œì „ì˜ í•µì‹¬ ì›ë¦¬ë¡œ ì‘ìš©í•  ê²ƒì…ë‹ˆë‹¤: ë”¥ëŸ¬ë‹ê³¼ì˜ ê²°í•©: í‘œí˜„ í•™ìŠµ ê¸°ë°˜ double robust ë°©ë²• ì‹¤ì‹œê°„ ì¶”ë¡ : ì˜¨ë¼ì¸ í•™ìŠµì—ì„œì˜ double robustness ë³µì¡í•œ ì²˜ì¹˜ êµ¬ì¡°: ë„¤íŠ¸ì›Œí¬, ì‹œê³„ì—´ì—ì„œì˜ í™•ì¥ Double Robustnessë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ í˜„ëŒ€ ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ ë” robustí•˜ê³  íš¨ìœ¨ì ì¸ ì¸ê³¼ ì¶”ë¡  ë°©ë²•ë“¤ì´ ê³„ì† ë°œì „í•  ê²ƒì…ë‹ˆë‹¤. ğŸ¯ --- ì°¸ê³ ë¬¸í—Œ: Robins, J.M., Rotnitzky, A., & Zhao, L.P. (1994). Estimation of regression coefficients when some regressors are not always observed. Bang, H. & Robins, J.M. (2005). Doubly robust estimation in missing data and causal inference models. Chernozhukov, V., et al. (2018). Double/debiased machine learning for treatment and structural parameters. Kennedy, E.H. (2020). Towards optimal doubly robust estimation of heterogeneous causal effects.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects",
    "title": "[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects",
    "date": "2025-07-17",
    "excerpt": "R-leaner ë°©ë²• ì†Œê°œì™€ ì´ê²Œ ê°€ì§€ëŠ” quasi-oracle propertyì— ëŒ€í•´ ì„¤ëª…",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link ë…¼ë¬¸ ë¦¬ë·° Abstract > Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì •í•˜ëŠ” ê²ƒì€ ë§ì€ ë¶„ì•¼ì—ì„œ í†µê³„ì  ë¬¸ì œì˜ í•µì‹¬ì…ë‹ˆë‹¤. ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì•Œê³  ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì•½ì²˜ë°©ì´ë‚˜ êµìœ¡ì •ì±… ê²°ì •, ìì› ë¶„ë°° ë“± ë§ì€ ë¬¸ì œì—ì„œ ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¥¼ ì¶”ì •í•˜ëŠ” ì¼ì€ ê½¤ë‚˜ ë³µì¡í•œ ì¼ì´ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” marginal effectsì™€ treatment propensityë¼ëŠ” ë‘ ê°€ì§€ nuisance componentë¥¼ ì¶”ì •í•˜ì—¬ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. > we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. ì´ëŸ¬í•œ ë°©ë²•ì€ quasi-oracle propertyë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤. <small> quasi-oracle property : marginal effectsì™€ treatment propensityì˜ ì¶”ì •ì´ ì •í™•í•˜ì§€ ì•Šë”ë¼ë„, ë§ˆì¹˜ ì´ ë‘ nuisance componentë¥¼ ë¯¸ë¦¬ ì•Œê³  ìˆëŠ” ê²ƒê³¼ ê°™ì€ ì˜¤ì°¨ ê²½ê³„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ì„±ì§ˆ</small> Introduction ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì : ë°©ë²•ë¡ ì  ë¶ˆì¼ì¹˜: ê´€ì°° ì—°êµ¬ì—ì„œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì— ì–´ë–»ê²Œ ì ìš©í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ë‹µì´ ì•„ì§ í™•ë¦½ë˜ì§€ ì•ŠìŒ ê°œë°œ ê³¼ì •ì˜ ë³µì¡ì„±: ì¸ê³¼ê´€ê³„ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ê°œë°œí•˜ëŠ” ê³¼ì •ì´ ë…¸ë™ì§‘ì•½ì ì´ë©°, ì „ë¬¸ ì—°êµ¬ìë“¤ì˜ ì°¸ì—¬ê°€ í•„ìˆ˜ì  ì´ë¡ ì  ê·¼ê±° ë¶€ì¡±: ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ë“¤ì´ ìˆ˜ì¹˜ì  ì‹¤í—˜ìœ¼ë¡œë§Œ ê²€ì¦ë˜ê³ , í˜•ì‹ì ì¸ ìˆ˜ë ´ ë³´ì¥ì´ë‚˜ ì˜¤ì°¨ ê²½ ì´ ë…¼ë¬¸ì˜ ìƒˆë¡œìš´ ì ‘ê·¼ë²•: ìë™í™”ëœ í”„ë ˆì„ì›Œí¬: ì„ì˜ì˜ ì†ì‹¤ ìµœì†Œí™” ì ˆì°¨ë¥¼ í†µí•´ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë¥¼ ì™„ì „ ìë™ìœ¼ë¡œ ëª…ì„¸í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ ì œê³µ Oracle ìˆ˜ì¤€ì˜ ì„±ëŠ¥: ë°ì´í„° ìƒì„± ë¶„í¬ì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ì•Œê³  ìˆëŠ” oracle ë°©ë²•ê³¼ ë¹„êµ ê°€ëŠ¥í•œ ì˜¤ì°¨ ê²½ê³„ ë‹¬ì„± ì´ë¡ ì  ê¸°ë°˜: ì´ì¤‘ ê°•ê±´ ì¶”ì •(double robust estimation), oracle ë¶€ë“±ì‹, êµì°¨ ê²€ì¦ì„ ê²°í•©í•˜ì—¬ ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ë„êµ¬ë¡œ ì›ë¦¬ì  í†µê³„ ì¶”ì •ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ê°œë°œ ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ ì‹¤ìš©ì  í•œê³„ì™€ ì´ë¡ ì  ë¶€ì¡±í•¨ì„ ëª¨ë‘ í•´ê²°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. A Loss Function for Treatment Effect Estimation > We formalize our problem in terms of the potential outcomes framework (Neyman, 1923; Rubin, 1974). potential outcomes frameworkë¥¼ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë°ì´í„° êµ¬ì¡°: ê´€ì°° ë°ì´í„°: $(X_i, Y_i, W_i)$ for $i = 1, ..., n$ $X_i \\in \\mathcal{X}$: ê°œì¸ë³„ íŠ¹ì„± (features) $Y_i \\in \\mathbb{R}$: ê´€ì°°ëœ ê²°ê³¼ (observed outcome) $W_i \\in \\{0, 1\\}$: ì²˜ì¹˜ í• ë‹¹ (treatment assignment) ì ì¬ ê²°ê³¼ (Potential Outcomes): ì ì¬ ê²°ê³¼: $\\{Y_i(0), Y_i(1)\\}$ $Y_i(0)$: ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²°ê³¼ $Y_i(1)$: ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ê²°ê³¼ ê´€ì°°ëœ ê²°ê³¼ì™€ì˜ ê´€ê³„: $Y_i = Y_i(W_i)$ ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜íš¨ê³¼ (CATE): ëª©í‘œ í•¨ìˆ˜: $\\tau^(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]$ íŠ¹ì„± $X = x$ì¸ ê°œì¸ë“¤ì˜ í‰ê·  ì²˜ì¹˜íš¨ê³¼ ì‹ë³„ ì¡°ê±´: Unconfoundedness: ì²˜ì¹˜ í• ë‹¹ì´ ê´€ì°°ë˜ì§€ ì•Šì€ confounding variableì— ì˜í•´ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê°€ì • ì´ ìˆ˜ì‹ë“¤ì€ ì¸ê³¼ì¶”ë¡ ì˜ í‘œì¤€ì ì¸ ì ì¬ê²°ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼ ë”°ë¥´ë©°, ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•œ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. > In order to identify $\\tau^(x)$, we assume unconfoundedness, i.e., the treatment assignment is randomized once we control for the features $X_i$ (Rosenbaum and Rubin, 1983). CATEë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ë¬´ì‘ìœ„ í• ë‹¹ì²˜ëŸ¼ unconfoundedness ê°€ì •ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ Xë¥¼ í†µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Assumption 1 : The treatment assignment $W_i$ is unconfounded, $\\{Y_i(0), Y_i(1)\\} \\perp \\!\\!\\! \\perp W_i \\mid X_i$ Treatment Propensity : $e^(x) = \\Pr(W = 1 \\mid X = x)$ Conditional Response Surfaces : $\\mu^{(w)}(x) = \\mathbb{E}[Y(w) \\mid X = x]$ for $w \\in \\{0, 1\\}$ Error Term : $\\varepsilon_i(w) := Y_i(w) - \\{\\mu^{(0)}(X_i) + w\\tau^(X_i)\\}$ ì„±ì§ˆ : unconfoundedness í•˜ì—ì„œ $\\mathbb{E}[\\varepsilon_i(W_i) \\mid X_i, W_i] = 0$ Conditional Mean Outcome : $m^(x) = \\mathbb{E}[Y \\mid X = x] = \\mu^{(0)}(x) + e^(x)\\tau^(x)$ $$ Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i \\tag{1} $$ (1)ì—ì„œ *propensity score $e^(X_i)$ë¥¼ í†µí•´ Xë¥¼ í†µì œí•©ë‹ˆë‹¤. $W_i - e^(X_i)$: ì‹¤ì œ ì²˜ì¹˜ í• ë‹¹ì—ì„œ ì˜ˆì¸¡ëœ ì²˜ì¹˜ í™•ë¥ ì„ ëº€ ê°’ ì´ëŠ” Xë¥¼ í†µì œí•œ í›„ì˜ \"ì²˜ì¹˜ í• ë‹¹ì˜ í¸ì°¨\"ë¥¼ ë‚˜íƒ€ëƒ„ ë§ˆì¹˜ Xê°€ ê°™ì€ ê·¸ë£¹ ë‚´ì—ì„œ ë¬´ì‘ìœ„ í• ë‹¹ëœ ê²ƒì²˜ëŸ¼ ë§Œë“¦ ë”°ë¼ì„œ ì´ ë“±ì‹ì€ propensity scoreë¥¼ í†µí•œ X í†µì œë¥¼ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. > The goal of this paper is to study how we can use the Robinsonâ€™s transfomation (1) for flexible treatment effect estimation that builds on modern machine learning approaches such as boosting or deep learning. ì´ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ë¶€ìŠ¤íŒ…ì´ë‚˜ ë”¥ëŸ¬ë‹ê³¼ ê°™ì€ í˜„ëŒ€ì ì¸ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìœ ì—°í•œ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•´ Robinson's transfomation (1)ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì—°êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. > Our main result is that we can use this representation to construct a loss function that captures heterogeneous treatment effects, and that we can then accurately estimate treatment effectsâ€”both in terms of empirical performance and asymptotic guaranteesâ€”by finding regularized minimizers of this loss function. ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê²°ê³¼ëŠ” ì´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ í¬ì°©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ì†ì‹¤ í•¨ìˆ˜ì˜ ì •ê·œí™”ëœ ìµœì†Œí™” í•´ë¥¼ ì°¾ì•„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. $$ \\tau^(\\cdot) = \\arg\\min_{\\tau} \\mathbb{E}\\left[\\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\} \\tau(X_i)\\right]^2 \\tag{2} $$ $$ \\tilde{\\tau}(\\cdot) = \\arg\\min_{\\tau} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( \\{ Y_i - m^(X_i) \\} - \\{ W_i - e^(X_i) \\} \\tau(X_i) \\right)^2 + \\Lambda_n[\\tau(\\cdot)] \\right\\} \\tag{3} $$ where the term $\\Lambda_n[\\tau(\\cdot)]$ is interpreted as a regularizer on the complexity of the $\\tau(\\cdot)$ function > This regularization could be explicit as in penalized regression, or implicit, e.g., as provided by a carefully designed deep neural network. ì •ê·œí™”í•­ì€ ëª¨ë¸ì˜ ì„¤ê³„ì— ë§ì¶° ì ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. > The difficulty, however, is that in practice we never know the weighted main effect function $m^(x)$ and usually donâ€™t know the treatment propensities $e^(x)$ either, and so the estimator (3) is not feasible. í•˜ì§€ë§Œ ì–´ë ¤ìš´ ì ì€ ë‹¹ì—°í•˜ê²Œë„ $m^(x)$ì™€ $e^(x)$ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. > Given these preliminaries, we here study the following class of two-step estimators using cross-fitting (Chernozhukov et al., 2018; Schick, 1986) motivated by the above oracle procedure: Cross-fittingì„ ì´ìš©í•œ 2ë‹¨ê³„ ì¶”ì • ë°©ë²•: Step 1: ë°ì´í„° ë¶„í•  ë° Nuisance Component ì¶”ì • ë°ì´í„°ë¥¼ Qê°œ(ë³´í†µ 5 ë˜ëŠ” 10)ì˜ ê· ë“±í•œ í¬ê¸°ì˜ foldë¡œ ë¶„í•  $q(\\cdot)$: $i = 1, \\ldots, n$ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¥¼ Qê°œì˜ foldì— ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ Cross-fittingì„ í†µí•´ $\\hat{m}$ê³¼ $\\hat{e}$ë¥¼ ìµœì  ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìœ„í•´ ì¡°ì •ëœ ë°©ë²•ìœ¼ë¡œ ì¶”ì • Step 2: Plug-in ì¶”ì • (3)ì˜ plug-in ë²„ì „ì„ í†µí•´ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • $\\hat{e}^{(-q(i))}(X_i)$ ë“±ì€ ië²ˆì§¸ í›ˆë ¨ ì˜ˆì œê°€ ì†í•œ foldë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë§Œë“  ì˜ˆì¸¡ê°’ $$ \\hat{\\tau}(\\cdot) = \\arg\\min_{\\tau} \\left\\{ \\hat{L}_n[\\tau(\\cdot)] + \\Lambda_n[\\tau(\\cdot)] \\right\\} \\tag{4} $$ $$ \\hat{L}_n[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\} - \\{W_i - \\hat{e}^{(-q(i))}(X_i)\\} \\tau(X_i) \\right]^2 \\tag{4a} $$ > In other words, the first step learns an approximation for the oracle objective, and the second step optimizes it. We refer to this approach as the R-learner in recognition of the work of Robinson (1988) and to emphasize the role of residualization. We will also refer to the squared loss $L_b^n[\\tau(\\cdot)]$ as the R-loss. 1ë‹¨ê³„: ì˜¤ë¼í´ ëª©ì í•¨ìˆ˜(ì´ë¡ ì ìœ¼ë¡œ ìµœì ì„ì„ ë³´ì¥í•˜ëŠ” í•¨ìˆ˜)ì˜ ê·¼ì‚¬ê°’ì„ í•™ìŠµ 2ë‹¨ê³„: ê·¸ ê·¼ì‚¬ ëª©ì í•¨ìˆ˜ë¥¼ ì‹¤ì œë¡œ ìµœì í™” ì´ ì „ì²´ ê³¼ì •ì„ R-learnerë¼ ë¶€ë¥´ê³ , ì†ì‹¤í•¨ìˆ˜ë¥¼ R-lossë¼ ë¶€ë¦„ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬: ë‹¤ì–‘í•œ ë°©ë²• ì ìš© ë° ì„±ëŠ¥ R-learnerë¥¼ í˜ë„í‹° íšŒê·€, ì»¤ë„ ë¦¿ì§€ íšŒê·€, ë¶€ìŠ¤íŒ… ë“± ë‹¤ì–‘í•œ ë°©ë²•ì— ì ìš©í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„. ì´ë¡ ì  ë³´ì¥ ì»¤ë„ íšŒê·€ì˜ ê²½ìš°, ì‹¤ì œ ì¶”ì •ê¸°(plug-in estimator)ì˜ ì˜¤ì°¨ ê²½ê³„ê°€ ì˜¤ë¼í´(ì´ë¡ ì  ìµœì ) ë°©ë²•ê³¼ ê±°ì˜ ì¼ì¹˜í•¨ì„ ì¦ëª…. íŠ¹íˆ, nuisance component($m^(x)$, $e^(x)$)ì˜ ì¶”ì • ì˜¤ì°¨ê°€ ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë©´, ìµœì¢… ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ì˜¤ì§ $\\tau^(x)$ì˜ ë³µì¡ë„ì—ë§Œ ì˜ì¡´í•¨. ì‹¤ìš©ì  ì¥ì  R-learnerëŠ” ì²˜ì¹˜í™•ë¥ ê³¼ ê²°ê³¼ ì˜ˆì¸¡ì˜ ìƒê´€ê´€ê³„ë¥¼ ì†ì‹¤í•¨ìˆ˜ êµ¬ì¡°ë¡œ ë¶„ë¦¬í•˜ì—¬, ë‘ ì‘ì—…(ìƒê´€ê´€ê³„ ì œê±°, ì²˜ì¹˜íš¨ê³¼ ì¶”ì •)ì„ ëª…í™•íˆ ë¶„ë¦¬í•¨. ì´ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë„êµ¬(ì˜ˆ: glmnet, XGBoost, TensorFlow ë“±)ë¥¼ ì†ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆê³ , ì†ì‹¤í•¨ìˆ˜(R-loss)ë§Œ ì˜ ìµœì†Œí™”í•˜ë©´ ë¨. ë³µì¡í•œ êµì°¨ê²€ì¦ ì—†ì´ë„ ì†ì‹¤í•¨ìˆ˜ ê¸°ë°˜ì˜ ê°„ë‹¨í•œ íŠœë‹ì´ ê°€ëŠ¥í•¨. Related Work Regularization bias: > However, the fact that both $\\hat{\\beta}^{(0)}$ and $\\hat{\\beta}^{(1)}$ are regularized towards 0 separately may inadvertently regularize the treatment effect estimate $\\hat{\\beta}^{(1)} - \\hat{\\beta}^{(0)}$ away from 0, even when $\\tau^(x) = 0$ everywhere CATE(ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜íš¨ê³¼)ëŠ” $\\tau^(x) = \\mu^{(1)}(x) - \\mu^{(0)}(x)$ë¡œ ì“¸ ìˆ˜ ìˆìŒ. í”íˆ $\\mu^{(1)}(x)$ì™€ $\\mu^{(0)}(x)$ë¥¼ ê°ê° ë”°ë¡œ ì¶”ì •í•œ ë’¤, ê·¸ ì°¨ì´ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ êµ¬í•˜ëŠ”ë°, ì´ë•Œ ë‘ í•¨ìˆ˜ë¥¼ ë³„ë„ë¡œ ì •ê·œí™”(regularization)í•˜ë©´ regularization biasê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ. ì˜ˆë¥¼ ë“¤ì–´, ë¼ì˜(lasso) íšŒê·€ë¥¼ ê°ê°ì˜ ì§‘ë‹¨(ì²˜ì¹˜/ë¹„ì²˜ì¹˜)ì— ë”°ë¡œ ì ìš©í•˜ë©´, ë‘ ì¶”ì •ì¹˜ ëª¨ë‘ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡ ì •ê·œí™”ë˜ì–´ ì‹¤ì œë¡œëŠ” ì²˜ì¹˜íš¨ê³¼ê°€ ì—†ì„ ë•Œë„ $\\hat{\\tau}(x) = \\hat{\\mu}^{(1)}(x) - \\hat{\\mu}^{(0)}(x)$ê°€ 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆìŒ. íŠ¹íˆ, ì²˜ì¹˜êµ°ê³¼ ëŒ€ì¡°êµ°ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ë‹¤ë¥¼ ë•Œ ì´ í˜„ìƒì´ ë” ì‹¬í•´ì§. $$ \\begin{align} \\hat{\\beta}^{(w)} = \\arg\\min_{\\beta^{(w)}} \\left\\{ \\sum_{i:W_i=w} \\left( Y_i - X_i^\\top \\beta^{(w)} \\right)^2 + \\lambda^{(w)} \\ \\beta^{(w)}\\ _1 \\right\\} \\tag{5} \\end{align} $$ ê°„ë‹¨í•˜ê²Œ ëŒ€ì¡°êµ°ê³¼ í†µì œêµ°ì„ ë”°ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ê²°ê³¼ë¥¼ ë¹„êµí•´ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì–»ìœ¼ë©´ ë˜ëŠ”ê±° ì•„ë‹ˆì•¼? ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ regularization biasë¼ê³  í•˜ëŠ”, ê³¼í•˜ê²Œ ì •ê·œí™”ë˜ì–´ ì²˜ì¹˜íš¨ê³¼ê°€ 0ì¼ ë•Œë„ 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. Regularization Biasë¥¼ í”¼í•˜ëŠ” ìµœê·¼ ë°©ë²•ë“¤: ìµœê·¼ ì—°êµ¬ë“¤ì€ regularization bias ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ êµ¬ì¡°ì (machine learning êµ¬ì¡° ìì²´ì˜) ê°œì„  ë°©ë²•ì„ ì œì•ˆí•¨. ì˜ˆë¥¼ ë“¤ì–´, Imai & Ratkovic (2013)ì€ ì‹ (5)ì²˜ëŸ¼ ì²˜ì¹˜êµ°/ëŒ€ì¡°êµ°ì„ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , ì•„ë˜ì™€ ê°™ì´ í•˜ë‚˜ì˜ ë¼ì˜(lasso) íšŒê·€ë¡œ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨: $$ \\begin{align} \\hat{b}, \\hat{\\delta} = \\arg\\min_{b, \\delta} \\left\\{ \\sum_{i=1}^n \\left( Y_i - X_i^\\top b + (W_i - 0.5) X_i^\\top \\delta \\right)^2 + \\lambda_b \\ b\\ _1 + \\lambda_\\delta \\ \\delta\\ _1 \\right\\} \\tag{6} \\end{align} $$ ì—¬ê¸°ì„œ ìµœì¢… ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì€ $\\hat{\\tau}(x) = x^\\top \\hat{\\delta}$ë¡œ ê³„ì‚°í•¨. ì´ ë°©ë²•ì€ $\\delta$ì—ë§Œ í¬ì†Œì„±(sparsity)ì„ ê°•ì œí•˜ì—¬, ì²˜ì¹˜íš¨ê³¼ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ë” ì˜ ë°˜ì˜í•  ìˆ˜ ìˆìŒ. ì´ì™¸ì—ë„ ì‹ ê²½ë§ ë“± ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì—ì„œ ì²˜ì¹˜íš¨ê³¼ ì´ì§ˆì„±(heterogeneity)ì„ ì˜ ì¶”ì •í•˜ë„ë¡ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ê³  ìˆìŒ(ì˜ˆ: Shalit et al., 2017). Loss Function(ì†ì‹¤í•¨ìˆ˜) ë³€ê²½ì„ í†µí•œ ì ‘ê·¼: > Here, instead of trying to modify the algorithms underlying different machine learning tools to improve their performance as treatment effect estimators, we focus on modifying the loss function used to training generic machine learning methods. ìµœê·¼ ì—°êµ¬ë“¤ì€ ëª¨ë¸ êµ¬ì¡° ìì²´ë¥¼ ë°”ê¾¸ëŠ” ëŒ€ì‹ , ì†ì‹¤í•¨ìˆ˜(R-loss)ë¥¼ ë°”ê¿”ì„œ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•ì— ì£¼ëª©í•˜ê³  ìˆìŒ. ì´ ì ‘ê·¼ì€ van der Laanê³¼ Dudoit(2003) ë“±ì—ì„œ ì‹œì‘ëœ ì—°êµ¬ íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ë“¤ì€ doubly robust objective(ì´ì¤‘ ê°•ê±´ ëª©ì í•¨ìˆ˜)ì— ëŒ€í•œ êµì°¨ê²€ì¦ì„ í†µí•´ ìµœì ì˜ í†µê³„ì  ê·œì¹™ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨. Luedtke & van der Laan(2016) ë“±ì€ ì´ëŸ¬í•œ ëª©ì í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê°œë³„í™” ì²˜ì¹˜ê·œì¹™(individualized treatment rules)ì´ë‚˜ ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼(heterogeneous treatment effects)ë¥¼ í•™ìŠµí•˜ëŠ” ë‹¤ì–‘í•œ ìœ íš¨í•œ ëª©ì í•¨ìˆ˜(oracle loss ë“±)ì˜ ì„±ì§ˆì„ ë¶„ì„í•¨. ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬ëŠ”, R-lossë¥¼ í™œìš©í•´ ë²”ìš© ê¸°ê³„í•™ìŠµ(generic machine learning)ìœ¼ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³ , ì»¤ë„ íë²„íŠ¸ ê³µê°„ì—ì„œì˜ ì •ê·œí™” íšŒê·€ ë“± ë„ë¦¬ ì“°ì´ëŠ” ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì— ëŒ€í•´ ê°•í•œ ì´ë¡ ì  ì˜¤ì°¨ ê²½ê³„(ìˆ˜ë ´ë¥  ë³´ì¥)ë¥¼ ì œê³µí•œ ê²ƒì„. meta-learning ë°©ë²•ë“¤: meta learner ê´€ë ¨ ì„¤ëª…ìë£Œ X-learner (KÃ¼nzel et al., 2019): ë¨¼ì € $\\hat{\\mu}^{(w)}(x)$ë¥¼ ë¹„ëª¨ìˆ˜ íšŒê·€ ë°©ë²•ìœ¼ë¡œ ì¶”ì • ì²˜ì¹˜ ê´€ì°°ê°’ì— ëŒ€í•´ pseudo-effects $D_i = Y_i - \\hat{\\mu}^{(-i)}{(0)}(X_i)$ë¥¼ ì •ì˜í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ $\\hat{\\tau}^{(1)}(X_i)$ë¥¼ ë¹„ëª¨ìˆ˜ íšŒê·€ë¡œ ì¶”ì • ëŒ€ì¡°êµ°ì— ëŒ€í•´ì„œë„ ìœ ì‚¬í•˜ê²Œ $\\hat{\\tau}^{(0)}(X_i)$ë¥¼ êµ¬í•˜ê³ , ë‘ ì¶”ì •ê¸°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê²°í•©: $$ \\hat{\\tau}(x) = \\{1 - \\hat{e}(x)\\} \\hat{\\tau}^{(1)}(x) + \\hat{e}(x) \\hat{\\tau}^{(0)}(x) \\tag{7} $$ U-learner (KÃ¼nzel et al., 2019): $U_i = \\frac{Y_i - m^(X_i)}{W_i - e^(X_i)}$ì— ëŒ€í•´ $\\mathbb{E}[U_i \\mid X_i = x] = \\tau(x)$ì„ì„ ì´ìš© $U_i$ë¥¼ $X_i$ì— ëŒ€í•´ ë²”ìš© ê¸°ê³„í•™ìŠµ ë°©ë²•ìœ¼ë¡œ íšŒê·€ Propensity Score ê°€ì¤‘ ë°©ë²•ë“¤: Athey & Imbens (2016), Tian et al. (2014) ë“±ì´ ì œì•ˆ ê²°ê³¼ë‚˜ ê³µë³€ëŸ‰ì„ propensity scoreë¡œ ê°€ì¤‘í•˜ì—¬ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • ì˜ˆ: $Y_i\\{W_i - e^(X_i)\\}/\\{e^(X_i)(1-e^(X_i))\\}$ë¥¼ $X_i$ì— ëŒ€í•´ íšŒê·€ ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬: R-learner ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë‹¤ì–‘í•œ ì„¤ì •ì—ì„œ baselineë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ê°œì„ ì„ ì œê³µ Quasi-oracle ì˜¤ì°¨ ê²½ê³„ë¥¼ ì œê³µí•˜ì—¬ $\\hat{\\tau}$ì˜ ì˜¤ì°¨ê°€ $\\hat{e}$ë‚˜ $\\hat{m}$ì˜ ì˜¤ì°¨ë³´ë‹¤ ë¹ ë¥´ê²Œ ê°ì†Œí•  ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¥ ê´€ë ¨ ì—°êµ¬ì™€ ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì : ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ ì—°êµ¬: Zhao, Small, and Ertefaie (2017): Robinson ë³€í™˜ê³¼ ë¼ì˜(lasso)ë¥¼ ê²°í•©í•´ ê³ ì°¨ì› ì„ í˜•ëª¨í˜•ì—ì„œ íš¨ê³¼ ìˆ˜ì •(effect modification)ì— ëŒ€í•œ ìœ íš¨í•œ ì‚¬í›„ì„ íƒ ì¶”ë¡ (post-selection inference)ì„ ì œê³µ í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ë§¥ë½ì—ì„œ ì†ì‹¤í•¨ìˆ˜ë¡œ Robinson ë³€í™˜ì„ í™œìš©í•œ ê²ƒì€ ë³¸ ë…¼ë¬¸ì´ ì²˜ìŒ ì´ë¡ ì  ê¸°ë°˜: ë³¸ ë…¼ë¬¸ì˜ ì´ë¡ ì  ê²°ê³¼ëŠ” Robinson(1988) ë“±ì—ì„œ ë°œì „ëœ ì¤€ëª¨ìˆ˜ì  íš¨ìœ¨ì„±(semiparametric efficiency)ê³¼ ì§êµ ëª¨ë©˜íŠ¸(orthogonal moments) ì´ë¡ ì— ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œëŠ” Targeted Maximum Likelihood Estimation(TMIE)ì™€ ìœ ì‚¬: nuisance componentë¥¼ ë¹„ëª¨ìˆ˜ì ìœ¼ë¡œ ì¶”ì • ì´ë¥¼ í™œìš©í•´ likelihood(ë˜ëŠ” ì†ì‹¤í•¨ìˆ˜)ë¥¼ ìµœì í™” Cross-fitting(í™€ë“œì•„ì›ƒ ì˜ˆì¸¡)ì€ ìµœê·¼ ì¤€ëª¨ìˆ˜ì  ì¶”ì •ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ë°©ë²• ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì : ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì£¼ë¡œ ë‹¨ì¼(ë˜ëŠ” ì €ì°¨ì›) íŒŒë¼ë¯¸í„° ì¶”ì •ì— ì´ˆì  ë³¸ ë…¼ë¬¸ì€ ë³µì¡í•œ í•¨ìˆ˜ì  ê°ì²´(ì¦‰, $\\tau^(\\cdot)$ ì „ì²´ í•¨ìˆ˜)ë¥¼ ì¶”ì •í•˜ëŠ” ë° ì´ˆì  ìµœì  ì²˜ì¹˜ í• ë‹¹ ê·œì¹™(optimal treatment allocation rule) ì¶”ì •ê³¼ë„ ê´€ë ¨ ìˆì§€ë§Œ, ëª©ì í•¨ìˆ˜(ì†ì‹¤)ê°€ ë‹¤ë¦„ ì¶”ê°€ ë…¼ì˜: ë³¸ ë…¼ë¬¸ì€ ëª¨ì§‘ë‹¨ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¶œëœ ìƒ˜í”Œì„ ê°€ì • ì—„ê²©í•œ ë¬´ì‘ìœ„í™” ì¶”ë¡ (randomization inference) í•˜ì—ì„œì˜ ë¹„ëª¨ìˆ˜ì  ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë„ í¥ë¯¸ë¡œìš´ ì£¼ì œì„ The R-Learner in Action 4.1 Application to a Voting Study íˆ¬í‘œì— ê´€í•œ ì—°êµ¬ì— ì´ë¥¼ ì ìš©í•´ë³¸ ì‚¬ë¡€ë¥¼ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ ë°°ê²½ ì›ë³¸ ì—°êµ¬: Arceneaux, Gerber, and Green (2006) ì—°êµ¬ ì£¼ì œ: ìœ ë£Œ íˆ¬í‘œ ë…ë ¤ ì „í™”ê°€ íˆ¬í‘œìœ¨ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ ì‹¤ì œ ê²°ê³¼: íˆ¬í‘œ ë…ë ¤ ì „í™”ì˜ íš¨ê³¼ê°€ ê±°ì˜ ì—†ìŒ (1% ë¯¸ë§Œ) ì‹¤í—˜ ì„¤ê³„ ë°ì´í„° êµ¬ì„± ì „ì²´ ìƒ˜í”Œ: 1,895,468ê°œ ê´€ì¸¡ì¹˜ ì²˜ì¹˜ ê·¸ë£¹:59,264ê°œ (ì „í™” ë°›ì€ ì‚¬ëŒ) ë¶„ì„ ìƒ˜í”Œ: 148160ê°œ (ëª¨ë“  ì²˜ì¹˜ ê·¸ë£¹ + ëœë¤ ëŒ€ì¡° ê·¸ë£¹) ì²˜ì¹˜ ë¹„ìœ¨: 2/5 (40 ë°ì´í„° ë¶„í•  í›ˆë ¨ ì„¸íŠ¸: 1000ê°œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: 25,0ê°œ í™€ë“œì•„ì›ƒ ì„¸íŠ¸: ë‚˜ë¨¸ì§€ ì¸ê³µì  ì²˜ì¹˜íš¨ê³¼ ì¶”ê°€ ì‹¤ì œë¡œëŠ” ì²˜ì¹˜íš¨ê³¼ê°€ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ, ì¸ê³µì ìœ¼ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ê°€: $$ \\tau^(X_i) = -\\frac{\\text{VOTE00}_i}{2 + \\frac{100}{\\text{AGE}_i}} $$ VOTE00_i: 200íˆ¬í‘œ ì—¬ë¶€ AGE_i: ë‚˜ì´ ëª©ì : ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•œ ì˜ë¯¸ìˆëŠ” ì‘ì—… ìƒì„± R-learner ì ìš© ê³¼ì • Step 1: Nuisance Components ì¶”ì • ë¶€ìŠ¤íŒ…ê³¼ ë¼ì˜ ë‘ ë°©ë²•ìœ¼ë¡œ $\\hat{e}(\\cdot)$ì™€ $\\hat{m}(\\cdot)$ ì¶”ì • êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ ê²°ê³¼: ë¶€ìŠ¤íŒ…ì´ ë‘ nuisance components ëª¨ë‘ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ Step2 R-loss ìµœì í™” ë¼ì˜: R-loss = 00.1816 (í›ˆë ¨), 0.1781 (í™€ë“œì•„ì›ƒ) ë¶€ìŠ¤íŒ…: R-loss = 00.1818 (í›ˆë ¨), 0.1783í™€ë“œì•„ì›ƒ) ìµœì¢… ì„ íƒ: ë¼ì˜ ê¸°ë°˜ $\\hat{\\tau}(\\cdot)$ ì„±ëŠ¥ ë¹„êµ Oracle Test Set MSE (Mean Squared Error) ë°©ë²• MSE ------ ----- R-learner (ë¼ì˜) 00.47 Ã—10â»Â³ R-learner (ë¶€ìŠ¤íŒ…) 1.2310 Single Lasso 00.61 Ã—10â»Â³ BART 40.05 Ã—10â»Â³ ì„±ëŠ¥ ë¶„ì„ R-learner (ë¼ì˜)ê°€ ìµœê³  ì„±ëŠ¥2. ë¼ì˜ vs ë¶€ìŠ¤íŒ…: ë¼ì˜: í¸í–¥ ìˆì§€ë§Œ ë¶„ì‚° ë‚®ìŒ ë¶€ìŠ¤íŒ…: ë…¸ì´ì¦ˆ ë§ì§€ë§Œ í¸í–¥ ì ìŒ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ì—ì„œ ë¼ì˜ê°€ ìœ ë¦¬ í•µì‹¬ ë°œê²¬ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì˜ ìš°ìˆ˜ì„± Nuisance components: ë¹„ëª¨ìˆ˜ì  ë°©ë²• (ë¶€ìŠ¤íŒ…) ì‚¬ìš© ì²˜ì¹˜íš¨ê³¼ ì¶”ì •: ê°„ë‹¨í•œ ë°©ë²• (ë¼ì˜) ì‚¬ìš© ì´ìœ : ê° ë‹¨ê³„ì— ìµœì í™”ëœ ë°©ë²• ì„ íƒ ê¸°ì¡´ ë°©ë²•ë“¤ì˜ í•œê³„ Single Lasso: ëª¨ë“  ê³³ì—ì„œ ì„ í˜• ëª¨ë¸ë§ â†’ ëª¨ë¸ ì˜¤ì§€ì • ê°€ëŠ¥ì„± BART: ëª¨ë“  ê³³ì—ì„œ ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ë§ â†’ ë¶ˆì•ˆì •í•œ $\\tau(\\cdot)$ ì¶”ì • R-learnerì˜ ì¥ì  ìœ ì—°ì„±: ê° ë‹¨ê³„ì— ì í•©í•œ ë°©ë²• ì„ íƒ ê°€ëŠ¥ ì•ˆì •ì„±: êµì°¨ê²€ì¦ì„ í†µí•œ ì•ˆì •ì ì¸ ëª¨ë¸ ì„ íƒ ì„±ëŠ¥: ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì¶”ì • ì •í™•ë„ ì‹¤ìš©ì  êµí›ˆ ë‹¨ê³„ë³„ ìµœì í™”: ê° ë‹¨ê³„ì— ê°€ì¥ ì í•©í•œ ë°©ë²• ì„ íƒ êµì°¨ê²€ì¦ì˜ ì¤‘ìš”ì„±: ê³¼ì í•© ë°©ì§€ì™€ ëª¨ë¸ ì„ íƒ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„: ë°ì´í„° í¬ê¸°ì™€ ë³µì¡ì„± ê³ ë ¤ ì‹¤ì œ ë°ì´í„° ì ìš©: ì´ë¡ ì  ë°©ë²•ì˜ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ 4.2 Model Averaging with the R-Learner ì—¬ëŸ¬ ëª¨ë¸ì„ í•©ì¹˜ëŠ” stacking ë°©ë²•ì„ ì ìš©í•œ ì‹œë„ë¥¼ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°°ê²½ê³¼ ë™ê¸° ì•ì„  ì„¹ì…˜ì—ì„œëŠ” R-learnerì˜ ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ì¶”ì • ì „ëµì„ ì„¸ì‹¬í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë¶„ì„ì˜ ì‹œì‘ì ìœ¼ë¡œ ê¸°ì„±í’ˆ(off-the-shelf) ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” stackingì˜ ë³€í˜•ì„ í†µí•´ í•©ì˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ êµ¬ì¶•í•˜ëŠ” R-learning ì ‘ê·¼ë²•ì„ ë…¼ì˜í•©ë‹ˆë‹¤. Stacking ê¸°ë°˜ í•©ì˜ ì¶”ì • ê¸°ë³¸ ì„¤ì • $k = 1, \\ldots, K$ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸° $\\hat{\\tau}_k$ ë³´ìœ  í›ˆë ¨ ì„¸íŠ¸ì—ì„œ out-of-fold ì¶”ì •ì¹˜ $\\hat{\\tau}_k^{(-i)}(X_i)$ ì ‘ê·¼ ê°€ëŠ¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” out-of-fold ì¶”ì •ì¹˜ $\\hat{e}^{(-i)}(X_i)$ì™€ $\\hat{m}^{(-i)}(X_i)$ ë³´ìœ  í•©ì˜ ì¶”ì • ë°©ë²• R-lossì— ë”°ë¼ $\\hat{\\tau}_k(\\cdot)$ì˜ ìµœì  ì–‘ì˜ ì„ í˜• ê²°í•©ì„ ì·¨í•˜ì—¬ í•©ì˜ ì¶”ì • $\\hat{\\tau}(\\cdot)$ë¥¼ êµ¬ì¶•: $$ \\hat{\\tau}(x) = \\hat{c} + \\sum_{k=1}^K \\alpha_k \\hat{\\tau}_k(x) \\tag{8a} $$ $$ (\\hat{b}, \\hat{c}, \\hat{\\alpha}) = \\arg\\min_{b, c, \\alpha} \\left\\{ \\sum_{i=1}^n \\left[ \\left(Y_i - \\hat{m}^{(-i)}(X_i)\\right) - b - \\left(c + \\sum_{k=1}^K \\alpha_k \\hat{\\tau}_k^{(-i)}(X_i)\\right) \\left(W_i - \\hat{e}^{(-i)}(X_i)\\right) \\right]^2 : \\alpha \\geq 0 \\right\\} \\tag{8b} $$ ìˆ˜ì‹ì˜ êµ¬ì„± ìš”ì†Œ $\\hat{c}$: ìƒìˆ˜ ì²˜ì¹˜íš¨ê³¼ í•­ (ìœ ì—°ì„±ì„ ìœ„í•´ ììœ ë¡­ê²Œ ì¡°ì •) $\\hat{b}$: ì ˆí¸ ($\\hat{m}$ì˜ ì ì¬ì  í¸í–¥ í¡ìˆ˜) $\\alpha_k$: ê° ì¶”ì •ê¸°ì˜ ê°€ì¤‘ì¹˜ (ë¹„ìŒ ì œì•½) ì‹¤í—˜ ì„¤ì • ë°ì´í„° ìƒì„± ê³¼ì • ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„° ìƒì„± ë¶„í¬ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤: $$ X_i \\sim N(0, I_{d \\times d}), \\quad W_i \\sim \\mathrm{Bernoulli}(0.5) \\tag{9a} $$ $$ Y_i \\mid X_i, W_i \\sim N\\left(\\frac{3}{1 + e^{X_{i3} - X_{i2}}} + (W_i - 0.5) \\tau^(X_i), \\sigma^2\\right) \\tag{9b} $$ ì‹¤í—˜ ì¡°ê±´: ìƒ˜í”Œ í¬ê¸°: $n = 10,000$ (ë¬´ì‘ìœ„ ì—°êµ¬ ì„¤ê³„) ì°¨ì›: $d = 10$ ë‹¤ì–‘í•œ $\\tau^(\\cdot)$ì™€ $\\sigma$ ì„ íƒ ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ ë‘ ê°€ì§€ ìœ í˜•ì˜ ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ë¥¼ ê³ ë ¤: ì—°ì† í•¨ìˆ˜: $\\tau^(X_i) = \\frac{1}{1 + e^{X_{i1} - X_{i2}}}$ ë¶ˆì—°ì† í•¨ìˆ˜: $\\tau^(X_i) = \\frac{\\mathbf{1}\\{X_{i1} > 0\\}}{1 + e^{-X_{i2}}}$ ë¹„êµ ë°©ë²• ì¶”ì • ë°©ë²•ë“¤: BART (Bayesian Additive Regression Trees) Causal Forests (ì¸ê³¼ê´€ê³„ ìˆ²) Stacking: ì‹ (8)ì„ ì´ìš©í•œ ë‘ ë°©ë²•ì˜ ê²°í•© ì‹¤í—˜ ì¡°ê±´: ë°ì´í„°ê°€ ë¬´ì‘ìœ„í™”ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ìê°€ ì•Œê³  ìˆë‹¤ê³  ê°€ì • í•„ìš”í•œ ê³³ì—ì„œëŠ” $\\hat{e}(x) = 0.5$ ì‚¬ìš© Stackingìš© $\\hat{m}(\\cdot)$ëŠ” random forestë¡œ ì¶”ì • ì‹¤í—˜ ê²°ê³¼ (Figure 2) ì—°ì† ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ì˜ ê²½ìš° BART: Causal Forestsë³´ë‹¤ ì•½ê°„ ìš°ìˆ˜í•œ ì„±ëŠ¥ Stacking: ë…¸ì´ì¦ˆ ìˆ˜ì¤€ $\\sigma$ê°€ ë§¤ìš° í´ ë•Œê¹Œì§€ ê°œë³„ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜ ê³ ë…¸ì´ì¦ˆ í™˜ê²½: ëª¨ë“  ë°©ë²•ì´ ìƒìˆ˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ ë¶ˆì—°ì† ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ì˜ ê²½ìš° Causal Forests: ë‚®ì€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì—ì„œ íŠ¹íˆ ìœ ë¦¬ Stacking: ë” ì •í™•í•œ ê¸°ë³¸ í•™ìŠµìì˜ ì„±ëŠ¥ì„ ìë™ìœ¼ë¡œ ë§¤ì¹­ í•µì‹¬ ë°œê²¬ Stackingì˜ ìë™ ì ì‘ì„± ê° ì„¤ì •ì—ì„œ ë” ë‚˜ì€ ê¸°ë³¸ í•™ìŠµìë¥¼ ìë™ìœ¼ë¡œ ì‹ë³„ ìˆ˜ë™ ì„ íƒ ì—†ì´ë„ ìµœì  ì„±ëŠ¥ì— ê·¼ì ‘ ë°©ë²•ë¡ ì  ìœ ì—°ì„± ê¸°ì¡´ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë“¤ì„ ê·¸ëŒ€ë¡œ í™œìš© ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì • ì—†ì´ ì„±ëŠ¥ ê°œì„  ì‹¤ìš©ì  ì¥ì  ìë™í™”: ë³µì¡í•œ ëª¨ë¸ ì„ íƒ ê³¼ì • ë‹¨ìˆœí™” ê²¬ê³ ì„±: ë‹¤ì–‘í•œ ë°ì´í„° íŠ¹ì„±ì— ëŒ€í•œ ì ì‘ë ¥ íš¨ìœ¨ì„±: ê¸°ì¡´ ë„êµ¬ë“¤ì˜ ê°•ì ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•© A Quasi-Oracle Error Bound ì´ ì„¹ì…˜ì—ì„œëŠ” R-learnerì˜ í•µì‹¬ ì´ë¡ ì  ê¸°ì—¬ì¸ quasi-oracle propertyì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. 5.1 Oracleê³¼ Quasi-Oracleì˜ ê°œë… Oracle Estimator ì´ìƒì ì¸ oracleì€ ë‹¤ìŒì„ ë¯¸ë¦¬ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤: True marginal effect: $m^(x) = \\mathbb{E}[Y \\mid X = x]$ True propensity score: $e^(x) = \\Pr(W = 1 \\mid X = x)$ Oracleì´ ì´ ì •ë³´ë¥¼ ì•Œê³  ìˆë‹¤ë©´, ë‹¤ìŒ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì§ì ‘ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$ \\mathcal{L}_n^{\\text{oracle}}[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\} \\tau(X_i) \\right]^2 $$ R-learner (Quasi-Oracle) ì‹¤ì œë¡œëŠ” $m^(x)$ì™€ $e^(x)$ë¥¼ ëª¨ë¥´ë¯€ë¡œ, ì¶”ì •ê°’ $\\hat{m}(\\cdot)$ê³¼ $\\hat{e}(\\cdot)$ì„ ì‚¬ìš©: $$ \\hat{\\mathcal{L}}_n[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\} - \\{W_i - \\hat{e}^{(-q(i))}(X_i)\\} \\tau(X_i) \\right]^2 $$ 5.2 Main Theoretical Result ë…¼ë¬¸ì˜ í•µì‹¬ ì´ë¡ ì  ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: Theorem (Informal) ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ, R-learner $\\hat{\\tau}$ëŠ” ë‹¤ìŒ ì„±ì§ˆì„ ë§Œì¡±í•©ë‹ˆë‹¤: $$ \\mathbb{E}\\left[\\ \\hat{\\tau} - \\tau^\\ ^2\\right] \\lesssim \\text{Oracle Rate} + \\text{Nuisance Rate} $$ ì—¬ê¸°ì„œ: Oracle Rate: oracleì´ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìµœì  ìˆ˜ë ´ë¥  Nuisance Rate: nuisance components ì¶”ì • ì˜¤ì°¨ì˜ ê³± Quasi-Oracle Property ë§Œì•½ nuisance componentsì˜ ì¶”ì •ì´ ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•œë‹¤ë©´: $$ \\text{Nuisance Rate} = \\ \\hat{m} - m^\\ \\cdot \\ \\hat{e} - e^\\ \\to 0 $$ ì´ ê²½ìš° R-learnerëŠ” oracle rateë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤: $$ \\mathbb{E}\\left[\\ \\hat{\\tau} - \\tau^\\ ^2\\right] \\lesssim \\text{Oracle Rate} $$ 5.3 Why Does This Work? Cross-fittingì˜ ì—­í•  Overfitting ë°©ì§€: ê°™ì€ ë°ì´í„°ë¡œ ì¶”ì •ê³¼ ì˜ˆì¸¡ì„ í•˜ì§€ ì•ŠìŒ Bias ê°ì†Œ: $\\mathbb{E}[\\hat{m}^{(-i)}(X_i) - m^(X_i)] \\approx 0$ Doubly Robust Structure R-lossëŠ” doubly robust êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤: $$ \\mathbb{E}\\left[\\left\\{Y_i - m^(X_i)\\right\\} - \\left\\{W_i - e^(X_i)\\right\\} \\tau^(X_i) \\mid X_i\\right] = 0 $$ ì´ëŠ” ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤: $m^(x)$ ë˜ëŠ” $e^(x)$ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ í¸í–¥ì´ ì—†ìŒ ë‘ ì¶”ì •ì˜ ì˜¤ì°¨ê°€ ê³±ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¨ Product Form of Error ìµœì¢… ì˜¤ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´ë©ë‹ˆë‹¤: $$ \\text{Total Error} \\approx \\text{Oracle Error} + \\ \\hat{m} - m^\\ \\times \\ \\hat{e} - e^\\ $$ ë”°ë¼ì„œ ë‘ nuisance componentsê°€ ëª¨ë‘ $o(1)$ ì†ë„ë¡œ ìˆ˜ë ´í•˜ë©´, ê³±ì€ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤. 5.4 Practical Implications Flexible Method Choice ê° ë‹¨ê³„ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ê¸°ê³„í•™ìŠµ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ $\\hat{m}$ì—ëŠ” random forests, $\\hat{e}$ì—ëŠ” logistic regression ë“± Robustness í•œ nuisance componentê°€ ë¶€ì •í™•í•´ë„ ë‹¤ë¥¸ ê²ƒì´ ë³´ì™„ ì™„ë²½í•œ ì¶”ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ Rate Optimality ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ minimax optimal ë‹¬ì„± ì´ë¡ ì ìœ¼ë¡œ ìµœì„ ì˜ ì„±ëŠ¥ ë³´ì¥ ë…¼ë¬¸ì—ì„œ ê¶ê¸ˆí•œ ì ë“¤ ì •ë¦¬ ê·¸ë˜ì„œ quasi-oracle propertyëŠ” ì–´ë–»ê²Œ ë³´ì¥ë˜ëŠ”ê±´ë°? í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ Robinson's Transformationì˜ ë§ˆë²• Robinson's transformation (1988)ì´ í•µì‹¬ì…ë‹ˆë‹¤: $$ Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i $$ ì´ ë³€í™˜ì˜ íŠ¹ë³„í•œ ì„±ì§ˆ: Orthogonality: $\\mathbb{E}[\\varepsilon_i \\mid X_i, W_i] = 0$ Causal Isolation: ì²˜ì¹˜íš¨ê³¼ë§Œ ë¶„ë¦¬ë¨ Cross-fittingì˜ í•µì‹¬ ì—­í•  `` Step 1: ë°ì´í„°ë¥¼ Kê°œ foldë¡œ ë¶„í•  Step 2: kë²ˆì§¸ foldë¥¼ ì œì™¸í•˜ê³  mÌ‚, Ãª ì¶”ì • Step 3: kë²ˆì§¸ foldì—ì„œ Ï„Ì‚ ì¶”ì • Step 4: ëª¨ë“  foldì— ëŒ€í•´ ë°˜ë³µ ` ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€? Independence: ì¶”ì •ì— ì‚¬ìš©ëœ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„°ê°€ ë…ë¦½ Bias Reduction: $\\mathbb{E}[\\hat{m}^{(-i)}(X_i) \\mid X_i] \\approx m^(X_i)$ Doubly Robustì˜ ìˆ˜í•™ì  êµ¬ì¡° í•µì‹¬ì€ ë‹¤ìŒ decompositionì…ë‹ˆë‹¤: $$ \\hat{\\mathcal{L}}_n - \\mathcal{L}_n^{\\text{oracle}} = \\text{Bias Term} + \\text{Variance Term} $$ Bias Term: $$ \\frac{2}{n} \\sum_{i=1}^n \\left[\\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\}\\tau^(X_i)\\right] \\times \\left[\\{\\hat{m}^{(-i)} - m^\\}(X_i) - \\{\\hat{e}^{(-i)} - e^\\}(X_i)\\tau^(X_i)\\right] $$ Cross-fittingì— ì˜í•´ ì´ termì˜ ê¸°ëŒ“ê°’ì´ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤. Variance Term: $$ \\left\\ \\{\\hat{m} - m^\\} - \\{\\hat{e} - e^\\}\\tau^\\right\\ ^2 $$ ì´ëŠ” $\\ \\hat{m} - m^\\ \\times \\ \\hat{e} - e^\\ $ì˜ orderì…ë‹ˆë‹¤. ìˆ˜ë ´ë¥ ì˜ ê³±ì…ˆ êµ¬ì¡° ë§Œì•½: $\\ \\hat{m} - m^\\ = O_p(r_m)$ $\\ \\hat{e} - e^\\ = O_p(r_e)$ ê·¸ëŸ¬ë©´: $$ \\ \\hat{\\tau} - \\tau^\\ = O_p(\\max\\{r_{\\text{oracle}}, r_m \\times r_e\\}) $$ ì˜ˆì‹œ: $r_m = r_e = n^{-1/4}$ (ëŠë¦° ìˆ˜ë ´) $r_m \\times r_e = n^{-1/2}$ (ë¹ ë¥¸ ìˆ˜ë ´!) Oracle rate $r_{\\text{oracle}} = n^{-1/3}$ì´ë©´ ìµœì¢… rate = $\\max\\{n^{-1/3}, n^{-1/2}\\} = n^{-1/3}$ âœ¨ ì‹¤ì œ ë™ì‘ ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ë²½í•œ Oracle `python Oracle knows true m(x) and e(x) oracle_loss = R_loss(Y, W, X, tau, m_true, e_true) oracle_rate = n^(-1/3) # optimal rate for Ï„ complexity ` ì‹œë‚˜ë¦¬ì˜¤ 2: R-learner with Imperfect Estimates `python Step 1: Estimate nuisances with cross-fitting m_hat = estimate_m_with_crossfit(Y, X) # rate: n^(-1/4) e_hat = estimate_e_with_crossfit(W, X) # rate: n^(-1/4) Step 2: Optimize R-loss tau_hat = optimize_R_loss(Y, W, X, m_hat, e_hat) Achieved rate: max(n^(-1/3), n^(-1/4) Ã— n^(-1/4)) = n^(-1/3) Same as oracle! ğŸ‰ `` ì™œ ì´ê²Œ \"Quasi-Oracle\"ì¸ê°€? Oracleê³¼ ê°™ì€ ìˆ˜ë ´ë¥ : $n^{-1/3}$ ë‹¬ì„± í•˜ì§€ë§Œ Oracleì´ ì•„ë‹˜: $m^, e^*$ë¥¼ ì‹¤ì œë¡œëŠ” ëª¨ë¦„ Automatic: ì¶”ê°€ì ì¸ íŠœë‹ ì—†ì´ ìë™ìœ¼ë¡œ ë‹¬ì„± Robust: nuisance ì¶”ì •ì´ ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ì‘ë™ ì´ê²ƒì´ ë°”ë¡œ \"quasi-oracle property\"ì˜ í•µì‹¬ì…ë‹ˆë‹¤! ğŸ¯",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/PropensityScoreNotebook",
    "title": "Propensity Score Notebook",
    "date": "2025-07-17",
    "excerpt": "Propensity Score Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "--- title: \"Propensity Score Notebook\" date: \"2025-07-17\" excerpt: \"Propensity Score Notebook\" category: \"Causal Inference\" tags: [\"Causal Inference\"] --- Propensity Score # The Psychology of Growth ì¹˜ë£Œ(treatment)ì™€ ê²°ê³¼(outcome) ë³€ìˆ˜ ì™¸ì—ë„, ì´ ì—°êµ¬ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ë¥¸ íŠ¹ì„±ë“¤ë„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤: schoolid: í•™ìƒì˜ í•™êµ ì‹ë³„ì; success_expect: ë¯¸ë˜ ì„±ê³µì— ëŒ€í•œ ìê°€ ë³´ê³  ê¸°ëŒ€ì¹˜, ì‚¬ì „ ì„±ì·¨ë„ì˜ ëŒ€ìš© ì§€í‘œë¡œ, ë¬´ì‘ìœ„ ë°°ì • ì „ì— ì¸¡ì •ë¨; gender: í•™ìƒì´ ì‹ë³„í•œ ì„±ë³„ì˜ ë²”ì£¼í˜• ë³€ìˆ˜; frst_in_family: í•™ìƒì˜ 1ì„¸ëŒ€ ëŒ€í•™ìƒ ì§€ìœ„ì— ëŒ€í•œ ë²”ì£¼í˜• ë³€ìˆ˜, ì¦‰ ê°€ì¡± ì¤‘ ëŒ€í•™ì— ì§„í•™í•œ ì²« ë²ˆì§¸ ì‚¬ëŒ; school_urbanicity: í•™êµì˜ ë„ì‹œí™” ì •ë„ì— ëŒ€í•œ í•™êµ ìˆ˜ì¤€ì˜ ë²”ì£¼í˜• ë³€ìˆ˜, ì¦‰ ë†ì´Œ, êµì™¸ ë“±; school_mindset: í•™ìƒë“¤ì˜ ê³ ì •ëœ ì‚¬ê³ ë°©ì‹ì˜ í•™êµ ìˆ˜ì¤€ í‰ê· , ë¬´ì‘ìœ„ ë°°ì • ì „ì— ë³´ê³ ë¨, í‘œì¤€í™”ë¨; school_achievement: í•™êµ ì„±ì·¨ ìˆ˜ì¤€, ì´ì „ 4ê°œ í•™ë…„ í•™ìƒë“¤ì˜ ì‹œí—˜ ì ìˆ˜ì™€ ëŒ€í•™ ì¤€ë¹„ë„ë¥¼ ì¸¡ì •í•œ ê²ƒìœ¼ë¡œ, í‘œì¤€í™”ë¨; school_ethnic_minority: í•™êµ ì¸ì¢…/ë¯¼ì¡± ì†Œìˆ˜ì êµ¬ì„±, ì¦‰ í‘ì¸, ë¼í‹°ë…¸, ë˜ëŠ” ì•„ë©”ë¦¬ì¹´ ì›ì£¼ë¯¼ì¸ í•™ìƒ ë¹„ìœ¨, í‘œì¤€í™”ë¨; school_poverty: í•™êµ ë¹ˆê³¤ ì§‘ì¤‘ë„, ì¦‰ ì—°ë°© ë¹ˆê³¤ì„  ì´í•˜ ì†Œë“ì„ ê°€ì§„ ê°€ì • ì¶œì‹  í•™ìƒë“¤ì˜ ë¹„ìœ¨, í‘œì¤€í™”ë¨; school_size: í•™êµì˜ ëª¨ë“  4ê°œ í•™ë…„ í•™ìƒë“¤ì˜ ì´ ìˆ˜, í‘œì¤€í™”ë¨. import pandas as pd pd.set_option(\"display.max_columns\", None) data = pd.read_csv(\"./data/learning_mindset.csv\") data.sample(5, random_state=5) > Although the study was randomised, it doesnâ€™t seem to be the case that this data is free from confounding > So, although the opportunity to participate was random, participation itself is not. We are dealing with a case of non-compliance here. ë§ˆì¸ë“œì…‹ì„ ë°”ê¾¸ëŠ” í”„ë¡œê·¸ë¨ ì°¸ì—¬ ì—¬ë¶€ëŠ” ëœë¤í•˜ê²Œ ì£¼ì–´ì¡Œì§€ë§Œ, ì‹¤ì œë¡œ ì°¸ì„í•˜ëŠ”ë°ì—ëŠ” í•™ìƒë“¤ì˜ ê°œë³„ì ì¸ ìš”ì¸ì˜ ì˜í–¥ì´ ìˆìœ¼ë‹ˆê¹Œ data.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) > One evidence of this is how the studentâ€™s success expectation is correlated with the participation in the seminar. > Students with higher self-reported success expectation are more likely to have joined the growth mindset seminar. ìŠ¤ìŠ¤ë¡œ ë” ë†’ì€ ê¸°ëŒ€ê°€ ìˆëŠ” í•™ìƒë“¤ì´ ì‹¤ì œë¡œ ë” ë§ì´ ì„¸ë¯¸ë‚˜ì— ì°¸ì„í•˜ë”ë¼ -> confounding biasê°€ ì¡´ì¬í•œë‹¤. > Still, letâ€™s see what the difference in means looks like. This will be a useful baseline to compare against. $$ \\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) = E[Y_i(1)] - E[Y_i(0)] $$ import statsmodels.formula.api as smf smf.ols(\"achievement_score ~ intervention\", data=data).fit().summary().tables[1] ì²˜ì¹˜ê°€ 0ë˜ëŠ” 1ì´ê¸° ë•Œë¬¸ì— ì²˜ì¹˜ë§Œì„ ê°€ì§€ê³  ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ì‹ì„ í†µí•´ $E[Y_i(1)]$ê³¼ $E[Y_i(0)]$ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì²˜ì¹˜ í˜¹ì€ ê°œì…ì´ ìˆëŠ” ê²½ìš° ì—†ëŠ” ê²½ìš°ë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ 0.4723ì •ë„ ë†’ë‹¤. ê·¼ë° ì´ì •ë„ ì°¨ì´ë©´ í°ê±´ê°€? > But is this big or small? I know that interpreting standardized outcomes can be challenging, but bear with me for a moment. > I think it is worth going through this because it wonâ€™t be the last time you will encounter standardized scores. scoreê°€ ì •ê·œí™”ë˜ì—ˆê¸° ë•Œë¬¸ì— ì´ ë‹¨ìœ„ëŠ” stdë‹¤. ë”°ë¼ì„œ 0.4723ì´ë€ ì˜ë¯¸ëŠ” ì•½ 0.5std ì •ë„ ì°¨ì´ê°€ ìˆë‹¤ëŠ” ì˜ë¯¸. > Since the treated group has an average standardized score of about 0.5, this means that they fall above 70% in terms of individual achievement. > Or, in other words, they are in the top 30% who achieve more. Here is what this looks like in a picture. import matplotlib.pyplot as plt plt.hist(data[\"achievement_score\"], bins=20, alpha=0.3, label=\"All\") plt.hist(data.query(\"intervention==0\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C2\") plt.hist(data.query(\"intervention==1\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C3\") plt.vlines(-0.1538, 0, 300, label=\"Untreated\", color=\"C2\") plt.vlines(-0.1538+0.4723, 0, 300, label=\"Treated\", color=\"C3\") plt.legend() > Of course, we still think this result is biased. > The difference between treated and untreated is probably smaller than this, because we think the bias is positive ê³¼ì—° ì´ì •ë„ ì°¨ì´ ë‚˜ëŠ”ê²Œ ë§ì„ê¹Œ? ì•ì„œ í™•ì¸í•œ ê²ƒì²˜ëŸ¼ í¸í–¥ì´ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ë¹„êµêµ° ì‚¬ì´ì— ìŠ¤ìŠ¤ë¡œ ê¸°ëŒ€í•˜ëŠ” ì •ë„ (success_expect)ê°€ ì°¨ì´ê°€ ìˆì—ˆê³ , ì´ëŠ” ì„±ê³µí™•ë¥ ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì´ ë  ìˆ˜ ìˆë‹¤. # Propensity Score > Propensity score comes from the realisation that you donâ€™t need to directly control for confounders X to achieve conditional independence $(Y_1, Y_0) \\perp T X$ > Instead, it is sufficient to control for a balancing score $E(T X)$ í¸í–¥ì„ ì œê±°í•˜ê³  ì¡°ê±´ë¶€ ë…ë¦½ì„±ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œëŠ” Xë¥¼ í†µì œí•´ì•¼í•˜ëŠ”ë°, ì´ê²Œ ì–´ë ¤ìš°ë‹ˆê¹Œ ì´ë¥¼ ëŒ€ì‹ í•´ì„œ Xì¼ë•Œ Tì¼ í™•ë¥ ($e(x)$, propensity score)ì„ í†µì œí•˜ìëŠ” ì»¨ì…‰ $$(Y_1, Y_0) \\perp T e(x)$$ import graphviz as gr g = gr.Digraph() g.edge(\"T\", \"Y\") g.edge(\"X\", \"Y\") g.edge(\"X\", \"e(x)\") g.edge(\"e(x)\", \"T\") g > If I know what e(x) is, X alone tells me nothing more that can help me learn what T would be. > Which means that controlling for e(x) acts the same way as controlling for X directly ë°ì´í„°ì˜ í¸í–¥ì„ ìƒê°í•´ë³´ë©´, success expectê°€ ë†’ì€ í•™ìƒì€ ë³¸ì¸ ì¼ë„ ì—´ì‹¬íˆí•˜ê³  ì„¸ë¯¸ë‚˜ì— ì°¸ì„í•  í™•ë¥ ë„ ë†’ì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ëœë¤ìœ¼ë¡œ ê¸°íšŒë¥¼ ì¤¬ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì›ë˜ ì„±ê³µí•  í™•ë¥ ì´ ë†’ì€ í•™ìƒì´ ì„¸ë¯¸ë‚˜ì— ì°¸ì„í•  í™•ë¥ ì´ ë” ë†’ìŒ. > Think about it. If they have the exact same probability of receiving the treatment, the only reason one of them received it and the other did not is pure chance. > Holding the propensity score constant acts in a way of making the data look as good as random. ì°¸ì„í•  í™•ë¥ ì„ ê°™ê²Œ ë§Œë“ ë‹¤ë©´, ì›ë˜ ëª©ì ì²˜ëŸ¼ ì°¸ì„í•  í™•ë¥ ì´ ëœë¤í•˜ê²Œ ëœë‹¤. # Propensity Weighting $$ E[Y X, T = 1] - E[Y X, T = 0] = E[\\frac{Y}{e(x)} X, T = 1]P(T) - E\\frac{Y}{(1 - e(x))} X, T = 0) $$ ì²˜ì¹˜ë¥¼ ë°›ì€ ê²ƒ ì¤‘ì— ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì„ ê²ƒ ê°™ì€ ìƒ˜í”Œ ($e(x)$ê°€ ë‚®ì€ ê²½ìš°)ëŠ” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³ , ë°˜ëŒ€ë¡œ ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì€ ê²ƒ ì¤‘ì— ì²˜ì¹˜ë¥¼ ë°›ì„ ê²ƒ ê°™ì€ ìƒ˜í”Œ ($e(x)$ê°€ ë†’ì€ ê²½ìš°)ëŠ” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ”. ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ëŠ” ì§ê´€ì ì¸ ìƒê°ê³¼ ì¼ì¹˜í•œë‹¤. ì™œëƒí•˜ë©´ ì²˜ì¹˜ë¥¼ ë°›ì€ ê²ƒ ì¤‘ì— ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì´ ë†’ì€ ê±´ ëœë¤ì— ê°€ê¹ê²Œ í•˜ë ¤ë©´ ë‚®ì¶°ì•¼ í•˜ê³ , ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì´ ë‚®ì€ ê±´ ëœë¤ì— ê°€ê¹ê²Œ í•˜ë ¤ë©´ ë†’ì—¬ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. > This estimator is called the Inverse Probability of Treatment Weighting (IPTW), since it scales each unit by the inverse probability of the treatment it received. !iptw ì ìš©í•œ ì˜ˆì œ",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/PropensityScore",
    "title": "Propensity Score",
    "date": "2025-07-17",
    "excerpt": "Propensity Score",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "Propensity Score ì´ ì±…ì˜ ì €ìê°€ ì°¸ ê¸€ì„ ì˜ì“´ë‹¤. ë˜ê²Œ ë§¤ë ¥ì ì¸ ì£¼ì œë¡œ ì´ ë‚´ìš©ì„ í’€ì–´ê°„ë‹¤. Growth Mindset vs Fixed Mindset ì‹¬ë¦¬í•™ìë“¤ì€ ë§ˆì¸ë“œì…‹ì´ ì¸ìƒì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ë‚´ ëŠ¥ë ¥ì´ ê³ ì •ë˜ì–´ìˆì§€ ì•Šê³  ì¶©ë¶„íˆ ë³€í™”í•  ìˆ˜ ìˆë‹¤ê³  ë¯¿ëŠ” ì‚¬ëŒë“¤ì€, ë‚´ ëŠ¥ë ¥ì´ ê³ ì •ë˜ì–´ìˆë‹¤ê³  ë¯¿ëŠ” ì‚¬ëŒë³´ë‹¤ ë” ì„±ê³µí•œë‹¤. ë‚˜ëŠ” ì´ ë¶€ë¶„ì— í¬ê²Œ ê³µê°í•˜ê³ , ì„±ì¥í˜• ë§ˆì¸ë“œì…‹ì„ ê°€ì§€ë ¤ê³  í•œë‹¤. ë‹¤ë§Œ ì¸ê³¼ì¶”ë¡ ì„ ë‹¤ë£¨ëŠ” ì…ì¥ì—ì„œëŠ” ì´ì— ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¬¸ì„ ë˜ì ¸ë³¼ ìˆ˜ ìˆë‹¤. > Is it that a growth mindset causes people to achieve more? Or is simply the case that people who achieve more are prone to develop a growth mindset as a result of their success? ì§„ì§œ ì„±ì¥í˜• ì‚¬ê³ ë°©ì‹ì´ ì‚¬ëŒë“¤ì„ ë” ì„±ê³µí•˜ê²Œ í–ˆë‚˜? ì•„ë‹ˆë©´ ì„±ê³µí•œ ì‚¬ëŒë“¤ì´ ê·¸ëŸ° ë§ˆì¸ë“œì…‹ì„ ê°€ì§€ê³  ìˆì„ í™•ë¥ ì´ ë†’ì€ê±´ê°€? ë˜ê²Œ ì¬ë°ŒëŠ” ì§ˆë¬¸ì´ë‹¤. ì´ ë¶€ë¶„ì„ ìœ„í•œ ì‹¤í—˜ì´ ì§„í–‰ëœê²Œ ìˆêµ¬ë‚˜. > To settle things, researchers designed the The National Study of Learning Mindsets. It is a randomised study conducted in U.S. public high schools which aims at finding the impact of a growth mindset. > The real data on this study is not publicly available in order to preserve studentsâ€™ privacy. However, we have a simulated dataset with the same statistical properties provided by Athey and Wager, so we will use that instead. ë…¸íŠ¸ë¶ ì°¸ê³ : Propensity Score Notebook",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimationNotebook",
    "title": "Doubly Robust Estimation Notebook",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "--- title: \"Doubly Robust Estimation Notebook\" date: \"2025-07-17\" excerpt: \"Doubly Robust Estimation Notebook\" category: \"Causal Inference\" tags: [\"Causal Inference\"] --- Doubly Robust Estimation import warnings warnings.filterwarnings('ignore') import pandas as pd import numpy as np from matplotlib import style from matplotlib import pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression, LinearRegression %matplotlib inline style.use(\"fivethirtyeight\") pd.set_option(\"display.max_columns\", None) # í•™ìƒë“¤ì˜ ë§ˆì¸ë“œì…‹ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì„¸ë¯¸ë‚˜ë¥¼ ì§„í–‰í–ˆëŠ”ë°, ì´ê²Œ ì–´ëŠì •ë„ ì˜í–¥ì´ ìˆì—ˆëŠ”ì§€ ê°€ìƒì˜ ë°ì´í„° ì„¸ë¯¸ë‚˜ ì°¸ì„ ì—¬ë¶€ -> intervention ì–¼ë§ˆë‚˜ ì„±ê³µí–ˆëŠ”ì§€ í˜¹ì€ ì„±ê³µí•  í™•ë¥  -> success_expect data = pd.read_csv(\"./data/learning_mindset.csv\") data.sample(5, random_state=5) # ì„±ê³µí•œ ì •ë„ (success expect)ê°€ ë†’ì„ìˆ˜ë¡ ì„¸ë¯¸ë‚˜ ì°¸ì„ ì—¬ë¶€(intervention)ê°€ ë†’ì€ì§€ data.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) í•™ìƒì˜ ìš”ì¸ì— ë”°ë¼ ì°¸ì„ ì—¬ë¶€ê°€ ê²°ì •ë˜ê³ , ë˜ ì´ ìš”ì¸ì´ ì„±ê³µí™•ë¥ ì„ ê²°ì •í•œë‹¤. -> confounding variable. ì¦‰, ë‹¨ìˆœíˆ ì°¸ì„ ì—¬ë¶€ì— ë”°ë¼ ì„±ê³µí™•ë¥ ì„ ë¹„êµí•´ ì„¸ë¯¸ë‚˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸°ì—” ì–´ë µë‹¤. í•™ìƒì˜ ë‹¤ë¥¸ ìš”ì¸ë“¤ì´ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìœ¼ë‹ˆê¹Œ # regressionì„ í†µí•´ í¸í–¥ì„ ì œê±°í•˜ê³  ì„¸ë¯¸ë‚˜ì˜ íš¨ê³¼ ë³´ê¸° ì´ë¥¼ ìœ„í•´ categorical featuresë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜ categ = [\"ethnicity\", \"gender\", \"school_urbanicity\"] cont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"] data_with_categ = pd.concat([ data.drop(columns=categ), # dataset without the categorical features pd.get_dummies(data[categ], columns=categ, drop_first=False) # categorical features converted to dummies ], axis=1) print(data_with_categ.shape)",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimation",
    "title": "Doubly Robust Estimation",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "Doubly Robust Estimation",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-16-ê·¸ë‚˜ì €ë‚˜ì†Œê°œíŒ…ì–´ë• ì–´",
    "title": "ê·¸ë‚˜ì €ë‚˜ ì†Œê°œíŒ… ì–´ë• ì–´?",
    "date": "2025-07-16",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ê³ ë¯¼ ë° ì¡°ì–¸í•˜ëŠ” í‘œí–”"
    ],
    "content": "ë°°ìš´ ê²ƒ How was it? ê·¸ë‚˜ì €ë‚˜, ê·¸ëŸ°ë° By the way, how was the blind date? BTW (By The Way) ì¹œêµ¬ ì‚¬ì´ì—ì„œ ê°€ë³ê²Œ ì“°ëŠ” ë§ By the way, how was the dinner yesterday? Blind date ì†Œê°œíŒ… Well, the thing is he is a vegeterian. ê¸€ì„, ë¬¸ì œëŠ” out of budget to make it somewhere : ì–´ë””ë“  ê°€ë‹¤ ê°„ë‹¨í•œ ì‘ë¬¸. Simple writing or essay Daily talk to myself as usual. By the way, How was your day? Did you do your best? Well, the thing is condition. Today, I was so tired. So I can fully concentrate. So I think I should have a sleep early. ìˆ˜ì •ëœ ë²„ì „ (Improved Version) Just having my usual daily conversation with myself. By the way, how was your day? Did you do your best? Well, the thing is my condition wasn't great. Today, I was so tired that I couldn't fully concentrate. So I think I should go to sleep early tonight. í”¼ë“œë°± (Feedback) ì£¼ìš” ìˆ˜ì • ì‚¬í•­: \"Daily talk to myself\" â†’ \"Having my usual daily conversation with myself\" ë” ì™„ì „í•œ ë¬¸ì¥ êµ¬ì¡°ë¡œ ìˆ˜ì • \"Well, the thing is condition\" â†’ \"Well, the thing is my condition wasn't great\" ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìˆ˜ì • ì†Œìœ ê²© \"my\" ì¶”ê°€ ë° êµ¬ì²´ì ì¸ ì„¤ëª… ì¶”ê°€ \"So I can fully concentrate\" â†’ \"So I couldn't fully concentrate\" ë§¥ë½ìƒ ë¶€ì •ë¬¸ì´ ë§ìŒ (í”¼ê³¤í•´ì„œ ì§‘ì¤‘í•  ìˆ˜ ì—†ì—ˆë‹¤ëŠ” ì˜ë¯¸) \"I should have a sleep early\" â†’ \"I should go to sleep early tonight\" \"go to sleep\"ì´ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ \"tonight\" ì¶”ê°€ë¡œ ì‹œê°„ ëª…ì‹œ ë¬¸ë²• í¬ì¸íŠ¸: \"so + í˜•ìš©ì‚¬/ë¶€ì‚¬ + that + ì£¼ì–´ + ë™ì‚¬\" êµ¬ë¬¸ ì‚¬ìš© ì‹œì œ ì¼ì¹˜ (ê³¼ê±° ìƒí™©ì„ ì„¤ëª…í•  ë•ŒëŠ” ê³¼ê±°í˜• ì‚¬ìš©) ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ í‘œí˜„ ì‚¬ìš©",
    "public": true
  },
  {
    "id": "Self_Development/Book/ë§Œë“¤ì–´ì§„ ì‹  (The God Delusion)/ì±…ì„ì½ê¸°ì‹œì‘í•˜ë©´ì„œ",
    "title": "ì±…ì„ ì½ê¸° ì‹œì‘í•˜ë©´ì„œ",
    "date": "2025-07-16",
    "excerpt": "ì±…ì„ ì½ê¸° ì „ì— ì–´ë–¤ ìƒê°ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ì •ë¦¬",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "ë§Œë“¤ì–´ì§„ ì‹ "
    ],
    "content": "ë‚˜ëŠ” í™•ì‹¤í•œ ë¬´ì‹ ë¡ ìë‹¤. ë¶€ëª¨ë‹˜ì˜ ì˜í–¥ë„ ìˆê² ì§€ë§Œ, ì¹œêµ¬ë”°ë¼ êµíšŒì— ë‹¤ë…”ë˜ ê²½í—˜, ë‹¤ì–‘í•œ ë§¤ì²´ì—ì„œ ì ‘í•œ í˜„ìƒë“¤ ë“±ë“±ì´ ì§€ê¸ˆ ë‚˜ì˜ ìƒê°ì„ ë§Œë“¤ì—ˆë‹¤. ì‹ ì´ ìˆëŠ”ê²Œ ë§ì´ ë˜ë‚˜? ë‚˜ì˜ ì‚¬ê³ ë°©ì‹ì€ ì§ì—…ì ì¸ ì˜í–¥ (ê°œë°œì or ë°ì´í„°ê³¼í•™ì) ë•Œë¬¸ì¸ì§€ ëª°ë¼ë„ ê·¼ê±°ì— ì§€ë°°ì ì´ë‹¤. ê·¼ê±°ê°€ ëª…í™•í•˜ë‹¤ë©´ ì§€ê¸ˆë„ ë‹¹ì¥ ë°”ë€” ìˆ˜ ìˆê³ , ê·¼ê±°ê°€ ì—†ë‹¤ë©´ ë°±ë‚  ì„¤ë“ì„ í•˜ë ¤í•´ë„ ì „í˜€ ì†Œìš©ì´ ì—†ì„ ê²ƒì´ë‹¤. ë‚´ê°€ ì´ ì±…ì„ ì½ê²Œ ëœ ì´ìœ ëŠ” ì‹ ì´ ì—†ëŠ” ì´ìœ ë¥¼ ì•Œê³  ì‹¶ë‹¤ê¸° ë³´ë‹¤ëŠ”, ì‚¬ëŒë“¤ì´ ì™œ ì‹ ì„ ë¯¿ëŠ”ì§€ê°€ ë” ê¶ê¸ˆí•œê²Œ í¬ë‹¤. ì–´ì¨Œë“  ì±…ì—ì„œëŠ” ì‚¬ëŒë“¤ì´ ì‹ ì„ ë¯¿ëŠ” ì´ìœ ë¥¼ ì–¸ê¸‰í•˜ê³  ì´ë¥¼ ë°˜ë°•í• í…Œë‹ˆê¹Œ. ì´ ì±…ì˜ ì˜ì–´ ì œëª©ì€ ë˜ê²Œ íŒŒê²©ì ì´ë‹¤. í—›ì›ƒìŒì´ ë‚˜ì˜¬ì •ë„ë¡œ. Delusion, ë§ìƒì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì“°ë‹¤ë‹ˆ... > ëˆ„êµ°ê°€ ë§ìƒì— ì‹œë‹¬ë¦¬ë©´ ì •ì‹  ì´ìƒì´ë¼ê³  í•œë‹¤. > ë‹¤ìˆ˜ê°€ ë§ìƒì— ì‹œë‹¬ë¦¬ë©´ ì¢…êµë¼ê³  í•œë‹¤. > -> ë¡œë²„íŠ¸ í¼ì‹œê·¸ ì‚¬ì‹¤ ë‚˜ë„ ë¹„ìŠ·í•œ ìƒê°ì´ê¸´ í•˜ë‹¤. ë‚´ê°€ ìƒê°í•˜ëŠ” ì‹ ì€ ê³µí†µì ìœ¼ë¡œ ê·¸ë¦¬ê³  ìˆëŠ” í—ˆìƒì´ë¼ê³  ìƒê°í•œë‹¤. ì˜ˆì „ì— ëª¨ì‹œë˜ í˜¸ë‘ì´ ì‹ ê³¼ ì˜ˆìˆ˜ëŠ” ì‚¬ì‹¤ ê°™ë‹¤ê³  ë³´ì—¬ì§„ë‹¤. ì‚¬ëŒë“¤ì˜ í¸ì˜ë¥¼ ìœ„í•´ ì–´ë–¤ í—ˆìƒì˜ ê°œë…ì„ ë§Œë“¤ê³ , ì´ë¥¼ í•¨ê»˜ ë¯¿ëŠ” ê²ƒ. ì£¼ìœ„ì— ì¢…êµë¥¼ ê°€ì§„ ì¹œêµ¬ë“¤ì´ ë§ì€ë° ê·¸ë“¤ì„ ë¬´ì‹œí•˜ëŠ” ìƒê°ì€ ì§„ì§œ ì „í˜€ì—†ë‹¤. ê·¸ëƒ¥ ë‚˜ì˜ ìƒê°ì´ ì´ëŸ´ ë¿. ë˜ í—ˆìƒì´ë©´ ì–´ë–»ë‚˜ ì‚¶ì— ë„ì›€ì´ ë˜ë©´ ê·¸ê²Œ ì¢‹ì€ê±°ì§€ ë¼ëŠ” ìƒê°ë„ í•œë‹¤. ì–´ì¨Œë“  ì±…ì„ ì½ì–´ë‚˜ê°€ë©´ì„œ ë‚˜ì˜ ìƒê°ì€ ì–´ë–»ê²Œ ë°”ë€Œê³ , ë˜ ì–´ë–¤ ê´€ì ì´ ì—´ë¦´ì§€ ê¸°ëŒ€ê°€ ëœë‹¤. ì­‰ ì½ì–´ë´ì•¼ê² ë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/tikhonov-regularization-and-convex-optimization",
    "title": "Tikhonov Regularizationê³¼ Convex Optimization: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½",
    "date": "2025-07-16",
    "excerpt": "Ill-posed problemì„ í•´ê²°í•˜ëŠ” Tikhonov regularizationê³¼ convex optimizationì˜ ì›ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Optimization",
      "Regularization"
    ],
    "content": "ê°œìš” --- ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ R-learnerë¥¼ ì ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Tikhonov regularizationê³¼ convex optimization ì´ë¡ ì´ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ëŸ¬í•œ ìˆ˜í•™ì  ë„êµ¬ë“¤ì˜ ì›ë¦¬ì™€ ì‘ìš©ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. Ill-posed Problemê³¼ Non-identification --- 1.1 Well-posed vs Ill-posed Problem Well-posed problem (ì˜ ì •ì˜ëœ ë¬¸ì œ) ì˜ 3ê°€ì§€ ì¡°ê±´: ì¡´ì¬ì„± (Existence): í•´ê°€ ì¡´ì¬í•œë‹¤ ìœ ì¼ì„± (Uniqueness): í•´ê°€ ìœ ì¼í•˜ë‹¤ ì•ˆì •ì„± (Stability): ì…ë ¥ì˜ ì‘ì€ ë³€í™”ê°€ í•´ì˜ ì‘ì€ ë³€í™”ë¥¼ ê°€ì ¸ì˜¨ë‹¤ Ill-posed problemì€ ì´ ì¤‘ í•˜ë‚˜ ì´ìƒì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” ë¬¸ì œì…ë‹ˆë‹¤. 1.2 ì—°ì†í˜• ì²˜ì¹˜ì—ì„œì˜ Non-identification ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œëŠ” ìœ ì¼ì„± ì¡°ê±´ì´ ìœ„ë°˜ë©ë‹ˆë‹¤: $$L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right]$$ ì´ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” í•´ì§‘í•©ì€: $$S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\}$$ > ë¬¸ì œ: ë¬´í•œíˆ ë§ì€ í•´ê°€ ì¡´ì¬í•˜ì—¬ ìœ ì¼í•œ CATE í•¨ìˆ˜ $\\tau(x, t)$ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ìŒ Tikhonov Regularization --- 2.1 ê¸°ë³¸ ì›ë¦¬ Tikhonov regularizationì€ ill-posed problemì„ well-posed problemìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì›ë˜ ë¬¸ì œ: $$\\min_{h} L(h)$$ ì •ê·œí™”ëœ ë¬¸ì œ: $$\\min_{h} L(h) + \\rho \\ h\\ ^2$$ ì—¬ê¸°ì„œ: $\\rho > 0$: ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ $\\ h\\ ^2$: ì •ê·œí™” í•­ (ì¼ë°˜ì ìœ¼ë¡œ $L_2$ norm) 2.2 ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œì˜ ì ìš© $$ \\begin{equation} \\tau_\\rho = \\arg \\min_{h \\in L_2^P(X,T)} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right] \\end{equation} $$ ì •ê·œí™” í•­ì˜ ì—­í• : $\\ h\\ _{L_2^P}^2 = E\\{h^2(X, T)\\}$ í•¨ìˆ˜ì˜ \"í¬ê¸°\"ë¥¼ ì œí•œí•˜ì—¬ ê³¼ì í•© ë°©ì§€ ë¬´í•œíˆ ë§ë˜ í•´ë¥¼ ìœ ì¼í•œ í•´ë¡œ ìˆ˜ë ´ì‹œí‚´ 2.3 Tikhonov ì •ë¦¬ ì •ë¦¬: ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ $\\rho > 0$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì •ê·œí™”ëœ ë¬¸ì œëŠ” ìœ ì¼í•œ í•´ $\\tau_\\rho$ë¥¼ ê°€ì§„ë‹¤. ì¦ëª… ì•„ì´ë””ì–´: $L_c(h)$ëŠ” convexí•˜ì§€ë§Œ strictly convexí•˜ì§€ ì•ŠìŒ $\\rho \\ h\\ _{L_2^P}^2$ëŠ” strictly convexí•¨ ë‘˜ì˜ í•©ì€ strictly convexê°€ ë˜ì–´ ìœ ì¼í•œ ìµœì†Ÿê°’ì„ ê°€ì§ Convex Optimization --- 3.1 Convex Functionì˜ ì •ì˜ í•¨ìˆ˜ $f: \\mathbb{R}^n \\to \\mathbb{R}$ê°€ convexí•˜ë‹¤ëŠ” ê²ƒì€: $$f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1-\\lambda) f(y)$$ ëª¨ë“  $x, y \\in \\text{dom}(f)$ì™€ $\\lambda \\in [0, 1]$ì— ëŒ€í•´ ì„±ë¦½í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 3.2 Strictly Convex Function í•¨ìˆ˜ $f$ê°€ strictly convexí•˜ë‹¤ëŠ” ê²ƒì€: $$f(\\lambda x + (1-\\lambda) y) < \\lambda f(x) + (1-\\lambda) f(y)$$ $x \\neq y$ì´ê³  $\\lambda \\in (0, 1)$ì¼ ë•Œ ë¶€ë“±í˜¸ê°€ ì—„ê²©í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 3.3 Convex Functionì˜ ì¤‘ìš”í•œ ì„±ì§ˆ ì„±ì§ˆ 1: ìœ ì¼í•œ ìµœì†Ÿê°’ Convex function: ì§€ì—­ ìµœì†Ÿê°’ = ì „ì—­ ìµœì†Ÿê°’ Strictly convex function: ìµœì†Ÿê°’ì´ ìœ ì¼í•¨ ì„±ì§ˆ 2: ìµœì í™”ì˜ ìš©ì´ì„± ê²½ì‚¬í•˜ê°•ë²•ì´ ì „ì—­ ìµœì†Ÿê°’ìœ¼ë¡œ ìˆ˜ë ´ ë³´ì¥ ìˆ˜ì¹˜ì  ì•ˆì •ì„± ì„±ì§ˆ 3: Jensen's Inequality $$E[f(X)] \\geq f(E[X])$$ 3.4 R-learnerì—ì„œì˜ Convexity ì›ë˜ ì†ì‹¤í•¨ìˆ˜ $L_c(h)$: Convexí•˜ì§€ë§Œ strictly convexí•˜ì§€ ì•ŠìŒ ë”°ë¼ì„œ ìµœì†Ÿê°’ì´ ìœ ì¼í•˜ì§€ ì•ŠìŒ ì •ê·œí™”ëœ ì†ì‹¤í•¨ìˆ˜ $L_c(h) + \\rho \\ h\\ ^2$: $\\rho \\ h\\ ^2$ê°€ strictly convexí•¨ ì „ì²´ í•¨ìˆ˜ê°€ strictly convexê°€ ë¨ ìœ ì¼í•œ ìµœì†Ÿê°’ ë³´ì¥ ìˆ˜í•™ì  ì¦ëª… --- 4.1 $\\ h\\ _{L_2^P}^2$ê°€ Strictly Convexì„ì„ ì¦ëª… ì •ë¦¬: $\\ h\\ _{L_2^P}^2 = E\\{h^2(X, T)\\}$ëŠ” strictly convexí•˜ë‹¤. ì¦ëª…: ì„ì˜ì˜ $h_1 \\neq h_2$ì™€ $\\lambda \\in (0, 1)$ì— ëŒ€í•´: $$ \\begin{align} \\ \\lambda h_1 + (1-\\lambda) h_2\\ _{L_2^P}^2 &= E\\{[\\lambda h_1(X,T) + (1-\\lambda) h_2(X,T)]^2\\} \\\\ &= E\\{\\lambda^2 h_1^2(X,T) + 2\\lambda(1-\\lambda) h_1(X,T)h_2(X,T) + (1-\\lambda)^2 h_2^2(X,T)\\} \\\\ &= \\lambda^2 E\\{h_1^2(X,T)\\} + 2\\lambda(1-\\lambda) E\\{h_1(X,T)h_2(X,T)\\} + (1-\\lambda)^2 E\\{h_2^2(X,T)\\} \\end{align} $$ Cauchy-Schwarz ë¶€ë“±ì‹ì— ì˜í•´: $$E\\{h_1(X,T)h_2(X,T)\\} \\leq \\sqrt{E\\{h_1^2(X,T)\\}} \\sqrt{E\\{h_2^2(X,T)\\}}$$ $h_1 \\neq h_2$ì¼ ë•Œ ë¶€ë“±í˜¸ê°€ strictí•˜ë¯€ë¡œ: $$\\ \\lambda h_1 + (1-\\lambda) h_2\\ _{L_2^P}^2 < \\lambda \\ h_1\\ _{L_2^P}^2 + (1-\\lambda) \\ h_2\\ _{L_2^P}^2$$ 4.2 ì •ê·œí™”ëœ ì†ì‹¤í•¨ìˆ˜ì˜ Strict Convexity ê²°ë¡ : $L_{c,\\ell_2}(h \\mid \\rho) = L_c(h) + \\rho \\ h\\ _{L_2^P}^2$ëŠ” $\\rho > 0$ì¼ ë•Œ strictly convexí•˜ë‹¤. ì´ìœ : $L_c(h)$ëŠ” convex $\\rho \\ h\\ _{L_2^P}^2$ëŠ” strictly convex Convex + Strictly convex = Strictly convex ì‹¤ì œ ì‘ìš©ê³¼ ì˜ë¯¸ --- 5.1 Two-step Procedure Step 1: Tikhonov regularizationìœ¼ë¡œ ì¤‘ê°„ í•¨ìˆ˜ ì¶”ì • $$\\tilde{\\tau}_\\rho = \\arg \\min_{h} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right]$$ Step 2: Zero-constraining operatorë¡œ ìµœì¢… CATE ì¶”ì • $$\\tau(x, t) = \\text{ZeroConstraint}(\\tilde{\\tau}_\\rho(x, t))$$ 5.2 ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ $\\rho$ì˜ ì—­í•  $\\rho$ê°€ ì‘ì„ ë•Œ: ì›ë˜ ë¬¸ì œì— ê°€ê¹Œì›€ BiasëŠ” ì‘ì§€ë§Œ varianceê°€ í´ ìˆ˜ ìˆìŒ $\\rho$ê°€ í´ ë•Œ: ì •ê·œí™” íš¨ê³¼ê°€ ê°•í•¨ VarianceëŠ” ì‘ì§€ë§Œ biasê°€ í´ ìˆ˜ ìˆìŒ ìµœì  $\\rho$ ì„ íƒ: Cross-validation Information criteria (AIC, BIC) Theoretical convergence rate 5.3 ìˆ˜ë ´ ì†ë„ì™€ ì ê·¼ì  ì„±ì§ˆ ì •ê·œí™”ëœ ì¶”ì •ëŸ‰ $\\hat{\\tau}_\\rho$ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš°ìˆ˜í•œ ì„±ì§ˆì„ ê°€ì§‘ë‹ˆë‹¤: Consistency: $\\hat{\\tau}_\\rho \\to \\tau$ as $n \\to \\infty$ Convergence rate: $O_p(n^{-r})$ for some $r > 0$ Asymptotic normality: $\\sqrt{n}(\\hat{\\tau}_\\rho - \\tau) \\xrightarrow{d} N(0, \\Sigma)$ ë‹¤ë¥¸ ì •ê·œí™” ë°©ë²•ê³¼ì˜ ë¹„êµ --- 6.1 Ridge vs Lasso vs Tikhonov ë°©ë²• ì •ê·œí™” í•­ íŠ¹ì§• ------ ----------- ------ Ridge $\\lambda \\ \\beta\\ _2^2$ ê³„ìˆ˜ ì¶•ì†Œ, ë³€ìˆ˜ ì„ íƒ X Lasso $\\lambda \\ \\beta\\ _1$ ë³€ìˆ˜ ì„ íƒ, í¬ì†Œì„± Tikhonov $\\rho \\ h\\ _{L_2^P}^2$ í•¨ìˆ˜ ê³µê°„ì—ì„œì˜ ì •ê·œí™” 6.2 ë‹¤ë¥¸ í•´ê²° ë°©ë²•ë“¤ Alternative 1: Truncated SVD íŠ¹ì´ê°’ ë¶„í•´ í›„ ì‘ì€ íŠ¹ì´ê°’ ì œê±° ìˆ˜ì¹˜ì  ë°©ë²• Alternative 2: Iterative methods Landweber iteration Conjugate gradient methods Alternative 3: Bayesian approach Prior distribution ì„¤ì • Posterior inference ê²°ë¡  --- Tikhonov regularizationì˜ í•µì‹¬ ì¥ì : ì´ë¡ ì  ë³´ì¥: Strictly convex optimizationìœ¼ë¡œ ìœ ì¼í•œ í•´ ë³´ì¥ ìˆ˜ì¹˜ì  ì•ˆì •ì„±: Well-conditioned ë¬¸ì œë¡œ ë³€í™˜ ì¼ë°˜ì„±: ë‹¤ì–‘í•œ ill-posed problemì— ì ìš© ê°€ëŠ¥ êµ¬í˜„ ìš©ì´ì„±: ê¸°ì¡´ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ í™œìš© ê°€ëŠ¥ ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œì˜ ì˜ë¯¸: Non-identification ë¬¸ì œì˜ ê·¼ë³¸ì  í•´ê²° ì‹¤ìš©ì ì¸ CATE ì¶”ì • ë°©ë²• ì œê³µ ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤ì œ êµ¬í˜„ì˜ êµëŸ‰ ì—­í•  ì´ëŸ¬í•œ ìˆ˜í•™ì  ë„êµ¬ë“¤ì€ ë‹¨ìˆœíˆ ê¸°ìˆ ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ì–´ë ¤ì›€ì„ ì´í•´í•˜ê³  ê·¹ë³µí•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. --- ì°¸ê³  ë¬¸í—Œ: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization Boyd, S., & Vandenberghe, L. (2004). Convex optimization Wahba, G. (1990). Spline models for observational data Engl, H. W., Hanke, M., & Neubauer, A. (2000). Regularization of inverse problems",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/DefinitionOfArt",
    "title": "Definition of Art",
    "date": "2025-07-15",
    "excerpt": "TOEFL Reading ë¬¸ì œ ê³µë¶€ - ì˜ˆìˆ ì˜ ì •ì˜",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›"
    ],
    "content": "Definition of Art ì˜ˆìˆ ì˜ ì •ì˜ Paragraph 1 <button class=\"toggle-button translations\" data-type=\"paragraph1\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph1-content\"> > How art is defined has varied throughout history, leading to a wide range of sometimes-contradictory meanings. <span class=\"translation\">ì˜ˆìˆ ì´ ì–´ë–»ê²Œ ì •ì˜ë˜ëŠ”ì§€ëŠ” ì—­ì‚¬ë¥¼ í†µí‹€ì–´ ë³€í™”í•´ì™”ìœ¼ë©°, ë•Œë¡œëŠ” ìƒë°˜ë˜ëŠ” ì˜ë¯¸ë“¤ì˜ ê´‘ë²”ìœ„í•œ ìŠ¤í™íŠ¸ëŸ¼ì„ ë§Œë“¤ì–´ëƒˆë‹¤.</span> > In the ancient world, the term was used in a broad sense that was not strictly related to visual arts. <span class=\"translation\">ê³ ëŒ€ ì„¸ê³„ì—ì„œ ì´ ìš©ì–´ëŠ” ì‹œê° ì˜ˆìˆ ì— ì—„ê²©í•˜ê²Œ êµ­í•œë˜ì§€ ì•ŠëŠ” ê´‘ë²”ìœ„í•œ ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.</span> > The application of the word \"art\" exclusively to works in the form of paintings, sculptures, and prints came about only in the Renaissance, and later, in the eighteenth century, the use of the term expanded to include music and poetry. <span class=\"translation\">\"ì˜ˆìˆ \"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ íšŒí™”, ì¡°ê°, íŒí™” í˜•íƒœì˜ ì‘í’ˆë“¤ì—ë§Œ ë…ì ì ìœ¼ë¡œ ì ìš©í•˜ëŠ” ê²ƒì€ ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ì— ë¹„ë¡œì†Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´í›„ 18ì„¸ê¸°ì—ëŠ” ì´ ìš©ì–´ì˜ ì‚¬ìš©ì´ ìŒì•…ê³¼ ì‹œë¥¼ í¬í•¨í•˜ë„ë¡ í™•ì¥ë˜ì—ˆë‹¤.</span> > Subsequently, in modern times, a new meaning came into fashion, obscuring the distinction between what is art and what is not. <span class=\"translation\">ê·¸ í›„ í˜„ëŒ€ì— ë“¤ì–´ì„œëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ê°€ ìœ í–‰í•˜ê²Œ ë˜ì—ˆê³ , ë¬´ì—‡ì´ ì˜ˆìˆ ì´ê³  ë¬´ì—‡ì´ ì˜ˆìˆ ì´ ì•„ë‹Œì§€ì˜ êµ¬ë³„ì„ ëª¨í˜¸í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤.</span> </div> Paragraph 2 <button class=\"toggle-button translations\" data-type=\"paragraph2\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph2-content\"> > In the ancient world, there was no clear difference between art and crafts. <span class=\"translation\">ê³ ëŒ€ ì„¸ê³„ì—ì„œëŠ” ì˜ˆìˆ ê³¼ ê³µì˜ˆ ì‚¬ì´ì— ëª…í™•í•œ ì°¨ì´ê°€ ì—†ì—ˆë‹¤.</span> > In Greek and Latin, the terms techne and ars both translated as \"art,\" and they stood for any activity that had rules and required expertise. <span class=\"translation\">ê·¸ë¦¬ìŠ¤ì–´ì™€ ë¼í‹´ì–´ì—ì„œ í…Œí¬ë„¤(techne)ì™€ ì•„ë¥´ìŠ¤(ars)ë¼ëŠ” ìš©ì–´ë“¤ì€ ë‘˜ ë‹¤ \"ì˜ˆìˆ \"ë¡œ ë²ˆì—­ë˜ì—ˆìœ¼ë©°, ê·œì¹™ì„ ê°€ì§€ê³  ì „ë¬¸ì„±ì„ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í™œë™ì„ ì˜ë¯¸í–ˆë‹¤.</span> > They were thus used in reference to a wide range of fields, from medicine and law to blacksmithing. <span class=\"translation\">ë”°ë¼ì„œ ì´ ìš©ì–´ë“¤ì€ ì˜í•™ê³¼ ë²•ë¥ ì—ì„œë¶€í„° ëŒ€ì¥ì¥ì´ ì¼ê¹Œì§€ ê´‘ë²”ìœ„í•œ ë¶„ì•¼ë“¤ì„ ì§€ì¹­í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤.</span> > Those engaged in the production of art were viewed as craftsmen, more similar to manual laborers than artists. <span class=\"translation\">ì˜ˆìˆ  ì œì‘ì— ì¢…ì‚¬í•˜ëŠ” ì‚¬ëŒë“¤ì€ ì¥ì¸ìœ¼ë¡œ ì—¬ê²¨ì¡Œìœ¼ë©°, ì˜ˆìˆ ê°€ë³´ë‹¤ëŠ” ìœ¡ì²´ ë…¸ë™ìì— ë” ê°€ê¹Œì› ë‹¤.</span> > To the ancients, art was a pursuit necessitating skills that were learned according to rules, but that were believed to require no unique inspiration. <span class=\"translation\">ê³ ëŒ€ì¸ë“¤ì—ê²Œ ì˜ˆìˆ ì€ ê·œì¹™ì— ë”°ë¼ ìŠµë“í•˜ëŠ” ê¸°ìˆ ì„ í•„ìš”ë¡œ í•˜ëŠ” ì¶”êµ¬ì˜€ì§€ë§Œ, ë…íŠ¹í•œ ì˜ê°ì„ ìš”êµ¬í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ê³  ì—¬ê²¨ì¡Œë‹¤.</span> > Painting and sculpture had no special place in higher learning, as only music and poetry were taught as academic subjects. <span class=\"translation\">íšŒí™”ì™€ ì¡°ê°ì€ ê³ ë“± í•™ë¬¸ì—ì„œ íŠ¹ë³„í•œ ìë¦¬ë¥¼ ì°¨ì§€í•˜ì§€ ëª»í–ˆëŠ”ë°, ìŒì•…ê³¼ ì‹œë§Œì´ í•™ë¬¸ ê³¼ëª©ìœ¼ë¡œ ê°€ë¥´ì³ì¡Œê¸° ë•Œë¬¸ì´ë‹¤.</span> > The visual arts were viewed simply as manual crafts, and because the ancients looked down on manual labor, artisans occupied the bottom of the social hierarchy. <span class=\"translation\">ì‹œê° ì˜ˆìˆ ì€ ë‹¨ìˆœíˆ ìˆ˜ê³µ ê¸°ìˆ ë¡œ ì—¬ê²¨ì¡Œìœ¼ë©°, ê³ ëŒ€ì¸ë“¤ì´ ìœ¡ì²´ ë…¸ë™ì„ ì—…ì‹ ì—¬ê²¼ê¸° ë•Œë¬¸ì— ì¥ì¸ë“¤ì€ ì‚¬íšŒ ê³„ì¸µì˜ ë§¨ ì•„ë˜ë¥¼ ì°¨ì§€í–ˆë‹¤.</span> </div> Paragraph 3 <button class=\"toggle-button translations\" data-type=\"paragraph3\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph3-content\"> > The medieval world inherited this view of art, but things began to change during the Renaissance. <span class=\"translation\">ì¤‘ì„¸ ì„¸ê³„ëŠ” ì´ëŸ¬í•œ ì˜ˆìˆ ê´€ì„ ë¬¼ë ¤ë°›ì•˜ì§€ë§Œ, ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ì— ìƒí™©ì´ ë³€í™”í•˜ê¸° ì‹œì‘í–ˆë‹¤.</span> > At the forefront of this transformation was the elevated position of painters, sculptors, and architects. <span class=\"translation\">ì´ ë³€í˜ì˜ ìµœì „ì„ ì—ëŠ” í™”ê°€, ì¡°ê°ê°€, ê±´ì¶•ê°€ë“¤ì˜ ì§€ìœ„ í–¥ìƒì´ ìˆì—ˆë‹¤.</span> > Renaissance artists increasingly employed scientific methods, and mathematics became central to their trade. <span class=\"translation\">ë¥´ë„¤ìƒìŠ¤ ì˜ˆìˆ ê°€ë“¤ì€ ì ì  ë” ê³¼í•™ì  ë°©ë²•ì„ ì‚¬ìš©í–ˆìœ¼ë©°, ìˆ˜í•™ì´ ê·¸ë“¤ ê¸°ì˜ˆì˜ ì¤‘ì‹¬ì´ ë˜ì—ˆë‹¤.</span> > Because their work was not simply mechanical but also intellectual, artists argued that they were superior to mere craftsmen, and their call for respect began to sway public opinion. <span class=\"translation\">ê·¸ë“¤ì˜ ì‘ì—…ì´ ë‹¨ìˆœíˆ ê¸°ê³„ì ì¸ ê²ƒì´ ì•„ë‹ˆë¼ ì§€ì ì´ê¸°ë„ í–ˆê¸° ë•Œë¬¸ì—, ì˜ˆìˆ ê°€ë“¤ì€ ìì‹ ë“¤ì´ ë‹¨ìˆœí•œ ì¥ì¸ë“¤ë³´ë‹¤ ìš°ì›”í•˜ë‹¤ê³  ì£¼ì¥í–ˆìœ¼ë©°, ì¡´ê²½ì„ ìš”êµ¬í•˜ëŠ” ê·¸ë“¤ì˜ ì™¸ì¹¨ì´ ì—¬ë¡ ì„ ì›€ì§ì´ê¸° ì‹œì‘í–ˆë‹¤.</span> > Artists gradually gained acceptance as creative geniuses, and their reputation rose to be on par with that of poets. <span class=\"translation\">ì˜ˆìˆ ê°€ë“¤ì€ ì ì°¨ ì°½ì¡°ì  ì²œì¬ë¡œ ì¸ì •ë°›ê²Œ ë˜ì—ˆìœ¼ë©°, ê·¸ë“¤ì˜ ëª…ì„±ì€ ì‹œì¸ë“¤ê³¼ ë™ë“±í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì˜¬ë¼ê°”ë‹¤.</span> > Painting, sculpture, and architecture were given their own place in academies, and art became synonymous with the label \"classical art.\" <span class=\"translation\">íšŒí™”, ì¡°ê°, ê±´ì¶•ì€ í•™ì›ì—ì„œ ìì‹ ë“¤ë§Œì˜ ìë¦¬ë¥¼ ë¶€ì—¬ë°›ì•˜ìœ¼ë©°, ì˜ˆìˆ ì€ \"ê³ ì „ ì˜ˆìˆ \"ì´ë¼ëŠ” ë¼ë²¨ê³¼ ë™ì˜ì–´ê°€ ë˜ì—ˆë‹¤.</span> > During the 1700s, the definition was slightly expanded as the concept of fine art emerged. <span class=\"translation\">1700ë…„ëŒ€ì—ëŠ” ìˆœìˆ˜ ì˜ˆìˆ ì˜ ê°œë…ì´ ë“±ì¥í•˜ë©´ì„œ ì •ì˜ê°€ ì•½ê°„ í™•ì¥ë˜ì—ˆë‹¤.</span> > Collectively, the classical arts, along with music and poetry, came to represent the fine arts. <span class=\"translation\">ì¢…í•©ì ìœ¼ë¡œ, ê³ ì „ ì˜ˆìˆ ë“¤ì€ ìŒì•…ê³¼ ì‹œì™€ í•¨ê»˜ ìˆœìˆ˜ ì˜ˆìˆ ì„ ëŒ€í‘œí•˜ê²Œ ë˜ì—ˆë‹¤.</span> </div> Paragraph 4 <button class=\"toggle-button translations\" data-type=\"paragraph4\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph4-content\"> > It was also in the Renaissance that the term \"artisan\" first appeared. <span class=\"translation\">\"ì¥ì¸\"ì´ë¼ëŠ” ìš©ì–´ê°€ ì²˜ìŒ ë“±ì¥í•œ ê²ƒë„ ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ì˜€ë‹¤.</span> > The word was used to clearly distinguish between arts and crafts. <span class=\"translation\">ì´ ë‹¨ì–´ëŠ” ì˜ˆìˆ ê³¼ ê³µì˜ˆë¥¼ ëª…í™•íˆ êµ¬ë³„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ì—ˆë‹¤.</span> > Art required academic training, and its goal was to create perfect and idealized representations of reality that followed established principles of beauty. <span class=\"translation\">ì˜ˆìˆ ì€ í•™ë¬¸ì  í›ˆë ¨ì„ ìš”êµ¬í–ˆìœ¼ë©°, ê·¸ ëª©í‘œëŠ” í™•ë¦½ëœ ì•„ë¦„ë‹¤ì›€ì˜ ì›ì¹™ì„ ë”°ë¥´ëŠ” í˜„ì‹¤ì˜ ì™„ë²½í•˜ê³  ì´ìƒí™”ëœ í‘œí˜„ì„ ì°½ì¡°í•˜ëŠ” ê²ƒì´ì—ˆë‹¤.</span> > This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes. <span class=\"translation\">ì´ëŠ” êµ¬ìŠ¬ ì„¸ê³µì´ë‚˜ ë°”êµ¬ë‹ˆ ë§Œë“¤ê¸°ì™€ ê°™ì€ ê³µì˜ˆì™€ ëŒ€ì¡°ë˜ëŠ”ë°, ì´ë“¤ì€ ì£¼ë¡œ ì¼ìƒì ì¸ ëª©ì ì„ ìœ„í•´ ì œì‘ë˜ì—ˆë‹¤.</span> > Craftsmen whose production consisted of such practical, or merely decorative, items were called artisans. <span class=\"translation\">ê·¸ëŸ¬í•œ ì‹¤ìš©ì ì´ê±°ë‚˜ ë‹¨ìˆœíˆ ì¥ì‹ì ì¸ ë¬¼í’ˆë“¤ì„ ìƒì‚°í•˜ëŠ” ì¥ì¸ë“¤ì€ ì•„í‹°ì”ì´ë¼ê³  ë¶ˆë ¸ë‹¤.</span> > They often learned their trade from master craftsmen, but they did not study art in the academies. <span class=\"translation\">ê·¸ë“¤ì€ ì¢…ì¢… ìˆ™ë ¨ëœ ì¥ì¸ë“¤ë¡œë¶€í„° ê¸°ìˆ ì„ ë°°ì› ì§€ë§Œ, í•™ì›ì—ì„œ ì˜ˆìˆ ì„ ê³µë¶€í•˜ì§€ëŠ” ì•Šì•˜ë‹¤.</span> > The clear division between arts and crafts saw its peak in the eighteenth century. <span class=\"translation\">ì˜ˆìˆ ê³¼ ê³µì˜ˆ ì‚¬ì´ì˜ ëª…í™•í•œ êµ¬ë¶„ì€ 18ì„¸ê¸°ì— ì ˆì •ì— ë‹¬í–ˆë‹¤.</span> </div> Paragraph 5 <button class=\"toggle-button translations\" data-type=\"paragraph5\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph5-content\"> > In the nineteenth century, modernists sought to promote the idea of \"art for art's sake\" and rejected classical definitions. <span class=\"translation\">19ì„¸ê¸°ì— ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì€ \"ì˜ˆìˆ ì„ ìœ„í•œ ì˜ˆìˆ \"ì´ë¼ëŠ” ì‚¬ìƒì„ ì´‰ì§„í•˜ë ¤ í–ˆìœ¼ë©° ê³ ì „ì  ì •ì˜ë“¤ì„ ê±°ë¶€í–ˆë‹¤.</span> > Artists felt hindered by the conventions of classicism and began to ignore form in favor of stylistic expression. <span class=\"translation\">ì˜ˆìˆ ê°€ë“¤ì€ ê³ ì „ì£¼ì˜ì˜ ê´€ìŠµì— ì˜í•´ ë°©í•´ë°›ëŠ”ë‹¤ê³  ëŠê¼ˆìœ¼ë©° í˜•íƒœë¥¼ ë¬´ì‹œí•˜ê³  ì–‘ì‹ì  í‘œí˜„ì„ ì„ í˜¸í•˜ê¸° ì‹œì‘í–ˆë‹¤.</span> > At the same time, they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society. <span class=\"translation\">ë™ì‹œì—, ê·¸ë“¤ì€ ì˜ˆìˆ ì´ ê·¼ë³¸ì ìœ¼ë¡œ ì‹¤ìš©ì ì´ê³  ì‚¬íšŒì— ìœ ìµí•´ì•¼ í•œë‹¤ê³  ì£¼ì¥í•˜ëŠ” ì¼ë¶€ ë™ì‹œëŒ€ì¸ë“¤ì˜ ì‹ ë…ì„ ê±°ë¶€í–ˆë‹¤.</span> > Modernists denied that purpose, whether moral or political, was an important criterion for art to have merit. <span class=\"translation\">ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì€ ë„ë•ì ì´ë“  ì •ì¹˜ì ì´ë“  ëª©ì ì´ ì˜ˆìˆ ì´ ê°€ì¹˜ë¥¼ ê°–ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê¸°ì¤€ì´ë¼ëŠ” ê²ƒì„ ë¶€ì¸í–ˆë‹¤.</span> > Accordingly, they believed all art had an intrinsic value of its own. <span class=\"translation\">ë”°ë¼ì„œ, ê·¸ë“¤ì€ ëª¨ë“  ì˜ˆìˆ ì´ ê·¸ ìì²´ë¡œ ë‚´ì¬ì  ê°€ì¹˜ë¥¼ ê°€ì§„ë‹¤ê³  ë¯¿ì—ˆë‹¤.</span> > In addition, artists experimented with new techniques and subject matter. <span class=\"translation\">ë˜í•œ, ì˜ˆìˆ ê°€ë“¤ì€ ìƒˆë¡œìš´ ê¸°ë²•ê³¼ ì£¼ì œë¥¼ ì‹¤í—˜í–ˆë‹¤.</span> > They began to include more impressionistic and abstract designs, emphasizing experimental over learned techniques. <span class=\"translation\">ê·¸ë“¤ì€ ë” ì¸ìƒì£¼ì˜ì ì´ê³  ì¶”ìƒì ì¸ ë””ìì¸ì„ í¬í•¨í•˜ê¸° ì‹œì‘í–ˆìœ¼ë©°, ë°°ìš´ ê¸°ë²•ë³´ë‹¤ ì‹¤í—˜ì ì¸ ê²ƒì„ ê°•ì¡°í–ˆë‹¤.</span> </div> Paragraph 6 <button class=\"toggle-button translations\" data-type=\"paragraph6\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph6-content\"> > This new perception of art as an end in itself eventually erased the fragmentation between artist and artisan. <span class=\"translation\">ê·¸ ìì²´ê°€ ëª©ì ì¸ ì˜ˆìˆ ì— ëŒ€í•œ ì´ ìƒˆë¡œìš´ ì¸ì‹ì€ ê²°êµ­ ì˜ˆìˆ ê°€ì™€ ì¥ì¸ ì‚¬ì´ì˜ ë¶„ì—´ì„ ì§€ì›Œë²„ë ¸ë‹¤.</span> > In a clear mockery of the classical tradition, the notion of art was extended to include anything a person creates. <span class=\"translation\">ê³ ì „ì  ì „í†µì— ëŒ€í•œ ëª…ë°±í•œ ì¡°ë¡±ìœ¼ë¡œ, ì˜ˆìˆ ì˜ ê°œë…ì€ ì‚¬ëŒì´ ì°½ì¡°í•˜ëŠ” ëª¨ë“  ê²ƒì„ í¬í•¨í•˜ë„ë¡ í™•ì¥ë˜ì—ˆë‹¤.</span> > Therefore, it effectively eliminated the distinction between crafts and the so-called fine arts. <span class=\"translation\">ë”°ë¼ì„œ, ê·¸ê²ƒì€ ê³µì˜ˆì™€ ì†Œìœ„ ìˆœìˆ˜ ì˜ˆìˆ  ì‚¬ì´ì˜ êµ¬ë³„ì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í–ˆë‹¤.</span> > A person who produced a basket and a person who painted a canvas in classical style were no different; they were both using their skills to create something. <span class=\"translation\">ë°”êµ¬ë‹ˆë¥¼ ë§Œë“  ì‚¬ëŒê³¼ ê³ ì „ì  ì–‘ì‹ìœ¼ë¡œ ìº”ë²„ìŠ¤ì— ê·¸ë¦¼ì„ ê·¸ë¦° ì‚¬ëŒì€ ë‹¤ë¥´ì§€ ì•Šì•˜ë‹¤; ê·¸ë“¤ì€ ë‘˜ ë‹¤ ë¬´ì–¸ê°€ë¥¼ ì°½ì¡°í•˜ê¸° ìœ„í•´ ìì‹ ì˜ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ê³  ìˆì—ˆë‹¤.</span> > This meant that any creative work could be viewed as genuine and prestigious. <span class=\"translation\">ì´ê²ƒì€ ëª¨ë“  ì°½ì¡°ì  ì‘ì—…ì´ ì§„ì •í•˜ê³  ê¶Œìœ„ ìˆëŠ” ê²ƒìœ¼ë¡œ ì—¬ê²¨ì§ˆ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í–ˆë‹¤.</span> > Further, art moved beyond the realm of productive creativity. <span class=\"translation\">ë” ë‚˜ì•„ê°€, ì˜ˆìˆ ì€ ìƒì‚°ì  ì°½ì¡°ì„±ì˜ ì˜ì—­ì„ ë„˜ì–´ì„°ë‹¤.</span> > Some artists randomly collected everyday objects and displayed them, completely unmodified, as \"creative sculpture.\" <span class=\"translation\">ì¼ë¶€ ì˜ˆìˆ ê°€ë“¤ì€ ì¼ìƒì ì¸ ë¬¼ê±´ë“¤ì„ ë¬´ì‘ìœ„ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì™„ì „íˆ ìˆ˜ì •í•˜ì§€ ì•Šì€ ì±„ë¡œ \"ì°½ì¡°ì  ì¡°ê°\"ìœ¼ë¡œ ì „ì‹œí–ˆë‹¤.</span> </div> Paragraph 7 <button class=\"toggle-button translations\" data-type=\"paragraph7\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph7-content\"> > Conceptions of art have changed remarkably and created much confusion. <span class=\"translation\">ì˜ˆìˆ ì— ëŒ€í•œ ê°œë…ë“¤ì€ í˜„ì €íˆ ë³€í™”í–ˆìœ¼ë©° ë§ì€ í˜¼ë€ì„ ë§Œë“¤ì–´ëƒˆë‹¤.</span> > Today there is no consensus on the meaning of the term, and it is applied very subjectively. <span class=\"translation\">ì˜¤ëŠ˜ë‚  ê·¸ ìš©ì–´ì˜ ì˜ë¯¸ì— ëŒ€í•œ í•©ì˜ê°€ ì—†ìœ¼ë©°, ê·¸ê²ƒì€ ë§¤ìš° ì£¼ê´€ì ìœ¼ë¡œ ì ìš©ëœë‹¤.</span> > Evidence can be seen in art museums and galleries devoted to everything from primitive artifacts to video productions to commercial graphics. <span class=\"translation\">ì¦ê±°ëŠ” ì›ì‹œ ìœ ë¬¼ì—ì„œë¶€í„° ë¹„ë””ì˜¤ ì œì‘ë¬¼, ìƒì—…ì  ê·¸ë˜í”½ì— ì´ë¥´ê¸°ê¹Œì§€ ëª¨ë“  ê²ƒì„ ë‹¤ë£¨ëŠ” ë¯¸ìˆ ê´€ê³¼ ê°¤ëŸ¬ë¦¬ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤.</span> > Certainly, the question of what constitutes art has become largely subjective. <span class=\"translation\">í™•ì‹¤íˆ, ë¬´ì—‡ì´ ì˜ˆìˆ ì„ êµ¬ì„±í•˜ëŠ”ê°€ë¼ëŠ” ì§ˆë¬¸ì€ ëŒ€ì²´ë¡œ ì£¼ê´€ì ì¸ ê²ƒì´ ë˜ì—ˆë‹¤.</span> </div> --- ğŸ“š í•µì‹¬ ë‹¨ì–´ ì‚¬ì „ <button class=\"toggle-button vocabulary\" data-type=\"vocabulary\">ğŸ” ë‹¨ì–´ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"vocabulary-content\"> ğŸ¨ ì˜ˆìˆ ì‚¬ ê´€ë ¨ ìš©ì–´ contradictory <span class=\"meaning\">(ìƒë°˜ë˜ëŠ”, ëª¨ìˆœì ì¸) - ì„œë¡œ ë°˜ëŒ€ë˜ëŠ”</span> exclusively <span class=\"meaning\">(ë…ì ì ìœ¼ë¡œ, ì˜¤ì§) - ë‹¤ë¥¸ ê²ƒì„ ë°°ì œí•˜ê³ </span> Renaissance <span class=\"meaning\">(ë¥´ë„¤ìƒìŠ¤) - 14-16ì„¸ê¸° ìœ ëŸ½ ë¬¸ì˜ˆë¶€í¥ì‹œëŒ€</span> obscure <span class=\"meaning\">(ëª¨í˜¸í•˜ê²Œ í•˜ë‹¤) - ë¶ˆë¶„ëª…í•˜ê²Œ ë§Œë“¤ë‹¤</span> techne <span class=\"meaning\">(í…Œí¬ë„¤) - ê·¸ë¦¬ìŠ¤ì–´ë¡œ \"ê¸°ìˆ , ê¸°ì˜ˆ\"</span> ars <span class=\"meaning\">(ì•„ë¥´ìŠ¤) - ë¼í‹´ì–´ë¡œ \"ê¸°ìˆ , ì˜ˆìˆ \"</span> expertise <span class=\"meaning\">(ì „ë¬¸ì„±) - ì „ë¬¸ì  ê¸°ìˆ ì´ë‚˜ ì§€ì‹</span> blacksmithing <span class=\"meaning\">(ëŒ€ì¥ì¥ì´ ì¼) - ì² ì„ ë‹¤ë£¨ëŠ” ê¸°ìˆ </span> craftsmen <span class=\"meaning\">(ì¥ì¸) - ìˆ˜ê³µì—…ì— ì¢…ì‚¬í•˜ëŠ” ì‚¬ëŒ</span> manual laborers <span class=\"meaning\">(ìœ¡ì²´ ë…¸ë™ì) - ì†ìœ¼ë¡œ ì¼í•˜ëŠ” ì‚¬ëŒë“¤</span> ğŸ›ï¸ í•™ë¬¸/êµìœ¡ ê´€ë ¨ ìš©ì–´ academic subjects <span class=\"meaning\">(í•™ë¬¸ ê³¼ëª©) - í•™êµì—ì„œ ê°€ë¥´ì¹˜ëŠ” ì •ê·œ ê³¼ëª©</span> higher learning <span class=\"meaning\">(ê³ ë“± í•™ë¬¸) - ëŒ€í•™ ìˆ˜ì¤€ì˜ êµìœ¡</span> social hierarchy <span class=\"meaning\">(ì‚¬íšŒ ê³„ì¸µ) - ì‚¬íšŒì˜ ë“±ê¸‰ êµ¬ì¡°</span> academies <span class=\"meaning\">(í•™ì›, ì•„ì¹´ë°ë¯¸) - ì˜ˆìˆ ì´ë‚˜ í•™ë¬¸ì„ ê°€ë¥´ì¹˜ëŠ” ê¸°ê´€</span> classical art <span class=\"meaning\">(ê³ ì „ ì˜ˆìˆ ) - ì „í†µì ì¸ ì–‘ì‹ì„ ë”°ë¥´ëŠ” ì˜ˆìˆ </span> fine arts <span class=\"meaning\">(ìˆœìˆ˜ ì˜ˆìˆ ) - ì‹¤ìš©ì„±ë³´ë‹¤ ë¯¸ì  ê°€ì¹˜ë¥¼ ì¶”êµ¬í•˜ëŠ” ì˜ˆìˆ </span> ğŸ”§ ê¸°ìˆ /ê³µì˜ˆ ê´€ë ¨ ìš©ì–´ artisan <span class=\"meaning\">(ì•„í‹°ì”, ìˆ˜ê³µì—…ì) - ìˆ™ë ¨ëœ ìˆ˜ê³µì—… ì¢…ì‚¬ì</span> beadwork <span class=\"meaning\">(êµ¬ìŠ¬ ì„¸ê³µ) - êµ¬ìŠ¬ì„ ì´ìš©í•œ ê³µì˜ˆ</span> basket making <span class=\"meaning\">(ë°”êµ¬ë‹ˆ ë§Œë“¤ê¸°) - ë°”êµ¬ë‹ˆ ì œì‘ ê¸°ìˆ </span> decorative <span class=\"meaning\">(ì¥ì‹ì ì¸) - ê¾¸ë¯¸ëŠ” ëª©ì ì˜</span> master craftsmen <span class=\"meaning\">(ìˆ™ë ¨ ì¥ì¸) - ê¸°ìˆ ì„ ì™„ì „íˆ ìµíŒ ì¥ì¸</span> ğŸ’­ ì² í•™/ì‚¬ìƒ ê´€ë ¨ ìš©ì–´ modernists <span class=\"meaning\">(ëª¨ë”ë‹ˆìŠ¤íŠ¸) - í˜„ëŒ€ì£¼ì˜ìë“¤</span> art for art's sake <span class=\"meaning\">(ì˜ˆìˆ ì„ ìœ„í•œ ì˜ˆìˆ ) - ì˜ˆìˆ ì˜ ììœ¨ì„±ì„ ì£¼ì¥í•˜ëŠ” ì‚¬ìƒ</span> conventions <span class=\"meaning\">(ê´€ìŠµ) - ì „í†µì ìœ¼ë¡œ ë”°ë¥´ëŠ” ë°©ì‹</span> classicism <span class=\"meaning\">(ê³ ì „ì£¼ì˜) - ê³ ì „ì  ì–‘ì‹ì„ ë”°ë¥´ëŠ” ì˜ˆìˆ  ì‚¬ì¡°</span> stylistic expression <span class=\"meaning\">(ì–‘ì‹ì  í‘œí˜„) - ê°œì¸ì  ìŠ¤íƒ€ì¼ì˜ í‘œí˜„</span> utilitarian <span class=\"meaning\">(ì‹¤ìš©ì£¼ì˜ì ì¸) - ì‹¤ìš©ì„±ì„ ì¤‘ì‹œí•˜ëŠ”</span> intrinsic value <span class=\"meaning\">(ë‚´ì¬ì  ê°€ì¹˜) - ê·¸ ìì²´ê°€ ê°€ì§„ ê°€ì¹˜</span> impressionistic <span class=\"meaning\">(ì¸ìƒì£¼ì˜ì ì¸) - ì¸ìƒì£¼ì˜ ì–‘ì‹ì˜</span> abstract <span class=\"meaning\">(ì¶”ìƒì ì¸) - êµ¬ì²´ì ì´ì§€ ì•Šì€</span> ğŸŒ í˜„ëŒ€ ì˜ˆìˆ  ê´€ë ¨ ìš©ì–´ fragmentation <span class=\"meaning\">(ë¶„ì—´, íŒŒí¸í™”) - ë‚˜ë‰˜ì–´ì§„ ìƒíƒœ</span> mockery <span class=\"meaning\">(ì¡°ë¡±) - ë¹„ì›ƒìŒ</span> genuine <span class=\"meaning\">(ì§„ì •í•œ) - ê°€ì§œê°€ ì•„ë‹Œ</span> prestigious <span class=\"meaning\">(ê¶Œìœ„ ìˆëŠ”) - ëª…ì„±ì´ ë†’ì€</span> realm <span class=\"meaning\">(ì˜ì—­) - ë¶„ì•¼ë‚˜ ë²”ìœ„</span> unmodified <span class=\"meaning\">(ìˆ˜ì •ë˜ì§€ ì•Šì€) - ë³€í™”ì‹œí‚¤ì§€ ì•Šì€</span> consensus <span class=\"meaning\">(í•©ì˜) - ì˜ê²¬ì˜ ì¼ì¹˜</span> subjectively <span class=\"meaning\">(ì£¼ê´€ì ìœ¼ë¡œ) - ê°œì¸ì  ê²¬í•´ì— ë”°ë¼</span> primitive artifacts <span class=\"meaning\">(ì›ì‹œ ìœ ë¬¼) - ê³ ëŒ€ì˜ ì¸ê³µ ì œì‘ë¬¼</span> constitutes <span class=\"meaning\">(êµ¬ì„±í•˜ë‹¤) - ì´ë£¨ë‹¤, í˜•ì„±í•˜ë‹¤</span> </div> --- ğŸ’¬ ì¤‘ìš”í•œ ì˜ì–´ í‘œí˜„ë“¤ <button class=\"toggle-button expressions\" data-type=\"expressions\">ğŸ” í‘œí˜„ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"expressions-content\"> ğŸ“Š ë³€í™”/ë°œì „ í‘œí˜„ has varied throughout history <span class=\"meaning\">(ì—­ì‚¬ë¥¼ í†µí‹€ì–´ ë³€í™”í•´ì™”ë‹¤) - \"How art is defined has varied throughout history\"</span> leading to <span class=\"meaning\">(~ë¡œ ì´ì–´ì§€ëŠ”) - \"leading to a wide range of meanings\"</span> came about <span class=\"meaning\">(ìƒê²¨ë‚˜ë‹¤, ë°œìƒí•˜ë‹¤) - \"came about only in the Renaissance\"</span> came into fashion <span class=\"meaning\">(ìœ í–‰í•˜ê²Œ ë˜ë‹¤) - \"a new meaning came into fashion\"</span> began to change <span class=\"meaning\">(ë³€í™”í•˜ê¸° ì‹œì‘í–ˆë‹¤) - \"things began to change during the Renaissance\"</span> at the forefront of <span class=\"meaning\">(~ì˜ ìµœì „ì„ ì—) - \"At the forefront of this transformation\"</span> ğŸ¯ ëŒ€ì¡°/ë¹„êµ í‘œí˜„ in contrast to <span class=\"meaning\">(~ì™€ ëŒ€ì¡°ì ìœ¼ë¡œ) - \"This is in contrast to crafts\"</span> more similar to <span class=\"meaning\">(~ì— ë” ê°€ê¹Œìš´) - \"more similar to manual laborers than artists\"</span> on par with <span class=\"meaning\">(~ì™€ ë™ë“±í•œ) - \"on par with that of poets\"</span> as opposed to <span class=\"meaning\">(~ì™€ëŠ” ë°˜ëŒ€ë¡œ) - implied in contrasts</span> in a clear mockery of <span class=\"meaning\">(~ì— ëŒ€í•œ ëª…ë°±í•œ ì¡°ë¡±ìœ¼ë¡œ) - \"In a clear mockery of the classical tradition\"</span> â° ì‹œê°„/ìˆœì„œ í‘œí˜„ subsequently <span class=\"meaning\">(ê·¸ í›„ì—) - \"Subsequently, in modern times\"</span> gradually <span class=\"meaning\">(ì ì°¨ì ìœ¼ë¡œ) - \"Artists gradually gained acceptance\"</span> increasingly <span class=\"meaning\">(ì ì  ë”) - \"increasingly employed scientific methods\"</span> eventually <span class=\"meaning\">(ê²°êµ­) - \"eventually erased the fragmentation\"</span> saw its peak <span class=\"meaning\">(ì ˆì •ì— ë‹¬í–ˆë‹¤) - \"saw its peak in the eighteenth century\"</span> ğŸ¨ ì˜ˆìˆ  ê´€ë ¨ í‘œí˜„ applied exclusively to <span class=\"meaning\">(~ì—ë§Œ ë…ì ì ìœ¼ë¡œ ì ìš©ë˜ë‹¤) - \"applied exclusively to works\"</span> became synonymous with <span class=\"meaning\">(~ì™€ ë™ì˜ì–´ê°€ ë˜ë‹¤) - \"became synonymous with classical art\"</span> came to represent <span class=\"meaning\">(~ë¥¼ ëŒ€í‘œí•˜ê²Œ ë˜ë‹¤) - \"came to represent the fine arts\"</span> in favor of <span class=\"meaning\">(~ì„ ì„ í˜¸í•˜ì—¬) - \"in favor of stylistic expression\"</span> beyond the realm of <span class=\"meaning\">(~ì˜ ì˜ì—­ì„ ë„˜ì–´ì„œ) - \"beyond the realm of productive creativity\"</span> ğŸ“ í•™ìˆ ì  í‘œí˜„ stood for <span class=\"meaning\">(~ë¥¼ ì˜ë¯¸í–ˆë‹¤) - \"they stood for any activity\"</span> were viewed as <span class=\"meaning\">(~ë¡œ ì—¬ê²¨ì¡Œë‹¤) - \"were viewed as craftsmen\"</span> necessitating <span class=\"meaning\">(~ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”) - \"necessitating skills\"</span> occupied the bottom of <span class=\"meaning\">(~ì˜ ë§¨ ì•„ë˜ë¥¼ ì°¨ì§€í–ˆë‹¤) - \"occupied the bottom of the social hierarchy\"</span> inherited this view <span class=\"meaning\">(ì´ ê²¬í•´ë¥¼ ë¬¼ë ¤ë°›ì•˜ë‹¤) - \"The medieval world inherited this view\"</span> ğŸ” ê°•ì¡°/ì„¤ëª… í‘œí˜„ it was also... that <span class=\"meaning\">(~í•œ ê²ƒë„ ë°”ë¡œ ...ì˜€ë‹¤) - \"It was also in the Renaissance that...\"</span> accordingly <span class=\"meaning\">(ë”°ë¼ì„œ) - \"Accordingly, they believed\"</span> in addition <span class=\"meaning\">(ë˜í•œ) - \"In addition, artists experimented\"</span> therefore <span class=\"meaning\">(ë”°ë¼ì„œ) - \"Therefore, it effectively eliminated\"</span> certainly <span class=\"meaning\">(í™•ì‹¤íˆ) - \"Certainly, the question has become\"</span> ğŸ’­ ì˜ê²¬/íŒë‹¨ í‘œí˜„ argued that <span class=\"meaning\">(~ë¼ê³  ì£¼ì¥í–ˆë‹¤) - \"artists argued that they were superior\"</span> believed to require <span class=\"meaning\">(~ë¥¼ ìš”êµ¬í•œë‹¤ê³  ì—¬ê²¨ì¡Œë‹¤) - \"believed to require no unique inspiration\"</span> denied that <span class=\"meaning\">(~ë¼ëŠ” ê²ƒì„ ë¶€ì¸í–ˆë‹¤) - \"Modernists denied that purpose\"</span> sought to promote <span class=\"meaning\">(~ë¥¼ ì´‰ì§„í•˜ë ¤ í–ˆë‹¤) - \"sought to promote the idea\"</span> felt hindered by <span class=\"meaning\">(~ì— ì˜í•´ ë°©í•´ë°›ëŠ”ë‹¤ê³  ëŠê¼ˆë‹¤) - \"felt hindered by the conventions\"</span> </div> --- ë¬¸ì œí’€ì´ TOEFL Reading Comprehension Questions Question 12 Which of the following is consistent with the ancient view of art as described in paragraph 2? (A) Visual art was not an undertaking worthy of esteem. (B) Art was an activity that required special inspiration. (C) Visual art occupied a special place in higher learning. (D) Art applied to any pursuit involving formal education. <button class=\"toggle-button answer\" data-type=\"answer12\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer12-content\"> ì •ë‹µ: (A) Visual art was not an undertaking worthy of esteem. í•´ì„¤: 2ë‹¨ë½ì—ì„œ ê³ ëŒ€ì¸ë“¤ì˜ ì˜ˆìˆ ê´€: í•µì‹¬ ê·¼ê±°: \"The visual arts were viewed simply as manual crafts\" \"because the ancients looked down on manual labor, artisans occupied the bottom of the social hierarchy\" \"Those engaged in the production of art were viewed as craftsmen, more similar to manual laborers than artists\" ê³ ëŒ€ì¸ë“¤ì€ ì‹œê° ì˜ˆìˆ ì„ ë‹¨ìˆœí•œ ìˆ˜ê³µ ê¸°ìˆ ë¡œ ì—¬ê²¼ê³ , ìœ¡ì²´ ë…¸ë™ì„ ì—…ì‹ ì—¬ê²¼ê¸° ë•Œë¬¸ì— ì‹œê° ì˜ˆìˆ ì€ ì¡´ê²½ë°›ì„ ë§Œí•œ ì¼ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ì˜¤ë‹µ ë¶„ì„: (B) \"believed to require no unique inspiration\" - íŠ¹ë³„í•œ ì˜ê°ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ê³  ì—¬ê¹€ (C) \"Painting and sculpture had no special place in higher learning\" - ê³ ë“± í•™ë¬¸ì—ì„œ íŠ¹ë³„í•œ ìë¦¬ ì—†ìŒ (D) ê³µì‹ êµìœ¡ì´ ì•„ë‹Œ \"rules and required expertise\"ë¥¼ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í™œë™ì— ì ìš© </div> Question 13 Which of the following is NOT listed as an example of fine art in paragraph 3? (A) Sculpture (B) Architecture (C) Mathematics (D) Music <button class=\"toggle-button answer\" data-type=\"answer13\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer13-content\"> ì •ë‹µ: (C) Mathematics í•´ì„¤: 3ë‹¨ë½ì—ì„œ ì–¸ê¸‰ëœ ìˆœìˆ˜ ì˜ˆìˆ (fine arts)ë“¤: ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê²ƒë“¤: \"Painting, sculpture, and architecture were given their own place in academies\" \"Collectively, the classical arts, along with music and poetry, came to represent the fine arts\" ìˆ˜í•™ì˜ ì—­í• : \"mathematics became central to their trade\"ë¡œ ì–¸ê¸‰ë˜ì§€ë§Œ, ì´ëŠ” ì˜ˆìˆ ê°€ë“¤ì´ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ë¡œì„œì˜ ìˆ˜í•™ì´ì§€, ìˆ˜í•™ ìì²´ê°€ ìˆœìˆ˜ ì˜ˆìˆ ë¡œ ë¶„ë¥˜ëœ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ìˆœìˆ˜ ì˜ˆìˆ ë¡œ ì–¸ê¸‰ëœ ê²ƒë“¤: (A) Sculpture âœ“ - ëª…ì‹œì  ì–¸ê¸‰ (B) Architecture âœ“ - ëª…ì‹œì  ì–¸ê¸‰ (D) Music âœ“ - \"along with music and poetry\" ì–¸ê¸‰ </div> Question 14 Which of the following is a distinguishing feature of crafts according to paragraph 4? (A) They were made for decoration. (B) They were produced in large quantities. (C) They were produced by artisans in the academies. (D) They were made to closely follow aesthetic principles. <button class=\"toggle-button answer\" data-type=\"answer14\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer14-content\"> ì •ë‹µ: (A) They were made for decoration. í•´ì„¤: 4ë‹¨ë½ì—ì„œ ê³µì˜ˆì˜ íŠ¹ì§•: í•µì‹¬ ê·¼ê±°: \"This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes\" \"Craftsmen whose production consisted of such practical, or merely decorative, items were called artisans\" ê³µì˜ˆëŠ” ì‹¤ìš©ì (practical)ì´ê±°ë‚˜ ë‹¨ìˆœíˆ ì¥ì‹ì (decorative)ì¸ ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œë‹¤ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆìˆ ê³¼ì˜ ëŒ€ì¡°: ì˜ˆìˆ : \"create perfect and idealized representations of reality that followed established principles of beauty\" ê³µì˜ˆ: \"fashioned largely for everyday purposes\" + \"merely decorative\" ì˜¤ë‹µ ë¶„ì„: (B) ëŒ€ëŸ‰ ìƒì‚°ì— ëŒ€í•œ ì–¸ê¸‰ ì—†ìŒ (C) \"they did not study art in the academies\" - í•™ì›ì—ì„œ ê³µë¶€í•˜ì§€ ì•ŠìŒ (D) ë¯¸ì  ì›ì¹™ì„ ë”°ë¥´ëŠ” ê²ƒì€ ì˜ˆìˆ ì˜ íŠ¹ì§• </div> Question 15 The word \"convictions\" in the passage is closest in meaning to (A) intense oppositions (B) strong beliefs (C) wrong assumptions (D) feeble suggestions <button class=\"toggle-button answer\" data-type=\"answer15\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer15-content\"> ì •ë‹µ: (B) strong beliefs í•´ì„¤: 5ë‹¨ë½ì˜ ë¬¸ë§¥ì—ì„œ \"convictions\" ì‚¬ìš©: ì›ë¬¸: \"they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society\" ë¬¸ë§¥ ë¶„ì„: ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ ê±°ë¶€í•œ ê²ƒì€ ë™ì‹œëŒ€ì¸ë“¤ì˜ ì‹ ë…/í™•ì‹ ì´ì—ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ì˜ˆìˆ ì´ ì‹¤ìš©ì ì´ê³  ì‚¬íšŒì— ìœ ìµí•´ì•¼ í•œë‹¤ê³  ê°•í•˜ê²Œ ë¯¿ì—ˆë˜ ì‚¬ëŒë“¤ì´ì—ˆìŠµë‹ˆë‹¤. ì–´íœ˜ ë¶„ì„: conviction = í™•ì‹ , ì‹ ë… (ê°•í•œ ë¯¿ìŒ) (A) intense oppositions: ê°•í•œ ë°˜ëŒ€ - ë¬¸ë§¥ì— ë§ì§€ ì•ŠìŒ (B) strong beliefs: ê°•í•œ ë¯¿ìŒ âœ“ (C) wrong assumptions: ì˜ëª»ëœ ê°€ì • - ê°€ì¹˜ íŒë‹¨ì  (D) feeble suggestions: ì•½í•œ ì œì•ˆ - convictionì˜ ê°•í•œ ì˜ë¯¸ì™€ ë°˜ëŒ€ </div> Question 16 What can be inferred from paragraph 5 about artists in the nineteenth century? (A) Some of them thought that the primary value of art was in its social purpose. (B) Most of them approached art as an academic discipline that required learning. (C) They placed a greater emphasis on aesthetics than did classical artists. (D) They viewed moral and political art as having no intrinsic value. <button class=\"toggle-button answer\" data-type=\"answer16\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer16-content\"> ì •ë‹µ: (A) Some of them thought that the primary value of art was in its social purpose. í•´ì„¤: 5ë‹¨ë½ì—ì„œ 19ì„¸ê¸° ì˜ˆìˆ ê°€ë“¤ì˜ ìƒí™©: í•µì‹¬ ê·¼ê±°: \"At the same time, they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society\" ì´ ë¬¸ì¥ì€ ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ ê±°ë¶€í•œ ë™ì‹œëŒ€ì¸ë“¤ì˜ ê²¬í•´ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. ì¦‰, 19ì„¸ê¸°ì—ëŠ” ì¼ë¶€ ì˜ˆìˆ ê°€ë“¤ì´ ì˜ˆìˆ ì˜ ì£¼ëœ ê°€ì¹˜ê°€ ì‚¬íšŒì  ëª©ì ì— ìˆë‹¤ê³  ìƒê°í–ˆë‹¤ëŠ” ê²ƒì„ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¦¬ êµ¬ì¡°: ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ \"art for art's sake\"ë¥¼ ì£¼ì¥ ê·¸ë“¤ì´ ê±°ë¶€í•œ ê²ƒ: ì˜ˆìˆ ì´ ì‹¤ìš©ì ì´ê³  ì‚¬íšŒì— ìœ ìµí•´ì•¼ í•œë‹¤ëŠ” ê²¬í•´ â†’ ë”°ë¼ì„œ ê·¸ëŸ° ê²¬í•´ë¥¼ ê°€ì§„ ì‚¬ëŒë“¤ì´ ì¡´ì¬í–ˆìŒì„ ì˜ë¯¸ ì˜¤ë‹µ ë¶„ì„: (B) ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì€ ì˜¤íˆë ¤ ì „í†µì  í•™ë¬¸ì  ì ‘ê·¼ì„ ê±°ë¶€ (C) í˜•íƒœë³´ë‹¤ ì–‘ì‹ì  í‘œí˜„ì„ ê°•ì¡° - ìˆœìˆ˜ ë¯¸í•™ì  ì ‘ê·¼ì€ ì•„ë‹˜ (D) ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ ê±°ë¶€í•œ ê²ƒì´ì§€, ëª¨ë“  ì˜ˆìˆ ê°€ê°€ ê·¸ëŸ° ê²ƒì€ ì•„ë‹˜ </div> Question 17 What is the purpose of paragraph 6 in the passage? (A) To introduce the reasons for the changes outlined in the preceding paragraphs (B) To describe the resulting outcome of the ideas described in the previous paragraph (C) To offer supporting evidence for the ideas introduced in the previous paragraph (D) To provide a transition between the preceding paragraphs and the final paragraph <button class=\"toggle-button answer\" data-type=\"answer17\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer17-content\"> ì •ë‹µ: (B) To describe the resulting outcome of the ideas described in the previous paragraph í•´ì„¤: ë‹¨ë½ë“¤ ê°„ì˜ ë…¼ë¦¬ì  ì—°ê²°: 5ë‹¨ë½ (ì›ì¸): ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì˜ ì‚¬ìƒ \"art for art's sake\" ì˜ˆìˆ ì˜ ë‚´ì¬ì  ê°€ì¹˜ ì‹¤í—˜ì  ê¸°ë²• ì¶”êµ¬ 6ë‹¨ë½ (ê²°ê³¼): ì´ëŸ¬í•œ ì‚¬ìƒì˜ ê²°ê³¼ \"This new perception of art as an end in itself eventually erased the fragmentation between artist and artisan\" \"effectively eliminated the distinction between crafts and the so-called fine arts\" ì˜ˆìˆ  ê°œë…ì˜ ê·¹ë‹¨ì  í™•ì¥ ë…¼ë¦¬ì  êµ¬ì¡°: 5ë‹¨ë½: ëª¨ë”ë‹ˆìŠ¤íŠ¸ ì‚¬ìƒ ì†Œê°œ (ì›ì¸) 6ë‹¨ë½: \"This new perception\" â†’ ê·¸ ê²°ê³¼ ì„¤ëª… ì—°ê²°ì–´: \"This\" = ì• ë‹¨ë½ì˜ ë‚´ìš©ì„ ë°›ìŒ ì˜¤ë‹µ ë¶„ì„: (A) ë³€í™”ì˜ ì´ìœ  ì†Œê°œ - 5ë‹¨ë½ì—ì„œ ì´ë¯¸ ë‹¤ë£¸ (C) ì§€ì§€ ì¦ê±° ì œê³µ - ìƒˆë¡œìš´ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒ (D) ì „í™˜ ì—­í•  - 6ë‹¨ë½ ìì²´ê°€ ì‹¤ì§ˆì  ë‚´ìš©ì„ ë‹´ê³  ìˆìŒ </div> Question 18 The word \"prestigious\" in the passage is closest in meaning to (A) obscure (B) ubiquitous (C) reputable (D) credulous <button class=\"toggle-button answer\" data-type=\"answer18\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer18-content\"> ì •ë‹µ: (C) reputable í•´ì„¤: 6ë‹¨ë½ì˜ ë¬¸ë§¥ì—ì„œ \"prestigious\" ì‚¬ìš©: ì›ë¬¸: \"This meant that any creative work could be viewed as genuine and prestigious\" ë¬¸ë§¥ ë¶„ì„: ëª¨ë“  ì°½ì¡°ì  ì‘ì—…ì´ ì§„ì •í•˜ê³ (genuine) ê¶Œìœ„ ìˆëŠ”(prestigious) ê²ƒìœ¼ë¡œ ì—¬ê²¨ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ \"prestigious\"ëŠ” ë†’ì€ ì§€ìœ„ë‚˜ ëª…ì„±ì„ ê°€ì§„ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤. ì–´íœ˜ ë¶„ì„: prestigious = ê¶Œìœ„ ìˆëŠ”, ëª…ì„±ì´ ë†’ì€ (A) obscure: ëª¨í˜¸í•œ, ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ - ì •ë°˜ëŒ€ ì˜ë¯¸ (B) ubiquitous: ì–´ë””ì—ë‚˜ ìˆëŠ” - ê´€ë ¨ ì—†ìŒ (C) reputable: í‰íŒì´ ì¢‹ì€, ëª…ì„± ìˆëŠ” âœ“ (D) credulous: ì˜ ë¯¿ëŠ”, ìˆœì§„í•œ - ê´€ë ¨ ì—†ìŒ ë™ì˜ì–´ ê´€ê³„: prestigious â‰ˆ reputable â‰ˆ respected â‰ˆ esteemed </div> Question 19 Look at the four squares [â– ] that indicate where the following sentence could be added to the passage. Other examples of crafts included studio pottery and metalworking. Where would the sentence best fit? <button class=\"toggle-button answer\" data-type=\"answer19\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer19-content\"> ì ì ˆí•œ ìœ„ì¹˜: 4ë‹¨ë½ì˜ ì„¸ ë²ˆì§¸ [â– ] ë’¤ í•´ì„¤: ì‚½ì…í•  ë¬¸ì¥: \"Other examples of crafts included studio pottery and metalworking.\" (ê³µì˜ˆì˜ ë‹¤ë¥¸ ì˜ˆì‹œë“¤ë¡œëŠ” ìŠ¤íŠœë””ì˜¤ ë„ìê¸°ì™€ ê¸ˆì† ê³µì˜ˆê°€ í¬í•¨ë˜ì—ˆë‹¤.) 4ë‹¨ë½ì˜ êµ¬ì¡°: \"artisan\" ìš©ì–´ì˜ ë“±ì¥ ì˜ˆìˆ ê³¼ ê³µì˜ˆì˜ êµ¬ë³„ [â–  ì—¬ê¸°] \"This is in contrast to crafts, like beadwork or basket making...\" ì•„í‹°ì”ì˜ ì •ì˜ ì•„í‹°ì”ì˜ êµìœ¡ ë°©ì‹ ì˜ˆìˆ ê³¼ ê³µì˜ˆ êµ¬ë¶„ì˜ ì ˆì • ê°€ì¥ ì ì ˆí•œ ìœ„ì¹˜ ë¶„ì„: ì„¸ ë²ˆì§¸ [â– ] ë’¤ê°€ ê°€ì¥ ì ì ˆ: ì• ë¬¸ì¥: \"This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes\" ì‚½ì… ë¬¸ì¥: \"Other examples of crafts included studio pottery and metalworking\" ë’¤ ë¬¸ì¥: \"Craftsmen whose production consisted of such practical...\" ë…¼ë¦¬ì  ì—°ê²°ì„±: ê³µì˜ˆì˜ ì²« ë²ˆì§¸ ì˜ˆì‹œ ì œì‹œ (beadwork, basket making) â†’ ê³µì˜ˆì˜ ì¶”ê°€ ì˜ˆì‹œ ì œê³µ (pottery, metalworking) â†’ ì´ëŸ¬í•œ ê³µì˜ˆí’ˆì„ ë§Œë“œëŠ” ì‚¬ëŒë“¤ì˜ ëª…ì¹­ ì„¤ëª… \"Other examples\"ë¼ëŠ” í‘œí˜„ì´ ì•ì„œ ì œì‹œëœ ì˜ˆì‹œë“¤ì„ ë°›ì•„ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì—­í•  </div> Question 20 Directions: An introductory sentence for a brief summary of the passage is provided below. Complete the summary by selecting the THREE answer choices that express the most important ideas in the passage. Some sentences do not belong in the summary because they express ideas that are not presented in the passage or are minor ideas in the passage. This question is worth 2 points. The term \"art\" has changed so much that its meaning has become rather subjective. Answer Choices: (A) In the Latin and Greek worlds, art was looked down upon as a form of manual labor. (B) Artists of the modern period began to experiment by combining classical and abstract techniques in their art. (C) After the medieval period, the number of artists attending academies rose due to increased prestige in the arts. (D) The ancients applied the term in a broad manner to any rule-based activity that required skill. (E) Modernists, hoping to revolutionize the way art was conceived, completely redefined the term. (F) As artists began to gain status, the distinction between arts and crafts became more apparent. <button class=\"toggle-button answer\" data-type=\"answer20\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer20-content\"> ì •ë‹µ: (D), (E), (F) í•´ì„¤: ì„ íƒëœ ë‹µì•ˆë“¤: (D) The ancients applied the term in a broad manner to any rule-based activity that required skill. âœ“ 2ë‹¨ë½: \"they stood for any activity that had rules and required expertise\" \"from medicine and law to blacksmithing\" ê³ ëŒ€ì˜ ê´‘ë²”ìœ„í•œ ì˜ˆìˆ  ê°œë…ì´ ì£¼ìš” ì£¼ì œ (E) Modernists, hoping to revolutionize the way art was conceived, completely redefined the term. âœ“ 5-6ë‹¨ë½: ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì˜ í˜ëª…ì  ë³€í™” \"art for art's sake\", ì˜ˆìˆ ê³¼ ê³µì˜ˆì˜ êµ¬ë¶„ ì œê±° í˜„ëŒ€ì  ì˜ˆìˆ  ê°œë…ì˜ í•µì‹¬ ë³€í™” (F) As artists began to gain status, the distinction between arts and crafts became more apparent. âœ“ 3-4ë‹¨ë½: ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ ì˜ˆìˆ ê°€ ì§€ìœ„ í–¥ìƒ \"artisan\" ìš©ì–´ ë“±ì¥, ì˜ˆìˆ ê³¼ ê³µì˜ˆì˜ ëª…í™•í•œ êµ¬ë¶„ ì—­ì‚¬ì  ë°œì „ ê³¼ì •ì˜ ì¤‘ìš”í•œ ë‹¨ê³„ ì œì™¸ëœ ë‹µì•ˆë“¤: (A) ê³ ëŒ€ì—ì„œ ì˜ˆìˆ ì´ ì—…ì‹ ì—¬ê²¨ì§„ ê²ƒì€ ë§ì§€ë§Œ, í•µì‹¬ì€ \"ê´‘ë²”ìœ„í•œ ì ìš©\" (B) ê³ ì „ê³¼ ì¶”ìƒ ê¸°ë²•ì˜ ê²°í•©ì€ ì–¸ê¸‰ë˜ì§€ ì•ŠìŒ (C) í•™ì› ì¶œì„ì ìˆ˜ ì¦ê°€ëŠ” ì„¸ë¶€ ì‚¬í•­, ì£¼ìš” ì•„ì´ë””ì–´ ì•„ë‹˜ ìš”ì•½ì˜ ë…¼ë¦¬ì  íë¦„: ê³ ëŒ€: ê´‘ë²”ìœ„í•œ ì ìš© (D) ë¥´ë„¤ìƒìŠ¤: ì§€ìœ„ í–¥ìƒê³¼ êµ¬ë¶„ ëª…í™•í™” (F) í˜„ëŒ€: ëª¨ë”ë‹ˆìŠ¤íŠ¸ì˜ ì¬ì •ì˜ (E) </div>",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-15-ì •ì‹ ë˜‘ë°”ë¡œì°¨ë¦¬ê³ ìˆì–´",
    "title": "ì •ì‹  ë˜‘ë°”ë¡œ ì°¨ë¦¬ê³  ìˆì–´",
    "date": "2025-07-15",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ê³ ë¯¼ ë° ì¡°ì–¸í•˜ëŠ” í‘œí–”"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Be on the lookout : ì •ì‹  ë˜‘ë°”ë¡œ ì°¨ë ¤ Be on the lookout for pickpockets Be on the lookout for voicefishing Be careful with Be careful with your bag on the subway. in + ìë™ì°¨ (íƒ€ì„œ ì•‰ìœ¼ë©´ ë) on + ë¹„í–‰ê¸°, ë°°, ì§€í•˜ì²  passport catch a cold Be careful not to catch a cold Be careful not to burn yourself Be careful not to hurt yourself During this outbreak? outbreak: ì§ˆë³‘ ë°œìƒê¸°ê°„ scammers : ì‚¬ê¸°ê¾¼ Make sure not to go ê°„ë‹¨í•œ ì‘ë¬¸ As usaul, I talk to myself. Hey MK, be on the lookout for ë‚˜íƒœí•¨. Be careful with your habit that do work later (ë¯¸ë£¨ëŠ” ìŠµê´€) Nice work today, just keep this pace. But don`t forget happy is around you. Feel happy with ì‚¬ì†Œí•œ thing around you, and victories you made in today. ìˆ˜ì •ëœ ì‘ë¬¸ (Improved Version) As usual, time for my daily self-talk. Hey MK, don't get lazy on me! Stop putting things off - let's break this procrastination habit once and for all. *procrastination: ê¾¸ë¬¼ê±°ë¦¬ë‹¤ ì˜ˆë¬¸: \"Stop procrastinating and start studying!\" (ë¯¸ë£¨ì§€ ë§ê³  ê³µë¶€ ì‹œì‘í•´!) \"Procrastination is the thief of time.\" (ë¯¸ë£¨ê¸°ëŠ” ì‹œê°„ ë„ë‘‘ì´ë‹¤) \"I need to overcome my procrastination habit.\" (ë¯¸ë£¨ëŠ” ìŠµê´€ì„ ê³ ì³ì•¼ í•´) You crushed it today, keep this energy going! And hey, don't forget to enjoy the little things around you. Take a moment to appreciate the small wins and celebrate what you accomplished today. í”¼ë“œë°± (Feedback) ê°œì„ ëœ ì ë“¤: ë¬¸ë²•ì  ìˆ˜ì • \"As usaul\" â†’ \"As usual\" (ì² ì ì˜¤ë¥˜ ìˆ˜ì •) \"do work later\" â†’ \"Stop putting things off\" (ë” ì§ì ‘ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"happy is around you\" â†’ \"enjoy the little things around you\" (ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ êµ¬ì¡°) ì–´íœ˜ ì„ íƒ ê°œì„  (ë” ìºì£¼ì–¼í•˜ê³  í˜„ëŒ€ì ìœ¼ë¡œ) \"ë‚˜íƒœí•¨\" â†’ \"don't get lazy on me!\" (ì¹œê·¼í•˜ê³  ì—ë„ˆì§€ ë„˜ì¹˜ëŠ” í‘œí˜„) \"Nice work\" â†’ \"You crushed it\" (í˜„ëŒ€ì ì´ê³  ì„íŒ©íŠ¸ ìˆëŠ” ì¹­ì°¬) \"keep this pace\" â†’ \"keep this energy going\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"ì‚¬ì†Œí•œ thing\" â†’ \"little things\", \"small wins\" (ì¼ìƒì ì¸ í‘œí˜„) ë¬¸ì¥ êµ¬ì¡° ê°œì„  ë„ˆë¬´ ê¸´ ë¬¸ì¥ì„ ì§§ê²Œ ë‚˜ëˆ„ì–´ ì½ê¸° ì‰½ê²Œ ë§Œë“¦ ëª…ë ¹í˜•ì„ ì‚¬ìš©í•´ ë” ì§ì ‘ì ì´ê³  ë™ê¸°ë¶€ì—¬ì ìœ¼ë¡œ í‘œí˜„ ì‹¤ì œ ìê¸° ëŒ€í™”ì—ì„œ ì“¸ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ ë³€ê²½ ì „ì²´ì ì¸ í†¤ ê°œì„  Formalí•œ í‘œí˜„ì„ ìºì£¼ì–¼í•˜ê³  ì¹œê·¼í•œ í‘œí˜„ìœ¼ë¡œ ë³€ê²½ ê°íƒ„ì‚¬(\"hey\") ì¶”ê°€ë¡œ ë” ìƒë™ê° ìˆëŠ” ëŒ€í™”ì²´ êµ¬í˜„ ì§§ê³  ì„íŒ©íŠ¸ ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ë™ê¸°ë¶€ì—¬ íš¨ê³¼ ê·¹ëŒ€í™” í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸: ìê¸° ëŒ€í™”ëŠ” ìºì£¼ì–¼í•˜ê³  ì§ì ‘ì ì¼ìˆ˜ë¡ ìì—°ìŠ¤ëŸ½ë‹¤ í˜„ëŒ€ì ì¸ ìŠ¬ë­ í‘œí˜„(\"crushed it\")ë„ ì ì ˆíˆ í™œìš©í•˜ê¸° ë„ˆë¬´ ê¸´ ë¬¸ì¥ë³´ë‹¤ëŠ” ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ì´ íš¨ê³¼ì  ê°ì •ì„ ë‹´ì€ í‘œí˜„(\"don't get lazy on me!\")ì´ ë” ë™ê¸°ë¶€ì—¬ê°€ ëœë‹¤ ì•ìœ¼ë¡œ ì—°ìŠµí•  ì : í•œêµ­ì–´ì™€ ì˜ì–´ê°€ ì„ì¸ ë¶€ë¶„ì„ í”¼í•˜ê³  ì™„ì „í•œ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê¸° í˜„ëŒ€ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ í‘œí˜„ ë” í•™ìŠµí•˜ê¸° ê°ì •ì„ ë‹´ì€ ìƒë™ê° ìˆëŠ” í‘œí˜„ ì—°ìŠµí•˜ê¸°",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/user-based-collaborative-filtering",
    "title": "User-Based Collaborative Filtering",
    "date": "2025-07-15",
    "excerpt": "User-Based Collaborative Filtering ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì„¤ëª…",
    "category": "Recommendation",
    "tags": [
      "ì¶”ì²œì‹œìŠ¤í…œ"
    ],
    "content": "User-Based Collaborative Filtering (ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§) ê°œìš” User-Based Collaborative Filteringì€ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ê°€ì¥ ì˜¤ë˜ë˜ê³  ì§ê´€ì ì¸ ì´ì›ƒ(neighborhood) ê¸°ë°˜ ë°©ë²•ì…ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” \"ë¹„ìŠ·í•œ ì·¨í–¥ì„ ê°€ì§„ ì‚¬ìš©ìë“¤ì´ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì¢‹ì•„í•  ê²ƒì´ë‹¤\" ë¼ëŠ” ê°€ì •ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ê¸°ë³¸ ê°œë… ì‚¬ìš©ì-ì•„ì´í…œ í‰ì  í–‰ë ¬ $R$: í–‰ì€ ì‚¬ìš©ì, ì—´ì€ ì•„ì´í…œ ì›ì†Œ $r_{u,i}$: ì‚¬ìš©ì $u$ê°€ ì•„ì´í…œ $i$ì— ë§¤ê¸´ í‰ì  (ë˜ëŠ” ì•”ì‹œì  í–‰ë™) ëª©í‘œ: ì‚¬ìš©ì $u$ì—ê²Œ ì•„ì´í…œ $i$ë¥¼ ì¶”ì²œí• ì§€ ê²°ì • ì•Œê³ ë¦¬ì¦˜ ì ˆì°¨ ìœ ì‚¬ë„(Similarity) ê³„ì‚° ì‚¬ìš©ì ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë“¤: Cosine Similarity $$\\text{sim}(u,v) = \\frac{\\mathbf{r}_u \\cdot \\mathbf{r}_v}{\\ \\mathbf{r}_u\\ \\times \\ \\mathbf{r}_v\\ }$$ Pearson Correlation $$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)(r_{v,i} - \\bar{r}_v)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_v)^2}}$$ ì—¬ê¸°ì„œ $I_{uv}$ëŠ” ë‘ ì‚¬ìš©ìê°€ ëª¨ë‘ í‰ê°€í•œ ì•„ì´í…œ ì§‘í•© Adjusted Cosine Similarity $$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)(r_{v,i} - \\bar{r}_i)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_i)^2}}$$ ì—¬ê¸°ì„œ $\\bar{r}_i$ëŠ” ì•„ì´í…œ $i$ì˜ í‰ê·  í‰ì  Top-K ì´ì›ƒ ì„ íƒ ì‚¬ìš©ì $u$ì™€ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ $K$ëª…ì˜ ì‚¬ìš©ì ì´ì›ƒ ì§‘í•© $N_u$ ì„ íƒ í‰ì  ì˜ˆì¸¡ / ì¶”ì²œ ì ìˆ˜ ê³„ì‚° ê°€ì¤‘ í‰ê·  ë°©ì‹ $$\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N_u} \\text{sim}(u,v) \\times (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N_u} \\text{sim}(u,v) }$$ ë‹¨ìˆœ ê°€ì¤‘í•© ë°©ì‹ $$\\hat{r}_{u,i} = \\frac{\\sum_{v \\in N_u} \\text{sim}(u,v) \\times r_{v,i}}{\\sum_{v \\in N_u} \\text{sim}(u,v) }$$ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì•„ì§ ì†Œë¹„í•˜ì§€ ì•Šì€ ì•„ì´í…œ ì¤‘ $\\hat{r}_{u,i}$ê°€ ë†’ì€ ìˆœìœ¼ë¡œ $N$ê°œ ë°˜í™˜ ì¥ì  ì§ê´€ì„±: êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€ New Item Cold-Start ëŒ€ì‘: ìƒˆë¡œìš´ ì•„ì´í…œì´ ë“¤ì–´ì™€ë„ ë¹„ìŠ·í•œ ì‚¬ìš©ìê°€ í‰ê°€í–ˆë‹¤ë©´ ê³§ë°”ë¡œ ì¶”ì²œ ê°€ëŠ¥ ë¯¸ë¬˜í•œ íŒ¨í„´ í¬ì°©: ì‚¬ìš©ì ì„ í˜¸ì˜ ì„¸ë°€í•œ íŒ¨í„´ì„ ì´ì›ƒ ê¸°ë°˜ìœ¼ë¡œ í¬ì°© ê°€ëŠ¥ í•´ì„ ê°€ëŠ¥ì„±: ì™œ ì´ ì•„ì´í…œì„ ì¶”ì²œí–ˆëŠ”ì§€ ì„¤ëª… ê°€ëŠ¥ ë‹¨ì  í¬ì†Œì„±(Sparsity): í‰ì  í–‰ë ¬ì´ í¬ì†Œí•˜ë©´ ê³µí†µ í‰ê°€ ì•„ì´í…œì´ ì ì–´ ìœ ì‚¬ë„ ê³„ì‚°ì´ ë¶ˆì•ˆì • í™•ì¥ì„±(Scalability): ì‚¬ìš©ì ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚° ë¹„ìš© $O( U ^2)$ ì‚¬ìš©ì Cold-Start: ìƒˆ ì‚¬ìš©ìëŠ” í‰ì ì´ ì—†ì–´ ì¶”ì²œ ë¶ˆê°€ Popularity Bias: ë§ì€ í‰ì ì„ ë‚¨ê¸´ í—¤ë¹„ ìœ ì €ì—ê²Œ ìœ ì‚¬ë„ê°€ ì§‘ì¤‘ë  ìˆ˜ ìˆìŒ Gray Sheep ë¬¸ì œ: ë…íŠ¹í•œ ì·¨í–¥ì„ ê°€ì§„ ì‚¬ìš©ìëŠ” ìœ ì‚¬í•œ ì´ì›ƒì„ ì°¾ê¸° ì–´ë ¤ì›€ ê°œì„  ê¸°ë²• Significance Weighting ê³µí†µ í‰ê°€ ì•„ì´í…œ ìˆ˜ê°€ ì ìœ¼ë©´ ìœ ì‚¬ë„ì— íŒ¨ë„í‹° ë¶€ì—¬ Neighborhood Size ìµœì í™” $K$ê°’ íŠœë‹ ìœ ì‚¬ë„ threshold ì„¤ì • ì •ê·œí™” ë° ë³´ì • Normalization (í‰ê· Â·í‘œì¤€í¸ì°¨ ì œê±°) Shrinkage Baseline ë³´ì • (ê¸€ë¡œë²Œ í‰ê·  + user bias + item bias) Hybrid ì ‘ê·¼ë²• ì‚¬ìš©ì ê¸°ë°˜ CF + ì•„ì´í…œ ê¸°ë°˜ CF ê²°í•© ì½˜í…ì¸  ê¸°ë°˜ íŠ¹ì§•ê³¼ ê²°í•© ëª¨ë¸ ê¸°ë°˜(í–‰ë ¬ ë¶„í•´)ê³¼ ìŠ¤ìœ„ì¹­ ì•„ì´í…œ ê¸°ë°˜ CFì™€ ë¹„êµ êµ¬ë¶„ User-Based CF Item-Based CF ------ --------------- --------------- ê°€ì • ë¹„ìŠ·í•œ ì‚¬ìš©ìê°€ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì¢‹ì•„í•¨ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì†Œë¹„í•œ ì‚¬ìš©ìëŠ” í–¥í›„ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì†Œë¹„ ì í•©í•œ ìƒí™© ì‚¬ìš©ì í–‰ë™ì´ í’ë¶€í•˜ê³  ë§ì¶¤í˜• ê´€ê³„ íŒŒì•… í•„ìš” ëŒ€ëŸ‰ ì‚¬ìš©ì, ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì•„ì´í…œ í™•ì¥ì„± ì‚¬ìš©ì ìˆ˜ì— ë”°ë¼ ì„±ëŠ¥ ì €í•˜ ì•„ì´í…œ ìˆ˜ì— ë”°ë¼ ì„±ëŠ¥ ì €í•˜ Cold-Start New Itemì— ê°•í•¨, New Userì— ì•½í•¨ New Userì— ê°•í•¨, New Itemì— ì•½í•¨ êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© Scikit-surprise: ë‹¤ì–‘í•œ CF ì•Œê³ ë¦¬ì¦˜ ì§€ì› Implicit: ì•”ì‹œì  í”¼ë“œë°± ê¸°ë°˜ CF LightFM: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„±ëŠ¥ ìµœì í™” ì˜¤í”„ë¼ì¸ ê³„ì‚°: ìœ ì‚¬ë„ í–‰ë ¬ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ìºì‹œ ìƒ˜í”Œë§: ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚°ëŸ‰ ê°ì†Œ Locality Sensitive Hashing: ê·¼ì‚¬ ìœ ì‚¬ë„ ê³„ì‚°ìœ¼ë¡œ ì†ë„ í–¥ìƒ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì£¼ê¸°ì  ì¬ê³„ì‚° + ìºì‹œë¡œ ëŒ€ì‘ ì¦ë¶„ ì—…ë°ì´íŠ¸ ë°©ì‹ ê³ ë ¤ A/B í…ŒìŠ¤íŠ¸ $K$ê°’, ìœ ì‚¬ë„ ì¢…ë¥˜, ì •ê·œí™” ë°©ì‹ íŠœë‹ ì¶”ì²œ í’ˆì§ˆê³¼ ì„±ëŠ¥ ê°„ ê· í˜•ì  ì°¾ê¸° Python êµ¬í˜„ ì˜ˆì‹œ ``python import numpy as np from scipy.spatial.distance import cosine from scipy.stats import pearsonr class UserBasedCF: def __init__(self, k=10): self.k = k self.user_similarities = None def fit(self, ratings_matrix): \"\"\"ì‚¬ìš©ì ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°\"\"\" n_users = ratings_matrix.shape[0] self.user_similarities = np.zeros((n_users, n_users)) for i in range(n_users): for j in range(i+1, n_users): # ê³µí†µ í‰ê°€ ì•„ì´í…œ ì°¾ê¸° common_items = np.where((ratings_matrix[i] != 0) & (ratings_matrix[j] != 0))[0] if len(common_items) < 2: similarity = 0 else: # Pearson correlation ê³„ì‚° user_i_ratings = ratings_matrix[i, common_items] user_j_ratings = ratings_matrix[j, common_items] try: similarity, _ = pearsonr(user_i_ratings, user_j_ratings) if np.isnan(similarity): similarity = 0 except: similarity = 0 self.user_similarities[i, j] = similarity self.user_similarities[j, i] = similarity def predict(self, user_id, item_id, ratings_matrix): \"\"\"íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¹ì • ì•„ì´í…œ í‰ì  ì˜ˆì¸¡\"\"\" if self.user_similarities is None: raise ValueError(\"Model must be fitted first\") # ì‚¬ìš©ìì˜ í‰ê·  í‰ì  user_ratings = ratings_matrix[user_id] user_mean = np.mean(user_ratings[user_ratings != 0]) # ì´ì›ƒ ì‚¬ìš©ìë“¤ ì°¾ê¸° similarities = self.user_similarities[user_id] neighbors = np.argsort(similarities)[::-1][1:self.k+1] numerator = 0 denominator = 0 for neighbor in neighbors: if similarities[neighbor] <= 0: continue neighbor_rating = ratings_matrix[neighbor, item_id] if neighbor_rating == 0: continue neighbor_mean = np.mean(ratings_matrix[neighbor][ratings_matrix[neighbor] != 0]) numerator += similarities[neighbor] * (neighbor_rating - neighbor_mean) denominator += abs(similarities[neighbor]) if denominator == 0: return user_mean predicted_rating = user_mean + (numerator / denominator) return max(1, min(5, predicted_rating)) # í‰ì  ë²”ìœ„ ì œí•œ `` ê²°ë¡  User-Based CFëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì˜ ê¸°ë³¸ì´ ë˜ëŠ” ì§ê´€ì ì´ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„° í¬ì†Œì„±ê³¼ í™•ì¥ì„± í•œê³„ë¡œ ì¸í•´, ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ì—ì„œëŠ” ë‹¤ë¥¸ ê¸°ë²•(ì•„ì´í…œ ê¸°ë°˜, í–‰ë ¬ ë¶„í•´, ë”¥ëŸ¬ë‹)ê³¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì‹¤ì œ ì ìš© ì‹œì—ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸, ë°ì´í„° íŠ¹ì„±, ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. --- ì°¸ê³  ìë£Œ: Collaborative Filtering for Implicit Feedback Datasets Item-based collaborative filtering recommendation algorithms Matrix Factorization Techniques for Recommender Systems",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/tilde-tau-explained",
    "title": "Why Do We Introduce the Centered Function tilde_tau(x,t)?",
    "date": "2025-07-15",
    "excerpt": "The role of the intermediate, mean-zero CATE in the continuous-treatment R-learner.",
    "category": "Causal Inference",
    "tags": [
      "Concept",
      "R-learner",
      "Continuous Treatment"
    ],
    "content": "The problem in one sentence R-lossë¥¼ ê·¸ëŒ€ë¡œ ìµœì†Œí™”í•˜ë©´ $$ h(x,t) = \\tau(x,t) + s(x) $$ í˜•íƒœë¡œ x ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë§ë¶™ì„ í•¨ìˆ˜ $s(x)$ ê°€ ìˆì–´ë„ ì†ì‹¤ê°’ì´ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ ìµœì†Œê°’ì´ ë¬´í•œíˆ ë§ì•„ì ¸ í•´ê°€ ìœ ì¼í•˜ì§€ ì•Šë‹¤. --- Where does s(x) come from? R-lossì— $\\mathbb{E}[ h(X,T) \\mid X ]$ í•­ì´ ë“¤ì–´ê°€ë¯€ë¡œ $h(x,t) + s(x)$ ë¥¼ ë„£ì–´ë„ $s(x)$ ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì‡„ëœë‹¤. ê·¸ ê²°ê³¼ ì†ì‹¤ê°’ì´ ë™ì¼í•˜ë‹ˆ $s(x)$ ë¥¼ ì–¼ë§ˆë“ ì§€ ë¶™ì¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ non-identification(ì‹ë³„ ë¶ˆëŠ¥) ë¬¸ì œì˜ ê·¼ì›ì´ë‹¤. --- Idea: â€œcenterâ€ the function ì¤‘ê°„ í•¨ìˆ˜ $\\tilde{\\tau}(x,t)$ ë¥¼ ë‹¤ìŒì²˜ëŸ¼ ì •ì˜í•œë‹¤. $$ \\tilde{\\tau}(x,t) = \\tau(x,t) - \\mathbb{E}[ \\tau(X,T) \\mid X = x ] $$ ì¦‰, ê° x ë§ˆë‹¤ t ì „ì—­ í‰ê· ì´ 0 ì´ ë˜ë„ë¡ ì¤‘ì‹¬í™”í•œë‹¤. mean-zero ì„±ì§ˆ ë•Œë¬¸ì— $h(x,t)$ ì™€ $h(x,t)+s(x)$ ì˜ ì°¨ì´ì ì´ R-loss ì•ˆì—ì„œëŠ” ì§€ì›Œì§€ì§€ ì•ŠëŠ”ë‹¤. ì‹¤ì œë¡œ $h(X,T)+s(X) - \\mathbb{E}[h(X,T)+s(X)\\mid X] = h(X,T)-\\mathbb{E}[h(X,T)\\mid X]$ ì´ë¯€ë¡œ, R-loss ê°’ì€ ê·¸ëŒ€ë¡œë‹¤. ë”°ë¼ì„œ ì¤‘ì‹¬í™”ë§Œìœ¼ë¡œëŠ” ì‹ë³„ ë¬¸ì œê°€ ì™„ì „íˆ ì‚¬ë¼ì§€ì§€ ì•ŠëŠ”ë‹¤. ê²°ì •ì ì¸ ì—´ì‡ ëŠ” ë‹¤ìŒ ë‹¨ê³„ì˜ L2 ì •ê·œí™”ë‹¤. L2 íŒ¨ë„í‹° $\\rho\\,\\ h\\ _2^2$ ê°€ ì¶”ê°€ë˜ë©´, $s(x)$ ë¥¼ ë¶™ì´ëŠ” ìˆœê°„ ë…¸ë¦„ì´ ì»¤ì ¸ì„œ ëª©ì í•¨ìˆ˜ê°€ ì¦ê°€í•œë‹¤. ìµœì í™”ëŠ” $s(x)=0$ ì„ ì„ íƒí•´ ìœ ì¼í•´ë¥¼ ë§Œë“ ë‹¤. ì¦‰ $\\tilde{\\tau}$ ëŠ” \"mean-zero\" ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í›„ë³´ ì¤‘ í•˜ë‚˜ì´ë©°, ì‹ë³„ì„ ì™„ì„±í•˜ëŠ” ê²ƒì€ ì •ê·œí™”ë¼ëŠ” ì ì„ ê¸°ì–µí•´ì•¼ í•œë‹¤. --- How regularisation fits in Step-1 (Tikhonov) ì •ê·œí™”ëœ ìµœì í™” ë¬¸ì œë¥¼ í‘¼ë‹¤. $$ \\tau_{\\rho} = \\operatorname{arg\\,min}_{h} \\Bigl\\{ \\text{R-loss}(h) + \\rho \\, \\ h \\ _2^2 \\Bigr\\} $$ L2 íŒ¨ë„í‹°ê°€ ë“¤ì–´ê°€ë©´ ë¬¸ì œê°€ ì—„ë°€íˆ convex í•´ì ¸ì„œ ìœ ì¼í•œ í•´ê°€ ë³´ì¥ëœë‹¤. Step-2 (Scaling) ì´ë¡ ì ìœ¼ë¡œ $\\tilde{\\tau} = (1 + \\rho)\\, \\tau_{\\rho}$ ì„ì´ ì¦ëª…ëœë‹¤(Theorem 1). Step-3 (Zero-constraining operator) ë§ˆì§€ë§‰ìœ¼ë¡œ $$ \\mathcal{C}(f)(x,t) = f(x,t) - f(x,0) $$ ë¥¼ ì ìš©í•˜ë©´ ì‹¤ì œ ê´€ì‹¬ í•¨ìˆ˜ $\\tau(x,t)$ ë¥¼ ë³µì›í•œë‹¤. --- Practical take-aways ëª¨ë¸ë§ â€“ ì‹ ê²½ë§ ê°™ì€ ìœ ì—°í•œ í•¨ìˆ˜ ê·¼ì‚¬ê¸°ë¥¼ ì“¸ ë•Œë„ $\\tilde{\\tau}$ ë¥¼ í•™ìŠµí•˜ë©´ ì‹ë³„ ë¬¸ì œê°€ ìë™ìœ¼ë¡œ í•´ê²°ëœë‹¤. ìˆ˜ì¹˜ ì•ˆì •ì„± â€“ L2 ì •ê·œí™”(Ï>0)ê°€ ì—†ìœ¼ë©´ ì—¬ì „íˆ ill-posed; Ï ê°’ì€ í¬ì§€ ì•Šì•„ë„ \"ìœ ì¼ ìµœì†Œ\" í™•ë³´ì— ì¶©ë¶„í•˜ë‹¤. í•´ì„ â€“ $\\tilde{\\tau}$ ëŠ” \"x ì¡°ê±´ë¶€ì—ì„œ t ë³€í™”ë§Œ ë‹´ì€ ìˆœìˆ˜ íš¨ê³¼\"ë¼ì„œ $s(x)$(x ì „ìš© í¸í–¥)ì™€ ê¹”ë”íˆ ë¶„ë¦¬ëœë‹¤. --- Mini-checklist for implementation ë‘ nuisance í•¨ìˆ˜ m(x) ì™€ varpi(t x) ì¶”ì • R-loss + L2 íŒ¨ë„í‹°ë¡œ $\\tau_{\\rho}$ ê³„ì‚° $$\\tilde{\\tau} = (1 + \\rho)\\, \\tau_{\\rho}$$ $$\\hat{\\tau}(x,t) = \\tilde{\\tau}(x,t) - \\tilde{\\tau}(x,0)$$ --- Related reading Robinson (1988) â€“ partially linear models Nie & Wager (2021) â€“ Quasi-Oracle R-learner Kennedy et al. (2017) â€“ continuous-treatment GPS Tikhonov (1963) â€“ regularisation for ill-posed problems --- 3.5 Why do we still center? L2 ì •ê·œí™”ê°€ ìµœì¢…ì ìœ¼ë¡œ $s(x)$ ë¥¼ ì œê±°í•˜ë”ë¼ë„, ì¤‘ì‹¬í™”ê°€ ì„ í–‰ë¼ì•¼ í•˜ëŠ” ì‹¤ìš©ì  ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤. Canonical representative â€“ ê°™ì€ R-loss ê°’ì„ ê°–ëŠ” ë™ì¹˜ë¥˜ $\\{h+s(x)\\}$ ì¤‘ì—ì„œ $\\mathbb{E}[h\\mid X]=0$ ì¸ í•¨ìˆ˜ê°€ ê°€ì¥ ì‘ì€ $L_2$ ë…¸ë¦„ì„ ê°€ì§€ë¯€ë¡œ, ì •ê·œí™”ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ê·¸ í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ê²Œ ëœë‹¤. ë‹«íŒ í˜•íƒœ í•´ â€“ Theorem 1 ì˜ $(1+\\rho)^{-1}$ ìŠ¤ì¼€ì¼ ê´€ê³„ëŠ” $\\mathbb{E}[h\\mid X]=0$ ì¡°ê±´ì´ ìˆì–´ì•¼ ì„±ë¦½í•œë‹¤. Zero-constraining operator â€“ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ $\\mathcal{C}(f)(x,t)=f(x,t)-f(x,0)$ ë¥¼ ì ìš©í•  ë•Œ, ì¤‘ì‹¬í™”ë¼ ìˆì–´ì•¼ ì •í™•íˆ $\\tau(x,t)$ ë¡œ ë³µì›ëœë‹¤. ---",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/ì¤€ë¹„ì „ëµì„œì¹˜",
    "title": "ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼í• ì§€ ì„œì¹˜í•˜ê³  ì •ë¦¬",
    "date": "2025-07-14",
    "excerpt": "OMSCS ì§€ì›ì„ ìœ„í•œ í† í”Œ ê³µë¶€",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/Fact and Negative Fact",
    "title": "Factì™€ Negative Fact ì°¨ì´",
    "date": "2025-07-14",
    "excerpt": "Factì™€ Negative Fact ìœ í˜• í’€ì´ë²•",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/BirdDialects",
    "title": "Bird Dialects",
    "date": "2025-07-14",
    "excerpt": "Diagnostic Test ë¬¸ì œ ê³µë¶€",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›"
    ],
    "content": "Bird Dialects ìƒˆ ë‹¤ì´ëŸ¬ë ‰ì¸ ? ìƒˆì˜ ë°©ì–¸ Paragraph 1 <button class=\"toggle-button translations\" data-type=\"paragraph1\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph1-content\"> > Birds learn acoustic information from their parents and environment from an early age. <span class=\"translation\">ìƒˆë“¤ì€ ì–´ë¦° ë‚˜ì´ë¶€í„° ë¶€ëª¨ì™€ í™˜ê²½ìœ¼ë¡œë¶€í„° ìŒí–¥ ì •ë³´ë¥¼ ë°°ìš´ë‹¤.</span> > The most complex form of avian vocalization is the birdsong, an often melodic series of communicative sounds produced primarily during breeding. <span class=\"translation\">ì¡°ë¥˜ì˜ ë°œì„± ì¤‘ ê°€ì¥ ë³µì¡í•œ í˜•íƒœëŠ” ìƒˆì˜ ë…¸ë˜ë¡œ, ë²ˆì‹ê¸°ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì¢…ì¢… ì„ ìœ¨ì ì¸ ì˜ì‚¬ì†Œí†µ ì†Œë¦¬ì˜ ì—°ì†ì´ë‹¤.</span> > In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects. <span class=\"translation\">ê°œì²´êµ°ì´ ì§€ë¦¬ì  ê²½ê³„ë¥¼ ê°€ì§„ ê²½ìš°, ë…¸ë˜ì—ì„œ ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ì§€ë¦¬ì  í•œê³„ê°€ ëšœë ·í•˜ë©´, í•œ ê°œì²´êµ° ë‚´ì—ì„œ ê³µìœ ë˜ëŠ” ë…¸ë˜ ìœ í˜•ì„ ë°©ì–¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</span> > Ornithologists are keenly interested in how these dialects form and to what extent genetic factors contribute to dialectical variation. <span class=\"translation\">ì¡°ë¥˜í•™ìë“¤ì€ ì´ëŸ¬í•œ ë°©ì–¸ì´ ì–´ë–»ê²Œ í˜•ì„±ë˜ëŠ”ì§€ì™€ ìœ ì „ì  ìš”ì¸ì´ ë°©ì–¸ì  ë³€í™”ì— ì–´ëŠ ì •ë„ ê¸°ì—¬í•˜ëŠ”ì§€ì— ê¹Šì€ ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆë‹¤.</span> > One of the best-studied songbirds in this regard is the white-crowned sparrow, Zonotrichia leucophrys, a species with several subspecies and many dialects. <span class=\"translation\">ì´ì™€ ê´€ë ¨í•˜ì—¬ ê°€ì¥ ì˜ ì—°êµ¬ëœ ëª…ê¸ˆ ì¤‘ í•˜ë‚˜ëŠ” í°ê´€ì°°ì°Œìƒˆ(Zonotrichia leucophrys)ë¡œ, ì—¬ëŸ¬ ì•„ì¢…ê³¼ ë§ì€ ë°©ì–¸ì„ ê°€ì§„ ì¢…ì´ë‹¤.</span> </div> Paragraph 2 <button class=\"toggle-button translations\" data-type=\"paragraph2\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph2-content\"> > In Zonotrichia leucophrys, males demonstrate the greatest dialectical diversity. Therefore, studying male birds offers the best opportunity for understanding dialect formation. <span class=\"translation\">í°ê´€ì°°ì°Œìƒˆ(Zonotrichia leucophrys)ì—ì„œ ìˆ˜ì»·ì´ ê°€ì¥ í° ë°©ì–¸ì  ë‹¤ì–‘ì„±ì„ ë³´ì¸ë‹¤. ë”°ë¼ì„œ ìˆ˜ì»· ìƒˆë¥¼ ì—°êµ¬í•˜ëŠ” ê²ƒì´ ë°©ì–¸ í˜•ì„±ì„ ì´í•´í•˜ëŠ” ë° ê°€ì¥ ì¢‹ì€ ê¸°íšŒë¥¼ ì œê³µí•œë‹¤.</span> > In order to make sense of birdsongs, scientists break them into smaller units. <span class=\"translation\">ìƒˆì˜ ë…¸ë˜ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ê³¼í•™ìë“¤ì€ ê·¸ê²ƒë“¤ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤.</span> > A phrase, for example, is a large category below the level of song, and specific sound types like whistles, pulses, and buzzes reflect the individual speech parts, or syllables, of a phrase. <span class=\"translation\">ì˜ˆë¥¼ ë“¤ì–´, êµ¬ë¬¸ì€ ë…¸ë˜ ìˆ˜ì¤€ ì•„ë˜ì˜ í° ë²”ì£¼ì´ë©°, íœ˜íŒŒëŒ, í„ìŠ¤, ìœ™ìœ™ê±°ë¦¬ëŠ” ì†Œë¦¬ì™€ ê°™ì€ íŠ¹ì • ì†Œë¦¬ ìœ í˜•ì€ êµ¬ë¬¸ì˜ ê°œë³„ ë°œí™” ë¶€ë¶„, ì¦‰ ìŒì ˆì„ ë°˜ì˜í•œë‹¤.</span> > The phrases are ordered according to their position in the song so that specific parts can be analyzed separately and compared across different individuals. <span class=\"translation\">êµ¬ë¬¸ë“¤ì€ ë…¸ë˜ì—ì„œì˜ ìœ„ì¹˜ì— ë”°ë¼ ìˆœì„œê°€ ì •í•´ì ¸ì„œ, íŠ¹ì • ë¶€ë¶„ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì„œë¡œ ë‹¤ë¥¸ ê°œì²´ë“¤ ê°„ì— ë¹„êµí•  ìˆ˜ ìˆë‹¤.</span> </div> Paragraph 3 <button class=\"toggle-button translations\" data-type=\"paragraph3\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph3-content\"> > Some phrases show more variability than others, and once scientists detect variability in a specific category, they can focus on that section of the songs in future studies. <span class=\"translation\">ì¼ë¶€ êµ¬ë¬¸ë“¤ì€ ë‹¤ë¥¸ êµ¬ë¬¸ë“¤ë³´ë‹¤ ë” ë§ì€ ë³€ì´ì„±ì„ ë³´ì´ë©°, ê³¼í•™ìë“¤ì´ íŠ¹ì • ë²”ì£¼ì—ì„œ ë³€ì´ì„±ì„ ë°œê²¬í•˜ë©´, í–¥í›„ ì—°êµ¬ì—ì„œ ë…¸ë˜ì˜ ê·¸ ë¶€ë¶„ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë‹¤.</span> > In other words, if the terminal phrase shows the greatest inconsistency, recordings can be edited to single it out for comparisons. <span class=\"translation\">ë‹¤ì‹œ ë§í•´ì„œ, ë§Œì•½ ë§ˆì§€ë§‰ êµ¬ë¬¸ì´ ê°€ì¥ í° ë¶ˆì¼ì¹˜ì„±ì„ ë³´ì¸ë‹¤ë©´, ë…¹ìŒì„ í¸ì§‘í•˜ì—¬ ë¹„êµë¥¼ ìœ„í•´ ê·¸ê²ƒë§Œ ë”°ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤.</span> > Most white-crowned sparrow songs begin similarly with one or two whistles; consequently, the initial sequence is generally the most consistent component across subgroups. <span class=\"translation\">ëŒ€ë¶€ë¶„ì˜ í°ê´€ì°°ì°Œìƒˆ ë…¸ë˜ëŠ” í•˜ë‚˜ ë˜ëŠ” ë‘ ê°œì˜ íœ˜íŒŒëŒìœ¼ë¡œ ë¹„ìŠ·í•˜ê²Œ ì‹œì‘í•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ˆê¸° ìˆœì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì•„ì¢…ë“¤ ê°„ì— ê°€ì¥ ì¼ê´€ëœ êµ¬ì„± ìš”ì†Œì´ë‹¤.</span> > In addition, a convenient feature of male white-crowned sparrows is that individual adults generally sing a single song, making analysis across groups a relatively simple task in comparison to other bird species that routinely learn multiple songs, such as song sparrows (Melospiza melodia). <span class=\"translation\">ë˜í•œ, ìˆ˜ì»· í°ê´€ì°°ì°Œìƒˆì˜ í¸ë¦¬í•œ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ê°œë³„ ì„±ì²´ê°€ ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ë…¸ë˜ë§Œ ë¶€ë¥¸ë‹¤ëŠ” ê²ƒìœ¼ë¡œ, ì´ëŠ” ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ë°°ìš°ëŠ” ë…¸ë˜ì°Œìƒˆ(Melospiza melodia)ì™€ ê°™ì€ ë‹¤ë¥¸ ì¡°ë¥˜ ì¢…ì— ë¹„í•´ ì§‘ë‹¨ ê°„ ë¶„ì„ì„ ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨í•œ ì‘ì—…ìœ¼ë¡œ ë§Œë“ ë‹¤.</span> </div> Paragraph 4 <button class=\"toggle-button translations\" data-type=\"paragraph4\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph4-content\"> > Variations present among groups led some scientists to wonder whether there might be a genetic basis for dialectical uniqueness. <span class=\"translation\">ì§‘ë‹¨ë“¤ ê°„ì— ì¡´ì¬í•˜ëŠ” ë³€ì´ë“¤ì€ ì¼ë¶€ ê³¼í•™ìë“¤ì´ ë°©ì–¸ì  ê³ ìœ ì„±ì— ìœ ì „ìì ì¸ ê¸°ì €ê°€ ìˆëŠ”ì§€ ê¶ê¸ˆí•˜ê²Œ í–ˆë‹¤.</span> > Perhaps through the evolutionary isolation of some populations (for example, female preference for males of their own dialect), dispersal of the gene pool has been slowed to the point that differences in song reflect expressions of varying genetic compositions. <span class=\"translation\">ì•„ë§ˆë„ ì¼ë¶€ ê°œì²´êµ°ì˜ ì§„í™”ì  ê²©ë¦¬ë¥¼ í†µí•´ (ì˜ˆë¥¼ ë“¤ì–´, ì•”ì»·ì´ ìì‹ ì˜ ë°©ì–¸ì„ ê°€ì§„ ìˆ˜ì»·ì„ ì„ í˜¸í•˜ëŠ” ê²ƒ), ìœ ì „ì í’€ì˜ ë¶„ì‚°ì´ ëŠë ¤ì ¸ì„œ ë…¸ë˜ì˜ ì°¨ì´ê°€ ë‹¤ì–‘í•œ ìœ ì „ì  êµ¬ì„±ì˜ í‘œí˜„ì„ ë°˜ì˜í•˜ê²Œ ë˜ì—ˆì„ ê²ƒì´ë‹¤.</span> > Though a genetic explanation seems plausible, laboratory experiments have found that white-crowned sparrow chicks of particular dialect groups can learn the songs of other groups in their own subspecies in addition to dialects of other subspecies. <span class=\"translation\">ìœ ì „ìì  í•´ì„ì´ ë§ì´ í”ŒëŸ¬ì„œë¸” í•˜ë‹¤ê³  ë³¼ ìˆ˜ë„ ìˆì§€ë§Œ, ì‹¤í—˜ì—ì„œ í•˜ì–€ êµ°ì¤‘ ê¹Œë§ˆê·€ëŠ” ë‹¤ë¥¸ ì•„ì¢…ì˜ ë…¸ë˜ë‚˜ ë°©ì–¸ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤.</span> > Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects, although the maximum number is approximately seven. <span class=\"translation\">ì„±ì²´ì˜ ë…¹ìŒì„ ì‚¬ìš©í•˜ì—¬, ê³¼í•™ìë“¤ì€ ì–´ë¦° ìƒˆë“¤ì´ ì—¬ëŸ¬ ë°©ì–¸ì„ ì¬í˜„í•˜ë„ë¡ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆì—ˆëŠ”ë°, ìµœëŒ€ ê°œìˆ˜ëŠ” ì•½ 7ê°œì˜€ë‹¤.</span> > In addition, comparisons of genetic material taken from blood sample DNA have revealed unsubstantial differences. <span class=\"translation\">ë˜í•œ, í˜ˆì•¡ ìƒ˜í”Œ DNAì—ì„œ ì¶”ì¶œí•œ ìœ ì „ ë¬¼ì§ˆì˜ ë¹„êµëŠ” ì‹¤ì§ˆì ì´ì§€ ì•Šì€ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.</span> > In some instances, genetic variation among individuals of a dialect group was found to be greater than genetic variation among larger groups, such as subspecies. <span class=\"translation\">ì–´ë–¤ ê²½ìš°ì—ëŠ”, ë°©ì–¸ ì§‘ë‹¨ ë‚´ ê°œì²´ë“¤ ê°„ì˜ ìœ ì „ì  ë³€ì´ê°€ ì•„ì¢…ê³¼ ê°™ì€ ë” í° ì§‘ë‹¨ë“¤ ê°„ì˜ ìœ ì „ì  ë³€ì´ë³´ë‹¤ ë” í° ê²ƒìœ¼ë¡œ ë°í˜€ì¡Œë‹¤.</span> </div> Paragraph 5 <button class=\"toggle-button translations\" data-type=\"paragraph5\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph5-content\"> > If genetic differences are insignificant in comparison to cultural influences, why do sparrows with overlapping territories sing only one song in the field? <span class=\"translation\">ë§Œì•½ ìœ ì „ì  ì°¨ì´ê°€ ë¬¸í™”ì  ì˜í–¥ì— ë¹„í•´ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ë©´, ì™œ ê²¹ì¹˜ëŠ” ì˜ì—­ì„ ê°€ì§„ ì°¸ìƒˆë“¤ì€ ì•¼ìƒì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ë…¸ë˜ë§Œ ë¶€ë¥´ëŠ”ê°€?</span> > Some, such as those of the sedentary subspecies known as Nuttall's sparrow, do encounter other dialect groups. <span class=\"translation\">ì¼ë¶€ëŠ”, ë„ˆíƒˆ ì°¸ìƒˆë¡œ ì•Œë ¤ì§„ ì •ì£¼ì„± ì•„ì¢… ê°™ì€ ê²½ìš°, ë‹¤ë¥¸ ë°©ì–¸ ê·¸ë£¹ê³¼ ë§ˆì£¼ì¹œë‹¤.</span> > Gambell's sparrow, a migratory subspecies, overwinters in the coastal California area that Nuttall's sparrow inhabits. <span class=\"translation\">ê°ë²¨ ì°¸ìƒˆëŠ”, ì´ë™ì„± ì•„ì¢…ìœ¼ë¡œ, ë„ˆíƒˆ ì°¸ìƒˆê°€ ì„œì‹í•˜ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ ì—°ì•ˆ ì§€ì—­ì—ì„œ ì›”ë™í•œë‹¤.</span> > Interestingly, despite contacts between October and March with Gambell's sparrow, male Nuttall's sparrows typically sing their own distinct songs. <span class=\"translation\">í¥ë¯¸ë¡­ê²Œë„, 10ì›”ê³¼ 3ì›” ì‚¬ì´ì— ê°ë²¨ ì°¸ìƒˆì™€ ì ‘ì´‰í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ìˆ˜ì»· ë„ˆíƒˆ ì°¸ìƒˆë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ìì‹ ë“¤ë§Œì˜ ë…íŠ¹í•œ ë…¸ë˜ë¥¼ ë¶€ë¥¸ë‹¤.</span> > This is because the encounters between the two subspecies occur outside the primary song crystallization period, which occurs during the first two months of a bird's life. <span class=\"translation\">ì´ê²ƒì€ ë‘ ì•„ì¢… ê°„ì˜ ë§Œë‚¨ì´ ì£¼ìš” ë…¸ë˜ ê²°ì •í™” ê¸°ê°„ ë°–ì—ì„œ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì¸ë°, ì´ ê¸°ê°„ì€ ìƒˆì˜ ìƒí›„ ì²« ë‘ ë‹¬ ë™ì•ˆ ë°œìƒí•œë‹¤.</span> </div> Paragraph 6 <button class=\"toggle-button translations\" data-type=\"paragraph6\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph6-content\"> > Detailed field research led to the discovery that some exceptional members of the species do master multiple songs, but their knowledge acquisition is contingent on contact with other groups in one of two ways. <span class=\"translation\">ìƒì„¸í•œ í˜„ì¥ ì—°êµ¬ëŠ” ì´ ì¢…ì˜ ì¼ë¶€ ì˜ˆì™¸ì ì¸ êµ¬ì„±ì›ë“¤ì´ ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ìˆ™ë‹¬í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆì§€ë§Œ, ê·¸ë“¤ì˜ ì§€ì‹ ìŠµë“ì€ ë‘ ê°€ì§€ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ë‹¤ë¥¸ ì§‘ë‹¨ê³¼ì˜ ì ‘ì´‰ì— ë‹¬ë ¤ìˆë‹¤.</span> > Either young birds come in contact with migratory subspecies during the formative period of language learning or they live on the margins of dialect borders. <span class=\"translation\">ì–´ë¦° ìƒˆë“¤ì´ ì–¸ì–´ í•™ìŠµì˜ í˜•ì„± ê¸°ê°„ ë™ì•ˆ ì´ë™ì„± ì•„ì¢…ê³¼ ì ‘ì´‰í•˜ê±°ë‚˜, ë°©ì–¸ ê²½ê³„ì˜ ê°€ì¥ìë¦¬ì— ì‚´ê³  ìˆë‹¤.</span> > Nuttall's sparrows hatching early or late in the season are more likely to be exposed to Gambell's sparrows, but broods hatching after March or before October will mostly only interact with other members of their own kind. <span class=\"translation\">ê³„ì ˆì— ì¼ì° ë˜ëŠ” ëŠ¦ê²Œ ë¶€í™”í•˜ëŠ” Nuttall's sparrowë“¤ì€ Gambell's sparrowë“¤ì— ë…¸ì¶œë  ê°€ëŠ¥ì„±ì´ ë” ë†’ì§€ë§Œ, 3ì›” ì´í›„ ë˜ëŠ” 10ì›” ì´ì „ì— ë¶€í™”í•˜ëŠ” ìƒˆë¼ë“¤ì€ ëŒ€ë¶€ë¶„ ìì‹ ì˜ ì¢…ë¥˜ì˜ ë‹¤ë¥¸ êµ¬ì„±ì›ë“¤ê³¼ë§Œ ìƒí˜¸ì‘ìš©í•  ê²ƒì´ë‹¤.</span> > In like fashion, individuals living near the peripheries of their dialectic range may regularly hear the songs of an adjacent group and develop multilingual abilities. <span class=\"translation\">ê°™ì€ ë°©ì‹ìœ¼ë¡œ, ë°©ì–¸ ë²”ìœ„ì˜ ì£¼ë³€ì— ì‚¬ëŠ” ê°œì²´ë“¤ì€ ì¸ì ‘í•œ ì§‘ë‹¨ì˜ ë…¸ë˜ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ë“£ê³  ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì„ ê°œë°œí•  ìˆ˜ ìˆë‹¤.</span> </div> Paragraph 7 <button class=\"toggle-button translations\" data-type=\"paragraph7\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph7-content\"> > In light of these findings, most scientists have concluded that geographic differences may mildly restrict gene flow, but that genetic isolation has not been strong enough to create a rigid linguistic barrier. <span class=\"translation\">ì´ëŸ¬í•œ ë°œê²¬ì„ ë°”íƒ•ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ì˜ ê³¼í•™ìë“¤ì€ ì§€ë¦¬ì  ì°¨ì´ê°€ ìœ ì „ì íë¦„ì„ ì•½ê°„ ì œí•œí•  ìˆ˜ ìˆì§€ë§Œ, ìœ ì „ì  ê²©ë¦¬ê°€ ì—„ê²©í•œ ì–¸ì–´ì  ì¥ë²½ì„ ë§Œë“¤ ë§Œí¼ ì¶©ë¶„íˆ ê°•í•˜ì§€ ì•Šì•˜ë‹¤ê³  ê²°ë¡ ì§€ì—ˆë‹¤.</span> > Dialectical variety in birds can thus be viewed as primarily a cultural phenomenon. <span class=\"translation\">ë”°ë¼ì„œ ìƒˆë“¤ì˜ ë°©ì–¸ì  ë‹¤ì–‘ì„±ì€ ì£¼ë¡œ ë¬¸í™”ì  í˜„ìƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.</span> </div> --- ğŸ“š í•µì‹¬ ë‹¨ì–´ ì‚¬ì „ <button class=\"toggle-button vocabulary\" data-type=\"vocabulary\">ğŸ” ë‹¨ì–´ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"vocabulary-content\"> ğŸ¦ ì¡°ë¥˜í•™ ê´€ë ¨ ìš©ì–´ avian vocalization <span class=\"meaning\">(ì¡°ë¥˜ì˜ ë°œì„±) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬</span> birdsong <span class=\"meaning\">(ìƒˆì˜ ë…¸ë˜) - ìƒˆê°€ ë¶€ë¥´ëŠ” ë³µì¡í•œ ì†Œë¦¬</span> dialect <span class=\"meaning\">(ë°©ì–¸) - ì§€ë¦¬ì  ê²½ê³„ì— ë”°ë¥¸ ë…¸ë˜ì˜ ì°¨ì´</span> phrase <span class=\"meaning\">(êµ¬ë¬¸) - ë…¸ë˜ì˜ êµ¬ì„± ë‹¨ìœ„</span> syllable <span class=\"meaning\">(ìŒì ˆ) - êµ¬ë¬¸ì˜ ê°œë³„ ë°œí™” ë¶€ë¶„</span> whistle <span class=\"meaning\">(íœ˜íŒŒëŒ) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬ ìœ í˜•</span> pulse <span class=\"meaning\">(í„ìŠ¤) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬ ìœ í˜•</span> buzz <span class=\"meaning\">(ìœ™ìœ™ê±°ë¦¬ëŠ” ì†Œë¦¬) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬ ìœ í˜•</span> ğŸ§¬ ìœ ì „í•™ ê´€ë ¨ ìš©ì–´ gene pool <span class=\"meaning\">(ìœ ì „ì í’€) - ì§‘ë‹¨ì˜ ì „ì²´ ìœ ì „ì  ë‹¤ì–‘ì„±</span> genetic isolation <span class=\"meaning\">(ìœ ì „ì  ê²©ë¦¬) - ìœ ì „ì êµí™˜ì˜ ì°¨ë‹¨</span> subspecies <span class=\"meaning\">(ì•„ì¢…) - ì¢…ì˜ í•˜ìœ„ ë¶„ë¥˜</span> genetic variation <span class=\"meaning\">(ìœ ì „ì  ë³€ì´) - ìœ ì „ì  ì°¨ì´</span> gene flow <span class=\"meaning\">(ìœ ì „ì íë¦„) - ì§‘ë‹¨ ê°„ ìœ ì „ì êµí™˜</span> ğŸ• ì‹œê°„ ê´€ë ¨ ìš©ì–´ formative period <span class=\"meaning\">(í˜•ì„± ê¸°ê°„) - í•™ìŠµì´ ì´ë£¨ì–´ì§€ëŠ” ì¤‘ìš”í•œ ì‹œê¸°</span> song crystallization period <span class=\"meaning\">(ë…¸ë˜ ê²°ì •í™” ê¸°ê°„) - ìƒí›„ 2ê°œì›”ì˜ í•µì‹¬ í•™ìŠµ ì‹œê¸°</span> overwinter <span class=\"meaning\">(ì›”ë™í•˜ë‹¤) - ê²¨ìš¸ì„ ë‚˜ë‹¤</span> ğŸ—ºï¸ ì§€ë¦¬ ê´€ë ¨ ìš©ì–´ geographical boundaries <span class=\"meaning\">(ì§€ë¦¬ì  ê²½ê³„) - ì§€ì—­ì  í•œê³„</span> overlapping territories <span class=\"meaning\">(ê²¹ì¹˜ëŠ” ì˜ì—­) - ì„œë¡œ ê²¹ì¹˜ëŠ” ì„œì‹ì§€</span> sedentary <span class=\"meaning\">(ì •ì£¼ì„±) - í•œ ê³³ì— ë¨¸ë¬´ë¥´ëŠ”</span> migratory <span class=\"meaning\">(ì´ë™ì„±) - ê³„ì ˆì— ë”°ë¼ ì´ë™í•˜ëŠ”</span> peripheries <span class=\"meaning\">(ì£¼ë³€ë¶€) - ê°€ì¥ìë¦¬ ì§€ì—­</span> </div> --- ğŸ’¬ ìœ ìš©í•œ ì˜ì–´ í‘œí˜„ë“¤ <button class=\"toggle-button expressions\" data-type=\"expressions\">ğŸ” í‘œí˜„ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"expressions-content\"> ğŸ”¬ ì—°êµ¬/ì‹¤í—˜ ê´€ë ¨ í‘œí˜„ in this regard <span class=\"meaning\">(ì´ì™€ ê´€ë ¨í•˜ì—¬) - \"One of the best-studied songbirds in this regard is...\"</span> led to the discovery <span class=\"meaning\">(ë°œê²¬ìœ¼ë¡œ ì´ì–´ì¡Œë‹¤) - \"Detailed field research led to the discovery that...\"</span> have been able to <span class=\"meaning\">(í•  ìˆ˜ ìˆì—ˆë‹¤) - \"Scientists have been able to train fledglings...\"</span> have found that <span class=\"meaning\">(ë°œê²¬í–ˆë‹¤) - \"Laboratory experiments have found that...\"</span> have revealed <span class=\"meaning\">(ë³´ì—¬ì£¼ì—ˆë‹¤) - \"Comparisons have revealed unsubstantial differences.\"</span> ğŸ“Š ë¹„êµ/ëŒ€ì¡° í‘œí˜„ in comparison to <span class=\"meaning\">(ë¹„êµí–ˆì„ ë•Œ) - \"If genetic differences are insignificant in comparison to cultural influences...\"</span> in addition <span class=\"meaning\">(ë˜í•œ) - \"In addition, a convenient feature...\"</span> though <span class=\"meaning\">(ë¹„ë¡...í•˜ì§€ë§Œ) - \"Though a genetic explanation seems plausible...\"</span> despite <span class=\"meaning\">(ì—ë„ ë¶ˆêµ¬í•˜ê³ ) - \"Despite contacts between October and March...\"</span> in light of <span class=\"meaning\">(ì„ ë°”íƒ•ìœ¼ë¡œ) - \"In light of these findings...\"</span> â° ì‹œê°„/ìˆœì„œ í‘œí˜„ from an early age <span class=\"meaning\">(ì–´ë¦° ë‚˜ì´ë¶€í„°) - \"Birds learn from an early age.\"</span> during <span class=\"meaning\">(ë™ì•ˆ) - \"Produced primarily during breeding.\"</span> between <span class=\"meaning\">(ì‚¬ì´ì—) - \"Between October and March.\"</span> after/before <span class=\"meaning\">(ì´í›„/ì´ì „) - \"After March or before October.\"</span> once <span class=\"meaning\">(ì¼ë‹¨...í•˜ë©´) - \"Once scientists detect variability...\"</span> ğŸ¯ ê°•ì¡°/ì„¤ëª… í‘œí˜„ in other words <span class=\"meaning\">(ë‹¤ì‹œ ë§í•´ì„œ) - \"In other words, if the terminal phrase...\"</span> for example <span class=\"meaning\">(ì˜ˆë¥¼ ë“¤ì–´) - \"For example, female preference for males...\"</span> such as <span class=\"meaning\">(ê°™ì€) - \"Such as song sparrows.\"</span> in like fashion <span class=\"meaning\">(ê°™ì€ ë°©ì‹ìœ¼ë¡œ) - \"In like fashion, individuals living near...\"</span> thus <span class=\"meaning\">(ë”°ë¼ì„œ) - \"Dialectical variety can thus be viewed as...\"</span> ğŸ” ì¡°ê±´/ê°€ì • í‘œí˜„ in cases where <span class=\"meaning\">(ì¸ ê²½ìš°ì—) - \"In cases where populations have geographical boundaries...\"</span> if <span class=\"meaning\">(ë§Œì•½) - \"If these geographical limits are distinct...\"</span> whether <span class=\"meaning\">(ì¸ì§€ ì•„ë‹Œì§€) - \"Whether there might be a genetic basis...\"</span> perhaps <span class=\"meaning\">(ì•„ë§ˆë„) - \"Perhaps through the evolutionary isolation...\"</span> may <span class=\"meaning\">(í•  ìˆ˜ ìˆë‹¤) - \"Geographic differences may mildly restrict gene flow.\"</span> ğŸ“ í•™ìˆ ì  í‘œí˜„ referred to as <span class=\"meaning\">(ë¼ê³  ë¶ˆë¦°ë‹¤) - \"Are referred to as dialects.\"</span> keenly interested in <span class=\"meaning\">(ì— ê¹Šì€ ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆë‹¤) - \"Ornithologists are keenly interested in...\"</span> make sense of <span class=\"meaning\">(ì´í•´í•˜ë‹¤) - \"In order to make sense of birdsongs...\"</span> contingent on <span class=\"meaning\">(ì— ë‹¬ë ¤ìˆë‹¤) - \"Knowledge acquisition is contingent on contact...\"</span> concluded that <span class=\"meaning\">(ë¼ê³  ê²°ë¡ ì§€ì—ˆë‹¤) - \"Scientists have concluded that...\"</span> </div> ë¬¸ì œí’€ì´ TOEFL Reading Comprehension Questions Question 1 The word \"primarily\" in the passage is closest in meaning to (A) abundantly (B) mainly (C) loudly (D) commonly <button class=\"toggle-button answer\" data-type=\"answer1\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer1-content\"> ì •ë‹µ: (B) mainly í•´ì„¤: \"primarily\"ëŠ” \"ì£¼ë¡œ, ì£¼ëœ\"ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì§€ë¬¸ì—ì„œ \"produced primarily during breeding\"ì€ \"ë²ˆì‹ê¸°ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ”\"ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (A) abundantly: í’ë¶€í•˜ê²Œ - ì–‘ì  ì˜ë¯¸ (B) mainly: ì£¼ë¡œ - ì£¼ëœ ëª©ì ì´ë‚˜ ì‹œê¸°ë¥¼ ë‚˜íƒ€ëƒ„ (C) loudly: í¬ê²Œ - ì†Œë¦¬ì˜ í¬ê¸° (D) commonly: í”íˆ - ë¹ˆë„ì˜ ì˜ë¯¸ </div> Question 2 Which of the sentences below best expresses the essential information in the highlighted sentence in the passage? Incorrect choices change the meaning in important ways or leave out essential information. ì›ë¬¸: \"In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects.\" (A) Dialects are those song types within a population that are shared by the group. (B) Shared song types within a population with clear geographical boundaries are called dialects. (C) Geographical boundaries may result in differences in song between distinct populations. (D) Populations having clear dialects result from the shared song types within the group. <button class=\"toggle-button answer\" data-type=\"answer2\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer2-content\"> ì •ë‹µ: (B) Shared song types within a population with clear geographical boundaries are called dialects. í•´ì„¤: ì›ë¬¸ì˜ í•µì‹¬ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: ì§€ë¦¬ì  ê²½ê³„ê°€ ìˆëŠ” ê°œì²´êµ°ì—ì„œëŠ” ë…¸ë˜ì˜ ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤ ì´ëŸ¬í•œ ì§€ë¦¬ì  í•œê³„ê°€ ëšœë ·í•˜ë©´, ê°œì²´êµ° ë‚´ì—ì„œ ê³µìœ ë˜ëŠ” ë…¸ë˜ ìœ í˜•ì„ ë°©ì–¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤ (A) ë°©ì–¸ì˜ ì •ì˜ë§Œ í¬í•¨, ì§€ë¦¬ì  ê²½ê³„ ì¡°ê±´ ëˆ„ë½ (B) ì§€ë¦¬ì  ê²½ê³„ì™€ ê³µìœ ëœ ë…¸ë˜ ìœ í˜•ì´ ë°©ì–¸ì´ë¼ëŠ” í•µì‹¬ ì •ë³´ ëª¨ë‘ í¬í•¨ (C) ì§€ë¦¬ì  ê²½ê³„ê°€ ì°¨ì´ë¥¼ ë§Œë“ ë‹¤ëŠ” ë¶€ë¶„ë§Œ, ë°©ì–¸ì˜ ì •ì˜ ëˆ„ë½ (D) ì¸ê³¼ê´€ê³„ê°€ ì˜ëª»ë¨ (ë°©ì–¸ì´ ê°œì²´êµ°ì„ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹˜) </div> Question 3 According to paragraph 2, scientists attempt to understand birdsongs by (A) dividing them into smaller categories (B) analyzing them as a single unit (C) listening to phrases in reverse order (D) labeling them as male or female <button class=\"toggle-button answer\" data-type=\"answer3\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer3-content\"> ì •ë‹µ: (A) dividing them into smaller categories í•´ì„¤: 2ë‹¨ë½ì—ì„œ \"In order to make sense of birdsongs, scientists break them into smaller units\"ë¼ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³¼í•™ìë“¤ì€ ìƒˆì˜ ë…¸ë˜ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ: ë…¸ë˜(song) â†’ êµ¬ë¬¸(phrase) â†’ ìŒì ˆ(syllables)ë¡œ ì„¸ë¶„í™” íœ˜íŒŒëŒ, í„ìŠ¤, ìœ™ìœ™ê±°ë¦¬ëŠ” ì†Œë¦¬ ë“± íŠ¹ì • ì†Œë¦¬ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜ ì´ëŠ” \"ë” ì‘ì€ ë²”ì£¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ\"ê³¼ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤. </div> Question 4 What can be inferred from paragraph 4 about white-crowned sparrow chicks? (A) The simplest dialects are the ones they learn first. (B) The number of songs they can be trained to learn is limited. (C) They learn songs of their own subspecies faster than those of other subspecies. (D) They learn more easily from laboratory recordings than in the wild. <button class=\"toggle-button answer\" data-type=\"answer4\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer4-content\"> ì •ë‹µ: (B) The number of songs they can be trained to learn is limited. í•´ì„¤: 4ë‹¨ë½ì—ì„œ \"Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects, although the maximum number is approximately seven\"ë¼ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³¼í•™ìë“¤ì´ ì–´ë¦° ìƒˆë“¤ì„ í›ˆë ¨ì‹œì¼œ ì—¬ëŸ¬ ë°©ì–¸ì„ ì¬í˜„í•˜ê²Œ í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, ìµœëŒ€ ê°œìˆ˜ëŠ” ì•½ 7ê°œë¼ê³  í•œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë…¸ë˜ì˜ ìˆ˜ê°€ ì œí•œì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (A) ê°€ì¥ ê°„ë‹¨í•œ ë°©ì–¸ì„ ë¨¼ì € ë°°ìš´ë‹¤ëŠ” ì–¸ê¸‰ ì—†ìŒ (C) ìì‹ ì˜ ì•„ì¢… ë…¸ë˜ë¥¼ ë” ë¹¨ë¦¬ ë°°ìš´ë‹¤ëŠ” ì–¸ê¸‰ ì—†ìŒ (D) ì‹¤í—˜ì‹¤ ë…¹ìŒìœ¼ë¡œ ë” ì‰½ê²Œ ë°°ìš´ë‹¤ëŠ” ë¹„êµ ì—†ìŒ </div> Question 5 According to paragraph 5, which of the following is true of Nuttall's sparrows? (A) They migrate to the same overwintering locations that Gambell's sparrows do. (B) They do not often encounter Gambell's sparrows during the initial period of song formation. (C) They only have contact with Gambell's sparrows during the breeding season. (D) They do not have the same inherent vocalization abilities that Gambell's sparrows do. <button class=\"toggle-button answer\" data-type=\"answer5\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer5-content\"> ì •ë‹µ: (B) They do not often encounter Gambell's sparrows during the initial period of song formation. í•´ì„¤: 5ë‹¨ë½ì—ì„œ í•µì‹¬ ì •ë³´: Nuttall's sparrowëŠ” ì •ì£¼ì„±(sedentary) ì•„ì¢… Gambell's sparrowëŠ” ì´ë™ì„±(migratory) ì•„ì¢…ìœ¼ë¡œ 10ì›”~3ì›”ì— Nuttall's ì„œì‹ì§€ì—ì„œ ì›”ë™ \"This is because the encounters between the two subspecies occur outside the primary song crystallization period, which occurs during the first two months of a bird's life\" ì¦‰, ë‘ ì•„ì¢…ì˜ ë§Œë‚¨ì€ ì£¼ìš” ë…¸ë˜ ê²°ì •í™” ê¸°ê°„(ìƒí›„ ì²« 2ê°œì›”) ë°–ì—ì„œ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì—, Nuttall's sparrowëŠ” ë…¸ë˜ í˜•ì„± ì´ˆê¸°ì— Gambell's sparrowì™€ ìì£¼ ë§ˆì£¼ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (A) Nuttall'sëŠ” ì´ë™í•˜ì§€ ì•ŠìŒ (sedentary) (C) 10ì›”~3ì›”ì— ì ‘ì´‰ (ë²ˆì‹ê¸°ê°€ ì•„ë‹˜) (D) ë°œì„± ëŠ¥ë ¥ì˜ ì°¨ì´ì— ëŒ€í•œ ì–¸ê¸‰ ì—†ìŒ </div> Question 6 The word \"exceptional\" in the passage is closest in meaning to (A) extraordinary (B) perceptual (C) intelligent (D) capable <button class=\"toggle-button answer\" data-type=\"answer6\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer6-content\"> ì •ë‹µ: (A) extraordinary í•´ì„¤: 6ë‹¨ë½ì—ì„œ \"some exceptional members of the species do master multiple songs\"ë¼ê³  ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ë§¥ìƒ ëŒ€ë¶€ë¶„ì˜ ê°œì²´ë“¤ì€ í•˜ë‚˜ì˜ ë…¸ë˜ë§Œ ë¶€ë¥´ëŠ”ë°, ì¼ë¶€ ì˜ˆì™¸ì ì¸ êµ¬ì„±ì›ë“¤ì€ ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ìˆ™ë‹¬í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (A) extraordinary: íŠ¹ë³„í•œ, ì˜ˆì™¸ì ì¸ (B) perceptual: ì§€ê°ì˜, ì¸ì‹ì˜ (C) intelligent: ì§€ëŠ¥ì ì¸ (D) capable: ëŠ¥ë ¥ìˆëŠ” \"exceptional\"ì€ \"ì¼ë°˜ì ì´ì§€ ì•Šì€, íŠ¹ë³„í•œ\"ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ \"extraordinary\"ì™€ ê°€ì¥ ì˜ë¯¸ê°€ ê°€ê¹ìŠµë‹ˆë‹¤. </div> Question 7 According to paragraph 6, which of the following does NOT contribute to multilingualism in Nuttall's sparrows? (A) Living on the edges of dialectical boundaries (B) Interacting with other subspecies (C) Hatching early or late in the season (D) Mating between March and October âœ“ <button class=\"toggle-button answer\" data-type=\"answer7\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer7-content\"> ì •ë‹µ: (D) Mating between March and October í•´ì„¤: 6ë‹¨ë½ì—ì„œ ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì— ê¸°ì—¬í•˜ëŠ” ìš”ì¸ë“¤: ê¸°ì—¬í•˜ëŠ” ê²ƒë“¤: (A) ë°©ì–¸ ê²½ê³„ì˜ ê°€ì¥ìë¦¬ì— ê±°ì£¼: \"individuals living near the peripheries of their dialectic range may regularly hear the songs of an adjacent group\" (B) ë‹¤ë¥¸ ì•„ì¢…ê³¼ì˜ ìƒí˜¸ì‘ìš©: \"young birds come in contact with migratory subspecies during the formative period\" (C) ê³„ì ˆ ì´ˆê¸°ë‚˜ ë§ê¸°ì— ë¶€í™”: \"Nuttall's sparrows hatching early or late in the season are more likely to be exposed to Gambell's sparrows\" ê¸°ì—¬í•˜ì§€ ì•ŠëŠ” ê²ƒ: (D) 3ì›”~10ì›” ì‚¬ì´ì˜ ì§ì§“ê¸°: ì§€ë¬¸ì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ì˜¤íˆë ¤ \"broods hatching after March or before October will mostly only interact with other members of their own kind\"ë¼ê³  í•˜ì—¬ ì´ ì‹œê¸°ì— ë¶€í™”í•˜ëŠ” ìƒˆë¼ë“¤ì€ ìì‹ ì˜ ì¢…ë¥˜ì™€ë§Œ ìƒí˜¸ì‘ìš©í•œë‹¤ê³  ëª…ì‹œ </div> Question 8 Why does the author include paragraph 7 in the passage? (A) To argue that genetic factors have nothing to do with linguistic ability in birds (B) To offer a concluding argument for why birds have a variety of dialects (C) To introduce a new issue that has emerged in research on bird dialects (D) To suggest that language acquisition in birds is still poorly understood <button class=\"toggle-button answer\" data-type=\"answer8\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer8-content\"> ì •ë‹µ: (B) To offer a concluding argument for why birds have a variety of dialects í•´ì„¤: 7ë‹¨ë½ì€ ì „ì²´ ì§€ë¬¸ì˜ ê²°ë¡  ë‹¨ë½ì…ë‹ˆë‹¤: \"In light of these findings, most scientists have concluded that geographic differences may mildly restrict gene flow, but that genetic isolation has not been strong enough to create a rigid linguistic barrier. Dialectical variety in birds can thus be viewed as primarily a cultural phenomenon.\" ì´ ë‹¨ë½ì—ì„œ ì €ìëŠ”: ì•ì„œ ì œì‹œëœ ì—°êµ¬ ê²°ê³¼ë“¤ì„ ì¢…í•© ìœ ì „ì  ê²©ë¦¬ë³´ë‹¤ëŠ” ë¬¸í™”ì  ìš”ì¸ì´ ë” ì¤‘ìš”í•˜ë‹¤ê³  ê²°ë¡  ìƒˆë“¤ì˜ ë°©ì–¸ì  ë‹¤ì–‘ì„±ì´ ì£¼ë¡œ ë¬¸í™”ì  í˜„ìƒì´ë¼ëŠ” ìµœì¢… ë‹µì„ ì œì‹œ ì´ëŠ” ìƒˆë“¤ì´ ì™œ ë‹¤ì–‘í•œ ë°©ì–¸ì„ ê°€ì§€ê²Œ ë˜ëŠ”ì§€ì— ëŒ€í•œ ê²°ë¡ ì  ì£¼ì¥ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. (A) ìœ ì „ì  ìš”ì¸ì´ ì „í˜€ ê´€ë ¨ì—†ë‹¤ê³  ì£¼ì¥í•˜ì§€ ì•ŠìŒ (mildly restrictë¼ê³  ì¸ì •) (C) ìƒˆë¡œìš´ ì´ìŠˆë¥¼ ì†Œê°œí•˜ëŠ” ê²ƒì´ ì•„ë‹˜ (D) ì–¸ì–´ ìŠµë“ì´ ì˜ ì´í•´ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ì œì‹œí•˜ì§€ ì•ŠìŒ </div> Question 9 Look at the four squares [â– ] that indicate where the following sentence could be added to the passage. Single adult males of this species have been recorded singing eleven different songs. Where would the sentence best fit? <button class=\"toggle-button answer\" data-type=\"answer9\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer9-content\"> ì ì ˆí•œ ìœ„ì¹˜: Paragraph 6ì˜ ì²« ë²ˆì§¸ ë¬¸ì¥ ë’¤ í•´ì„¤: ì‚½ì…í•  ë¬¸ì¥: \"Single adult males of this species have been recorded singing eleven different songs.\" (ì´ ì¢…ì˜ ì„±ì²´ ìˆ˜ì»· ê°œì²´ë“¤ì´ 11ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë…¸ë˜ë¥¼ ë¶€ë¥´ëŠ” ê²ƒì´ ê¸°ë¡ë˜ì—ˆë‹¤.) ê°€ì¥ ì ì ˆí•œ ìœ„ì¹˜ëŠ” 6ë‹¨ë½ ì²« ë¬¸ì¥ ë’¤ì…ë‹ˆë‹¤: \"Detailed field research led to the discovery that some exceptional members of the species do master multiple songs, but their knowledge acquisition is contingent on contact with other groups in one of two ways. [â–  ì—¬ê¸°] Either young birds come in contact with migratory subspecies...\" ì´ìœ : ë…¼ë¦¬ì  ì—°ê²°: ì• ë¬¸ì¥ì—ì„œ \"ì¼ë¶€ ì˜ˆì™¸ì ì¸ êµ¬ì„±ì›ë“¤ì´ ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ìˆ™ë‹¬í•œë‹¤\"ê³  í•˜ê³ , ì‚½ì… ë¬¸ì¥ì€ ê·¸ êµ¬ì²´ì ì¸ ì˜ˆì‹œ(11ê°œ ë…¸ë˜)ë¥¼ ì œê³µ êµ¬ì²´ì  ì¦ê±°: \"some exceptional members\"ì˜ êµ¬ì²´ì ì¸ ì‚¬ë¡€ ë¬¸ë§¥ì˜ ì¼ê´€ì„±: ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì— ëŒ€í•œ ë…¼ì˜ê°€ ì´ì–´ì§€ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ìœ„ì¹˜ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì€ ë¬¸ë§¥ìƒ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. </div> Question 10 Directions: An introductory sentence for a brief summary of the passage is provided below. Complete the summary by selecting the THREE answer choices that express the most important ideas in the passage. Some sentences do not belong in the summary because they express ideas that are not presented in the passage or are minor ideas in the passage. This question is worth 2 points. In some bird populations, there are regional dialects in the birds' songs. Answer Choices: (A) White-crowned sparrows are useful subjects for studying bird dialects because they usually only learn a single song and have many dialects. (B) Using recordings in a laboratory setting, scientists have demonstrated that white-crowned sparrow chicks are capable of learning multiple dialects. (C) Research conducted on Nuttall's sparrows and Gambell's sparrows suggests that the two subspecies have developed different dialects because one is sedentary while the other is migratory. (D) Scientists thought that genetic factors may have contributed to dialectical diversity, but research has shown that genetic factors are limited. (E) Field studies of two Zonotrichia leucophrys subspecies revealed that multilingualism is dependent upon the timing and nature of interactions between dialectical group members. (F) One thing that is clear about the various subspecies of Zonotrichia leucophrys is that they all seem to begin learning songs for the first time after two months of age. <button class=\"toggle-button answer\" data-type=\"answer10\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer10-content\"> ì •ë‹µ: (A), (B), (D) í•´ì„¤: ì„ íƒëœ ë‹µì•ˆë“¤: (A) White-crowned sparrows are useful subjects for studying bird dialects because they usually only learn a single song and have many dialects. âœ“ 1ë‹¨ë½: \"One of the best-studied songbirds... is the white-crowned sparrow\" 3ë‹¨ë½: \"individual adults generally sing a single song\" ì—°êµ¬ ëŒ€ìƒìœ¼ë¡œì„œì˜ ìœ ìš©ì„±ì´ ì£¼ìš” ì£¼ì œ (B) Using recordings in a laboratory setting, scientists have demonstrated that white-crowned sparrow chicks are capable of learning multiple dialects. âœ“ 4ë‹¨ë½: \"Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects\" ì‹¤í—˜ì‹¤ ì—°êµ¬ì˜ ì¤‘ìš”í•œ ë°œê²¬ (D) Scientists thought that genetic factors may have contributed to dialectical diversity, but research has shown that genetic factors are limited. âœ“ 4ë‹¨ë½: ìœ ì „ì  ì„¤ëª…ì´ ê·¸ëŸ´ë“¯í•´ ë³´ì˜€ì§€ë§Œ 7ë‹¨ë½: \"genetic isolation has not been strong enough... primarily a cultural phenomenon\" ì§€ë¬¸ì˜ í•µì‹¬ ê²°ë¡  ì œì™¸ëœ ë‹µì•ˆë“¤: (C) ì •ì£¼ì„±/ì´ë™ì„±ì˜ ì°¨ì´ê°€ ë°©ì–¸ ë°œë‹¬ì˜ ì›ì¸ì´ë¼ê³  ì œì‹œí•˜ì§€ ì•ŠìŒ (E) ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì€ ë¶€ì°¨ì  ì£¼ì œ (F) 2ê°œì›” í›„ì— í•™ìŠµì„ ì‹œì‘í•œë‹¤ëŠ” ë‚´ìš© ì—†ìŒ (ì˜¤íˆë ¤ ì²« 2ê°œì›”ì´ ì¤‘ìš”í•œ ì‹œê¸°) </div>",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-14-ê±±ì •í•˜ëŠ”í‘œí˜„",
    "title": "ê±±ì •í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-14",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ worried love life : ì—°ì•  ì‚¬ì—… I'm worried about my love life. worry about : (í•­ìƒ) ~ë¥¼ ê±±ì •í•´ I worry about my future. 'm worried about : (ìš”ì¦˜) ê±±ì • ë¼ I'm worried about my love life. I'm worried about my job interview. I'm worried about the traffic. ì°¨ê°€ ë§‰íê¹Œë°”ã…— I'm worried about my presentation tommorow. ì¡°ê¸ˆ ê±±ì •ëœë‹¤ I'm a little worried about ì—„ì²­ ê±±ì •ëœë‹¤ I'm really worried about ì‹ ê²½ì“°ì¸ë‹¤ I'm concerned about the new flu virus. I'm concerned about the news lately. travel ban What are some things that are on your mind? as well , ë‚˜ë„ ê°„ë‹¨í•œ ì‘ë¬¸ Nowadays, I'm worried about my future of career. ë‚´ê°€ ì‹œì¥ ê°€ì¹˜ê°€ ìˆì„ì§€? ë‹¤ë¥¸ íšŒì‚¬ì— ê°ˆ ìˆ˜ ìˆì„ì§€?ì— ëŒ€í•´ ê³ ë¯¼ì„ í•˜ê³  ìˆì–´. I'm preparing the job transition, but if fails in the resume screening. So I can believe my self. ì•ìœ¼ë¡œ ë” ë‚˜ì•„ì§ˆì§€. My ì—°ë´‰ can be better than now? I have a some concerns, but what I have to do or I can to is just focus on the today, just step ahead. So í˜¸í¡ì„ ê°€ë‹¤ë“¬ê³ , í˜„ì¬ì— ì§‘ì¤‘í•˜ë ¤ê³  ë…¸ë ¥í•œë‹¤. ìˆ˜ì •ëœ ì˜ì–´ ê¸€ Nowadays, I'm worried about my career path. I keep wondering if I'm competitive enough in the job market to move to another company. I'm preparing for a job transition, but I'm afraid of failing at the resume screening stage. It makes me doubt myself and wonder if I can actually improve from here. I also question if I can earn a better salary than I do now. Even with these concerns, I know that all I can really do is focus on today and take things one step at a time. So, I'm trying to take a deep breath and concentrate on the present. í”¼ë“œë°± ë° í•´ì„¤ my future of career â†’ my career path í”¼ë“œë°±: careerëŠ” ì´ë¯¸ ë¯¸ë˜ì˜ ê¸¸(path)ì´ë¼ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ìš”. my career path (ë‚˜ì˜ ì»¤ë¦¬ì–´ ê²½ë¡œ) ë˜ëŠ” my future career (ë‚˜ì˜ ë¯¸ë˜ ì»¤ë¦¬ì–´)ë¼ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ìì—°ìŠ¤ëŸ¬ì›Œìš”. ì˜ˆì‹œ: \"I need some advice on my career path.\" (ì œ ì»¤ë¦¬ì–´ ê²½ë¡œì— ëŒ€í•´ ì¡°ì–¸ì´ í•„ìš”í•´ìš”.) if fails in the resume screening â†’ I'm afraid of failing at the resume screening stage í”¼ë“œë°±: ì›ë˜ ë¬¸ì¥ì€ \"ë§Œì•½ ì„œë¥˜ ì‹¬ì‚¬ì—ì„œ ë–¨ì–´ì§€ë©´\"ì´ë¼ëŠ” ì¡°ê±´ì ˆë§Œ ìˆì–´ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ì „í•˜ì§€ ì•Šì•„ìš”. \"ì„œë¥˜ ì‹¬ì‚¬ ë‹¨ê³„ì—ì„œ ë–¨ì–´ì§€ëŠ” ê²ƒì´ ë‘ë µë‹¤\"ëŠ” ì˜ë¯¸ë¡œ I'm afraid of failing...ê³¼ ê°™ì´ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì—ˆì–´ìš”. íŒ: afraid of + (ë™)ëª…ì‚¬ëŠ” \"~í•˜ëŠ” ê²ƒì´ ë‘ë µë‹¤\"ëŠ” í‘œí˜„ìœ¼ë¡œ ìì£¼ ì“°ì—¬ìš”. So I can believe my self â†’ It makes me doubt myself í”¼ë“œë°±: ë¬¸ë§¥ìƒ \"ë‚˜ ìì‹ ì„ ë¯¿ì„ ìˆ˜ ìˆì„ê¹Œ?\"ë¼ë©° ìŠ¤ìŠ¤ë¡œë¥¼ ì˜ì‹¬í•˜ëŠ” ì˜ë¯¸ë¡œ í•´ì„ë¼ìš”. doubt myself (ë‚˜ ìì‹ ì„ ì˜ì‹¬í•˜ë‹¤)ë¼ëŠ” í‘œí˜„ì´ ë” ëª…í™•í•˜ê²Œ ì˜ë¯¸ë¥¼ ì „ë‹¬í•´ìš”. ë˜í•œ, myselfëŠ” í•­ìƒ í•œ ë‹¨ì–´ë¡œ ë¶™ì—¬ ì¨ì•¼ í•©ë‹ˆë‹¤. ì˜ˆì‹œ: \"Don't doubt yourself. You can do it!\" (ìŠ¤ìŠ¤ë¡œë¥¼ ì˜ì‹¬í•˜ì§€ ë§ˆ. ë„Œ í•  ìˆ˜ ìˆì–´!) My ì—°ë´‰ can be better than now? â†’ if I can earn a better salary than I do now í”¼ë“œë°±: \"ë‚´ ì—°ë´‰ì´ ì§€ê¸ˆë³´ë‹¤ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆì„ê¹Œ?\"ë¼ëŠ” ì§ˆë¬¸ì„ if (~ì¸ì§€ ì•„ë‹Œì§€)ë¥¼ ì‚¬ìš©í•´ \"ë” ë‚˜ì€ ì—°ë´‰ì„ ë°›ì„ ìˆ˜ ìˆì„ì§€ ê¶ê¸ˆí•˜ë‹¤/ì˜ë¬¸ì´ë‹¤\"ëŠ” ì˜ë¯¸ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ì— ë…¹ì—¬ëƒˆì–´ìš”. ì˜ˆì‹œ: \"I wonder if I can finish this on time.\" (ì´ê±¸ ì œì‹œê°„ì— ëë‚¼ ìˆ˜ ìˆì„ì§€ ëª¨ë¥´ê² ì–´.) I have a some concerns â†’ Even with these concerns í”¼ë“œë°±: aì™€ someì€ í•¨ê»˜ ì“¸ ìˆ˜ ì—†ì–´ìš”. I have some concerns (ëª‡ ê°€ì§€ ê±±ì •ì´ ìˆë‹¤)ê°€ ë§ìŠµë‹ˆë‹¤. ë¬¸ë§¥ì˜ íë¦„ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ í•˜ê¸° ìœ„í•´ \"ì´ëŸ° ê±±ì •ì—ë„ ë¶ˆêµ¬í•˜ê³ \"ë¼ëŠ” ì˜ë¯¸ì˜ Even with these concernsë¡œ ìˆ˜ì •í–ˆì–´ìš”. what I have to do or I can to is... â†’ all I can really do is... í”¼ë“œë°±: ì›ë˜ ë¬¸ì¥ì˜ I can toëŠ” ë¬¸ë²•ì ìœ¼ë¡œ í‹€ë¦° í‘œí˜„ì´ì—ìš”. I can doê°€ ë§ìŠµë‹ˆë‹¤. ì˜ë¯¸ë¥¼ ì‚´ë ¤ì„œ \"ë‚´ê°€ ì •ë§ë¡œ í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ê²ƒì€\"ì´ë¼ëŠ” all I can really do is...ë¡œ ë‹¤ë“¬ì—ˆì–´ìš”. focus on the today, just step ahead â†’ focus on today and take things one step at a time í”¼ë“œë°±: today, yesterday, tomorrow ì•ì—ëŠ” ë³´í†µ theë¥¼ ë¶™ì´ì§€ ì•Šì•„ìš”. just step aheadë„ ì¢‹ì€ í‘œí˜„ì´ì§€ë§Œ, \"ì°¨ê·¼ì°¨ê·¼ ë‚˜ì•„ê°€ë‹¤\"ë¼ëŠ” ì˜ë¯¸ë¥¼ ë” ì˜ ì‚´ë¦¬ëŠ” ê´€ìš©êµ¬ì¸ take things one step at a timeì´ ì´ëŸ° ìƒí™©ì— ì•„ì£¼ ì˜ ì–´ìš¸ë ¤ìš”.",
    "public": true
  },
  {
    "id": "Data Science/Clean Code Principles/dependency-injection-pattern",
    "title": "ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒí•˜ê¸°",
    "date": "2025-07-14",
    "excerpt": "ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì„ í†µí•´ ì½”ë“œì˜ ê²°í•©ë„ë¥¼ ë‚®ì¶”ê³  í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì„ ì‹¤ì œ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.",
    "category": "Data Science",
    "tags": [
      "python",
      "design-pattern",
      "dependency-injection",
      "software-architecture",
      "clean-code",
      "testing"
    ],
    "content": "ë°°ê²½ ìµœê·¼ì— ëŒ€í™”í˜• AI ì±—ë´‡ í”„ë¡œì íŠ¸ë¥¼ ê°œë°œí•˜ë©´ì„œ ê²ªì—ˆë˜ ì½”ë“œ êµ¬ì¡°ì˜ ë¬¸ì œì ë“¤ê³¼, ì´ë¥¼ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ê°œì„ í•œ ê²½í—˜ì„ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ìƒí™© FastAPI ê¸°ë°˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ OpenAI GPT ëª¨ë¸ì„ í™œìš©í•œ ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©ìë³„ ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ê²ªì—ˆë˜ ë¬¸ì œì ë“¤ ê°•í•œ ê²°í•© (Tight Coupling) í´ë˜ìŠ¤ë“¤ì´ ì„œë¡œ ë„ˆë¬´ ë°€ì ‘í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆì–´ì„œ, í•˜ë‚˜ë¥¼ ìˆ˜ì •í•˜ë©´ ë‹¤ë¥¸ ê²ƒë“¤ë„ í•¨ê»˜ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í•˜ë“œì½”ë”©ëœ ì„¤ì • ëª¨ë¸ëª…, ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ, API í‚¤ ë“±ì´ ì½”ë“œì— ì§ì ‘ ì‘ì„±ë˜ì–´ ìˆì–´ì„œ í™˜ê²½ë³„ë¡œ ì„¤ì •ì„ ë°”ê¾¸ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ì–´ë ¤ì›Œì„œ, ì „ì²´ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•´ì•¼ë§Œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ê°œë…ë“¤ ì •ë¦¬ ê°•í•œ ê²°í•© (Tight Coupling) - ì‰½ê²Œ ì´í•´í•˜ê¸° ğŸ  ì§‘ ì§“ê¸° ë¹„ìœ  ê°•í•œ ê²°í•©ì˜ ì˜ˆì‹œ: ``python ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ - ê°•í•œ ê²°í•© class ì§‘: def __init__(self): # ì§‘ì´ ì§ì ‘ ë²½ëŒê³µì¥ì„ ë§Œë“¤ì–´ì„œ ë²½ëŒì„ ê°€ì ¸ì˜´ self.ë²½ëŒê³µì¥ = ë²½ëŒê³µì¥() self.ë²½ëŒ = self.ë²½ëŒê³µì¥.ë²½ëŒë§Œë“¤ê¸°() # ì§‘ì´ ì§ì ‘ ëª©ìˆ˜ê³µì¥ì„ ë§Œë“¤ì–´ì„œ ë¬¸ì„ ê°€ì ¸ì˜´ self.ëª©ìˆ˜ê³µì¥ = ëª©ìˆ˜ê³µì¥() self.ë¬¸ = self.ëª©ìˆ˜ê³µì¥.ë¬¸ë§Œë“¤ê¸°() ` ë¬¸ì œì : ì§‘ì´ ë²½ëŒê³µì¥ê³¼ ëª©ìˆ˜ê³µì¥ì„ ì§ì ‘ ì•Œê³  ìˆì–´ì•¼ í•¨ ë‹¤ë¥¸ ì¬ë£Œ(ì½˜í¬ë¦¬íŠ¸, ì•Œë£¨ë¯¸ëŠ„)ë¡œ ë°”ê¾¸ë ¤ë©´ ì§‘ í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•´ì•¼ í•¨ í…ŒìŠ¤íŠ¸í•  ë•Œ ì§„ì§œ ê³µì¥ ëŒ€ì‹  ê°€ì§œ ê³µì¥ì„ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€ í•´ê²°ì±… - ëŠìŠ¨í•œ ê²°í•©: `python ê°œì„ ëœ ì½”ë“œ - ëŠìŠ¨í•œ ê²°í•© class ì§‘: def __init__(self, ë²½ëŒì œê³µì, ë¬¸ì œê³µì): # ì™¸ë¶€ì—ì„œ ë²½ëŒê³¼ ë¬¸ì„ ë°›ì•„ì˜´ self.ë²½ëŒ = ë²½ëŒì œê³µì.ë²½ëŒë§Œë“¤ê¸°() self.ë¬¸ = ë¬¸ì œê³µì.ë¬¸ë§Œë“¤ê¸°() ì‚¬ìš©í•  ë•Œ ì§‘1 = ì§‘(ë²½ëŒê³µì¥(), ëª©ìˆ˜ê³µì¥()) ì§‘2 = ì§‘(ì½˜í¬ë¦¬íŠ¸ê³µì¥(), ì•Œë£¨ë¯¸ëŠ„ê³µì¥()) # ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŒ! ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python ê°•í•œ ê²°í•© - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class ChatBot: def __init__(self): # ì§ì ‘ ì˜ì¡´ì„±ì„ ìƒì„± self.llm = OpenAI(model=\"gpt-4\") # OpenAIì— ê°•í•˜ê²Œ ê²°í•© self.database = ChromaDB(path=\"./db\") # ChromaDBì— ê°•í•˜ê²Œ ê²°í•© self.scheduler = APScheduler() # APSchedulerì— ê°•í•˜ê²Œ ê²°í•© ëŠìŠ¨í•œ ê²°í•© - ê°œì„ ëœ ì½”ë“œ class ChatBot: def __init__(self, llm_provider, database, scheduler): # ì™¸ë¶€ì—ì„œ ì˜ì¡´ì„±ì„ ì£¼ì…ë°›ìŒ self.llm = llm_provider self.database = database self.scheduler = scheduler ` í•˜ë“œì½”ë”©ëœ ì„¤ì • - ì™œ ë¬¸ì œì¸ê°€? ğŸ• í”¼ì ì£¼ë¬¸ ë¹„ìœ  í•˜ë“œì½”ë”©ëœ ì„¤ì •ì˜ ë¬¸ì œ: `python ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class í”¼ìì§‘: def __init__(self): self.í† í•‘ = \"í˜í¼ë¡œë‹ˆ\" # í•˜ë“œì½”ë”©! self.í¬ê¸° = \"ë¼ì§€\" # í•˜ë“œì½”ë”©! self.ê°€ê²© = 20000 # í•˜ë“œì½”ë”©! ëª¨ë“  í”¼ìê°€ ë˜‘ê°™ì´ ë‚˜ì˜´ ğŸ˜ í”¼ì1 = í”¼ìì§‘() # í•­ìƒ í˜í¼ë¡œë‹ˆ ë¼ì§€ 20000ì› í”¼ì2 = í”¼ìì§‘() # í•­ìƒ í˜í¼ë¡œë‹ˆ ë¼ì§€ 20000ì› ` ê°œì„ ëœ ì½”ë“œ: `python ì„¤ì • íŒŒì¼ ì‚¬ìš© class í”¼ìì„¤ì •: def __init__(self): self.ê¸°ë³¸í† í•‘ = \"í˜í¼ë¡œë‹ˆ\" self.ê¸°ë³¸í¬ê¸° = \"ë¼ì§€\" self.ê¸°ë³¸ê°€ê²© = 20000 class í”¼ìì§‘: def __init__(self, ì„¤ì •): self.í† í•‘ = ì„¤ì •.ê¸°ë³¸í† í•‘ self.í¬ê¸° = ì„¤ì •.ê¸°ë³¸í¬ê¸° self.ê°€ê²© = ì„¤ì •.ê¸°ë³¸ê°€ê²© í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥ ê°œë°œì„¤ì • = í”¼ìì„¤ì •() ê°œë°œì„¤ì •.ê¸°ë³¸í† í•‘ = \"ì¹˜ì¦ˆ\" # ê°œë°œí™˜ê²½ì€ ì¹˜ì¦ˆ ìš´ì˜ì„¤ì • = í”¼ìì„¤ì •() ìš´ì˜ì„¤ì •.ê¸°ë³¸í† í•‘ = \"í˜í¼ë¡œë‹ˆ\" # ìš´ì˜í™˜ê²½ì€ í˜í¼ë¡œë‹ˆ ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python í•˜ë“œì½”ë”©ëœ ì„¤ì • - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class AIì±—ë´‡: def __init__(self): self.model = \"gpt-4o-mini-2024-07-18\" # í•˜ë“œì½”ë”©! self.api_key = \"sk-1234567890abcdef\" # í•˜ë“œì½”ë”©! self.db_path = \"./db/chroma_db\" # í•˜ë“œì½”ë”©! ê°œì„ ëœ ì½”ë“œ - ì„¤ì • íŒŒì¼ ì‚¬ìš© import os from pydantic import BaseSettings class Settings(BaseSettings): model: str = \"gpt-4o-mini-2024-07-18\" api_key: str = \"\" db_path: str = \"./db/chroma_db\" class Config: env_file = \".env\" # í™˜ê²½ë³€ìˆ˜ íŒŒì¼ì—ì„œ ì½ì–´ì˜´ class AIì±—ë´‡: def __init__(self, settings: Settings): self.model = settings.model self.api_key = settings.api_key self.db_path = settings.db_path ì‚¬ìš© settings = Settings() ì±—ë´‡ = AIì±—ë´‡(settings) ` ì˜ì¡´ì„± ì£¼ì… (Dependency Injection) - í•µì‹¬ ê°œë… ğŸ¯ ì˜ì¡´ì„± ì£¼ì…ì´ë€? ì˜ì¡´ì„±(Dependency): ê°ì²´ê°€ ë™ì‘í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë‹¤ë¥¸ ê°ì²´ë“¤ ì£¼ì…(Injection): ì™¸ë¶€ì—ì„œ ê·¸ ê°ì²´ë“¤ì„ ì œê³µí•´ì£¼ëŠ” ê²ƒ ğŸš— ìë™ì°¨ ë¹„ìœ  `python ì˜ì¡´ì„± ì£¼ì… ì—†ì´ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class ìë™ì°¨: def __init__(self): # ìë™ì°¨ê°€ ì§ì ‘ ì—”ì§„ì„ ë§Œë“¦ self.ì—”ì§„ = ê°€ì†”ë¦°ì—”ì§„() # ê°€ì†”ë¦°ì—”ì§„ì— ê°•í•˜ê²Œ ê²°í•© self.íƒ€ì´ì–´ = ë¯¸ì‰ë¦°íƒ€ì´ì–´() # ë¯¸ì‰ë¦°íƒ€ì´ì–´ì— ê°•í•˜ê²Œ ê²°í•© ë¬¸ì œì : ì „ê¸°ì°¨ë¡œ ë°”ê¾¸ë ¤ë©´ ìë™ì°¨ í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•´ì•¼ í•¨ í…ŒìŠ¤íŠ¸í•  ë•Œ ì§„ì§œ ì—”ì§„ ëŒ€ì‹  ê°€ì§œ ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ ë‹¤ë¥¸ ë¸Œëœë“œ íƒ€ì´ì–´ë¡œ ë°”ê¾¸ê¸° ì–´ë ¤ì›€ ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš© - ê°œì„ ëœ ì½”ë“œ class ìë™ì°¨: def __init__(self, ì—”ì§„, íƒ€ì´ì–´): # ì™¸ë¶€ì—ì„œ ì—”ì§„ê³¼ íƒ€ì´ì–´ë¥¼ ë°›ì•„ì˜´ self.ì—”ì§„ = ì—”ì§„ self.íƒ€ì´ì–´ = íƒ€ì´ì–´ ì‚¬ìš©í•  ë•Œ ê°€ì†”ë¦°ì°¨ = ìë™ì°¨(ê°€ì†”ë¦°ì—”ì§„(), ë¯¸ì‰ë¦°íƒ€ì´ì–´()) ì „ê¸°ì°¨ = ìë™ì°¨(ì „ê¸°ì—”ì§„(), í•œêµ­íƒ€ì´ì–´()) # ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŒ! í…ŒìŠ¤íŠ¸í•  ë•Œ í…ŒìŠ¤íŠ¸ì°¨ = ìë™ì°¨(ê°€ì§œì—”ì§„(), ê°€ì§œíƒ€ì´ì–´()) # ê°€ì§œ ë¶€í’ˆìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥! ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python ì˜ì¡´ì„± ì£¼ì… ì—†ì´ class ChatBot: def __init__(self): self.llm = OpenAI(model=\"gpt-4\") # ì§ì ‘ ìƒì„± self.db = ChromaDB(path=\"./db\") # ì§ì ‘ ìƒì„± ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš© class ChatBot: def __init__(self, llm_provider, database): self.llm = llm_provider # ì™¸ë¶€ì—ì„œ ì£¼ì… self.database = database # ì™¸ë¶€ì—ì„œ ì£¼ì… ì‚¬ìš©í•  ë•Œ ì±—ë´‡1 = ChatBot(OpenAI(model=\"gpt-4\"), ChromaDB(path=\"./db\")) ì±—ë´‡2 = ChatBot(Anthropic(model=\"claude\"), Pinecone(index=\"my-index\")) í…ŒìŠ¤íŠ¸í•  ë•Œ í…ŒìŠ¤íŠ¸ì±—ë´‡ = ChatBot(MockLLM(), MockDatabase()) ` íŒ©í† ë¦¬ íŒ¨í„´ (Factory Pattern) - ê°ì²´ ìƒì‚° ê³µì¥ ğŸ­ ê³µì¥ ë¹„ìœ  íŒ©í† ë¦¬ íŒ¨í„´ì´ë€? ê°ì²´ë¥¼ ë§Œë“œëŠ” ì „ìš© ê³µì¥ì„ ë”°ë¡œ ë‘ëŠ” íŒ¨í„´ `python íŒ©í† ë¦¬ ì—†ì´ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class í”¼ìì§‘: def __init__(self): # í”¼ìì§‘ì´ ì§ì ‘ ì¬ë£Œë¥¼ ì¤€ë¹„ self.ë„ìš° = ë°€ê°€ë£¨ë„ìš°() self.ì†ŒìŠ¤ = í† ë§ˆí† ì†ŒìŠ¤() self.ì¹˜ì¦ˆ = ëª¨ì§œë ë¼ì¹˜ì¦ˆ() ë¬¸ì œì : í”¼ìì§‘ì´ ì¬ë£Œ ì¤€ë¹„ ë°©ë²•ì„ ëª¨ë‘ ì•Œì•„ì•¼ í•¨ íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš© - ê°œì„ ëœ ì½”ë“œ class ì¬ë£Œê³µì¥: def ë„ìš°ë§Œë“¤ê¸°(self): return ë°€ê°€ë£¨ë„ìš°() def ì†ŒìŠ¤ë§Œë“¤ê¸°(self): return í† ë§ˆí† ì†ŒìŠ¤() def ì¹˜ì¦ˆë§Œë“¤ê¸°(self): return ëª¨ì§œë ë¼ì¹˜ì¦ˆ() class í”¼ìì§‘: def __init__(self, ì¬ë£Œê³µì¥): self.ê³µì¥ = ì¬ë£Œê³µì¥ self.ë„ìš° = self.ê³µì¥.ë„ìš°ë§Œë“¤ê¸°() self.ì†ŒìŠ¤ = self.ê³µì¥.ì†ŒìŠ¤ë§Œë“¤ê¸°() self.ì¹˜ì¦ˆ = self.ê³µì¥.ì¹˜ì¦ˆë§Œë“¤ê¸°() ì‚¬ìš© ê³µì¥ = ì¬ë£Œê³µì¥() í”¼ìì§‘ = í”¼ìì§‘(ê³µì¥) ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python LLM Factory class LLMFactory: def __init__(self, config): self.config = config def create_refine_llm(self): return OpenAI( model=self.config.get(\"refine_model\", \"gpt-4o-mini\"), api_key=self.config.get(\"api_key\") ) def create_answer_llm(self): return OpenAI( model=self.config.get(\"answer_model\", \"gpt-4o\"), api_key=self.config.get(\"api_key\") ) ì‚¬ìš© config = {\"refine_model\": \"gpt-4o-mini\", \"answer_model\": \"gpt-4o\"} factory = LLMFactory(config) refine_llm = factory.create_refine_llm() answer_llm = factory.create_answer_llm() ` ì‹±ê¸€í†¤ íŒ¨í„´ (Singleton Pattern) - í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ëŠ” ê°ì²´ ğŸ‘‘ ì™• ë¹„ìœ  ì‹±ê¸€í†¤ íŒ¨í„´ì´ë€? í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥í•˜ëŠ” íŒ¨í„´ `python ì¼ë°˜ì ì¸ í´ë˜ìŠ¤ - ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ê°€ëŠ¥ class ì¼ë°˜ì‚¬ëŒ: def __init__(self, ì´ë¦„): self.ì´ë¦„ = ì´ë¦„ ì‚¬ëŒ1 = ì¼ë°˜ì‚¬ëŒ(\"ê¹€ì² ìˆ˜\") ì‚¬ëŒ2 = ì¼ë°˜ì‚¬ëŒ(\"ì´ì˜í¬\") print(ì‚¬ëŒ1 is ì‚¬ëŒ2) # False - ë‹¤ë¥¸ ê°ì²´ ì‹±ê¸€í†¤ íŒ¨í„´ - í•˜ë‚˜ë§Œ ì¡´ì¬ class ì™•: _instance = None # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ def __new__(cls): if cls._instance is None: cls._instance = super().__new__(cls) cls._instance.ì´ë¦„ = \"í˜„ì¬ì™•\" return cls._instance ì™•1 = ì™•() ì™•2 = ì™•() print(ì™•1 is ì™•2) # True - ê°™ì€ ê°ì²´! ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° - ì‹±ê¸€í†¤ íŒ¨í„´ class DatabaseConnection: _instance = None def __new__(cls): if cls._instance is None: cls._instance = super().__new__(cls) cls._instance.connection = create_database_connection() return cls._instance def get_connection(self): return self.connection ì‚¬ìš© db1 = DatabaseConnection() db2 = DatabaseConnection() print(db1 is db2) # True - ê°™ì€ ì—°ê²° ê°ì²´ ` í”„ë¡œí† ì½œ (Protocol) - ì¸í„°í˜ì´ìŠ¤ ì •ì˜ ğŸ“‹ ê³„ì•½ì„œ ë¹„ìœ  í”„ë¡œí† ì½œì´ë€? ê°ì²´ê°€ ì–´ë–¤ ë©”ì„œë“œë¥¼ ê°€ì ¸ì•¼ í•˜ëŠ”ì§€ ì •ì˜í•˜ëŠ” \"ê³„ì•½ì„œ\" `python í”„ë¡œí† ì½œ ì—†ì´ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class ChatBot: def __init__(self, llm): self.llm = llm def ëŒ€í™”í•˜ê¸°(self, ë©”ì‹œì§€): # llmì´ generate ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì • return self.llm.generate(ë©”ì‹œì§€) # ëŸ°íƒ€ì„ì— ì—ëŸ¬ ê°€ëŠ¥! ë¬¸ì œì : llm ê°ì²´ê°€ generate ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ í”„ë¡œí† ì½œ ì‚¬ìš© - ê°œì„ ëœ ì½”ë“œ from typing import Protocol class LLMProvider(Protocol): \"\"\"LLM ì œê³µìëŠ” ë°˜ë“œì‹œ generate ë©”ì„œë“œë¥¼ ê°€ì ¸ì•¼ í•¨\"\"\" def generate(self, message: str) -> str: ... class ChatBot: def __init__(self, llm: LLMProvider): # íƒ€ì… íŒíŠ¸ë¡œ ê²€ì¦ self.llm = llm def ëŒ€í™”í•˜ê¸°(self, ë©”ì‹œì§€): return self.llm.generate(ë©”ì‹œì§€) # ì•ˆì „! ì‚¬ìš© class OpenAI: def generate(self, message: str) -> str: return f\"OpenAI: {message}\" class Anthropic: def generate(self, message: str) -> str: return f\"Anthropic: {message}\" ë‘˜ ë‹¤ LLMProvider í”„ë¡œí† ì½œì„ ë§Œì¡±í•˜ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥ ì±—ë´‡1 = ChatBot(OpenAI()) ì±—ë´‡2 = ChatBot(Anthropic()) ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python from typing import Protocol, List, Dict, Any class Retriever(Protocol): \"\"\"ê²€ìƒ‰ê¸° ì¸í„°í˜ì´ìŠ¤\"\"\" def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: \"\"\"ì¿¼ë¦¬ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰\"\"\" ... class ChromaRetriever: def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: # ChromaDBë¡œ ê²€ìƒ‰í•˜ëŠ” ì‹¤ì œ êµ¬í˜„ return [{\"content\": \"ê²€ìƒ‰ëœ ë¬¸ì„œ\", \"score\": 0.9}] class PineconeRetriever: def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: # Pineconeìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ì‹¤ì œ êµ¬í˜„ return [{\"content\": \"ê²€ìƒ‰ëœ ë¬¸ì„œ\", \"score\": 0.8}] ë‘˜ ë‹¤ Retriever í”„ë¡œí† ì½œì„ ë§Œì¡±í•˜ë¯€ë¡œ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥ def search_documents(retriever: Retriever, query: str): return retriever.search(query, max_results=5) ì‚¬ìš© chroma_results = search_documents(ChromaRetriever(), \"ê²€ìƒ‰ì–´\") pinecone_results = search_documents(PineconeRetriever(), \"ê²€ìƒ‰ì–´\") ` ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒí•˜ê¸° ê°œìš” ì˜ì¡´ì„± ì£¼ì…(Dependency Injection, DI)ì€ ê°ì²´ê°€ í•„ìš”ë¡œ í•˜ëŠ” ì˜ì¡´ì„±ì„ ì™¸ë¶€ì—ì„œ ì œê³µë°›ëŠ” ë””ìì¸ íŒ¨í„´ì…ë‹ˆë‹¤. ì´ íŒ¨í„´ì„ í†µí•´ ì½”ë“œì˜ ê²°í•©ë„ë¥¼ ë‚®ì¶”ê³ , í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ ë†’ì´ë©°, ìœ ì§€ë³´ìˆ˜ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì½”ë“œì˜ ë¬¸ì œì  ê°•í•œ ê²°í•© (Tight Coupling) `python ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ ì˜ˆì‹œ class MyApp: def __init__(self): self.app = FastAPI() # ì§ì ‘ ì˜ì¡´ì„± ìƒì„± - ê°•í•œ ê²°í•© self.retriever = ChromadbRetriever(\"./db/chroma_db\") self.scheduler = BackgroundScheduler(timezone=\"Asia/Seoul\") self.llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\") ` ë¬¸ì œì : ëª¨ë“  ì˜ì¡´ì„±ì„ í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì§ì ‘ ìƒì„± ë‹¤ë¥¸ êµ¬í˜„ì²´ë¡œ êµì²´í•˜ê¸° ì–´ë ¤ì›€ í…ŒìŠ¤íŠ¸ ì‹œ Mock ê°ì²´ ì£¼ì… ë¶ˆê°€ëŠ¥ í•˜ë“œì½”ë”©ëœ ì„¤ì • `python class ConversationalRetrievalChain: def __init__(self, retriever: ChromadbRetriever): # í•˜ë“œì½”ë”©ëœ ëª¨ë¸ëª…ë“¤ self.refine_llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\", system_message=\"\") self.answer_llm = LLM_openai(model=\"gpt-4o-2024-05-13\", system_message=\"\") self.lang_detect_llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\", system_message=\"\") ` ë¬¸ì œì : ëª¨ë¸ëª…ì´ ì½”ë“œì— í•˜ë“œì½”ë”©ë¨ í™˜ê²½ë³„ ì„¤ì • ë³€ê²½ ì–´ë ¤ì›€ ì„¤ì • ê´€ë¦¬ì˜ ì¤‘ì•™í™” ë¶€ì¡± ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì ìš© ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (Protocol ì‚¬ìš©) ë¨¼ì € ì˜ì¡´ì„±ë“¤ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤: `python interfaces/llm_provider.py from typing import Protocol, List, Dict, Any class LLMProvider(Protocol): \"\"\"LLM ì œê³µì ì¸í„°í˜ì´ìŠ¤\"\"\" def generate(self, user_message: str) -> str: \"\"\"ë‹¨ì¼ ë©”ì‹œì§€ ìƒì„±\"\"\" ... def generate_with_chat_history(self, history: List, user_message: str) -> str: \"\"\"ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ë©”ì‹œì§€ ìƒì„±\"\"\" ... class Retriever(Protocol): \"\"\"ê²€ìƒ‰ê¸° ì¸í„°í˜ì´ìŠ¤\"\"\" def get_docs_with_decomposition(self, query: str, max_k: int, threshold: float, lang: str) -> Dict[str, Any]: \"\"\"ì¿¼ë¦¬ ë¶„í•´ë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\" ... class Scheduler(Protocol): \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì¸í„°í˜ì´ìŠ¤\"\"\" def add_job(self, func, trigger, kwargs): \"\"\"ì‘ì—… ì¶”ê°€\"\"\" ... def get_job(self, job_id: str): \"\"\"ì‘ì—… ì¡°íšŒ\"\"\" ... def start(self): \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘\"\"\" ... ` Factory í´ë˜ìŠ¤ë“¤ ìƒì„± ì˜ì¡´ì„± ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” Factory í´ë˜ìŠ¤ë“¤ì„ ë§Œë“­ë‹ˆë‹¤: `python factories/llm_factory.py from typing import Dict from utils.llm_providers import LLM_openai from interfaces.llm_provider import LLMProvider class LLMFactory: \"\"\"LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” Factory\"\"\" def __init__(self, config: Dict[str, str]): self.config = config def create_refine_llm(self) -> LLMProvider: \"\"\"ì •ì œìš© LLM ìƒì„±\"\"\" return LLM_openai( model=self.config.get(\"refine_model\", \"gpt-4o-mini-2024-07-18\"), system_message=\"\" ) def create_answer_llm(self) -> LLMProvider: \"\"\"ë‹µë³€ìš© LLM ìƒì„±\"\"\" return LLM_openai( model=self.config.get(\"answer_model\", \"gpt-4o-2024-05-13\"), system_message=\"\" ) def create_lang_detect_llm(self) -> LLMProvider: \"\"\"ì–¸ì–´ ê°ì§€ìš© LLM ìƒì„±\"\"\" return LLM_openai( model=self.config.get(\"lang_detect_model\", \"gpt-4o-mini-2024-07-18\"), system_message=get_lang_detect_message(type=\"system\") ) factories/retriever_factory.py from chains.custom_retriever import ChromadbRetriever from interfaces.retriever import Retriever class RetrieverFactory: \"\"\"ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” Factory\"\"\" def __init__(self, db_path: str): self.db_path = db_path def create_retriever(self) -> Retriever: \"\"\"ê²€ìƒ‰ê¸° ìƒì„±\"\"\" return ChromadbRetriever(self.db_path) ` ì„¤ì • ê´€ë¦¬ ê°œì„  ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ë¥¼ ìœ„í•´ Pydanticì„ ì‚¬ìš©í•©ë‹ˆë‹¤: `python config/settings.py from pydantic import BaseSettings from typing import Dict class Settings(BaseSettings): \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •\"\"\" # LLM ì„¤ì • refine_model: str = \"gpt-4o-mini-2024-07-18\" answer_model: str = \"gpt-4o-2024-05-13\" lang_detect_model: str = \"gpt-4o-mini-2024-07-18\" # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • chroma_db_path: str = \"./db/chroma_db\" # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • timezone: str = \"Asia/Seoul\" # API ì„¤ì • api_host: str = \"0.0.0.0\" api_port: int = 8000 class Config: env_file = \".env\" case_sensitive = False ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ settings = Settings() ` ì˜ì¡´ì„± ì»¨í…Œì´ë„ˆ ìƒì„± ì˜ì¡´ì„±ë“¤ì„ ê´€ë¦¬í•˜ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“­ë‹ˆë‹¤: `python di/container.py from typing import Dict from fastapi import FastAPI from apscheduler.schedulers.background import BackgroundScheduler from factories.llm_factory import LLMFactory from factories.retriever_factory import RetrieverFactory from config.settings import settings class DependencyContainer: \"\"\"ì˜ì¡´ì„± ì£¼ì… ì»¨í…Œì´ë„ˆ\"\"\" def __init__(self): self._services: Dict[str, object] = {} self._factories: Dict[str, object] = {} self._setup_factories() def _setup_factories(self): \"\"\"Factory ì´ˆê¸°í™”\"\"\" self._factories[\"llm\"] = LLMFactory({ \"refine_model\": settings.refine_model, \"answer_model\": settings.answer_model, \"lang_detect_model\": settings.lang_detect_model }) self._factories[\"retriever\"] = RetrieverFactory(settings.chroma_db_path) def get_retriever(self): \"\"\"ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)\"\"\" if \"retriever\" not in self._services: self._services[\"retriever\"] = self._factories[\"retriever\"].create_retriever() return self._services[\"retriever\"] def get_scheduler(self): \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)\"\"\" if \"scheduler\" not in self._services: scheduler = BackgroundScheduler(timezone=settings.timezone) scheduler.start() self._services[\"scheduler\"] = scheduler return self._services[\"scheduler\"] def get_llm_factory(self): \"\"\"LLM Factory ë°˜í™˜\"\"\" return self._factories[\"llm\"] def reset(self): \"\"\"í…ŒìŠ¤íŠ¸ìš© ë¦¬ì…‹ ë©”ì„œë“œ\"\"\" self._services.clear() ì „ì—­ ì»¨í…Œì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤ container = DependencyContainer() ` ë¦¬íŒ©í† ë§ëœ MyApp í´ë˜ìŠ¤ ì˜ì¡´ì„± ì£¼ì…ì„ ì ìš©í•œ MyApp í´ë˜ìŠ¤: `python app.py (ë¦¬íŒ©í† ë§ í›„) from fastapi import FastAPI from di.container import container from interfaces.retriever import Retriever from interfaces.scheduler import Scheduler class MyApp: \"\"\"ì˜ì¡´ì„± ì£¼ì…ì´ ì ìš©ëœ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤\"\"\" def __init__(self, retriever: Retriever = None, scheduler: Scheduler = None): self.app = FastAPI() # ì˜ì¡´ì„± ì£¼ì… self.retriever = retriever or container.get_retriever() self.scheduler = scheduler or container.get_scheduler() # ì‚¬ìš©ìë³„ ì„¸ì…˜ ê´€ë¦¬ self.user_conversations: Dict[str, ConversationalRetrievalChain] = {} # êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—°ë™ self.sheet = None self.setup_events() def setup_events(self): \"\"\"ì´ë²¤íŠ¸ ì„¤ì •\"\"\" select_sheet(self) self.scheduler.add_job( select_sheet, \"cron\", hour=0, minute=0, args=[self], id=\"create new sheet\" ) def get_conversation_chain(self, user_id: str) -> ConversationalRetrievalChain: \"\"\"ì‚¬ìš©ìë³„ ëŒ€í™” ì²´ì¸ ë°˜í™˜\"\"\" if user_id not in self.user_conversations: self.user_conversations[user_id] = ConversationalRetrievalChain( retriever=self.retriever ) return self.user_conversations[user_id] í…ŒìŠ¤íŠ¸ìš© íŒ©í† ë¦¬ í•¨ìˆ˜ def create_app(retriever=None, scheduler=None) -> MyApp: \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\"\"\" return MyApp(retriever=retriever, scheduler=scheduler) ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± myapp = create_app() ` ë¦¬íŒ©í† ë§ëœ ConversationalRetrievalChain í´ë˜ìŠ¤ ì˜ì¡´ì„± ì£¼ì…ì„ ì ìš©í•œ ì²´ì¸ í´ë˜ìŠ¤: `python chains/main_chain.py (ë¦¬íŒ©í† ë§ í›„) from interfaces.retriever import Retriever from factories.llm_factory import LLMFactory from di.container import container class ConversationalRetrievalChain: \"\"\"ì˜ì¡´ì„± ì£¼ì…ì´ ì ìš©ëœ ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸\"\"\" def __init__(self, retriever: Retriever, llm_factory: LLMFactory = None): self.retriever = retriever # Factoryë¥¼ í†µí•œ ì˜ì¡´ì„± ìƒì„± factory = llm_factory or container.get_llm_factory() self.refine_llm = factory.create_refine_llm() self.answer_llm = factory.create_answer_llm() self.lang_detect_llm = factory.create_lang_detect_llm() # ì²´ì»¤ ì²´ì¸ë“¤ self.checker_runnable_executor = get_combined_checker_chain( model_name=settings.lang_detect_model ) self.checker_reservation_runnable_executor = get_combined_checker_chain_reservation( model_name=settings.lang_detect_model ) self.chat_history = [] def process_query(self, user_message: str, user_id: str) -> str: \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬\"\"\" # ì–¸ì–´ ê°ì§€ detected_lang = self.detect_language(user_message) # ë¬¸ì„œ ê²€ìƒ‰ docs = self.retriever.get_docs_with_decomposition( query=user_message, max_k=5, threshold=0.7, lang=detected_lang ) # ë‹µë³€ ìƒì„± response = self.generate_response(user_message, docs) # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ self.update_chat_history(user_message, response) return response ` í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ Mock ê°ì²´ë¥¼ ì‚¬ìš©í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ `python tests/test_myapp.py import pytest from unittest.mock import Mock, MagicMock from app import create_app class TestMyApp: \"\"\"MyApp í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸\"\"\" def test_app_initialization_with_mocks(self): \"\"\"Mock ê°ì²´ë¥¼ ì‚¬ìš©í•œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸\"\"\" # Mock ê°ì²´ ìƒì„± mock_retriever = Mock() mock_scheduler = Mock() # ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì•± ìƒì„± app = create_app(retriever=mock_retriever, scheduler=mock_scheduler) # ê²€ì¦ assert app.retriever == mock_retriever assert app.scheduler == mock_scheduler assert isinstance(app.app, FastAPI) def test_conversation_chain_creation(self): \"\"\"ëŒ€í™” ì²´ì¸ ìƒì„± í…ŒìŠ¤íŠ¸\"\"\" mock_retriever = Mock() app = create_app(retriever=mock_retriever) # ì‚¬ìš©ìë³„ ì²´ì¸ ìƒì„± chain = app.get_conversation_chain(\"user123\") assert chain is not None assert chain.retriever == mock_retriever tests/test_conversational_chain.py class TestConversationalRetrievalChain: \"\"\"ConversationalRetrievalChain í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸\"\"\" def test_chain_initialization_with_mock_factory(self): \"\"\"Mock Factoryë¥¼ ì‚¬ìš©í•œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸\"\"\" mock_retriever = Mock() mock_factory = Mock() # Mock LLM ì¸ìŠ¤í„´ìŠ¤ë“¤ mock_refine_llm = Mock() mock_answer_llm = Mock() mock_lang_detect_llm = Mock() mock_factory.create_refine_llm.return_value = mock_refine_llm mock_factory.create_answer_llm.return_value = mock_answer_llm mock_factory.create_lang_detect_llm.return_value = mock_lang_detect_llm # ì²´ì¸ ìƒì„± chain = ConversationalRetrievalChain( retriever=mock_retriever, llm_factory=mock_factory ) # ê²€ì¦ assert chain.retriever == mock_retriever assert chain.refine_llm == mock_refine_llm assert chain.answer_llm == mock_answer_llm assert chain.lang_detect_llm == mock_lang_detect_llm ` í†µí•© í…ŒìŠ¤íŠ¸ `python tests/test_integration.py import pytest from di.container import container class TestIntegration: \"\"\"í†µí•© í…ŒìŠ¤íŠ¸\"\"\" def setup_method(self): \"\"\"í…ŒìŠ¤íŠ¸ ì „ ì»¨í…Œì´ë„ˆ ë¦¬ì…‹\"\"\" container.reset() def test_container_dependency_resolution(self): \"\"\"ì»¨í…Œì´ë„ˆ ì˜ì¡´ì„± í•´ê²° í…ŒìŠ¤íŠ¸\"\"\" retriever = container.get_retriever() scheduler = container.get_scheduler() llm_factory = container.get_llm_factory() assert retriever is not None assert scheduler is not None assert llm_factory is not None # ì‹±ê¸€í†¤ íŒ¨í„´ ê²€ì¦ retriever2 = container.get_retriever() assert retriever is retriever2 ` í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • `bash .env.development REFINE_MODEL=gpt-4o-mini-2024-07-18 ANSWER_MODEL=gpt-4o-2024-05-13 LANG_DETECT_MODEL=gpt-4o-mini-2024-07-18 CHROMA_DB_PATH=./db/chroma_db_dev TIMEZONE=Asia/Seoul API_HOST=0.0.0.0 API_PORT=8000 ` `bash .env.production REFINE_MODEL=gpt-4o-2024-05-13 ANSWER_MODEL=gpt-4o-2024-05-13 LANG_DETECT_MODEL=gpt-4o-mini-2024-07-18 CHROMA_DB_PATH=/data/chroma_db_prod TIMEZONE=Asia/Seoul API_HOST=0.0.0.0 API_PORT=80 ` ì„¤ì • ë¡œë”© ê°œì„  `python config/settings.py (ê°œì„ ëœ ë²„ì „) import os from pydantic import BaseSettings from typing import Dict class Settings(BaseSettings): \"\"\"í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬\"\"\" # í™˜ê²½ ì„¤ì • environment: str = \"development\" # LLM ì„¤ì • refine_model: str = \"gpt-4o-mini-2024-07-18\" answer_model: str = \"gpt-4o-2024-05-13\" lang_detect_model: str = \"gpt-4o-mini-2024-07-18\" # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • chroma_db_path: str = \"./db/chroma_db\" # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • timezone: str = \"Asia/Seoul\" # API ì„¤ì • api_host: str = \"0.0.0.0\" api_port: int = 8000 class Config: env_file = f\".env.{os.getenv('ENVIRONMENT', 'development')}\" case_sensitive = False settings = Settings() `` ì¥ì ê³¼ íš¨ê³¼ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± Mock ê°ì²´ë¥¼ ì‰½ê²Œ ì£¼ì…í•  ìˆ˜ ìˆìŒ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ì™€ í†µí•© í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ ê°€ëŠ¥ í…ŒìŠ¤íŠ¸ ê²©ë¦¬ ë³´ì¥ ìœ ì—°ì„± ëŸ°íƒ€ì„ì— ë‹¤ë¥¸ êµ¬í˜„ì²´ë¡œ êµì²´ ê°€ëŠ¥ ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€ ìš©ì´ ì„¤ì • ë³€ê²½ ì‹œ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš” ìœ ì§€ë³´ìˆ˜ì„± ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ ëª…í™•í•œ ì˜ì¡´ì„± ê´€ê³„ ë‹¨ì¼ ì±…ì„ ì›ì¹™ ì¤€ìˆ˜ í™•ì¥ì„± ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ì˜í–¥ ìµœì†Œí™” í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ ê°€ëŠ¥ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì „í™˜ ìš©ì´ ê²°ë¡  ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì„ ì ìš©í•¨ìœ¼ë¡œì¨ ì½”ë“œì˜ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ: ê²°í•©ë„ ê°ì†Œ: í´ë˜ìŠ¤ ê°„ì˜ ì˜ì¡´ì„±ì´ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬ë¨ í…ŒìŠ¤íŠ¸ ìš©ì´ì„±: Mock ê°ì²´ ì£¼ì…ìœ¼ë¡œ ê²©ë¦¬ëœ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ ì„¤ì • ê´€ë¦¬: ì¤‘ì•™í™”ëœ ì„¤ì •ìœ¼ë¡œ í™˜ê²½ë³„ ê´€ë¦¬ ìš©ì´ í™•ì¥ì„±**: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ì˜í–¥ ìµœì†Œí™” ì´ëŸ¬í•œ íŒ¨í„´ì€ ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ì—ì„œ íŠ¹íˆ ìœ ìš©í•˜ë©°, ì½”ë“œì˜ ì¥ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì°¸ê³  ìë£Œ Python Protocol Classes Pydantic Settings Management FastAPI Dependency Injection Clean Architecture by Robert C. Martin",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/why-we-need-causal-inference",
    "title": "ì¸ê³¼ ì¶”ë¡  ë„ì…ë°°ê²½",
    "date": "2025-07-14",
    "excerpt": "ì¸ê³¼ì¶”ë¡ ì„ ì™œ ì‚¬ìš©í•´ì•¼í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³  ì´ë¥¼ ë„ì…í•˜ê¸° ìœ„í•´ ì„¤ë“í•˜ëŠ” ê¸€",
    "category": "Causal Inference",
    "tags": [
      "ICL-Lenze-sizing"
    ],
    "content": "Abstract - ë°°ê²½, ìš”ì•½ --- í™˜ìì—ê²Œ ë Œì¦ˆ ì‚½ì…ìˆ ì„ ìœ„í•œ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ”, ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œëŠ” ì˜ˆì¸¡ ëª¨ë¸ì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì–´ì™”ë‹¤. í™˜ìì˜ ëˆˆì˜ ì¡°ê±´ê³¼ ì‚½ì…í•œ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥ ë°›ì•„ ìˆ˜ìˆ ê²°ê³¼(vaulting, ë Œì¦ˆ í›„ë©´ê³¼ ìˆ˜ì •ì²´ ì „ë©´ê¹Œì§€ì˜ ê±°ë¦¬)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ê³ , ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ í†µí•´ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•˜ëŠ” ë°©ì‹ì´ë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì€ í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ì´ë²ˆ ë³´ê³ ì„œë¥¼ í†µí•´ ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì ì„ ìš”ì•½í•˜ë©´ ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ” ì¸ê³¼ì ì¸ ì˜í–¥ë„ë¥¼ ì¶”ì •í•˜ì§€ ëª»í•˜ë©°, ë‹¨ìˆœíˆ ì˜ˆì¸¡ í‰ê°€ì§€í‘œë§Œì„ ê³ ë ¤í•˜ê³  ìˆì–´ ì¶”ì²œì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ ë„ì…í•  ê²ƒì„ ì œì•ˆí•œë‹¤. ì´ë•Œ ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆê³  ì´ë¥¼ í†µí•´ ê°œì¸í™”ëœ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ì¶”ì²œì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•  ìˆ˜ ìˆë‹¤. Introduction - ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì  ë¶„ì„ --- ë Œì¦ˆ ì‚½ì…ìˆ ì„ ìœ„í•œ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì°¾ëŠ” ì¼ì€ ì˜ì‚¬ì˜ ë†’ì€ ìˆ˜ì¤€ì˜ ê²½í—˜ê³¼ ë…¸í•˜ìš°ì— ì˜ì¡´í•˜ëŠ” ì¼ì´ë‹¤. ì´ë¥¼ ë„ì™€ì£¼ê¸° ìœ„í•´ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ” ëª¨ë¸ì´ ê°œë°œë˜ì–´ì™”ë‹¤. ê¸°ì¡´ì˜ ìµœì ì˜ ì‚¬ì´ì¦ˆë¥¼ ì°¾ê¸° ìœ„í•œ ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œëŠ” ì˜ˆì¸¡ ëª¨ë¸ì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì–´ì™”ë‹¤. í™˜ìì˜ ëˆˆì˜ ì¡°ê±´ê³¼ ì‚½ì…í•œ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥ ë°›ì•„ ìˆ˜ìˆ ê²°ê³¼(vaulting, ë Œì¦ˆ í›„ë©´ê³¼ ìˆ˜ì •ì²´ ì „ë©´ê¹Œì§€ì˜ ê±°ë¦¬)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©í–¥ì„ ì§„í–‰ë˜ì—ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ MAEì™€ ê°™ì´ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ í‰ê°€í•˜ëŠ” ì˜ˆì¸¡í‰ê°€ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì´ë¥¼ ê°œì„ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆë‹¤. MAE: Mean Absolute Error, ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ì˜ ì ˆëŒ€ê°’ì˜ í‰ê·  í•˜ì§€ë§Œ ì˜ˆì¸¡ ëª¨ë¸ ê¸°ë°˜ì˜ ë Œì¦ˆ ì‚¬ì´ì§• ëª¨ë¸ì¸ ê¸°ì¡´ ì—°êµ¬ëŠ” ë¶„ëª…í•œ í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ë¨¼ì € ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒì˜ ë¬¸ì œë¥¼ ì´í•´í•˜ê³  êµ¬ë¶„í•´ì•¼ í•œë‹¤. 1.1 ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒì˜ ë¬¸ì œ ì°¨ì´ --- ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒì˜ ë¬¸ì œëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì„±ê²©ì„ ê°€ì§„ë‹¤. ë¨¼ì € ì˜ˆì¸¡ ë¬¸ì œëŠ” ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \"í™˜ì Aê°€ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ìœ¼ë©´ vaultingì´ ì–¼ë§ˆê°€ ë ê¹Œ?\"ì™€ ê°™ì€ ë¬¸ì œê°€ ì˜ˆì¸¡ ë¬¸ì œì— í•´ë‹¹í•œë‹¤. ì˜ˆì¸¡ ë¬¸ì œì˜ ëª©í‘œëŠ” ì •í™•í•œ ì˜ˆì¸¡ì´ë‹¤. ì˜ˆì¸¡ ë¬¸ì œì˜ ì„±ëŠ¥ ì§€í‘œëŠ” MAE, RMSE, RÂ² ë“±ì´ ìˆë‹¤. ì„ íƒì˜ ë¬¸ì œëŠ” ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ ìµœì ì˜ ì„ íƒì„ í•˜ëŠ” ë¬¸ì œë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \"í™˜ì Aì—ê²Œ 12.1mm, 12.6mm, 13.2mm ì¤‘ ì–´ë–¤ ë Œì¦ˆë¥¼ ì¶”ì²œí• ê¹Œ?\"ì™€ ê°™ì€ ë¬¸ì œê°€ ì„ íƒì˜ ë¬¸ì œì— í•´ë‹¹í•œë‹¤. ì„ íƒ ë¬¸ì œì˜ ëª©í‘œëŠ” ìµœì ì˜ ì„ íƒì´ë‹¤. ì„ íƒ ë¬¸ì œì˜ ì„±ëŠ¥ ì§€í‘œëŠ” ì²˜ì¹˜ íš¨ê³¼, ê°œì¸í™”ëœ ì¶”ì²œ ì •í™•ë„ ë“±ì´ ìˆë‹¤. ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒ ë¬¸ì œì˜ ê°€ì¥ í° ì°¨ì´ì ì€ í†µì œ ë³€ìˆ˜(Control Variable)ì˜ ì¡´ì¬ ì—¬ë¶€ì´ë‹¤. í†µì œ ë³€ìˆ˜ëŠ” ì—°êµ¬ìê°€ ì§ì ‘ ì¡°ì‘í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë¡œ, ì‹¤í—˜ ì„¤ê³„ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•œë‹¤. ì´ëŠ” ì¸ê³¼ì¶”ë¡ ì—ì„œ ì²˜ì¹˜(Treatment)ì™€ ë™ì¼í•œ ê°œë…ìœ¼ë¡œ, ì—°êµ¬ìê°€ ì˜ë„ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë…ë¦½ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œëŠ” ë Œì¦ˆì˜ ì‚¬ì´ì¦ˆê°€ ì²˜ì¹˜ì— í•´ë‹¹í•˜ë©° ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²˜ì¹˜íš¨ê³¼ë¼ê³  í•œë‹¤. ì˜ˆì¸¡ ë¬¸ì œì—ì„œëŠ” ëª¨ë“  ì…ë ¥ ë³€ìˆ˜ê°€ ì£¼ì–´ì§„ ê°’ìœ¼ë¡œ ê³ ì •ë˜ì–´ ìˆë‹¤. ì¦‰, ëª¨ë¸ì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ ì¡°ê±´ í•˜ì—ì„œ ê²°ê³¼ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \"í™˜ì Aê°€ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ìœ¼ë©´ vaultingì´ ì–¼ë§ˆê°€ ë ê¹Œ?\"ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ë Œì¦ˆ ì‚¬ì´ì¦ˆ(12.6mm)ëŠ” ì´ë¯¸ ê²°ì •ëœ ê³ ì •ê°’ì´ë©°, ì´ë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤. ì´ ê²½ìš° ëª¨ë¸ì€ í™˜ìì˜ ëˆˆ ì¡°ê±´ê³¼ ê³ ì •ëœ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ vaulting ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµí•œë‹¤. ì„ íƒ ë¬¸ì œì—ì„œëŠ” í†µì œ ë³€ìˆ˜ê°€ í•µì‹¬ ì—­í• ì„ í•œë‹¤. ì—°êµ¬ìëŠ” ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ì—ì„œ ìµœì ì˜ ì„ íƒì„ í•˜ê¸° ìœ„í•´ í†µì œ ë³€ìˆ˜ì˜ ê°’ì„ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤. \"í™˜ì Aì—ê²Œ 12.1mm, 12.6mm, 13.2mm ì¤‘ ì–´ë–¤ ë Œì¦ˆë¥¼ ì¶”ì²œí• ê¹Œ?\"ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ë Œì¦ˆ ì‚¬ì´ì¦ˆëŠ” í†µì œ ë³€ìˆ˜ë¡œ, ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ì´ë‹¤. ì´ ê²½ìš° ëª¨ë¸ì€ ê° ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì˜µì…˜ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê³ , ìµœì ì˜ ì„ íƒì„ ì œì•ˆí•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤. ì˜ˆì¸¡ ë¬¸ì œëŠ” ì¡°ê±´ë¶€ í™•ë¥  $P(Y X)$ë¥¼ ì¶”ì •í•˜ëŠ” ë¬¸ì œì´ê³ , ì„ íƒ ë¬¸ì œëŠ” ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜ íš¨ê³¼ $E[Y(T=t) X]$ë¥¼ ì¶”ì •í•˜ëŠ” ë¬¸ì œì´ë‹¤. ì—¬ê¸°ì„œ $T$ëŠ” í†µì œ ë³€ìˆ˜(ì²˜ì¹˜)ë¥¼, $X$ëŠ” í™˜ì íŠ¹ì„±ì„, $Y$ëŠ” ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜ íš¨ê³¼ë€ í†µì œ ë³€ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì²˜ì¹˜ íš¨ê³¼ì˜ í‰ê· ì„ ì˜ë¯¸í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì§•ì€ ë³¸ì§ˆì ìœ¼ë¡œ ì„ íƒì˜ ë¬¸ì œì´ë‹¤. í™˜ìì—ê²Œ ìµœì ì˜ ë Œì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ì—°êµ¬ëŠ” ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì„ íƒì˜ ë¬¸ì œë¥¼ ì œëŒ€ë¡œ í•´ê²°í•˜ì§€ ëª»í•œë‹¤. 1.2 ì˜ˆì¸¡ ëª¨ë¸ì´ ì„ íƒì˜ ë¬¸ì œì— í™œìš©ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ  --- ë Œì¦ˆ ì‚¬ì´ì§•ì´ ì„ íƒì˜ ë¬¸ì œì„ì„ í™•ì¸í–ˆìœ¼ë¯€ë¡œ, ì´ì œ ì˜ˆì¸¡ ëª¨ë¸ì´ ì„ íƒì˜ ë¬¸ì œì— í™œìš©ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ë¥¼ ë¶„ì„í•œë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì´ ê°€ì§€ëŠ” í•œê³„ì ì€ ë¬´ì—‡ì¸ì§€, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒì„ í•  ë•Œì˜ ìœ„í—˜ì„±ì— ëŒ€í•´ ì´ì•¼ê¸°í•œë‹¤. 1.2.1 ìƒê´€ê´€ê³„ì— ê¸°ë°˜í•œ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ ì˜ˆì¸¡ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ê°„ì˜ ë™ì‹œ ë³€ë™ íŒ¨í„´, ì¦‰ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•œë‹¤. ìƒê´€ê´€ê³„ëŠ” ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ì¼ì •í•œ íŒ¨í„´ì´ë‚˜ ë™ë°˜ ë³€ë™ì´ ì¡´ì¬í•¨ì„ ì˜ë¯¸í•˜ë©°, í•œ ë³€ìˆ˜ê°€ ì»¤ì§ˆ ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ì¼ì •í•˜ê²Œ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ê²½í–¥ì„ ë³´ì´ëŠ” í†µê³„ì  ê´€ê³„ë¥¼ ë§í•œë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì€ ìƒê´€ê´€ê³„ë§Œì„ í•™ìŠµí•˜ë¯€ë¡œ ìˆ¨ê²¨ì§„ êµë€ë³€ìˆ˜(Confounding Variable)ì˜ ì¡´ì¬ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•œë‹¤. êµë€ë³€ìˆ˜(Confounding Variable) : ì²˜ì¹˜(Treatment)ì™€ ê²°ê³¼(Outcome) ëª¨ë‘ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì œ3ì˜ ë³€ìˆ˜ë¡œ, ì²˜ì¹˜ì™€ ê²°ê³¼ ì‚¬ì´ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ì™œê³¡ì‹œí‚¤ëŠ” ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. êµë€ë³€ìˆ˜ëŠ” ì²˜ì¹˜ ì„ íƒê³¼ ê²°ê³¼ì— ë™ì‹œì— ì˜í–¥ì„ ì£¼ì–´ ê°€ì§œ ì¸ê³¼ê´€ê³„ë¥¼ ë§Œë“¤ì–´ë‚´ê±°ë‚˜ ì‹¤ì œ ì¸ê³¼ê´€ê³„ë¥¼ ìˆ¨ê¸¸ ìˆ˜ ìˆë‹¤. êµë€í¸í–¥(Confounding Bias) : êµë€ë³€ìˆ˜ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” í†µê³„ì  í¸í–¥ìœ¼ë¡œ, ì²˜ì¹˜ì™€ ê²°ê³¼ ì‚¬ì´ì˜ ì‹¤ì œ ì¸ê³¼ê´€ê³„ê°€ ì™œê³¡ë˜ì–´ ë‚˜íƒ€ë‚˜ëŠ” í˜„ìƒì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” ì²˜ì¹˜ ê·¸ë£¹ ê°„ì— êµë€ë³€ìˆ˜ì˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ë•Œ ë°œìƒí•˜ë©°, ì²˜ì¹˜ íš¨ê³¼ì˜ ì •í™•í•œ ì¶”ì •ì„ ë°©í•´í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìˆ™ë°• ê°€ê²©ê³¼ ë§¤ì¶œ ë°ì´í„°ì—ì„œ ê°€ê²©ì´ ë†’ì„ìˆ˜ë¡ ë§¤ì¶œì´ ë†’ì•„ ë³´ì´ëŠ” íŒ¨í„´ì´ ê´€ì°°ë˜ë©´, ì˜ˆì¸¡ ëª¨ë¸ì€ \"ê°€ê²©ì„ ì˜¬ë¦¬ë©´ ë§¤ì¶œì´ ì˜¤ë¥¸ë‹¤\"ê³  ê²°ë¡ ì„ ë‚´ë¦°ë‹¤. í•˜ì§€ë§Œ ì´ íŒ¨í„´ ë’¤ì—ëŠ” ì„±ìˆ˜ê¸°/ë¹„ìˆ˜ê¸° ì—¬ë¶€ë¼ëŠ” ìˆ¨ê²¨ì§„ êµë€ë³€ìˆ˜ê°€ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œëŠ” ì„±ìˆ˜ê¸°ì— ê°€ê²©ê³¼ ë§¤ì¶œì´ ë™ì‹œì— ìƒìŠ¹í•˜ê³ , ë¹„ìˆ˜ê¸°ì— ê°€ê²©ê³¼ ë§¤ì¶œì´ ë™ì‹œì— í•˜ë½í•˜ëŠ” ê²ƒì´ë©°, ê°€ê²©ê³¼ ë§¤ì¶œ ì‚¬ì´ì—ëŠ” ì§ì ‘ì ì¸ ì¸ê³¼ê´€ê³„ê°€ ì—†ë‹¤. ì´ëŸ¬í•œ ìƒê´€ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ì˜¤í•´í•˜ë©´ \"ë§¤ì¶œì„ ì˜¬ë¦¬ë ¤ë©´ ê°€ê²©ì„ ì˜¬ë ¤ì•¼ í•œë‹¤\"ëŠ” ì˜ëª»ëœ ê²°ë¡ ì— ì´ë¥¼ ìˆ˜ ìˆê³ , ì‹¤ì œ íš¨ê³¼ê°€ ìˆëŠ” ê°œì…(ì˜ˆ: ë¹„ìˆ˜ê¸° í”„ë¡œëª¨ì…˜) ëŒ€ì‹  ë¶€ì ì ˆí•œ ê°€ê²© ì •ì±…ì„ ì„ íƒí•˜ê²Œ ë§Œë“ ë‹¤. ì¸ê³¼ê´€ê³„: í•œ ë³€ìˆ˜ê°€ ë³€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ê·¸ ë³€í™”ì— ì˜í•´ ì§ì ‘ì Â·ì²´ê³„ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ì•„ ë³€í™”í•˜ëŠ” ê´€ê³„ë¥¼ ì˜ë¯¸ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  í•´ì„œ ì¸ê³¼ê´€ê³„ë¥¼ ì˜ ì°¾ì•˜ë‹¤ëŠ” ë³´ì¥ì´ ì—†ìœ¼ë©°, ë Œì¦ˆ ì‚¬ì´ì§•ê³¼ ê°™ì€ ì„ íƒ ë¬¸ì œì—ì„œëŠ” ì¸ê³¼ê´€ê³„ê°€ í•„ìš”í•˜ì§€ë§Œ ì˜ˆì¸¡ ëª¨ë¸ì€ ì´ë¥¼ ì œê³µí•˜ì§€ ëª»í•œë‹¤. ë”°ë¼ì„œ ì˜ˆì¸¡ ëª¨ë¸ì— ê¸°ë°˜í•´ ì„ íƒì„ í•˜ëŠ” ê²ƒì€ ìœ„í—˜í•˜ë©°, ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì´ í•„ìš”í•˜ë‹¤. ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì„ í†µí•´ ì²˜ì¹˜ (í†µì œë³€ìˆ˜, ë Œì¦ˆì‚¬ì´ì¦ˆ)ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆê³  ì´ë¥¼ í†µí•´ ì˜¬ë°”ë¥¸ ì„ íƒì„ í•  ìˆ˜ ìˆë‹¤. 1.2.2 ë°ì´í„° ì•ˆì— ì¡´ì¬í•˜ëŠ” í†µê³„ì  í¸í–¥ ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ë°ì´í„°ì— í†µê³„ì  í¸í–¥(êµë€í¸í–¥)ì´ ì¡´ì¬í•˜ë©´ ì˜ˆì¸¡ ëª¨ë¸ì€ ì˜ëª»ëœ íŒ¨í„´, ì¦‰ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œ ë°ì´í„°ëŠ” í†µê³„ì  í¸í–¥ì„ ê°€ì§€ê³  ìˆë‹¤. ì´ëŠ” ë Œì¦ˆ í¬ê¸°ë³„(ì²˜ì¹˜ë³„)ë¡œ í™˜ìì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤ëŠ” ì˜ë¯¸ì´ë©°, ë°ì´í„°ì—ì„œ í™•ì¸ëœë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„_Feature_DensityPlot.png\" alt=\"ì²˜ì¹˜ë³„ Feature Density Plot\"> <figcaption>ê·¸ë¦¼ 1: ì²˜ì¹˜ë³„ í™˜ì ëˆˆì˜ í¬ê¸° ë¶„í¬ ë¹„êµ</figcaption> </figure> ê·¸ë¦¼ 1ì€ ì²˜ì¹˜(ë Œì¦ˆ ì‚¬ì´ì¦ˆ)ë³„ë¡œ í™˜ìì˜ ëˆˆ í¬ê¸°ì— ëŒ€í•œ ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ê·¸ë¦¼ 1ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì²˜ì¹˜ë³„ë¡œ í™˜ìì˜ ëˆˆ í¬ê¸° ë¶„í¬(ATA, ACD)ê°€ ëª…í™•íˆ ë‹¤ë¥´ë‹¤. ì´ëŠ” ì˜ì‚¬ê°€ í™˜ìì˜ ê°œë³„ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ë Œì¦ˆë¥¼ ì„ íƒí•˜ëŠ” ì„ìƒì  íŒë‹¨ì— ë”°ë¥¸ ê²°ê³¼ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëˆˆì´ ì‘ì€ í™˜ìì—ê²ŒëŠ” ì‘ì€ ë Œì¦ˆë¥¼, ëˆˆì´ í° í™˜ìì—ê²ŒëŠ” í° ë Œì¦ˆë¥¼ ì„ íƒí•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì´ëŸ¬í•œ ì„ íƒ í¸í–¥ì€ êµë€í¸í–¥ì„ ìœ ë°œí•œë‹¤. í™˜ìì˜ ëˆˆ í¬ê¸°, ë‚˜ì´, ê°ë§‰ ë‘ê»˜ ë“±ì€ ë Œì¦ˆ ì„ íƒê³¼ ìˆ˜ìˆ  ê²°ê³¼ ëª¨ë‘ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” êµë€ë³€ìˆ˜ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì•ì„œ 1.2.1ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´, ë°ì´í„°ì— í†µê³„ì  í¸í–¥ì´ ì¡´ì¬í•  ê²½ìš° ì˜ˆì¸¡ ëª¨ë¸ì€ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ ì˜ëª»ëœ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ íŒ¨í„´ì— ê¸°ë°˜í•œ ì„ íƒì€ ìœ„í—˜í•˜ë‹¤. ë˜í•œ í†µê³„ì  í¸í–¥ì€ ë Œì¦ˆì˜ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì¸ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì™œê³¡í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì¦ˆë³„ë¡œ í™˜ì ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë‹¨ìˆœíˆ ê²°ê³¼ë§Œ ë¹„êµí•´ì„œëŠ” ë Œì¦ˆ ì‚¬ì´ì¦ˆì˜ íš¨ê³¼ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤. í™˜ìì˜ ê°œë³„ íŠ¹ì„±ì„ ë™ì¼í•˜ê²Œ ë§ì¶˜ ìƒíƒœì—ì„œ ë Œì¦ˆ íš¨ê³¼ë¥¼ ë¹„êµí•´ì•¼ í•œë‹¤. ì´ëŠ” ì‹¤í—˜ì„ ì„¤ê³„í•  ë•Œ ëŒ€ì¡°êµ°ê³¼ ì²˜ì¹˜êµ°ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ë§Œ ì‹¤í—˜ ì¡°ê±´ë§Œì˜ ì˜í–¥ì„ ì¶”ì •í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ ëŒ€ì¡° ì‹¤í—˜(RCT, Randomized Controlled Trial)ì„ ì§„í–‰í•  ìˆ˜ ì—†ë‹¤. ë¬´ì‘ìœ„ ëŒ€ì¡° ì‹¤í—˜ : ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•˜ëŠ” ë°©ë²• ë”°ë¼ì„œ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê³  ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•´ì•¼ í•œë‹¤. ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì€ ì´ëŸ¬í•œ êµë€í¸í–¥ì„ í†µê³„ì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ì‹¤ì œ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•œë‹¤. êµ¬ì²´ì ì¸ ë°©ë²•ì€ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ë‹¤ë£¬ë‹¤. 1.3 ì˜ˆì¸¡ í‰ê°€ì§€í‘œì˜ í•œê³„ --- ì˜ˆì¸¡ ëª¨ë¸ì˜ ë˜ ë‹¤ë¥¸ í•œê³„ëŠ” ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°(ë°˜ì‚¬ì‹¤, Counterfactual)ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì˜ˆì¸¡ ëª¨ë¸ì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì˜ˆì¸¡ ì„±ëŠ¥ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ìµœì ì˜ ë Œì¦ˆë¥¼ ì¶”ì²œí•˜ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ìˆ ë°›ì€ ë Œì¦ˆ ì™¸ì— ë‹¤ë¥¸ ë Œì¦ˆ í¬ê¸°ì— ëŒ€í•œ ê²°ê³¼ë„ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ëŠ” ì‹¤ì œë¡œ ì¼ì–´ë‚˜ì§€ ì•Šì€ ìƒí™©ì— ëŒ€í•œ í‰ê°€ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í™˜ì Aê°€ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ê³  ì´ì— ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  ê°€ì •í•œë‹¤. ì´ë•Œ ì¶”ì²œ ëª¨ë¸ì´ ì¦ëª…í•´ì•¼ í•˜ëŠ” ê²ƒì€ \"ë§Œì•½ ì´ í™˜ìê°€ 13.2mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ë‹¤ë©´ ì–´ë–¤ ê²°ê³¼ê°€ ë‚˜ì™”ì„ê¹Œ?\"ì´ë‹¤. í•˜ì§€ë§Œ ì´ í™˜ìëŠ” ì´ë¯¸ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì—, 13.2mm ë Œì¦ˆì˜ ê²°ê³¼ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤. ì´ë ‡ê²Œ ì‹¤ì œë¡œëŠ” ê´€ì°°ë˜ì§€ ì•Šì€ ê°€ìƒì˜ ìƒí™©ì„ \"ë°˜ì‚¬ì‹¤(Counterfactual)\"ì´ë¼ê³  í•œë‹¤. ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì€ ê´€ì°°ëœ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, ì´ëŸ° ë°˜ì‚¬ì‹¤ì  ìƒí™©ì— ëŒ€í•œ ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•œ í‰ê°€ì§€í‘œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì´ëŸ° ë°˜ì‚¬ì‹¤ì  ìƒí™©ì„ í†µê³„ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œê³µí•˜ë©°, ì´ì— ëŒ€í•œ í‰ê°€ì§€í‘œë„ í•¨ê»˜ ê³ ë ¤í•´ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•œë‹¤. Method & Result - ì¸ê³¼ì¶”ë¡ ì— ëŒ€í•œ ì†Œê°œì™€ ì¸ê³¼ì¶”ë¡ ì„ í†µí•´ ì´ë¥¼ í•´ê²°í•œ ë°©ë²• ì†Œê°œ --- ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ì„ íƒì˜ ë¬¸ì œì—ì„œ ì˜ˆì¸¡ ëª¨ë¸ì€ í•œê³„ì ì„ ê°€ì§„ë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ ë„ì…í•˜ê³ ì í•œë‹¤. ì¸ê³¼ì¶”ë¡ ì´ë€ ì˜í•™, ë§ˆì¼€íŒ…, êµìœ¡ ë“±ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ ì •ì±… ë° ë¹„ì¦ˆë‹ˆìŠ¤ì— ìˆ˜ë°˜ëœ ìˆ˜ë§ì€ ì˜ì‚¬ê²°ì •ë“¤ì˜ ì›ì¸ê³¼ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì˜ì‚¬ê²°ì •ì„ ë•ëŠ” ë°©ë²•ë¡ ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì •ë°€ì˜í•™ì—ì„œëŠ” í™˜ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ìˆ˜ìˆ ë°©ë²•ì„ ì„ íƒì„ ê²°ì •í•˜ê³ , êµìœ¡ì—ì„œëŠ” í•™ìƒë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ êµìœ¡ ë°©ë²•ì„ ê²°ì •í•˜ê³ , ì˜¨ë¼ì¸ ë§ˆì¼€íŒ…ì—ì„œëŠ” ì‚¬ìš©ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ë§ì¶¤í˜• ê´‘ê³ ë¥¼ ì œê³µí•˜ê³ , ì˜¤í”„ë¼ì¸ ì •ì±… í‰ê°€ì—ì„œëŠ” ì§€ì—­ë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì •ì±…ì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì²˜ì¹˜ë€ ì¸ê³¼ì¶”ë¡ ì—ì„œ ì˜ì‚¬ê²°ì •ì˜ ëŒ€ìƒ(ì˜ˆ: ìˆ˜ìˆ  ë°©ë²•, ì•½ë¬¼ íˆ¬ì—¬, êµìœ¡ì •ì±… ë“±)ì„ ì˜ë¯¸í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œëŠ” ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ì²˜ì¹˜ì— í•´ë‹¹í•œë‹¤. ì¸ê³¼ì¶”ë¡ ì„ í†µí•´ ë Œì¦ˆì˜ ì‚¬ì´ì¦ˆì™€ ê²°ê³¼ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ í†µí•´ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ê²°ì •í•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì¸ê³¼ì¶”ë¡ ì„ ë„ì…í•¨ìœ¼ë¡œì¨ ë‹¤ìŒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤. ì²«ì§¸, í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê³  ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆì–´ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì˜í–¥ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ë‘˜ì§¸, ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°(ë°˜ì‚¬ì‹¤ì  ìƒí™©)ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤. ì…‹ì§¸, ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼(Heterogeneous Treatment Effects)ë¥¼ ì¶”ì •í•˜ì—¬ í™˜ìë³„ë¡œ ìµœì í™”ëœ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•  ìˆ˜ ìˆë‹¤. ì´ë²ˆ ë³´ê³ ì„œì—ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ê²°ê³¼ì— ëŒ€í•´ì„œë§Œ ì†Œê°œí•˜ë©°, ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ê°œì¸í™”ëœ ì¶”ì²œì„ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œëŠ” ì¶”í›„ì— ë‹¤ë£¨ë„ë¡ í•œë‹¤. 2.1 ì¸ê³¼ì¶”ë¡  ë°©ë²•ì„ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±° ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì¸ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì²˜ëŸ¼ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ëŠ” í†µì œí•´ì•¼ í•œë‹¤. ì¦‰ ë Œì¦ˆ ì‚¬ì´ì¦ˆë³„ë¡œ í™˜ìì˜ ëˆˆì˜ í¬ê¸°ì— ëŒ€í•œ ë¶„í¬ê°€ ê°™ì•„ì•¼ í•œë‹¤. í•˜ì§€ë§Œ ì•ì„œ í™•ì¸í•œ ê²ƒì²˜ëŸ¼ ë°ì´í„°ì—ëŠ” í¸í–¥ì´ ì¡´ì¬í•˜ë©°, ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ RCTë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ë‹¤. ì´ë•Œ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•  ìˆ˜ ìˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„ê²°ê³¼ë¹„êµ.png\" alt=\"ì²˜ì¹˜ë³„ ê²°ê³¼ ë¹„êµ\"> <figcaption>ê·¸ë¦¼ 2: ì²˜ì¹˜ë³„ ê²°ê³¼ ë¶„í¬ ë¹„êµ</figcaption> </figure> ê·¸ë¦¼ 2ëŠ” ì²˜ì¹˜ë³„ ê²°ê³¼ì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ì•ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ì²˜ì¹˜ë³„ë¡œ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì²˜ì¹˜ë³„ ë¶„í¬ë¥¼ ë¹„êµí•´ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ì—†ë‹¤. ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•´ì•¼ í•œë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì‚¬ì´ì¦ˆë³„ì”ì°¨ë¶„í¬.png\" alt=\"ê²°ê³¼ì—ì„œ êµë€ë³€ìˆ˜ì— ëŒ€í•œ ì˜í–¥ ì œê±°í›„ ë¶„í¬\"> <figcaption>ê·¸ë¦¼ 3: ì”ì°¨í™”ë¥¼ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ê²°ê³¼ ë¶„í¬</figcaption> </figure> ê·¸ë¦¼ 3ì€ ê·¸ë¦¼ 2ì˜ ê²°ê³¼ì—ì„œ ì”ì°¨í™”ë¥¼ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ê²ƒì´ë‹¤. ì”ì°¨í™”ë€ ê²°ê³¼(vaulting)ì— ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ë³€ìˆ˜(êµë€ë³€ìˆ˜)ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œê±°í•´ êµë€ë³€ìˆ˜ë¥¼ í†µì œí•œ ê²ƒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë§Œë“œëŠ” ë°©ë²•ë¡ ì´ë‹¤. ì”ì°¨í™”ë¥¼ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ë©´ ë Œì¦ˆ ì‚¬ì´ì¦ˆë³„ë¡œ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ê°™ì•„ì§€ë¯€ë¡œ ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì´ì²˜ëŸ¼ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì€ ë°ì´í„°ê°€ ê°€ì§€ê³  ìˆëŠ” í†µê³„ì  í¸í–¥ì„ í™•ì¸í•˜ê³  ì´ë¥¼ ì œê±°í•´ ì²˜ì¹˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê³  ì¸ê³¼ì ì¸ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤. Conclusion - ì—°êµ¬ ìš”ì•½ ë° í–¥í›„ ê³„íš --- ë³¸ ì—°êµ¬ì—ì„œëŠ” ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œ ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì˜ ë„ì…ì„ ì œì•ˆí•˜ì˜€ë‹¤. ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì€ ì„ íƒì˜ ë¬¸ì œì—ì„œ í†µê³„ì  í¸í–¥ì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ê³  ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ ë„ì…í•˜ì—¬ í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê³  ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ì˜€ë‹¤. ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì˜ ë„ì…ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤. ì²«ì§¸, í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ì—¬ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì¸ê³¼ì  ì˜í–¥ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ë‘˜ì§¸, ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°(ë°˜ì‚¬ì‹¤ì  ìƒí™©)ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤. ì…‹ì§¸, ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼(Heterogeneous Treatment Effects)ë¥¼ ì¶”ì •í•˜ì—¬ í™˜ìë³„ë¡œ ìµœì í™”ëœ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•  ìˆ˜ ìˆë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼ ì¶”ì •ì„ í†µí•œ ê°œì¸í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ ê°œë°œì— ì§‘ì¤‘í•  ì˜ˆì •ì´ë‹¤. ì´ë¥¼ í†µí•´ í™˜ìë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì¶”ì²œì´ ê°€ëŠ¥í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ ì í•œë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Fitness/Workout Log/250713",
    "title": "ê°„ë‹¨í•œ ë‹¬ë¦¬ê¸° + ì‚°ì±…",
    "date": "2025-07-13",
    "excerpt": "ìš´ë™ì¼ì§€",
    "category": "Workout Log",
    "tags": [],
    "content": "í™•ì‹¤íˆ ëŸ°ë‹ ìì£¼ ì•ˆí•˜ë‹ˆê¹Œ ì§„ì§œ ì•ˆë›°ì–´ì§„ë‹¤. ê¾¸ì¤€íˆ ë›°ì–´ë³´ì.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/í† í”Œì¤€ë¹„",
    "title": "TOEFL 100ì  ì´ìƒ ëª©í‘œë¡œ ê³µë¶€",
    "date": "2025-07-13",
    "excerpt": "OMSCS ì§€ì›ì„ ìœ„í•œ í† í”Œ ê³µë¶€",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›"
    ],
    "content": "ë°°ê²½ OMSCSë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ì„œëŠ” í† í”Œ 100ì  ì´ìƒì´ í•„ìš”í•œë°ìš”, ì´ë¥¼ ìœ„í•´ì„œ í† í”Œ ê³µë¶€ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê³µë¶€í•˜ë©´ì„œ ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼í• ì§€ ì •ë¦¬í•´ë‚˜ê°€ê³  ìˆìŠµë‹ˆë‹¤. ëª©í‘œ ê¸°ê°„ì€ 3~4ê°œì›” ì •ë„ì…ë‹ˆë‹¤. ì˜¬í•´ 11ì›”ë¶€í„°ëŠ” OSMCì— ì„œë¥˜ë¥¼ ë„£ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤. í•™ìŠµ ê³„íš ì„¸ì›Œë³´ê¸° ê³µë¶€ ì „ëµ ìœ íŠœë¸Œ ë§í¬ - ğŸ€í† í”Œ ë…í•™ì„ ë§ˆìŒë¨¹ìœ¼ì‹  ë¶„ë“¤ì„ ìœ„í•œ ì²« ë²ˆì§¸ ê°€ì´ë“œ (ì‹œí—˜ ì´í•´, ê³µë¶€ ë°©ë²•, ë‘ë ¤ì›€ ê·¹ë³µ, ì°¸ê³  ë§í¬) ëª…í™•í•œ ëª©í‘œ ê¸°ê°„ì„ ì¡ê³  ì§‘ì¤‘í•´ì„œ ê³µë¶€í•˜ê¸° ì‹œí—˜êµ¬ì„±ì— ëŒ€í•´ ì •í™•íˆ ì´í•´í•˜ê³  ì‹œì‘í•˜ê¸° ì‹œí—˜ êµ¬ì„± Reading 2ê°œ ì§€ë¬¸ x 10ë¬¸ì œ. ì§€ë¬¸í•˜ë‚˜ë‹¹ ì•½ 700ë‹¨ì–´. ì´ 35ë¶„ì •ë„. Listening 2ê°œ ëŒ€í™” (ê° 3ë¶„) x 5ë¬¸ì œ 3ê°œ ê°•ì˜ (ê° 3-5ë¶„) x 6ë¬¸ì œ ì´ 36ë¶„ ì •ë„. Speaking (1) ë…ë¦½í˜• (15ì´ˆ ì¤€ë¹„, 45ì´ˆ ë‹µë³€) (2), (3) í†µí•©í˜• - ì½+ë“£+ë§ (30ì´ˆ ì¤€ë¹„, 60ì´ˆ ë‹µë³€) (4) í†µí•©í˜• - ë“£+ë§ (20ì´ˆ ì¤€ë¹„, 60ì´ˆ ë‹µë³€) ì´ 16ë¶„ ì •ë„. Writing (1) í†µí•©í˜• - ì½+ë“£+ì“° (20ë¶„) (2) í† ë¡ í˜• - ì½+ì“° (10ë¶„) ì´ 30ë¶„ì •ë„. ì°¸ê³  ì‚¬ì´íŠ¸ ğŸ”—ì°¸ê³  ì‚¬ì´íŠ¸ ë§í¬ ETS TOEFL ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì‹œí—˜ êµ¬ì„± ë° ë¬¸ì œ ìœ í˜• ì•ˆë‚´ https://www.ets.org/toefl/test-takers... ETS TOEFL TestReady ì›¹ì‚¬ì´íŠ¸ (ëª¨ì˜ê³ ì‚¬, ì•¡í‹°ë¹„í‹°) https://testready.ets.org/test-prep-l... í…ŒìŠ¤íŠ¸ê¸€ë¼ì´ë” ì›¹ì‚¬ì´íŠ¸ https://www.testglider.com/ko/toefl ì°¸ê³ ìë£Œ [[í† í”Œê³µë¶€/í† í”Œ ê³µë¶€ë²•] ì´ˆë³´ìë¥¼ ìœ„í•œ í† í”Œ ë…í•™ ê°€ì´ë“œ : A to Z](https://beyondourlimit.tistory.com/68) : ë¸”ë¡œê·¸, ê¸°ë³¸ì ì¸ ê°œë… ì•Œê¸° ì¢‹ë‹¤",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-13-ì¡°ì–¸í•˜ëŠ” í‘œí˜„",
    "title": "ì¡°ì–¸í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-13",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Try to drink more water Drink more water (X, ëª…ë ¹í•˜ëŠ” ëŠë‚Œì´ ë‚¨) ë°œìŒ íŒ! t -> d Try to get more sleep Try to cut down on smoking cut down on : ~ì„ ì¤„ì´ë‹¤ Try not to skip breakfast. Make sure to + Make sure to take probiotics every day. Make sure to take Vitamin C. Make sure not to eat too much junk food. wipe down : ë§ë”íˆ ë‹¦ë‹¤ come to work stock up : ëŒ€ëŸ‰ìœ¼ë¡œ ì‚¬ë†“ë‹¤. ê°„ë‹¨í•œ ì‘ë¬¸ I read a book , atomic habits that tell me how to make habit. The book says that try to split the things to very small things. Because it make me to do more. So the lesson in the book is that make sure to make things, clear, attractive, easy. í”¼ë“œë°±. AI ì‘ë¬¸ í”¼ë“œë°±: \"I read a book , atomic habits\" â†’ ì‰¼í‘œ ì• ê³µë°± ì œê±°, ì±… ì œëª©ì€ ë”°ì˜´í‘œ ì‚¬ìš© \"that tell me\" â†’ ë‹¨ìˆ˜ ì£¼ì–´ì´ë¯€ë¡œ \"tells me\" \"how to make habit\" â†’ \"how to build habits\" (ë³µìˆ˜í˜•, ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"says that try to\" â†’ \"says to try to\" ë˜ëŠ” \"says that I should try to\" \"split the things to very small things\" â†’ \"break things down into smaller pieces\" \"Because it make me to do more\" â†’ \"makes\" (ë‹¨ìˆ˜), \"make me do\" (to ë¶ˆí•„ìš”) \"make sure to make things, clear, attractive, easy\" â†’ ë¬¸ì¥ êµ¬ì¡° ê°œì„  í•„ìš” ìˆ˜ì •ëœ ì‘ë¬¸: I read a book called \"Atomic Habits\" that tells me how to build habits. The book says to try to break things down into smaller pieces. This makes me do more because it reduces the psychological burden. So the lesson in the book is to make sure to make things clear, attractive, and easy. ì˜ì–´ ì‘ë¬¸ ì—°ìŠµ (í•œê¸€ ë‚´ìš© ê¸°ë°˜) I think habits are the most important thing in life. According to research by Professor Wendy Wood at Duke University, about 45% of our behavior comes from habits. The book \"Atomic Habits\" suggests the 2-minute rule. Try to break down what you need to do into very small tasks that can be finished in 2 minutes. I applied this to my life for about 1-2 months and it was really effective. The biggest reason is that it reduces psychological burden. Usually, we procrastinate because of psychological pressure. When I break down tasks into small 2-minute pieces, this burden almost disappears. By achieving small things, I can gradually do more things with that sense of accomplishment. For breaking bad habits, you should make them difficult to do. Human willpower has limits, so environment is the most important thing. I had a bad habit of looking at my phone late at night and sleeping late. To solve this, I bought an alarm clock. Now I don't bring my phone to bed and turn it off as soon as I get home. The key lessons are: Try to break tasks into very small pieces to reduce psychological burden Make sure to create an environment where you can't do bad habits Remember that willpower is limited, so focus on changing your environment",
    "public": true
  },
  {
    "id": "Self_Development/Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™",
    "title": "ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™",
    "date": "2025-07-13",
    "excerpt": "ì§€ê¸ˆë³´ë‹¤ ì¡°ê¸ˆ ë” ì–´ë ¤ìš´ ì¼ì— ë„ì „í•˜ì",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì—ì„œëŠ” ê¾¸ì¤€í•¨ì„ ì–´ë–»ê²Œ ì´ì–´ê°ˆ ê²ƒì¸ì§€, ê·¸ë¦¬ê³  ì´ì— ë„ì›€ì„ ì£¼ëŠ” ë„êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ì—ì„œ 'ê³¨ë”” ë½ìŠ¤ì˜ ë²•ì¹™'ì— ëŒ€í•´ ì´ì•¼ê¸°í•œ ë¶€ë¶„ì„ ì •ë¦¬í•´ë´…ë‹ˆë‹¤. <figure> <img src=\"/post/ê³¨ë””ë½ìŠ¤ì˜ë²•ì¹™/ê³¨ë””ë½ìŠ¤ì˜ë²•ì¹™_ê·¸ë˜í”„_ì´ë¯¸ì§€.jpg\" alt=\"ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ ê·¸ë˜í”„\" /> <figcaption>ê·¸ë¦¼1.ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™: ë„ˆë¬´ ì‰¬ìš°ë©´ ì§€ë£¨í•˜ê³ , ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í¬ê¸°í•˜ê²Œ ëœë‹¤. ì ë‹¹íˆ ë„ì „ì ì¸ ìˆ˜ì¤€ì´ ê°€ì¥ íš¨ê³¼ì ì´ë‹¤.</figcaption> </figure> ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™(Goldilocks Rule)ì˜ ë²•ì¹™ì´ë€ ì¼ì˜ ë‚œì´ë„ê°€ ë„ˆë¬´ ì‰¬ìš°ë©´ ì§€ë£¨í•˜ê³ , ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í¬ê¸°í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì—, ì¡°ê¸ˆ ì–´ë ¤ìš´ ì •ë„ì˜ ë‚œì´ë„ê°€ ì¼ì— ëª°ì…í•˜ëŠ”ë° íš¨ê³¼ì ì´ë¼ëŠ” ì´ì•¼ê¸°ì…ë‹ˆë‹¤. > ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì´ë€ ì¸ê°„ì€ ìì‹ ì´ í•  ìˆ˜ ìˆëŠ” ì í•©í•œ ì¼ì„ í•  ë•Œ ë™ê¸°ê°€ ê·¹ëŒ€í™”ë˜ëŠ” ê²½í—˜ì„ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. > ì§€ë‚˜ì¹˜ê²Œ ì–´ë ¤ì›Œì„œë„ ì•ˆë˜ë©° ì§€ë‚˜ì¹˜ê²Œ ì‰¬ì›Œì„œë„ ì•ˆëœë‹¤. ë”± ë“¤ì–´ë§ì•„ì•¼ í•œë‹¤. > > â€” ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ ë²•ì¹™ì„ ì´í•´í•˜ê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì–´ë–¤ ì¼ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ”ê±°ë¼ë©´, ê·¸ ìì²´ë¡œ ì‹¬ë¦¬ì  ë¶€ë‹´ê°ì´ í¬ê³  ë‚œì´ë„ê°€ ì–´ë µë‹¤ê³  ëŠê»´ì§€ê¸° ë•Œë¬¸ì— ì •ë§ ì‰¬ìš´ ì¼ë¡œ ì‘ê²Œ ìª¼ê°œëŠ”ê²Œ íš¨ê³¼ì ì¼ ê²ƒì…ë‹ˆë‹¤. ê·¸ í›„ì— ì–´ëŠì •ë„ ë°˜ë³µí•˜ë©´ì„œ ìŠµê´€ì´ ë˜ì—ˆë‹¤ë©´ ì¡°ê¸ˆì”© ë‚œì´ë„ë¥¼ ì˜¬ë ¤ê°€ì•¼ ì§€ë£¨í•¨ì„ ëŠë¼ì§€ ì•Šê³  ê¾¸ì¤€íˆ í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì´ ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì€ ë‡Œì˜ ê¸´ì¥ë„ë¥¼ ì–´ë–»ê²Œ ìœ ì§€í•  ê²ƒì¸ì§€ì™€ë„ ì—°ê²°ë©ë‹ˆë‹¤. ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™ê³¼ì˜ ì—°ê²° ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì€ ì‹¬ë¦¬í•™ì˜ \"ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™(Yerkes-Dodson Law)\"ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/ê³¨ë””ë½ìŠ¤ì˜ë²•ì¹™/ì—¬í‚¤ìŠ¤ë„ìŠ¨ë²•ì¹™_ê·¸ë˜í”„.png\" alt=\"ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™ ê·¸ë˜í”„\" /> <figcaption>ê·¸ë¦¼2. ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™: ê¸´ì¥ë„(ê°ì„± ìˆ˜ì¤€)ì™€ ì„±ê³¼ì˜ ê´€ê³„. ì ì ˆí•œ ê¸´ì¥ë„ì—ì„œ ìµœê³ ì˜ ì„±ê³¼ë¥¼ ë³´ì´ì§€ë§Œ, ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ì€ ê¸´ì¥ë„ì—ì„œëŠ” ì„±ê³¼ê°€ ë–¨ì–´ì§„ë‹¤.</figcaption> </figure> > ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™ì´ë€ ê¸´ì¥ë„(ê°ì„± ìˆ˜ì¤€)ì™€ ì„±ê³¼ ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ë²•ì¹™ìœ¼ë¡œ, ì ì ˆí•œ ê¸´ì¥ë„ì—ì„œ ìµœê³ ì˜ ì„±ê³¼ë¥¼ ë³´ì´ì§€ë§Œ, ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ì€ ê¸´ì¥ë„ì—ì„œëŠ” ì„±ê³¼ê°€ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ë„ˆë¬´ ì‰¬ìš´ ì¼: ê¸´ì¥ë„ê°€ ë‚®ì•„ì„œ ì§€ë£¨í•¨ì„ ëŠë¼ê³  ì„±ê³¼ê°€ ë–¨ì–´ì§ ë„ˆë¬´ ì–´ë ¤ìš´ ì¼: ê¸´ì¥ë„ê°€ ë„ˆë¬´ ë†’ì•„ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ì™€ ë¶ˆì•ˆìœ¼ë¡œ ì„±ê³¼ê°€ ë–¨ì–´ì§ ì ë‹¹íˆ ë„ì „ì ì¸ ì¼: ì ì ˆí•œ ê¸´ì¥ë„ë¥¼ ìœ ì§€í•˜ì—¬ ìµœê³ ì˜ ì„±ê³¼ì™€ ëª°ì…ì„ ê²½í—˜ ì‚¬ëŒë§ˆë‹¤ ìµœì ì˜ ê¸´ì¥ë„ ìˆ˜ì¤€ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, ìì‹ ì—ê²Œ ë§ëŠ” ë‚œì´ë„ë¥¼ ì°¾ì•„ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì‰¬ìš´ ê²ƒë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ë‚œì´ë„ë¥¼ ë†’ì—¬ê°€ë©° ìì‹ ë§Œì˜ \"ê³¨ë””ë½ìŠ¤ ì¡´\"ì„ ì°¾ì•„ê°€ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤. ì •ë¦¬ ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì„ ì‹¤ìƒí™œì— ì ìš©í•˜ëŠ” ë°©ë²•: ë‚œì´ë„ ì¡°ì ˆ ì „ëµ ì§€ë£¨í•¨ì„ ëŠë‚„ ë•Œ: ë‚œì´ë„ë¥¼ 10-15% ë†’ì—¬ì„œ ë„ì „ê°ì„ ë§Œë“¤ì–´ë¼ ì••ë„ë‹¹í•  ë•Œ: ë‚œì´ë„ë¥¼ ë‚®ì¶”ê³  ë” ì‘ì€ ë‹¨ìœ„ë¡œ ìª¼ê°œë¼ ì ì • ìˆ˜ì¤€: ì•½ê°„ì˜ ê¸´ì¥ê°ì´ ìˆì§€ë§Œ í¬ê¸°í•˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ ì‹¤ì²œ ë°©ë²• ìŠµê´€ í˜•ì„± ì´ˆê¸°: ì •ë§ ì‰¬ìš´ ê²ƒë¶€í„° ì‹œì‘ (ì˜ˆ: ë§¤ì¼ 1ë¶„ ëª…ìƒ) ìŠµê´€ ì•ˆì •í™” í›„: ì ì§„ì ìœ¼ë¡œ ë‚œì´ë„ ìƒìŠ¹ (ì˜ˆ: 5ë¶„ â†’ 10ë¶„ â†’ 15ë¶„) ì§€ì†ì  ëª¨ë‹ˆí„°ë§: ìì‹ ì˜ ëª°ì…ë„ì™€ ë™ê¸°ë¥¼ ì²´í¬í•˜ë©° ì¡°ì ˆ í•µì‹¬ ì›ì¹™ > \"ë„ˆë¬´ ì‰¬ìš°ë©´ ì§€ë£¨í•˜ê³ , ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í¬ê¸°í•œë‹¤. ì ë‹¹íˆ ë„ì „ì ì¸ ìˆ˜ì¤€ì´ ê°€ì¥ íš¨ê³¼ì ì´ë‹¤.\"",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/250713",
    "title": "ë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°±",
    "date": "2025-07-12",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [],
    "content": "í¬ë¡œë§¤í‹± 80bpm 16bit. ì²˜ìŒì—” ì˜ ì•ˆëœë‹¤ê°€ ë‚˜ì¤‘ì— ì†í’€ë¦¬ë‹ˆê¹Œ ì˜ ë¨ ìŠ¤ì¼€ì¼ 0~4. ì´ì œ ë‹¤ ì™¸ìš¸ ë•Œê°€ ëëŠ”ë°.. ã…ã… ê³¡ ì—°ìŠµ ë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°±. ë§‰ ë¹¡ì„¸ê²ŒëŠ” ì•ˆí•˜ê³  ê·¸ëƒ¥ í•œë²ˆì”© ì³ë³´ëŠ” ì •ë„?",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/250712",
    "title": "ì–´ë–»ê²Œ",
    "date": "2025-07-12",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [],
    "content": "ì—°ìŠµ ì²´í¬ 7ì›” 12. í†  [ ] í¬ë¡œë©”í‹±",
    "public": true
  },
  {
    "id": "Self_Development/Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ìŠµê´€ì¶”ì ",
    "title": "ìŠµê´€ì¶”ì ì— ëŒ€í•˜ì—¬",
    "date": "2025-07-12",
    "excerpt": "ìŠµê´€ì¶”ì ì€ ì–´ë–¤ ìŠµê´€ì„ ë§Œë“¤ê¸° ìœ„í•œ í›Œë¥­í•œ ë„êµ¬ë‹¤",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì—ì„œ ì´ì•¼ê¸°í•˜ëŠ” ì»¨ì…‰ì€ ì•„ì£¼ ëª…í™•í•©ë‹ˆë‹¤. ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” 'ëª…í™•í•´ì•¼ í•˜ë©°', 'í•˜ê¸° ì‰¬ì›Œì•¼ í•˜ê³ ', 'ë§Œì¡±ê°ì„ ì¤˜ì•¼ í•œë‹¤'. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ì¢‹ì€ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ìŠµê´€ ì¶”ì ì„ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìŠµê´€ì„ ì¶”ì í•œë‹¤ëŠ” ê±´ ë‚´ê°€ ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê³  ìˆëŠ”ì§€ì™€ ì–´ë–¤ ì¼ì„ í•´ì•¼í•˜ëŠ”ì§€ë¥¼ ì ê³  í™•ì¸í•´ë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‚´ê°€ ì–´ë–¤ ì¼ì„ í•´ì•¼í•˜ëŠ”ì§€ ì ì–´ì„œ í•´ì•¼í•  ì¼ì„ ëª…í™•íˆ ë§Œë“¤ê³ , í–ˆì„ ë•ŒëŠ” ì´ë¥¼ ê¸°ë¡í•˜ë©´ì„œ ì„±ì·¨ê°ì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” ì„±ì·¨ê°ì„ ë°›ì•„ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ ìŠµê´€ì¶”ì ì€ ì¢‹ì€ ë„êµ¬ë‹¤. íŠ¹íˆ ì €ì—ê²ŒëŠ” ë§¤ì¼ í•´ë‚˜ê°€ë©´ì„œ ê¾¸ì¤€íˆ ìŒ“ì´ëŠ” ëŠë‚Œì´ ê°•ë ¥í•œ ë™ê¸°ë¶€ì—¬ë¡œ ì‘ìš©í•©ë‹ˆë‹¤. ê°œë°œì„ í•˜ëŠ” ì…ì¥ì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ê²Œ ê¹ƒí—™ì˜ ì”ë””ë°­ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. <figure> <img src=\"/post/ìŠµê´€ì¶”ì /ê¹ƒí—™ì”ë””ë°­.png\" alt=\"ê¹ƒí—™ ì”ë””ë°­\"> <figcaption>ê¹ƒí—™ ì”ë””ë°­</figcaption> </figure> ë‚´ê°€ ê¾¸ì¤€íˆ ì»¤ë°‹(ì½”ë“œ ì‘ì—…)ì„ í•˜ê³  ìˆëŠ”ê²Œ í•œëˆˆì— ë³´ì´ê³ , ì´ê²Œ ì´ ì¼ì„ ë” ê¾¸ì¤€íˆ í•˜ëŠ”ë° ë™ê¸°ë¶€ì—¬ë¥¼ í•´ì¤ë‹ˆë‹¤. > ê¾¸ì¤€íˆ ìŒ“ì—¬ê°€ëŠ” ëŠë‚Œì€ êµ‰ì¥í•œ ë™ê¸°ë¶€ì—¬ë¥¼ ì œê³µí•œë‹¤. ë˜í•œ ì´ë ‡ê²Œ ìŒ“ì—¬ê°€ëŠ” ëŠë‚Œì„ ë°›ê¸° ìœ„í•´ ë¸”ë¡œê·¸ë„ ì‘ì„±í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ê³µë¶€í•œ ê²ƒë“¤ì„ ê¸€ë¡œ ì‘ì„±í•˜ëŠ” ì—°ìŠµì„ í•˜ê¸°ë„ í•˜ê³ , ë˜ ì´ë ‡ê²Œ ê³µë¶€í•œ ê²ƒë“¤ì´ í•˜ë‚˜ì”© ìŒ“ì—¬ê°€ëŠ”ê²Œ ëˆˆì— ë³´ì´ê¸° ë•Œë¬¸ì— ì„±ì·¨ê°ì„ ì¤ë‹ˆë‹¤. ê·¸ë˜ì„œ ê·¸ëƒ¥ ê³µë¶€í•˜ëŠ” ê²ƒë³´ë‹¤ ì €ëŠ” ë¸”ë¡œê·¸ì— ê¸€ë¡œ ë‚¨ê¸°ë©´ì„œ í•˜ë‚˜ì”© ìŒ“ì—¬ê°€ëŠ” ê±¸ ëˆˆìœ¼ë¡œ ë³¼ ë•Œ ë” í° ì„±ì·¨ê°ê³¼ ë™ê¸°ë¶€ì—¬ë¥¼ ë°›ìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-11-ê¸ì •ì ì¸í”¼ë“œë°±ì£¼ê¸°",
    "title": "ê¸ì •ì ì¸ í”¼ë“œë°± ì£¼ëŠ” í‘œí˜„ë“¤ ë³µìŠµ",
    "date": "2025-07-11",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë¦¬ë·° ë°ì´ I like the way I appreciate it I know that its hard I want you to know that you call me any time. This too shall pass. I was really impressed with. It couldnt be any better I couldnt have done it without you. You did a great job on work Im happy to see that I can tell that I believe in you Youve made it this far. You`re almost there",
    "public": true
  },
  {
    "id": "Self_Development/Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ë§Œì¡±ìŠ¤ëŸ½ê²Œë§Œë“¤ì–´ë¼",
    "title": "ë§Œì¡±ìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ë¼",
    "date": "2025-07-11",
    "excerpt": "ì§€ì†í•˜ê¸° ìœ„í•´ì„œëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì´, ê·¸ë§Œë‘ê¸° ìœ„í•´ì„œëŠ” ì¦‰ê°ì ì¸ í˜ë„í‹°ê°€ í•„ìš”í•˜ë‹¤",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "ì–´ë–¤ ì¼ì„ ì§€ì†í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•œ ê±´ ë­˜ê¹Œ? ë§ì€ ì‚¬ëŒë“¤ì´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” 'ê¾¸ì¤€í•¨'ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ë§í•©ë‹ˆë‹¤. ì´ ê¾¸ì¤€í•¨ì€ ì–´ë–»ê²Œ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œìš”? ê°•ë ¥í•œ ì˜ì§€ì™€ ë‹¤ì§ì´ í•„ìš”í•œ ê±¸ê¹Œìš”? 'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì—ì„œëŠ” ì›í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ë²•ë“¤ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ëŠ” í•´ì•¼í•˜ëŠ” ì¼ì„ ì•„ì£¼ ì‘ê²Œ ìª¼ê°œë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‰¬ì›Œì•¼ ë‹¬ë¼ì§„ë‹¤/ì‰¬ì›Œì•¼ë‹¬ë¼ì§„ë‹¤) ì´ì— ì´ì–´ì„œ ì €ìëŠ” ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” ë§Œì¡±ìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ë¼ ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ë‡Œë¥¼ ì´í•´í•´ë³´ì ìš°ë¦¬ì˜ ë‡ŒëŠ” ê³¼ê±°ì— ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤. ë‡Œê³¼í•™ì—ì„œ ë°í˜€ì§„ ë°”ì— ë”°ë¥´ë©´, ì¸ê°„ì˜ ë‡ŒëŠ” ìˆ˜ì‹­ë§Œ ë…„ ë™ì•ˆ ìˆ˜ë µì±„ì§‘ ì‹œëŒ€ì˜ í™˜ê²½ì— ìµœì í™”ë˜ì–´ ì§„í™”í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ í˜„ëŒ€ ë¬¸ëª…ì— ì‚´ê¸° ì‹œì‘í•œ ê²ƒì€ ë¶ˆê³¼ ëª‡ ì²œ ë…„ì— ë¶ˆê³¼í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§„í™”ì  ë°°ê²½ ë•Œë¬¸ì— ìš°ë¦¬ì˜ ë‡ŒëŠ” ì¥ê¸°ì ì¸ ëª©í‘œë³´ë‹¤ëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì— í›¨ì”¬ ë” ê°•í•˜ê²Œ ë°˜ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì›ì‹œ ì‹œëŒ€ì—ëŠ” ì§€ê¸ˆ ë‹¹ì¥ ì–»ì„ ìˆ˜ ìˆëŠ” ìŒì‹, ì•ˆì „, ë²ˆì‹ ê¸°íšŒê°€ ìƒì¡´ì— ì§ê²°ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ê°€ ì¥ê¸°ì ì¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë ¤ë©´ ì´ëŸ¬í•œ ë‡Œì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³  í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤. > ìš°ë¦¬ì˜ ë‡ŒëŠ” ë‹¹ì¥ ì¦ê±°ìš´ë©´ í•˜ê³  ë‹¹ì¥ ê³ í†µìŠ¤ëŸ¬ìš°ë©´ ì•ˆí•œë‹¤. ìš°ë¦¬ì˜ ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆê¸° ë•Œë¬¸ì—, ì´ë¥¼ ì´í•´í•˜ê³  í™œìš©í•´ì„œ ê¾¸ì¤€íˆ í•  ìˆ˜ ìˆëŠ” í™˜ê²½ í˜¹ì€ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. > ìš°ë¦¬ëŠ” ë³¸ëŠ¥ì˜ ì”¨ì•—ë“¤ì— ë°˜ê¸°ë¥¼ ë“¤ì§€ ëª»í•˜ë©° ê·¸ê²ƒë“¤ê³¼ í•¨ê»˜ ë‚˜ì•„ê°€ì•¼ í•œë‹¤. ê°€ì¥ ìµœì„ ì˜ ë°©ë²•ì€ ì¥ê¸°ì ìœ¼ë¡œ ë³´ìƒì„ ì£¼ëŠ” ìŠµê´€ì—ëŠ” ì¦‰ì‹œì ì¸ ê¸°ì¨ì˜ ì¡°ê°ë“¤ì„ ë§ë¶™ì´ê³ , ê·¸ë ‡ì§€ ì•Šì€ ìŠµê´€ì—ëŠ” ê³ í†µì˜ ì¡°ê°ë“¤ì„ ë§ë¶™ì´ëŠ” ê²ƒì´ë‹¤. > > - ã€Šì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ã€‹ ì›í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ë ¤ë©´ ì¦ê²ê²Œ ë§Œë“¤ì > ìŠµê´€ì„ ê³„ì† ìœ ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œì„ í•„ìˆ˜ì ìœ¼ë¡œ ë°›ì•„ì•¼ í•œë‹¤. ë¹„ë¡ ì•„ì£¼ ì‚¬ì†Œí•œ ë°©ì‹ì¼ì§€ë¼ë„ ë§ì´ë‹¤. ì„±ê³µí–ˆë‹¤ê³  ëŠë¼ëŠ” ê²ƒì€ ìŠµê´€ì´ ì„±ê³¼ë¥¼ ëƒˆê³ , ê·¸ ì¼ì´ ë…¸ë ¥í•  ë§Œí•œ ê°€ì¹˜ê°€ ìˆë‹¤ëŠ” ì‹ í˜¸ì´ê¸° ë•Œë¬¸ì´ë‹¤. > > - ã€Šì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ã€‹ ìš°ë¦¬ëŠ” ì¦‰ê°ì ìœ¼ë¡œ ì¦ê±°ì›€ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ì¼ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì›í•˜ëŠ” ëª©í‘œë“¤ì€ ì§€ì—°ì ì¸ ë³´ìƒì„ ì œê³µí•˜ëŠ” ì¼ë“¤ì´ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì›í•˜ëŠ” ëª©í‘œë¡œ ê°€ëŠ” ê¸¸ì„ ì•„ì£¼ ì˜ê²Œ ìª¼ê°œê³ , ê° ë‹¨ê³„ì—ì„œ ì¦‰ê°ì ì¸ ë³´ìƒì„ ë°›ë„ë¡ ì„¤ê³„í•´ì•¼í•©ë‹ˆë‹¤. ê°€ì¥ ì¢‹ì€ ê±´ \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì„ ë°›ë„ë¡ ì„¤ê³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. > ì‹¤ì œë¡œ ì €ëŠ” ì´ ì „ëµìœ¼ë¡œ ìš´ë™í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ì—ˆê³ , 6ë…„ì •ë„ ê¾¸ì¤€íˆ ì´ì–´ì„œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë§ì€ ì‚¬ëŒë“¤ì´ ìš´ë™ì„ ì‹œì‘í•  ë•Œ \"ì§„ì§œ ìš´ë™ ì—´ì‹¬íˆ í• ê±°ì•¼, ì§„ì§œ ë¹¡ì„¸ê²Œ í•´ì•¼ì§€\" ë“±ì˜ ë‹¤ì§ì„ í•˜ê³  ì‹œì‘í•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë§‰ìƒ ìš´ë™ì„ í•˜ê²Œ ë˜ë©´ í˜ë“¤ê³  í”¼ê³¤í•˜ê¸° ë•Œë¬¸ì— ë¶€ì •ì ì¸ ê°ì •ì„ ê²½í—˜í•˜ê²Œ ë˜ê³  í–‰ë™ì„ ì´ì–´ê°€ì§€ ëª»í•©ë‹ˆë‹¤. ì œê°€ ìš´ë™ì„ ì‹œì‘í•  ë•Œì˜ ëª©í‘œëŠ” \"í—¬ìŠ¤ì¥ì— ë“¤ì–´ê°€ì„œ í‘¸ì‰¬ì—… 1ê°œë§Œ í•˜ê³  ì¹­ì°¬í•˜ê¸°\" ì˜€ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì•„ë¬´ ìƒê°ì—†ì´ í—¬ìŠ¤ì¥ì— ê°€ì„œ ì§„ì§œ í‘¸ì‰¬ì—… 1ê°œë§Œ í•˜ê³  \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì„ ë°›ìœ¼ë©° ìŠ¤ìŠ¤ë¡œë¥¼ ì¹­ì°¬í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë‹¤ë³´ë‹ˆ í—¬ìŠ¤ì¥ì— ê°€ëŠ” ê±´ ì €ì—ê²Œ ì„±ì·¨ê°ì„ ì£¼ëŠ” ì¬ë°ŒëŠ” ì¼ì´ ë˜ì—ˆê³ , ì ì  ìš•ì‹¬ì„ ëŠê»´ ê°•ë„ë¥¼ ëŠ˜ë ¤ë‚˜ê°”ìŠµë‹ˆëŒœ. í‘¸ì‰¬ì—…ì„ ê¸°ì¤€ìœ¼ë¡œ ë³´ë©´ ì²˜ìŒì—ëŠ” í•œë²ˆì— 10ê°œì •ë„ ë°–ì— ëª»í–ˆì§€ë§Œ ì§€ê¸ˆì€ 100ê°œì •ë„ëŠ” í•  ìˆ˜ ìˆëŠ”, 10ë°° ì´ìƒì˜ ê·¼ë ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬í•˜ë©´ ì›í•˜ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì„ ë°›ë„ë¡ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ° í™˜ê²½, ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ì•¼í•©ë‹ˆë‹¤. ë§¤ì¼í•  ìˆ˜ ìˆëŠ” ì‘ì€ ëª©í‘œë“¤ë¡œ ìª¼ê°œê³ , ì´ë¥¼ í–ˆì„ ë•Œ \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì„ ë°›ë„ë¡ ì„¤ê³„ í•´ì•¼í•˜ê³ , ì´ë•Œ ì´ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚˜ìœ ìŠµê´€ì„ ë§Œë“¤ë ¤ë©´ í˜ë„í‹°ë¥¼ ì£¼ì ë°˜ëŒ€ë¡œ ë‚˜ìœ ìŠµê´€ì„ ì—†ì• ë ¤ë©´ ë³´ìƒì„ ì§€ì—°ì‹œí‚¤ê±°ë‚˜ íŒ¨ë„í‹°ë¥¼ ì£¼ë©´ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì œ ìƒê°ì— ì´ ë¶€ë¶„ì€ ì ìš©í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. íœ´ëŒ€í°ì´ë‚˜ ë…¸íŠ¸ë¶ìœ¼ë¡œ ë„ˆë¬´ë‚˜ë„ ì‰½ê²Œ ì¦‰ê°ì ì¸ ë³´ìƒì„ ì–»ì„ ìˆ˜ ìˆëŠ” ì„¸ìƒì´ê³ , ì´ë¥¼ ì§€ì—°ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì•„ì§ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì œê°€ ë‚˜ë¦„ëŒ€ë¡œ ì ìš©í•˜ê³  ìˆëŠ” ë¶€ë¶„ì€ \"íšŒì˜ì ì¸ ìƒê°\"ì„ ì ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \"ìœ íŠœë¸Œ ë´ì„œ ë­í•´? ì¬ë¯¸ë„ ì—†ëŠ”ê±°\" ì´ëŸ°ì‹ìœ¼ë¡œ í”¼í•´ì•¼í•˜ëŠ” í–‰ë™ë“¤ì— ëŒ€í•´ì„œëŠ” í•˜ê³  ì‹¶ì§€ë§Œ ì°¸ëŠ”ê²Œ ì•„ë‹Œ í•˜ëŠ”ê²Œ ì˜ë¯¸ê°€ ì—†ë‹¤ê³  ìƒê°í•˜ë ¤ê³  í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ ì´ëŸ° ìƒê°ì„ ê°€ì§€ê³  ë‚˜ë‹ˆ, í•´ë´¤ì ì˜ë¯¸ì—†ëŠ” í–‰ë™ë“¤ì€ í•˜ì§€ ì•Šê²Œ ë˜ì—ˆê³ , ì‹¤ì§ˆì ìœ¼ë¡œ ë‚˜ì—ê²Œ ì„±ì·¨ê°ì„ ì£¼ëŠ” í–‰ë™ë“¤ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬ ìš°ë¦¬ì˜ ë‡ŒëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì— ì§‘ì¤‘í•œë‹¤. ë¯¸ë˜ì˜ ë³´ìƒì—ëŠ” ì§‘ì¤‘í•˜ì§€ ëª»í•œë‹¤. ì´ë¥¼ í™œìš©í•´ ì „ëµì„ ì„¸ìš°ê³  í™˜ê²½ì„ ë§Œë“¤ì. ì˜ì§€ë ¥ì˜ ë¬¸ì œê°€ ì•„ë‹ˆë‹¤! ì‘ì€ ëª©í‘œë“¤ë¡œ ìª¼ê°œê³  ì´ë¥¼ í•´ë‚˜ê°€ë©° \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì— ì§‘ì¤‘í•˜ì í”¼í•´ì•¼ í•˜ëŠ” ì¼ë“¤ì—ëŠ” íšŒì˜ì ì¸ ìƒê°ì„ ì ìš©í•´ ë¶ˆí¸í•œ ëŠë‚Œì„ ê°€ì§€ë„ë¡ í•˜ì",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/ill-conditioned-matrix-theory",
    "title": "Ill-conditioned í–‰ë ¬ê³¼ ê³ ìœ ê°’: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½",
    "date": "2025-07-11",
    "excerpt": "ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œì™€ ill-conditioned í–‰ë ¬, ê³ ìœ ê°’ì˜ ê´€ê³„ë¥¼ ìì„¸íˆ ì„¤ëª…",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Linear Algebra",
      "Matrix Analysis"
    ],
    "content": "Ill-conditioned í–‰ë ¬ê³¼ ê³ ìœ ê°’: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½ ê°œìš” ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ill-conditioned í–‰ë ¬ ë¬¸ì œë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ ê°œë…ë“¤ì„ ë‹¨ê³„ë³„ë¡œ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ê³ ìœ ê°’(Eigenvalue)ì´ë€? 1.1 ê¸°ë³¸ ê°œë… ê³ ìœ ê°’ì€ í–‰ë ¬ì˜ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤ì¹¼ë¼ ê°’ì…ë‹ˆë‹¤. ìˆ˜í•™ì  ì •ì˜: í–‰ë ¬ Aì— ëŒ€í•´, 0ì´ ì•„ë‹Œ ë²¡í„° vì™€ ìŠ¤ì¹¼ë¼ Î»ê°€ ë‹¤ìŒì„ ë§Œì¡±í•  ë•Œ: $$Av = \\lambda v$$ Î»ë¥¼ Aì˜ ê³ ìœ ê°’, vë¥¼ ê³ ìœ ë²¡í„°ë¼ê³  í•©ë‹ˆë‹¤. 1.2 ì§ê´€ì  ì´í•´ ê³ ìœ ê°’ì€ í–‰ë ¬ì´ ë²¡í„°ë¥¼ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì´ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜ˆì‹œ: $$ A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix} $$ ê³ ìœ ê°’: $\\lambda_1 = 2$, $\\lambda_2 = 3$ ê³ ìœ ë²¡í„°: $v_1 = [1, 0]$, $v_2 = [0, 1]$ Î»â‚ = 2: xì¶• ë°©í–¥ìœ¼ë¡œ 2ë°° ëŠ˜ë¦¼ Î»â‚‚ = 3: yì¶• ë°©í–¥ìœ¼ë¡œ 3ë°° ëŠ˜ë¦¼ 1.3 ê³ ìœ ê°’ì˜ ì˜ë¯¸ Î» > 1: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ í™•ëŒ€ 0 < Î» < 1: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ì¶•ì†Œ Î» = 0: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ì™„ì „íˆ ì••ì¶• (ì •ë³´ ì†ì‹¤) Î» < 0: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ë°˜ì „ Ill-conditioned í–‰ë ¬ì´ë€? 2.1 ì¡°ê±´ìˆ˜(Condition Number) ì¡°ê±´ìˆ˜ëŠ” í–‰ë ¬ì´ ì–¼ë§ˆë‚˜ \"ë¶ˆì•ˆì •í•œì§€\"ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì •ì˜: $$ \\kappa(A) = \\ A\\ \\times \\ A^{-1}\\ $$ ì—¬ê¸°ì„œ $\\ A\\ $ëŠ” í–‰ë ¬ Aì˜ ë…¸ë¦„(norm)ì…ë‹ˆë‹¤. 2.2 Ill-conditionedì˜ ì˜ë¯¸ Well-conditioned: ì¡°ê±´ìˆ˜ê°€ ì‘ìŒ (â‰ˆ 1) Ill-conditioned: ì¡°ê±´ìˆ˜ê°€ í¼ (â‰« 1) 2.3 ì™œ ë¬¸ì œê°€ ë˜ëŠ”ê°€? Ill-conditioned í–‰ë ¬ì—ì„œëŠ” ì‘ì€ ì…ë ¥ ë³€í™”ê°€ í° ì¶œë ¥ ë³€í™”ë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤. ì˜ˆì‹œ: $$ A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{bmatrix} $$ $$ A^{-1} \\approx \\begin{bmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{bmatrix} $$ ì¡°ê±´ìˆ˜ $\\approx 4000$ (ë§¤ìš° í¼!) ì…ë ¥ì— ì‘ì€ ë…¸ì´ì¦ˆê°€ ìˆìœ¼ë©´ í•´ê°€ í¬ê²Œ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ Ill-conditionedê°€ ë°œìƒí•˜ëŠ” ì´ìœ  3.1 Non-identification ë¬¸ì œ ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” ë¬´í•œíˆ ë§ì€ ì²˜ì¹˜ ìˆ˜ì¤€ì´ ì¡´ì¬í•©ë‹ˆë‹¤: ì´ì§„ ì²˜ì¹˜: $T \\in \\{0, 1\\}$ (2ê°œ ê°’) ì—°ì†í˜• ì²˜ì¹˜: $T \\in [0, 1]$ (ë¬´í•œ ê°œ ê°’) 3.2 Generalized R-lossì˜ íŠ¹ì„± ì´ì§„ ì²˜ì¹˜ R-loss: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X)(T - \\pi(X)))^2]$$ ì—°ì†í˜• ì²˜ì¹˜ R-loss: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X,T)(T - \\pi(X)))^2]$$ 3.3 í•µì‹¬ ì°¨ì´ì  ì´ì§„ ì²˜ì¹˜: Ï„(X)ëŠ” Xì—ë§Œ ì˜ì¡´ (ìœ í•œì°¨ì›) ì—°ì†í˜• ì²˜ì¹˜: Ï„(X,T)ëŠ” Xì™€ T ëª¨ë‘ì— ì˜ì¡´ (ë¬´í•œì°¨ì›) 3.4 í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´ ì—°ì†í˜• ì²˜ì¹˜ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´: $$ \\begin{bmatrix} \\tau(x_1,t_1) & \\tau(x_1,t_2) & \\cdots & \\tau(x_1,t_n) \\\\ \\tau(x_2,t_1) & \\tau(x_2,t_2) & \\cdots & \\tau(x_2,t_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\tau(x_m,t_1) & \\tau(x_m,t_2) & \\cdots & \\tau(x_m,t_n) \\end{bmatrix} $$ ì´ í–‰ë ¬ì€ ë§¤ìš° í° í¬ê¸°ë¥¼ ê°€ì§€ë©°, ë§ì€ í–‰ë“¤ì´ ê±°ì˜ ì„ í˜• ì¢…ì†ì…ë‹ˆë‹¤. ê³ ìœ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ì´ìœ  4.1 ì„ í˜• ì¢…ì†ì„± ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ Ï„(x,t) í•¨ìˆ˜ë“¤ì€ ì„œë¡œ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤: $$\\tau(x, 0.1) \\approx \\tau(x, 0.11) \\approx \\tau(x, 0.12) \\cdots$$ ì´ëŠ” í–‰ë ¬ì˜ í–‰ë“¤ì´ ê±°ì˜ ì„ í˜• ì¢…ì†ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 4.2 ê³ ìœ ê°’ì˜ ì˜ë¯¸ ê³ ìœ ê°’ì´ 0ì— ê°€ê¹Œì›€ = í•´ë‹¹ ë°©í–¥ì˜ ì •ë³´ê°€ ê±°ì˜ ì—†ìŒ ì˜ˆì‹œ: í–‰ë ¬ Aì˜ ê³ ìœ ê°’: $\\lambda_1 = 10$, $\\lambda_2 = 0.001$, $\\lambda_3 = 0.0001$ $\\lambda_1 = 10$: ì´ ë°©í–¥ì€ ì •ë³´ê°€ í’ë¶€ $\\lambda_2 = 0.001$: ì´ ë°©í–¥ì€ ì •ë³´ê°€ ê±°ì˜ ì—†ìŒ $\\lambda_3 = 0.0001$: ì´ ë°©í–¥ì€ ê±°ì˜ ì •ë³´ ì—†ìŒ 4.3 ì—°ì†í˜• ì²˜ì¹˜ì—ì„œì˜ ìƒí™© ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ”: ëŒ€ë¶€ë¶„ì˜ ê³ ìœ ê°’ì´ 0ì— ê°€ê¹Œì›€ ëª‡ ê°œì˜ ê³ ìœ ê°’ë§Œ ì˜ë¯¸ ìˆìŒ í–‰ë ¬ì´ ê±°ì˜ íŠ¹ì´(singular)ì— ê°€ê¹Œì›€ ì™œ ë¬¸ì œê°€ ë˜ëŠ”ê°€? 5.1 ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± ì‘ì€ ë…¸ì´ì¦ˆ â†’ í° ì˜¤ì°¨ ì›ë˜ ë¬¸ì œ: $Ax = b$ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë¬¸ì œ: $(A + \\epsilon)\\tilde{x} = b + \\delta$ ê²°ê³¼: $\\ x - \\tilde{x}\\ $ê°€ ë§¤ìš° í´ ìˆ˜ ìˆìŒ 5.2 í•´ì˜ ìœ ì¼ì„± ë¶€ì¡± ë¬´ìˆ˜íˆ ë§ì€ í•´ê°€ ì¡´ì¬: $$\\tau(x,t) = \\tau_0(x,t) + \\epsilon(x,t)$$ ì—¬ê¸°ì„œ $\\epsilon(x,t)$ëŠ” R-lossë¥¼ ê±°ì˜ ë³€í™”ì‹œí‚¤ì§€ ì•ŠëŠ” í•¨ìˆ˜ 5.3 ì¶”ì •ì˜ ì–´ë ¤ì›€ ì–´ë–¤ í•´ê°€ \"ì •ë‹µ\"ì¸ì§€ ì•Œ ìˆ˜ ì—†ìŒ í•´ê²° ë°©ë²•: Tikhonov ì •ê·œí™” 6.1 ê¸°ë³¸ ì•„ì´ë””ì–´ ì •ê·œí™” í•­ ì¶”ê°€: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X,T)(T - \\pi(X)))^2] + \\lambda\\ \\tau\\ ^2$$ 6.2 ìˆ˜í•™ì  íš¨ê³¼ í–‰ë ¬ í‘œí˜„: $$(A^TA + \\lambda I)x = A^Tb$$ ê³ ìœ ê°’ ë³€í™”: ì›ë˜ ê³ ìœ ê°’: $\\lambda_i$ ì •ê·œí™” í›„: $\\lambda_i + \\lambda$ 6.3 ì™œ ë„ì›€ì´ ë˜ëŠ”ê°€? 0ì— ê°€ê¹Œìš´ ê³ ìœ ê°’ë“¤ì´ Î»ë§Œí¼ ì¦ê°€ í–‰ë ¬ì´ ë” ì•ˆì •ì (well-conditioned)ì´ ë¨ ìœ ì¼í•œ í•´ ë³´ì¥ ì‹¤ì œ ì˜ˆì‹œ 7.1 ê°„ë‹¨í•œ ì˜ˆì‹œ ì—°ì†í˜• ì²˜ì¹˜ í•¨ìˆ˜: $$\\tau(x,t) = 2t + 0.1\\sin(10t)$$ ë¬¸ì œ: sin(10t) í•­ì´ R-lossì— ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ í•´ê²°: ì •ê·œí™”ë¥¼ í†µí•´ \"ë¶€ë“œëŸ¬ìš´\" í•¨ìˆ˜ ì„ í˜¸ 7.2 ìˆ˜ì¹˜ì  ì˜ˆì‹œ ì›ë˜ í–‰ë ¬ Aì˜ ê³ ìœ ê°’: $[10, 0.1, 0.01, 0.001, \\ldots]$ ì •ê·œí™” í›„ ($\\lambda=1$): $[11, 1.1, 1.01, 1.001, \\ldots]$ ì¡°ê±´ìˆ˜ê°€ í¬ê²Œ ê°œì„ ë©ë‹ˆë‹¤. ê²°ë¡  ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œëŠ”: ë¬´í•œì°¨ì› í•¨ìˆ˜ ê³µê°„ì—ì„œ ë°œìƒ Ill-conditioned í–‰ë ¬ë¡œ ìˆ˜í•™ì  í‘œí˜„ ê³ ìœ ê°’ë“¤ì´ 0ì— ê°€ê¹Œì›€ìœ¼ë¡œ íŠ¹ì§• Tikhonov ì •ê·œí™”ë¡œ í•´ê²° ì´ëŸ¬í•œ ì´í•´ëŠ” ì—°ì†í˜• ì²˜ì¹˜ íš¨ê³¼ ì¶”ì •ì˜ ì´ë¡ ì  ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì°¸ê³  ìë£Œ ì¶”ì²œ ë„ì„œ: Golub, G. H., & Van Loan, C. F. (2013). Matrix computations. JHU press. Trefethen, L. N., & Bau, D. (1997). Numerical linear algebra. SIAM. í•µì‹¬ ë…¼ë¬¸: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. Hansen, P. C. (1998). Rank-deficient and discrete ill-posed problems: numerical aspects of linear inversion. ì˜¨ë¼ì¸ ìë£Œ: MIT OpenCourseWare: Linear Algebra Khan Academy: Eigenvalues and Eigenvectors",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Towards-R-learner-with-Continuous-Treatments",
    "title": "[Paper Review] Towards R-learner with Continuous Treatments",
    "date": "2025-07-11",
    "excerpt": "ì—°ì†í˜• ì²˜ì¹˜ë¥¼ ìœ„í•œ R-learnerë¥¼ ì–´ë–»ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ë…¼ì˜",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link ë…¼ë¬¸ì˜ ë°°ê²½ --- Quasi-Oracle Estimation of Heterogeneous Treatment Effects ì—ì„œ ê°œì¸í™” ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. <small> * ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ : ì–´ë–¤ ì²˜ì¹˜ë¥¼ í–ˆì„ ë•Œ ê°œì¸ë³„ë¡œ ì–´ë–¤ íš¨ê³¼ê°€ ìˆì„ì§€ ì¶”ì •í•œ ê²ƒ</small> ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ê±´ ì¸ê³¼ì¶”ë¡ ì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¬¸ì œì´ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì •ë°€ì˜í•™ì—ì„œëŠ” í™˜ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì²˜ì¹˜ ì„ íƒì„ ê²°ì •í•˜ê³ , êµìœ¡ì—ì„œëŠ” í•™ìƒë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ êµìœ¡ ë°©ë²•ì„ ê²°ì •í•˜ê³ , ì˜¨ë¼ì¸ ë§ˆì¼€íŒ…ì—ì„œëŠ” ì‚¬ìš©ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ë§ì¶¤í˜• ê´‘ê³ ë¥¼ ì œê³µí•˜ê³ , ì˜¤í”„ë¼ì¸ ì •ì±… í‰ê°€ì—ì„œëŠ” ì§€ì—­ë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì •ì±…ì„ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ì¦‰, ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì•Œê²Œ ë˜ë©´ ì–´ë–¤ ì„ íƒì— ëŒ€í•œ ê·¼ê±°ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì´ì§„ ì²˜ì¹˜ì˜ ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸°í–ˆê³ , ì´ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ í™•ì¥í•´ì„œ ì—°ì†í˜• ì²˜ì¹˜ì— ëŒ€í•´ì„œë„ ì´ë¥¼ ì ìš©í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ì„ ì´ ë…¼ë¬¸ì—ì„œ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. [[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects](/posts/Data%20Science/Causal%20Inference/Paper%20Review/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects) <- ì´ ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·°ëŠ” ì—¬ê¸°ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ê¸°ì¡´ì˜ ë°©ë²•ì„ í™•ì¥í•  ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ ì´ë¥¼ í•´ê²°í•œ ë°©ë²•ë¡ ì— ëŒ€í•œ ì´ì•¼ê¸°ì— ì§‘ì¤‘í•´ì„œ ì´ ë…¼ë¬¸ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ê°„ë‹¨íˆ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ ì´ì§„ ì²˜ì¹˜ì—ì„œëŠ” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ ìˆì—ˆëŠ”ë°, ì´ë¥¼ ì—°ì†í˜• ì²˜ì¹˜ë¡œ í™•ì¥í•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•œë‹¤. (ìœ ì¼í•´ë¥¼ ê°€ì§€ì§€ ì•ŠëŠ” ë¬¸ì œ) ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ 2ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. Tikhonov regularization zero-constraining operatorë¥¼ í†µí•´ì„œ í•´ê²°í•œë‹¤. ë…¼ë¬¸ ë‚´ìš© ì •ë¦¬ --- Abstract --- > However, extending the R-learner framework from binary to continuous treatments introduces a non-identifiability issue, as the functional zero constraint inherent to the conditional average treatment effect cannot be directly imposed in the R-loss under continuous treatments ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ì£¼ì¥ì…ë‹ˆë‹¤. > binary ë¥¼ continuousë¡œ í™•ì¥í•˜ë©´ non-identifiability issueê°€ ë°œìƒí•œë‹¤. ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•œ ê³¼ì •ì„ identification strategyë¼ê³  ì¹­í•˜ê³  ìˆìŠµë‹ˆë‹¤. 2ê°€ì§€ ê³¼ì •ì„ í†µí•´ ì´ë¥¼ êµ¬í˜„í•œë‹¤ê³  í•©ë‹ˆë‹¤. Tikhonov regularization zero-constraining operator ì™œ ì´ìŠˆê°€ ë°œìƒí•˜ê³  ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ”ê²Œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì´ë¼ê³  ì´í•´ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ë…¼ë¬¸ì—ì„œ generalized R-lossë¥¼ ì •ì˜í–ˆëŠ”ë°, ì´ ìˆ˜ì‹ì´ ì–´ë–»ê²Œ ìœ ë„ë˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ë¶€ë¶„ë„ í° ë„ì›€ì´ ë©ë‹ˆë‹¤. Introduction --- > Estimating heterogeneous treatment effects is fundamental in causal inference and provides insights into various fields, including precision medicine, education, online marketing, and offline policy evaluation ê°œì¸ë³„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ê±´ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•´ì¤„ ìˆ˜ ìˆê¸°ì— ì¤‘ìš”í•œ ë¬¸ì œì…ë‹ˆë‹¤. The treatment effect heterogeneity can be quantified by: $$ \\begin{equation} \\tau(x, t) = E[Y^{(t)} - Y^{(0)} X = x] \\end{equation} $$ $t = 0$ : reference treatment level heterogeneous treatment effectsëŠ” conditional average treatment effect (CATE)ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì´ëŠ” ì¡°ê±´ë¶€ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ê³„ì‚°í•œ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. (í™˜ìì˜ ì¡°ê±´ì´ xì¼ ë•Œì˜ ATE) > On the contrary, the R-learner and its variants (Kennedy, 2023) target the treatment effect estimatio Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects ê¸°ì¡´ì˜ ë‹¤ë¥¸ ì—°êµ¬ë“¤ì€ ì´ CATEë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ì§€ ì•Šì•˜ì§€ë§Œ R-learner ëª¨ë¸ì€ ì´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. > The R-learner capitalizes on the decomposition of the outcome model initially proposed by Robinson (1988) in partially linear models and extends for machine learning-based treatment effect estimation (Nie and Wager, 2021) R-learnerëŠ” partially linear modelì—ì„œ ì œì•ˆí•œ ìˆ˜í•™ì  ë¶„í•´ ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¼ì•„ì„œ ì´ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ìœ¼ë¡œ í™•ì¥ëœ ê°œë…ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ê°„ë‹¨í•œ ì •ë¦¬ëŠ” What is FWL?ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¸ê³  ë…¼ë¬¸: Robinson, P. M. (1988). Root-N-consistent semiparametric regression. Econometrica, 56(4), 931-954. [ë…¼ë¬¸ ë§í¬] Nie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319. [ë…¼ë¬¸ ë§í¬] > Notably, when using the two nuisance functions estimated under flexible models, the R-learner preserves the oracle property of treatment effect estimation as though the nuisance functions were known. Despite these advantages, the current R-learner framework applies only to binary or categorical treatments. R-learnerì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ìœ ì—°í•œ ëª¨ë¸ì„ í†µí•´ ë‘ ê°œì˜ nuisance functionì„ ì¶”ì •í•˜ë”ë¼ë„, ë§ˆì¹˜ ì´ í•¨ìˆ˜ë“¤ì´ ì •í™•íˆ ì•Œë ¤ì§„ ê²ƒì²˜ëŸ¼ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì˜ oracle ì„±ì§ˆì„ ìœ ì§€í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ì¥ì ì—ë„ ë¶ˆêµ¬í•˜ê³ , í˜„ì¬ R-learner í”„ë ˆì„ì›Œí¬ëŠ” ì´ì§„í˜• ë˜ëŠ” ë²”ì£¼í˜• ì²˜ì¹˜ì—ë§Œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. > In this article, we extend the R-learner framework to estimate the conditional average treatment effect flexibly with continuous treatments. This extension is nontrivial in both identification and estimation. Echoing the approach of Nie and Wager (2021), we focus on adapting the generalized R-learner loss function with continuous treatments. ì´ ë…¼ë¬¸ì—ì„œëŠ” R-learner í”„ë ˆì„ì›Œí¬ë¥¼ ì—°ì†í˜• ì²˜ì¹˜ì— ëŒ€í•´ì„œ í™•ì¥í•´ì„œ CATEë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í™•ì¥ì€ ì‹ë³„ê³¼ ì¶”ì • ëª¨ë‘ì—ì„œ ì–´ë ¤ìš´ ë¬¸ì œì…ë‹ˆë‹¤. Nieì™€ Wager (2021)ì˜ ì ‘ê·¼ ë°©ì‹ì„ ë”°ë¼ì„œ ì—°ì†í˜• ì²˜ì¹˜ì— ëŒ€í•œ ì¼ë°˜í™”ëœ R-learner ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì´ì§„ì²˜ì¹˜ ($T = 1$ or $T = 0$ ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°)ì™€ ë‹¤ë¥´ê²Œ ì—°ì†í˜•ì¼ ê²½ìš°ì— ë°œìƒí•˜ëŠ” ì–´ë ¤ì›€ë“¤ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ë©° ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. > Unlike the binary-treatment case, we demonstrate that directly minimizing the generalized R-loss does not uniquely identify $\\tau (x, t)$ but instead identifies a broad class of functions. ì´ì§„ì²˜ì¹˜ì™€ ë‹¬ë¦¬, ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” R-lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ìœ ì¼í•œ CATE í•¨ìˆ˜ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ê³ , ëŒ€ì‹  í•¨ìˆ˜ë“¤ì˜ ë„“ì€ ì§‘í•©ë§Œì„ ì‹ë³„í•  ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ non-identifiability issueì˜ í•µì‹¬ì…ë‹ˆë‹¤. ì´ì§„ì²˜ì¹˜ì˜ ê²½ìš° zero condition ($\\tau (x, t): \\tau (x, 0) \\equiv 0$)ì„ ë§Œì¡±í•˜ëŠ” ê²ƒì´ ì‰½ì§€ë§Œ, ì—°ì†í˜• ì²˜ì¹˜ì˜ ê²½ìš°ì—ëŠ” ì´ë¥¼ ë§Œì¡±í•˜ëŠ” ê²ƒì´ ì‰½ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ë³´ë©´, ì´ì§„ì²˜ì¹˜ì˜ ê²½ìš° zero conditionì„ ë§Œì¡±í•˜ê³  ì´ê²Œ cate functionì„ ìœ ì¼í•˜ê²Œ ì¶”ì •í•˜ë„ë¡ í•´ì£¼ì§€ë§Œ, ì—°ì†í˜• ì²˜ì¹˜ì˜ ê²½ìš°ì—ëŠ” zero conditionì„ ë§Œì¡±í•˜ëŠ” ê²ƒì´ ì‰½ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ë¥¼ ë§Œì¡±í•˜ëŠ” í•¨ìˆ˜ë“¤ì˜ ë„“ì€ ì§‘í•©ë§Œì„ ì¶”ì •í•˜ê²Œ ëœë‹¤ê³  ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ non-identifiability ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ â„“2-ì •ê·œí™”ëœ R-learnerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ Tikhonov ì •ê·œí™” ì›ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. Tikhonov ì •ê·œí™”ë€? Tikhonov ì •ê·œí™”ëŠ” ill-posed problem(ì˜ ì •ì˜ë˜ì§€ ì•Šì€ ë¬¸ì œ)ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” non-identifiability ë¬¸ì œê°€ ë°”ë¡œ ì´ëŸ° ill-posed problemì˜ ëŒ€í‘œì ì¸ ì˜ˆì…ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´: ë¬¸ì œê°€ ë„ˆë¬´ ìœ ì—°í•´ì„œ í•´ê°€ ë¬´ìˆ˜íˆ ë§ì„ ë•Œ ì¶”ê°€ì ì¸ ì œì•½ì¡°ê±´(ì •ê·œí™”)ì„ ë„ì…í•˜ì—¬ í•´ë¥¼ ìœ ì¼í•˜ê²Œ ë§Œë“¦ ìˆ˜í•™ì ìœ¼ë¡œëŠ” f Â² ê°™ì€ ì •ê·œí™” í•­ì„ ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€ 2ë‹¨ê³„ ì¶”ì • ê³¼ì • ë…¼ë¬¸ì˜ í•µì‹¬ì€ 2ë‹¨ê³„ ì¶”ì • ê³¼ì •ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤: 1ë‹¨ê³„: ì¤‘ê°„ í•¨ìˆ˜ ì¶”ì • $$\\tilde{\\tau}(x, t) = \\tau(x, t) - \\mathbb{E}\\{\\tau(X, T) \\mid X = x\\}$$ 2ë‹¨ê³„: Zero-constraining operatorë¥¼ í†µí•œ ë³€í™˜ $$\\tau(x, t) = \\text{ë³€í™˜}(\\tilde{\\tau}(x, t))$$ Zero-constraining operatorì˜ ì—­í•  ì´ operatorëŠ” ì¶”ì •ëœ í•¨ìˆ˜ê°€ í•­ìƒ zero conditionì„ ë§Œì¡±í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤: Ï„(x, 0) = 0 (ê¸°ì¤€ ì²˜ì¹˜ ìˆ˜ì¤€ì—ì„œì˜ íš¨ê³¼ëŠ” 0) ì´ë¥¼ í†µí•´ ìœ ì¼í•œ í•´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ Sieve ë°©ë²•ë¡  Sieve ë°©ë²•ì€ ë¬´í•œì°¨ì› í•¨ìˆ˜ ê³µê°„ì„ ìœ í•œì°¨ì›ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤: ê¸°ë³¸ ì•„ì´ë””ì–´: ë¬´í•œì°¨ì› í•¨ìˆ˜ë¥¼ ìœ í•œê°œì˜ ê¸°ì € í•¨ìˆ˜ë“¤ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ê·¼ì‚¬ ì˜ˆ: ë‹¤í•­ì‹, ìŠ¤í”Œë¼ì¸, í‘¸ë¦¬ì— ê¸‰ìˆ˜ ë“± ìˆ˜í•™ì  í‘œí˜„: $$\\tau(x, t) \\approx \\sum_i \\beta_i \\phi_i(x, t)$$ ì—¬ê¸°ì„œ $\\phi_i(x, t)$ëŠ” ê¸°ì € í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤. ì €ì°¨ì› í–‰ë ¬ê³¼ Non-identification ì¼ë°˜ì ì¸ sieve íšŒê·€ì™€ ë‹¬ë¦¬, ì´ ë…¼ë¬¸ì—ì„œëŠ” ì €ì°¨ì› í–‰ë ¬ì´ ë“±ì¥í•©ë‹ˆë‹¤. ì´ëŠ” ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œì˜ ìˆ˜í•™ì  í‘œí˜„ì…ë‹ˆë‹¤. í•µì‹¬ ê°œë…: Generalized R-lossì˜ non-identification íŠ¹ì„± ë•Œë¬¸ì— í–‰ë ¬ì´ ill-conditionedê°€ ë¨ í–‰ë ¬ì˜ ê³ ìœ ê°’ë“¤ì´ 0ì— ê°€ê¹Œì›Œì ¸ì„œ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± ë°œìƒ ì´ëŠ” í–‰ë ¬ êµë€ ì´ë¡ (matrix perturbation theory)ê³¼ ìŠ¤í™íŠ¸ëŸ´ ë¶„ì„ì´ í•„ìš”í•œ ì´ìœ  ìì„¸í•œ ì´ë¡ ì  ë°°ê²½: Ill-conditioned í–‰ë ¬ê³¼ ê³ ìœ ê°’: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½ ìˆ˜ë ´ ì†ë„ì˜ íŠ¹ì§• í•µì‹¬ ê²°ê³¼: > \"nuisance í•¨ìˆ˜ë“¤ì´ oP(n^(-1/4)) ìˆ˜ë ´ ì†ë„ë¡œ ê·¼ì‚¬ë  ìˆ˜ ìˆë‹¤ë©´, ì¶”ì •ëŸ‰ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ê²°ê³¼ ëª¨ë¸ì˜ ë§¤ë„ëŸ¬ì›€ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ì˜¤ì§ CATEì™€ propensity score í•¨ìˆ˜ì˜ ë§¤ë„ëŸ¬ì›€ì—ë§Œ ì˜ì¡´í•œë‹¤\" ì˜ë¯¸: ê²°ê³¼ ëª¨ë¸ì˜ ë³µì¡ì„±ì— ê´€ê³„ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ ë³´ì¥ CATEì™€ propensity scoreë§Œ ì˜ ì¶”ì •ë˜ë©´ ë¨ ì´ëŠ” double robustnessì˜ ì—°ì†í˜• ë²„ì „ ì ê·¼ì  ì •ê·œì„±ê³¼ ì¶”ë¡  ì ê·¼ì  ì •ê·œì„±: ì¶”ì •ëŸ‰ì´ ì •ê·œë¶„í¬ë¡œ ìˆ˜ë ´ ì´ë¥¼ í†µí•´ ì‹ ë¢°êµ¬ê°„ê³¼ ê°€ì„¤ê²€ì • ê°€ëŠ¥ ë‹«íŒ í˜•íƒœ ë¶„ì‚° ì¶”ì •ëŸ‰: ë³µì¡í•œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì—†ì´ë„ ë¶„ì‚° ê³„ì‚° ê°€ëŠ¥ ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ ì°¸ê³  ë…¼ë¬¸: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. Doklady Akademii Nauk SSSR, 151(3), 501-504. [ë…¼ë¬¸ ë§í¬] Bhatia, R. (2013). Matrix analysis. Springer Science & Business Media. [ì±… ë§í¬] Chen, X. (2007). Large sample sieve estimation of semi-nonparametric models. Handbook of econometrics, 6, 5549-5632. [ë…¼ë¬¸ ë§í¬] í•µì‹¬ ê°œë…ë“¤: Ill-posed problem: í•´ê°€ ìœ ì¼í•˜ì§€ ì•Šê±°ë‚˜ ë¶ˆì•ˆì •í•œ ë¬¸ì œ Tikhonov regularization: ì •ê·œí™”ë¥¼ í†µí•œ ill-posed problem í•´ê²° Sieve method: ë¬´í•œì°¨ì›ì„ ìœ í•œì°¨ì›ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ë²• Matrix perturbation theory: í–‰ë ¬ì˜ ì‘ì€ ë³€í™”ê°€ ê³ ìœ ê°’ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì—°êµ¬ Spectral analysis: í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° ë¶„ì„ 1.1 Setup and notation ì´ë¡ ì  ë°°ê²½ê³¼ ê´€ë ¨í•œ ìˆ˜ì‹ì„ ì •ë¦¬í•©ë‹ˆë‹¤. $\\{Z_i = (X_i, T_i, Y_i)\\}_{i=1}^n$ : independent and identically distributed samples from the distribution of $(X, T, Y)$ $X = (X^{(1)}, \\ldots, X^{(d)})$ : $d$-dimensional vector of covariates. $Y^{(t)}$ : potential outcome had the unit received treatment level $T = t \\in \\mathbb{R}$ $\\tau(x, t)$ : causal estimand defined in (1) Under Rubin's causal model framework (Rubin, 1974), $$ \\begin{equation} \\tau(x, t) = E[Y^{(t)} - Y^{(0)} X = x] \\end{equation} $$ > Due to the fundamental problem in causal inference that not all potential outcomes can be observed for a particular unit, $\\tau(x, t)$ is not identifiable without further assumptions. We employ common assumptions for continuous treatments (Kennedy et al., 2017). ì¸ê³¼ì¶”ë¡ ì—ì„œëŠ” ëª¨ë“  ê´€ì¸¡ê°’ì„ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì—ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ 3ê°€ì§€ì˜ ê°€ì •ì´ ì¡´ì¬í•©ë‹ˆë‹¤. Assumption 1 (No unmeasured confounding). We have $\\{Y^{(t)}\\}_{t \\in \\mathcal{T}} \\perp\\!\\!\\!\\perp T \\mid X$ Assumption 2 (Stable unit and treatment value). When $T = t \\in \\mathcal{T}$, we have $Y = Y^{(t)}$ Assumption 3 (Positivity). There exists an $\\epsilon > 0$ such that the generalized propensity score $f(T = t \\mid X = x) \\in (\\epsilon, 1/\\epsilon)$ for any $(x, t) \\in \\mathcal{X} \\times \\mathcal{T}$. Notation: For any vector $v$, $\\ v\\ $ denotes its $\\ell_2$ norm For any random variable $W \\in \\mathcal{W}$, $f(w)$ and $P(w)$ denote its probability density function and probability measure For any function $g(w)$: $P_n\\{g(W)\\} = \\frac{1}{n}\\sum_{i=1}^n g(W_i)$ denotes its empirical expectation $\\ g\\ _{L_2} = \\left\\{\\int_{w \\in \\mathcal{W}} g^2(w) dw\\right\\}^{1/2}$ denotes its $L_2$ norm $\\ g\\ _{L_2^P} = \\left\\{\\int_{w \\in \\mathcal{W}} g^2(w) dP(w)\\right\\}^{1/2}$ denotes its $L_2^P$ norm $\\ g\\ _{\\mathcal{W}} = \\sup_{w \\in \\mathcal{W}} g(w) $ denotes its $L_\\infty$ norm $L_2^P(\\mathcal{W})$ represents the function space of all $g(w)$ with a bounded $L_2^P$ norm When $g(w)$ is a multivariate function, denote $\\ g\\ _{\\mathcal{W}} = \\sup_{w \\in \\mathcal{W}} \\ g(w)\\ $ Nuisance Functions: Conditional outcome mean: $m(x) = E(Y \\mid X = x)$ Generalized propensity score: $\\varpi(t \\mid x) = f(T = t \\mid X = x)$ Full conditional outcome mean model: $$\\mu(x, t) = E(Y \\mid X = x, T = t)$$ Observation noises: $$\\varepsilon_i = Y_i - \\mu(X_i, T_i), \\quad i = 1, \\ldots, n$$ where $E(\\varepsilon_i \\mid X_i, T_i) = 0$, following the definition of $\\mu(x, t)$. Generalized R-learner --- 2.1 The generalized R-loss --- > We first generalize the idea of the Robinson's residual (Robinson, 1988; Nie and Wager, 2021) to the continuous-treatment scenario. ì´ë²ˆ ì ˆì—ì„œëŠ” ì¼ë°˜í™”ëœ R-lossë¥¼ ì–´ë–»ê²Œ ìœ ë„í•˜ëŠ”ì§€ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. The unconfoundedness and stable unit and treatment value imply: $$ \\begin{equation} Y_i^{(T_i)} = \\mu(X_i, T_i) + \\varepsilon_i = \\mu(X_i, 0) + \\tau(X_i, T_i) + \\varepsilon_i \\tag{3} \\end{equation} $$ > í•´ì„: > - ì²« ë²ˆì§¸ ë“±ì‹: Assumption 2ì™€ equation (2)ì—ì„œ ìœ ë„ > - ë‘ ë²ˆì§¸ ë“±ì‹: Assumption 1ê³¼ $\\tau(x, t)$ì˜ ì •ì˜ì—ì„œ ìœ ë„ > - íŠ¹ì§•: ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ì´ë©° ì¶”ê°€ì ì¸ êµ¬ì¡°ì  ê°€ì •ì´ ì—†ìŒ Step 2: ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ ê³„ì‚°: Given $X_i$, taking the conditional expectation on (3) leads to: $$ \\begin{equation} m(X_i) = E\\left[Y_i^{(T_i)} \\mid X = X_i\\right] = \\mu(X_i, 0) + E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} \\tag{4} \\end{equation} $$ > ì „ì²´ ê¸°ëŒ“ê°’ ë²•ì¹™ ì ìš©: > $$E(\\varepsilon_i \\mid X_i) = E\\left[E(\\varepsilon_i \\mid X_i, T_i) \\mid X_i\\right] = E[0 \\mid X_i] = 0$$ ì¼ë°˜í™”ëœ Propensity Score ê¸°ëŒ“ê°’: The notation $E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\}$ in (4) highlights the dependency of the conditional expectation on the generalized propensity score as: $$E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} = \\int_{t \\in \\mathcal{T}} \\tau(X_i, t) \\varpi(t \\mid X_i) dt$$ > í•µì‹¬: ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” ì ë¶„ì„ í†µí•´ ëª¨ë“  ì²˜ì¹˜ ìˆ˜ì¤€ì— ëŒ€í•œ ê°€ì¤‘ í‰ê· ì„ ê³„ì‚° Step 3: ì”ì°¨ ë„ì¶œ: By subtracting (4) from (3) on both left- and right-hand sides, we have: $$ \\begin{equation} Y_i^{(T_i)} - m(X_i) = \\tau(X_i, T_i) - E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} + \\varepsilon_i \\tag{5} \\end{equation} $$ > í•´ì„: > - ì¢Œë³€: ê´€ì°°ëœ ê²°ê³¼ì—ì„œ ì¡°ê±´ë¶€ í‰ê· ì„ ëº€ ê°’ (Robinson's residual) > - ìš°ë³€: ì²˜ì¹˜íš¨ê³¼ì—ì„œ í‰ê·  ì²˜ì¹˜íš¨ê³¼ë¥¼ ëº€ ê°’ + ë…¸ì´ì¦ˆ Step 4: ì†ì‹¤ í•¨ìˆ˜ ë„ì¶œ: By treating the left-hand side of (5) as the response and the right-hand side except $\\varepsilon_i$ as the mean function, we derive the following population loss function: $$ \\begin{equation} L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right] \\tag{6} \\end{equation} $$ > í•µì‹¬ íŠ¹ì§•: > - ìµœì í•´: $h = \\tau$ì—ì„œ ìµœì†Œí™”ë¨ > - ì¼ë°˜í™”: ì´ì§„ ì²˜ì¹˜ R-learnerì˜ ìì—°ìŠ¤ëŸ¬ìš´ í™•ì¥ > - ì°¸ê³ : Nie and Wager (2021, Â§7)ì˜ ë‹¤ì¤‘ ì²˜ì¹˜ ì„¤ì •ì—ì„œ ìœ ì‚¬í•œ ì†ì‹¤í•¨ìˆ˜ ë“±ì¥ ì´ì§„ ì²˜ì¹˜ì™€ì˜ ì—°ê²°: > In particular, under the binary-treatment case, $\\tau(x, t)$ reduces to $\\{\\tau(x, 0), \\tau(x, 1)\\}$, where $\\tau(x, 0) = E(Y^{(0)} - Y^{(0)} \\mid X = x) = 0$ for any $x \\in \\mathcal{X}$, and $\\tau(x, 1)$ is the conditional average treatment effect of interest. It suffices to estimate $\\tau(x, 1)$ by solving the $h(\\cdot, 1)$ that minimizes (6), after imposing a zero condition of $h(\\cdot, 0)$: $$ \\begin{equation} h(x, 0) = 0, \\quad \\text{for any } x \\in \\mathcal{X} \\tag{7} \\end{equation} $$ > Zero condition: ê¸°ì¤€ ì²˜ì¹˜ ìˆ˜ì¤€ì—ì„œì˜ íš¨ê³¼ëŠ” 0ìœ¼ë¡œ ì„¤ì • ì´ì§„ ì²˜ì¹˜ R-lossë¡œì˜ í™˜ì›: ìœ ë„ëœ ì¼ë°˜í™”ëœ R-lossê°€ ê¸°ì¡´ì˜ ì´ì§„ ì²˜ì¹˜ì—ì„œì˜ loss functionìœ¼ë¡œ ìœ ë„ë  ìˆ˜ ìˆìŒì„ ì„¤ëª…í•©ë‹ˆë‹¤. > More specifically, observing that under (7) one has $h(X, T) - E_e\\{h(X, T) \\mid X\\} = \\{T - e(X)\\}h(X, 1)$ a.s., where $e(x) = \\text{pr}(T = 1 \\mid X = x)$ is the propensity score, the R-loss function (6) reduces to: $$ \\begin{equation} L_b(h) = E\\left[\\left\\{Y - m(X) - \\{T - e(X)\\}h(X, 1)\\right\\}^2\\right] \\tag{8} \\end{equation} $$ > í•µì‹¬: > - ì¡°ê±´: $h(x, 0) = 0$ (zero condition) > - ê²°ê³¼: $h(X, T) - E_e\\{h(X, T) \\mid X\\} = \\{T - e(X)\\}h(X, 1)$ > - ì˜ë¯¸: ì¼ë°˜í™”ëœ R-lossê°€ ì´ì§„ ì²˜ì¹˜ì˜ ê³ ì „ì  R-lossë¡œ í™˜ì›ë¨ > í•µì‹¬: Tê°€ 1 or 0ì¸ ìƒí™©ì—ì„œ zero conditionì„ ê³ ë ¤í•´ (6)ë²ˆ ìˆ˜ì‹ì„ ì „ê°œí•˜ë©´ ê¸°ì¡´ì˜ ì´ì§„ ì²˜ì¹˜ì—ì„œì˜ ì†ì‹¤í•¨ìˆ˜ ìˆ˜ì‹ê³¼ ê°™ì€ (8)ë²ˆ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 2.2 Non-identification of the generalized R-loss --- > The R-learner for continuous treatment will have poor estimation performance, due to the non-unique identifiability of the generalized R-loss í•µì‹¬ ë¬¸ì œ: ì´ ì ˆì—ì„œëŠ” non-identification ë¬¸ì œì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ì¼ë°˜í™”ëœ R-lossë¥¼ ì§ì ‘ ìµœì†Œí™”í•˜ë©´ ìœ ì¼í•œ í•´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ í•µì‹¬ ë¬¸ì œì…ë‹ˆë‹¤. í•´ì§‘í•© ì •ì˜: $$ \\begin{equation} S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\} \\tag{9} \\end{equation} $$ > í•´ì„: $S$ëŠ” ëª©í‘œ í•¨ìˆ˜ $\\tau(X, T)$ì— ê³µë³€ëŸ‰ $X$ì˜ í•¨ìˆ˜ $s(X)$ë¥¼ ë”í•œ ëª¨ë“  í•¨ìˆ˜ë“¤ì˜ ì§‘í•© ê²€ì¦ ê³¼ì •: > It is easy to check that for any $h \\in S$: $$ Y - m(X) - [h(X, T) - E_{\\varpi}\\{h(X, T) \\mid X\\}] = Y - m(X) - [\\tau(X, T) - E_{\\varpi}\\{\\tau(X, T) \\mid X\\}] \\text{ a.s.} $$ > ì˜ë¯¸: $S$ì— ì†í•œ ëª¨ë“  í•¨ìˆ˜ $h$ê°€ ë™ì¼í•œ ì†ì‹¤ê°’ì„ ê°€ì§ Non-identification ë¬¸ì œ: > From (6), any function $h \\in S$ minimizes the generalized R-loss $L_c(\\cdot)$. Therefore, when $T$ is continuous, directly minimizing the generalized R-loss fails to uniquely identify the target estimand $\\tau(x, t)$, as there are infinitely many solutions in $S$. > í•µì‹¬ ë¬¸ì œ: > - ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” ë¬´í•œíˆ ë§ì€ í•´ê°€ ì¡´ì¬ > - ìœ ì¼í•œ CATE í•¨ìˆ˜ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ìŒ > - ì´ëŠ” ill-posed problemì˜ ì „í˜•ì ì¸ ì˜ˆ ì´ë¡ ì  ê·¼ê±°: > This result theoretically substantiates the ill-posedness of estimating $\\tau(x, t)$ by minimizing the empirical counterpart of $L_c(\\cdot)$ using nonparametric estimators, and also explains the failure-to-estimate issue illustrated in Fig. 1. > Part (i) of Proposition 1 below rigorously proves that $S$ in fact contains all minima of $L_c(\\cdot)$ in $L_2^P(X, T)$. ì´ì§„ ì²˜ì¹˜ì™€ì˜ ëŒ€ë¹„: > In contrast, minimizing the binary-treatment R-loss (8) which incorporates the zero condition (7), can successfully identify $\\tau$, because (7) narrows the general solution set $S$ into: $$ \\begin{equation} S^\\natural = \\{h \\mid h(X, T) = \\tau(X, T) \\text{ a.s.}\\} \\tag{10} \\end{equation} $$ > í•µì‹¬ ì°¨ì´: > - ì—°ì†í˜•: $S$ (ë¬´í•œíˆ ë§ì€ í•´) > - ì´ì§„í˜•: $S^\\natural$ (ìœ ì¼í•œ í•´) > - ì›ì¸: Zero condition (7)ì´ í•´ì§‘í•©ì„ ì¢í˜€ì¤Œ Proposition 1 - Suppose Assumptions 1â€“2 hold. Then, the following statements hold: ì´ ë¶€ë¶„ì€ generalized loss function (6) ì„ ìµœì†Œí™”í•˜ë©´ hëŠ” ë‹¤ì–‘í•œ í•´ë¥¼ ê°€ì§€ì§€ë§Œ, zero-conditionì„ ë…¹ì—¬ë‚´ ì •ë¦¬í•œ binary loss function (8) ì„ ìµœì†Œí™”í•˜ë©´ hëŠ” ìœ ì¼í•œ í•´ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. > Part (i): ì²˜ì¹˜ Tê°€ ì—°ì†í˜•ì¸ ê²½ìš°, SëŠ” ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ë‹µí•´ë‹¤. > - ìµœì í™” ë¬¸ì œ: $$ \\begin{equation} \\arg\\min_{h \\in L_2^P(X,T)} L_c(h) \\tag{11} \\end{equation} $$ > - í•´ì§‘í•©: $S$ > Part (ii): ì²˜ì¹˜ Tê°€ ì´ì§„í˜•ì¸ ê²½ìš°, SëŠ” ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ë‹µí•´ë‹¤. > - ìµœì í™” ë¬¸ì œ: $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ > - í•´ì§‘í•©: $S^\\natural$ > ê²°ê³¼: ê´€ì‹¬ í•¨ìˆ˜ë“¤ì˜ ì§‘í•© $\\mathcal{L}_b = \\{h \\mid h(\\cdot, 1) \\in L_2^P(X) \\text{ and } h(X, 0) = 0 \\text{ a.s.}\\}$ ì¤‘ì—ì„œ, (10)ì˜ $S^\\natural$ê°€ ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ì§‘í•©ì´ë‹¤: > - ìµœì í™” ë¬¸ì œ: $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ > - í•´ì§‘í•©: $S^\\natural$ > ì˜ë¯¸: ì´ì§„í˜• ì²˜ì¹˜ì—ì„œëŠ” zero conditionì´ ì ìš©ëœ R-loss $L_b(h)$ì˜ ìµœì†Œí™”ê°€ ìœ ì¼í•œ í•´ $S^\\natural$ë¥¼ ì œê³µ í•µì‹¬ ì°¨ì´ì : êµ¬ë¶„ ì—°ì†í˜• ì²˜ì¹˜ ì´ì§„í˜• ì²˜ì¹˜ ------ ------------- ------------- ìµœì í™” ë¬¸ì œ $\\arg\\min_{h \\in L_2^P(X,T)} L_c(h)$ $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ í•´ì§‘í•© $S$ (ë¬´í•œíˆ ë§ì€ í•´) $S^\\natural$ (ìœ ì¼í•œ í•´) ì œì•½ì¡°ê±´ ì—†ìŒ $h(X, 0) = 0$ (zero condition) ì‹ë³„ ê°€ëŠ¥ì„± âŒ ë¶ˆê°€ëŠ¥ âœ… ê°€ëŠ¥ 2.3 One-step nonparametric identification with a functional zero constraint > ì´ì§„ ì²˜ì¹˜ì˜ ê²½ìš° zero-conditionë¥¼ loss functionì— ë…¹ì—¬ë‚¼ ìˆ˜ ìˆì–´ ìœ ì¼í•´ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆì§€ë§Œ, ì—°ì† ì²˜ì¹˜ì˜ ê²½ìš° ìœ ì¼í•´ë¥¼ ì–»ì§€ ëª»í•œë‹¤. ê·¸ëŸ¼ optimization problemì„ í’€ ë•Œ zero conditionì„ ì ìš©í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ? ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. $$ \\begin{equation} \\arg \\min_{h \\in L_2^P(X,T) \\cap \\{h \\mid h(x,0) = 0 \\text{ for any } x \\in \\mathcal{X}\\}} L_c(h) \\tag{12} \\end{equation} $$ ê·¸ëŸ¬ë‚˜ Proposition 2ëŠ” ì´ ì „ëµë„ $\\tau(x, t)$ì˜ ë¹„ëª¨ìˆ˜ì  ì‹ë³„ì„ ë‹¬ì„±í•˜ëŠ” ë° ê³„ì† ì‹¤íŒ¨í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Proposition 2. Assumptions 1â€“2ê°€ ì„±ë¦½í•˜ê³ , $(X, T)$ê°€ ìœ ê³„ ë°€ë„í•¨ìˆ˜ë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ì. ì¦‰, $$\\sup_{(x,t) \\in \\mathcal{X} \\times \\mathcal{T}} f(x, t) < \\infty$$ $\\tau^{\\vee}(x, t \\mid s)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ì·¨í•˜ëŠ” í•¨ìˆ˜ë¼ê³  í•˜ì: $$ \\begin{equation} \\tau^{\\vee}(x, t \\mid s) = \\begin{cases} \\tau(x, t) + s(x) & \\text{when } t \\neq 0 \\\\ 0 & \\text{when } t = 0 \\end{cases} \\tag{13} \\end{equation} $$ ì—¬ê¸°ì„œ $s$ëŠ” $L_2^P(X)$ì˜ ì„ì˜ì˜ í•¨ìˆ˜ì´ë‹¤. ê·¸ëŸ¬ë©´ ì„ì˜ì˜ $s \\in L_2^P(X)$ì— ëŒ€í•´ $\\tau^{\\vee}(x, t \\mid s)$ëŠ” (12)ë¥¼ í•´ê²°í•œë‹¤. > í•µì‹¬: ì„ì˜ì˜ $\\tau^{\\vee}(x, t \\mid s)$ëŠ” zero condition (7)ì„ ë§Œì¡±í•˜ë©´ì„œë„ ì—¬ì „íˆ ì§‘í•© $S$ì— ì†í•  ìˆ˜ ìˆë‹¤. ì¦‰, zero conditionì„ ìµœì í™”ë¬¸ì œì— ê°•ì œë¡œ ì ìš©í•´ë„ $t \\neq 0$ì¸ ë¶€ë¶„ì—ì„œëŠ” ì—¬ì „íˆ ê°™ì€ non-identification ë¬¸ì œë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. 2.4 Two-step Tikhonov identification and â„“â‚‚ regularization R-learner --- 2.3ì—ì„œ ê°•ì œë¡œ zero conditionì„ ì ìš©í•´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆê³ , ì´ì œ ì •ê·œí™”í•­ì„ ë„ì…í•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. Step I: ëª¨ì§‘ë‹¨ ìˆ˜ì¤€ì—ì„œ ì£¼ì–´ì§„ $\\rho > 0$ì— ëŒ€í•´ (11)ì˜ $\\ell_2$-ì •ê·œí™” ë³€í˜•ì„ í•´ê²°í•©ë‹ˆë‹¤: $$ \\begin{equation} \\tau_\\rho = \\arg \\min_{h \\in L_2^P(X,T)} L_{c,\\ell_2}(h \\mid \\rho) = \\arg \\min_{h \\in L_2^P(X,T)} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right] \\tag{14} \\end{equation} $$ > The new loss $L_{c,\\ell_2}(h \\mid \\rho)$ is strictly convex over $L_2^P(X, T)$ due to the addition of a strictly convex functional $\\rho \\ h\\ _{L_2^P}^2 = \\rho E\\{h^2(X, T)\\}$. > Thus minimizing $L_{c,\\ell_2}(h \\mid \\rho)$ becomes well-posed and yields a unique functional minimum $\\tau_\\rho$. Theorem 1 explicitly characterizes this unique minimum. ê°•í•œ (ì—„ê²©í•œ) convex functionì„ loss functionì— ì¶”ê°€í•˜ì—¬, loss functionì„ convexí•˜ê²Œ ë§Œë“¤ì–´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì•¼ê¸°ì…ë‹ˆë‹¤. ì´ë¡ ì  ë°°ê²½: Tikhonov regularizationê³¼ convex optimizationì˜ ìì„¸í•œ ì›ë¦¬ëŠ” Tikhonov Regularizationê³¼ Convex Optimizationì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Theorem 1. ì§‘í•© $S$ ë‚´ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¤‘ê°„ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì: $$ \\begin{equation} \\tilde{\\tau}(x, t) = \\tau(x, t) - E\\{\\tau(X, T) \\mid X = x\\} \\tag{15} \\end{equation} $$ Assumptions 1â€“2ê°€ ì„±ë¦½í•˜ê³  $\\tau \\in L_2^P(X, T)$ì¼ ë•Œ, ì£¼ì–´ì§„ $\\rho > 0$ì— ëŒ€í•´ (14)ì˜ í•´ì§‘í•©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤: $$S_\\rho = \\{h \\mid h(X, T) = \\tau_\\rho(X, T) \\text{ a.s.}\\}$$ ì—¬ê¸°ì„œ $$\\tau_\\rho(x, t) = (1 + \\rho)^{-1} \\tilde{\\tau}(x, t)$$ Theorem 1ì˜ ì˜ë¯¸: Theorem 1ì€ $\\tau_\\rho$ì— $(1 + \\rho)$ ì¸ìˆ˜ë¥¼ ê³±í•˜ì—¬ ì§‘í•© $S$ ë‚´ì˜ ì¤‘ê°„ í•¨ìˆ˜ $\\tilde{\\tau}$ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤: $$\\tilde{\\tau} = (1 + \\rho)\\tau_\\rho$$ ì´ $\\tilde{\\tau}$ëŠ” ì›ë˜ R-loss $L_c(h)$ì˜ í•´ê°€ ë©ë‹ˆë‹¤. Step II: Zero-constraining Operator Step IIì—ì„œëŠ” $\\tilde{\\tau}$ë¥¼ zero-constraining operator $\\mathcal{C}(\\cdot): L_2^P(X, T) \\to L_2^P(X, T)$ë¥¼ í†µí•´ ë³€í™˜í•©ë‹ˆë‹¤: $$ \\begin{equation} \\mathcal{C}(h)(x, t) = h(x, t) - h(x, 0) \\quad \\text{for any } h \\in L_2^P(X, T) \\end{equation} $$ ë”°ë¼ì„œ: $$\\mathcal{C}(\\tilde{\\tau})(x, t) = \\mathcal{C}((1 + \\rho)\\tau_\\rho)(x, t) = (1 + \\rho)\\{\\tau_\\rho(x, t) - \\tau_\\rho(x, 0)\\}$$ Zero-constraining Operatorì˜ ì—­í• : Zero-constraining Operatorì˜ í•µì‹¬ ì›ë¦¬: ì—°ì‚°ì $\\mathcal{C}(\\cdot)$ëŠ” ë³€í™˜ì„ ê±°ì¹˜ëŠ” ëª¨ë“  í•¨ìˆ˜ê°€ zero condition (7)ì„ ë§Œì¡±í•˜ë„ë¡ ë³´ì¥ $\\tau$ëŠ” ì§‘í•© $S$ì—ì„œ zero condition (7)ì„ ë§Œì¡±í•˜ëŠ” ìœ ì¼í•œ í•¨ìˆ˜ ë”°ë¼ì„œ í•´ì§‘í•© $S$ì˜ ì„ì˜ í•¨ìˆ˜ë¥¼ ë³€í™˜í•˜ë©´ $\\tau$ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŒ $\\tilde{\\tau} \\in S$ì´ë¯€ë¡œ, Step IIì˜ $\\mathcal{C}(\\tilde{\\tau})$ëŠ” ê¶ê·¹ì ìœ¼ë¡œ $\\tau$ë¥¼ ì‹ë³„í•¨ > í•µì‹¬: 2ë‹¨ê³„ ê³¼ì •ì„ í†µí•´ non-identification ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìœ ì¼í•œ CATE í•¨ìˆ˜ $\\tau(x, t)$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Supplementary material for \"Towards R-learner with Continuous Treatments\" S1 : $l_2$ regularization R-learner : Formal algorithm and more discussions ë…¼ë¬¸ì—ì„œ ì˜ë¬¸ì´ ë“¤ì—ˆë˜ ë¶€ë¶„ë“¤ ì •ë¦¬ --- ì™œ ì´ì§„ ì²˜ì¹˜ì—ì„œëŠ” Zero Conditionì´ ì ìš©ë˜ê³ , ì—°ì† ì²˜ì¹˜ì—ì„œëŠ” ì ìš©ë˜ì§€ ì•Šì„ê¹Œ? ì´ì§„ ì²˜ì¹˜ì—ì„œì˜ Zero Condition ì´ì§„ ì²˜ì¹˜ì˜ íŠ¹ì„±: ì²˜ì¹˜ ìˆ˜ì¤€: $T \\in \\{0, 1\\}$ (ì´ì‚°ì ) ëª©í‘œ: $\\tau(x, 1)$ (ì²˜ì¹˜ íš¨ê³¼)ë§Œ ì¶”ì • ê¸°ì¤€ì : $T = 0$ (í†µì œ ê·¸ë£¹) Zero Conditionì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì ìš©: $$\\tau(x, 0) = E[Y^{(0)} - Y^{(0)} \\mid X = x] = 0$$ > ì´ìœ : > - $T = 0$ì€ \"ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì€ ìƒíƒœ\"ë¥¼ ì˜ë¯¸ > - ê°™ì€ ìƒíƒœì—ì„œì˜ ì°¨ì´ëŠ” ë‹¹ì—°íˆ 0 > - ì´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì œì•½ì¡°ê±´ì´ ë¨ ìˆ˜í•™ì  íš¨ê³¼: í•´ì§‘í•© $S$ì—ì„œ $h(x, 0) = 0$ ì¡°ê±´ì„ ì¶”ê°€ $S^\\natural = \\{h \\mid h(X, T) = \\tau(X, T) \\text{ a.s.}\\}$ë¡œ ì¶•ì†Œ ìœ ì¼í•œ í•´ ë³´ì¥ ì—°ì† ì²˜ì¹˜ì—ì„œì˜ ë¬¸ì œ ì—°ì† ì²˜ì¹˜ì˜ íŠ¹ì„±: ì²˜ì¹˜ ìˆ˜ì¤€: $T \\in \\mathbb{R}$ (ì—°ì†ì ) ëª©í‘œ: $\\tau(x, t)$ for all $t \\in \\mathcal{T}$ (ëª¨ë“  ì²˜ì¹˜ ìˆ˜ì¤€ì—ì„œì˜ íš¨ê³¼) ê¸°ì¤€ì : $T = 0$ì´ ì„ì˜ì ì¼ ìˆ˜ ìˆìŒ Zero Condition ì ìš©ì˜ í•œê³„: > $L_b$ë¥¼ í†µí•´ ì´ì§„ì²˜ì¹˜ì—ì„œëŠ” zero-condtionì„ loss í•¨ìˆ˜ì— ë…¹ì—¬ë‚¼ ìˆ˜ ìˆì—ˆëŠ”ë°, ì—°ì†í˜•ì—ì„œëŠ” ì´ë¥¼ loss functionì— ë…¹ì—¬ë‚¼ ìˆ˜ ì—†ë‹¤. ë¬¸ì œëŠ” â€œzero conditionì„ R-loss ì•ˆì—ì„œ ì§ì ‘ êµ¬í˜„í•˜ê¸° ì–´ë µë‹¤â€ëŠ” ì ì…ë‹ˆë‹¤. Binary Tì—ì„œëŠ” Tâˆˆ{0, 1}ì´ë¼ (T-e(X))h(X,1) í˜•íƒœë¡œ ì‹ì„ ì¬ì •ë¦¬í•˜ë©´ h(x,0)=0ì´ ìë™ìœ¼ë¡œ ë‚´ì¬ë©ë‹ˆë‹¤. í•µì‹¬ ì°¨ì´ì  ìš”ì•½ êµ¬ë¶„ ì´ì§„ ì²˜ì¹˜ ì—°ì† ì²˜ì¹˜ ------ ----------- ----------- ì²˜ì¹˜ íŠ¹ì„± ì´ì‚°ì  $\\{0, 1\\}$ ì—°ì†ì  $\\mathbb{R}$ ê¸°ì¤€ì  $T = 0$ (ìì—°ìŠ¤ëŸ¬ì›€) $T = 0$ (ì„ì˜ì ) Zero Condition ìˆ˜ì‹ì—ì„œ ì‰½ê²Œ ê°•ì œ ê°€ëŠ¥ ë¬´í•œì°¨ì› ì œì•½ â†’ êµ¬í˜„ ë‚œì´ë„ ë†’ìŒ í•´ì§‘í•©(ì´ë¡ ) $S^\\natural$ (ìœ ì¼) $S$ (ë¬´í•œ) ì‹ë³„ ê°€ëŠ¥ì„±(ì‹¤ì „) âœ… ê°€ëŠ¥ âŒ ì¶”ê°€ ì¥ì¹˜ í•„ìš” \"h(x, 0)=0ë§Œ ê°•ì œí•˜ë©´ ë˜ì§€ ì•Šë‚˜?\" â€“ ì™œ ì‹¤ì „ì—ì„œëŠ” ì•ˆ í†µí• ê¹Œ? --- ìˆ˜í•™ì ìœ¼ë¡œëŠ” $h(x,0)=0$ì„ ì •í™•íˆ ë¶€ê³¼í•˜ë©´ $s(x)=âˆ’\\tau(x,0)=0$ì´ ë˜ì–´ í•´ê°€ ìœ ì¼í•©ë‹ˆë‹¤. ë¬¸ì œëŠ” ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì´ ë¬´í•œì°¨ì› ì œì•½ì„ ê·¸ëŒ€ë¡œ êµ¬í˜„í•˜ê¸° ì–´ë µë‹¤ëŠ” ì ì— ìˆìŠµë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” $h$ë¥¼ ì‹ ê²½ë§Â·íŠ¸ë¦¬Â·ì»¤ë„ ë“± ìœ ì—°í•œ í•¨ìˆ˜ ê·¼ì‚¬ê¸°ë¡œ íŒŒë¼ë¯¸í„°í™”í•˜ê³ , ì†ì‹¤ì„ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì†Œí™”í•©ë‹ˆë‹¤. ì´ë•Œ â€œ$t=0$ì—ì„œ ë°˜ë“œì‹œ 0â€ì´ë¼ëŠ” ì œì•½ì„ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ ì „ì²´ì— ê°•ì œí•˜ê¸°ê°€ ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ í•™ìŠµ ê³¼ì •ì—ì„œëŠ” $\\hat{h}(x,t) = \\tau(x,t) + s(x)$ ê¼´ì˜ í•¨ìˆ˜ë„ ì†ì‹¤ì„ ë™ì¼í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ìœ : R-loss ì•ˆì˜ $E[h(X,T)\\mid X]$ í•­ì—ì„œ $s(x)$ê°€ ìƒì‡„ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰, zero conditionì„ ì½”ë“œì— ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ ì—¬ì „íˆ ì‹ë³„ ì‹¤íŒ¨(ill-posed)ê°€ ë°œìƒí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì €ìë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‘ ë‹¨ê³„ ì ˆì°¨ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. 1ë‹¨ê³„: ì œì•½ ì—†ì´ $\\tilde{h}$ë¥¼ í•™ìŠµ 2ë‹¨ê³„: íŠ¹ìˆ˜í•œ ì‚¬ìƒ(\"zero-constraining operator\")ìœ¼ë¡œ $\\tilde{h}$ë¥¼ $t=0$ì—ì„œ 0ì´ ë˜ë„ë¡ ë³€í™˜ ì¶”ê°€ë¡œ Tikhonov ì •ê·œí™”ë¥¼ ë„£ì–´ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤. ì™œ theorem 1ì—ì„œ ì¤‘ê°„í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ”ê±°ì§€? ì™œ êµ³ì´ â€˜ì¤‘ê°„ í•¨ìˆ˜(tilde tau)â€™ë¥¼ í•œ ë²ˆ ë” ì •ì˜í•˜ëŠëƒë¥¼ í•œë§ˆë””ë¡œ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ìš”. ë¹„ì‹ë³„(non-identification)ì˜ ì›ì¸ R-lossë¥¼ ê·¸ëŒ€ë¡œ ìµœì†Œí™”í•˜ë©´ $$h(x,t) = Ï„(x,t) + s(x)$$ ì²˜ëŸ¼ â€œx ì—ë§Œ ì˜ì¡´í•˜ëŠ” í•¨ìˆ˜â€ê°€ ì–¼ë§ˆë“ ì§€ ë§ë¶™ì–´ë„ ì†ì‹¤ê°’ì´ ë˜‘ê°™ìŠµë‹ˆë‹¤. ì´ s(x) ë•Œë¬¸ì— ìµœì†Œê°’ì´ ë¬´í•œíˆ ë§ì•„ì ¸ì„œ í•´ê°€ ìœ ì¼í•˜ì§€ ì•Šì•„ìš”. ì•„ì´ë””ì–´: â€˜x í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ”â€™ í•¨ìˆ˜ë¡œ ì¤‘ì‹¬í™”(center)í•˜ê¸° s(x)ëŠ” t ì™€ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ Ï„ ì—ì„œ â€œx ì— ë”°ë¥¸ í‰ê· ì¹˜â€ë§Œ ì™ ë¹¼ë²„ë¦¬ë©´ s(x)ê°€ ë” ì´ìƒ ë¶™ì„ ìˆ˜ ì—†ê² ì£ ? ê·¸ê²Œ ë°”ë¡œ $$\\tilde{\\tau}(x,t) = \\tau(x,t) âˆ’ E[ \\tau(X,T) X=x ]$$ ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ tilde tau ëŠ” ì–´ë–¤ x ì— ëŒ€í•´ì„œë„ t ê°’ ì „ì²´ë¥¼ í‰ê·  ë‚´ë©´ 0 ì´ ë©ë‹ˆë‹¤. ì¦‰ â€œë§ë¶™ì¼ ìˆ˜ ìˆëŠ” s(x)â€ ì—¬ì§€ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤. Why Do We Introduce the Centered Function tilde_tau(x,t)? ì •ê·œí™”ì™€ì˜ ì—°ê²° L2 ì •ê·œí™”(Tikhonov)ë¡œ $$argmin_h { R-loss(h) + \\rhoâ€–hâ€–_2^2 }$$ ë¥¼ í’€ë©´ ìœ ì¼í•œ í•´ $\\tau_\\rho$ ê°€ ë‚˜ì˜µë‹ˆë‹¤. Theorem 1 ì€ â€œê·¸ ìœ ì¼í•œ í•´ê°€ ë°”ë¡œ tilde tau ë¥¼ (1+Ï) ë¡œ ë‚˜ëˆˆ ê²ƒâ€ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¦‰ $\\tilde{\\tau} = (1+\\rho)\\tau_\\rho$. ê²°êµ­ 1ë‹¨ê³„ì—ì„œ ì •ê·œí™”ë¡œ ìœ ì¼í•˜ê²Œ ì°¾ì€ Ï„_Ï ë¡œë¶€í„° 2ë‹¨ê³„ì—ì„œ ë‹¤ì‹œ (1+Ï)ì„ ê³±í•´ tilde tau ë¥¼ ë³µì›í•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ zero-constraining operator ë¡œ ì›ë˜ Ï„ ë¥¼ ì–»ìŠµë‹ˆë‹¤. í•œ ì¤„ ê²°ë¡  ì¤‘ê°„ í•¨ìˆ˜ tilde tau ë¥¼ ì“°ëŠ” ì´ìœ ëŠ” â€œx ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë¶ˆí•„ìš”í•œ s(x)ë¥¼ ì œê±°í•´ ì‹ë³„ ë¬¸ì œë¥¼ ê¹¨ë—í•˜ê²Œ ë§Œë“¤ì–´ ì£¼ê¸°â€ì…ë‹ˆë‹¤. ê·¸ë ‡ê²Œ ì¤‘ì‹¬í™”í•œ ë’¤ ì •ê·œí™”ë¥¼ ê±¸ì–´ì•¼ ìœ ì¼í•œ í•´ë¥¼ ì•ˆì „í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆì–´ìš”. ì¶”ì •ê°’ì´ ì‘ì•„ì§€ëŠ” ê±¸ ë³´ì™„í•˜ê¸° ìœ„í•´ (1+Ï)ì„ ê³±í•˜ëŠ” ì´ìœ  ì•„ë˜ ìˆœì„œëŒ€ë¡œ â€˜ì™œ (1 + Ï)ë¥¼ ê³±í•´ì•¼ í•˜ë‚˜?â€™ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í’€ì–´ë³¼ê²Œìš”. (ì—¬ê¸°ì„œ ÏëŠ” ì •ê·œí™” ì„¸ê¸°, pë¼ê³  ì“°ì˜€ë˜ ê¸°í˜¸ì…ë‹ˆë‹¤.) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì•„ì£¼ ë‹¨ìˆœí•œ 1-ì°¨ì› ì˜ˆë¡œ ì‹œì‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ ëª©í‘œ : ì–´ë–¤ â€œì§„ì§œ ê°’â€ Î¸ ë¥¼ ì•Œì•„ë‚´ê³  ì‹¶ë‹¤. â€¢ í•˜ì§€ë§Œ ë¬¸ì œ(ë°ì´í„°)ê°€ ë¶ˆì•ˆì •í•´ì„œ ì •ê·œí™”ë¥¼ ë„£ì–´ í’€ê¸°ë¡œ í–ˆë‹¤. ì •ê·œí™”ëœ ìµœì†Œí™” ë¬¸ì œ (í‰ê· ì œê³±ì˜¤ì°¨ + Ï Ã— ê°’^2) â†“ F(h) = (h âˆ’ Î¸)Â² / 2 + Ï hÂ² â†’ ì—¬ê¸°ì„œ hê°€ ìš°ë¦¬ê°€ êµ¬í•  ì¶”ì •ì¹˜. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ ë¬¸ì œë¥¼ ì§ì ‘ í’€ì–´ë³´ì â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ F(h)ë¥¼ hì— ëŒ€í•´ ë¯¸ë¶„í•´ì„œ 0ìœ¼ë¡œ ë‘ë©´ (h âˆ’ Î¸) + 2Ïh = 0 â‡’ h (1 + 2Ï) = Î¸ â‡’ h = Î¸ / (1 + 2Ï) â€» ë…¼ë¬¸Â·í¬ìŠ¤íŠ¸ì—ì„œëŠ” 2 ëŒ€ì‹  1ì´ ë¶™ë„ë¡ (1/2)ê³„ìˆ˜ ë“±ìœ¼ë¡œ ë§ì¶° ë†“ì•˜ê¸° ë•Œë¬¸ì— ìµœì¢…ê³„ìˆ˜ëŠ” â€œ1 + Ïâ€ê°€ ë©ë‹ˆë‹¤. ì´ë¦„ë§Œ ë‹¤ë¥´ê³  ë³¸ì§ˆì€ ê°™ì•„ìš”. ê²°êµ­ ì •ê·œí™”ë¥¼ ë„£ê³  ìµœì†Œí™”í•˜ë©´ h = Î¸ / (1 + Ï) ì¦‰, ì§„ì§œ ê°’ì´ (1 + Ï)ë°°ë§Œí¼ ì‘ì•„ì ¸ ë²„ë¦° ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™œ ì‘ì•„ì§€ë‚˜? ì§ê´€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ ì •ê·œí™” í•­ Ï hÂ² ëŠ” â€œh ê°’ì„ 0ì— ê°€ê¹ê²Œ ëŒì–´ë‹¹ê¸°ëŠ” ìŠ¤í”„ë§â€ ê°™ì€ ì—­í• . â€¢ ì†ì‹¤ì‹ì€ â‘  (hâˆ’Î¸)Â² â†’ â€œì§„ì§œ ê°’ Î¸ì™€ ê°€ê¹Œì›Œì ¸ë¼â€ â‘¡ Ï hÂ² â†’ â€œê°’ì´ í´ìˆ˜ë¡ í˜ë„í‹°, 0ì´ ì¢‹ë‹¤â€ ë‘ í˜ì˜ íƒ€í˜‘ì ì´ Î¸ì˜ ì¶•ì†ŒíŒ h = Î¸/(1+Ï) ë¡œ ë‚˜íƒ€ë‚œ ê²ƒ. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¶•ì†Œ(shrinkage) í¸í–¥ì„ ì–´ë–»ê²Œ ì—†ì•¨ê¹Œ? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°©ë²•ì€ ê°„ë‹¨í•˜ë‹¤. ì •ê·œí™”ë¥¼ ë„£ì–´ì„œ ì–»ì€ ê²°ê³¼ hë¥¼ ë‹¤ì‹œ (1 + Ï)ë°° í‚¤ì›Œ ì£¼ë©´ ëœë‹¤. Î¸ (ì›ë˜ ê°’) = (1 + Ï) Ã— h (ì •ê·œí™” í•´) ê·¸ë˜ì„œ ì •ê·œí™” ë’¤ ë³´ì • ë‹¨ê³„ì—ì„œ (1 + Ï)ë¥¼ ê³±í•œë‹¤. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•¨ìˆ˜(tilde Ï„, Ï„Ï) ë¡œ ëŒì•„ì™€ì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ í•¨ìˆ˜ ì „ì²´ê°€ ìŠ¤ì¹¼ë¼ Î¸ ëŒ€ì‹  â€œtilde_tau(x,t)â€ ë¼ê³  ìƒê°í•˜ë©´ ìœ„ ê³„ì‚°ì´ ì ë§ˆë‹¤ ë™ì‹œì— ì¼ì–´ë‚œë‹¤. â€¢ ì •ê·œí™”ê°€ ëë‚˜ê³  ì–»ì€ í•¨ìˆ˜ Ï„Ï(x,t)ëŠ” tilde_tau(x,t) / (1 + Ï). â€¢ ê·¸ëŸ¬ë¯€ë¡œ ì›í•˜ëŠ” tilde_tauë¥¼ ë˜ì°¾ìœ¼ë ¤ë©´ tilde_tau = (1 + Ï) Â· Ï„Ï. â† ë°”ë¡œ ì´ ì‹ì´ì—ˆì£ . â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€˜ê¼­ ê³±í•´ì•¼ í•˜ë‚˜?â€™ì— ëŒ€í•œ ê²°ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ ì ˆëŒ€ í¬ê¸°ê°€ ì¤‘ìš”í•œ ë¬¸ì œ(ì˜ˆ: ì˜ì•½ ìš©ëŸ‰, ê°€ê²© ì±…ì •)ë¼ë©´ ì¶•ì†Œëœ ê°’ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ íš¨ê³¼ë¥¼ ê³¼ì†Œí‰ê°€ â†’ ì˜ëª»ëœ ì˜ì‚¬ê²°ì •. â€¢ Ïê°€ ì•„ì£¼ ì‘ì•„ì„œ (1+Ï) â‰ˆ 1 ë¼ë©´ ìƒëµí•´ë„ í° ì˜í–¥ì€ ì—†ìŒ. â€¢ ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ìœ¼ë¡œëŠ” ì •ê·œí™” â†’ ì¶•ì†Œ â†’ (1+Ï) ë°° ë˜ëŒë¦¬ê¸° ìˆœì„œë¥¼ ì§€ì¼œ ì£¼ëŠ” ê²ƒì´ í¸í–¥ ì—†ëŠ” ì¶”ì •ì„ ë³´ì¥í•œë‹¤. ì¦‰, (1 + Ï)ë¥¼ ê³±í•´ ì£¼ëŠ” ì´ìœ ëŠ” â€œì •ê·œí™”ê°€ ë§Œë“  â€˜ì¶•ì†Œ í¸í–¥â€™ì„ ì •í™•íˆ ë°˜ëŒ€ë¡œ ìƒì‡„í•´ ì›ë˜ ìŠ¤ì¼€ì¼(tilde_tau), ë” ë‚˜ì•„ê°€ ìµœì¢… Ï„(x,t)ì„ ë˜ì°¾ê¸° ìœ„í•´ì„œâ€ì…ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-10-ê²©ë ¤ì™€ì‹ ë¢°í•œë‹¤ëŠ”í‘œí˜„",
    "title": "ê²©ë ¤ì™€ ì‹ ë¢°í•œë‹¤ëŠ” í‘œí˜„",
    "date": "2025-07-10",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ ê²ƒë“¤ ì •ë¦¬ I believe in you. I believe in your ability to come out ahead. I believe in your ability to stay focused. I believe in your ability to persevere. You've made it this far. You're almost there. ê°„ë‹¨í•œ ì‘ë¬¸ As I did yesterday (ì–´ì œì²˜ëŸ¼), I wanna talk to myself. Because I need some ê²©ë ¤. I believe in you MK. I believe in your ability to come out ahead. ì§€ê¸ˆì€ í˜ë“¤ë”ë¼ë„, you will made it. Youve made it this far. Just focus on today, one step ahead. Youre almost there. í”¼ë“œë°± ì˜í•œ ì ë“¤ âœ¨ í•™ìŠµí•œ í‘œí˜„ í™œìš©: ì˜¤ëŠ˜ ë°°ìš´ \"I believe in you\" ì‹œë¦¬ì¦ˆë¥¼ ì‹¤ì œ ì‘ë¬¸ì— ë°”ë¡œ ì ìš©í•œ ì ì´ í›Œë¥­í•©ë‹ˆë‹¤ ê°ì • í‘œí˜„: ìì‹ ì—ê²Œ ê²©ë ¤ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì†”ì§í•œ ë§ˆìŒì„ ì˜ì–´ë¡œ í‘œí˜„í•œ ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤ ê°œì¸ì  ë©”ì‹œì§€: MKë¼ëŠ” ì´ë¦„ì„ ë„£ì–´ì„œ ë” ê°œì¸ì ì´ê³  ì§„ì‹¬ì–´ë¦° ëŠë‚Œì„ ë§Œë“¤ì–´ëƒˆìŠµë‹ˆë‹¤ ê°œì„ í•  ë¶€ë¶„ ğŸ”§ ë¬¸ë²• ì˜¤ë¥˜: you will made it â†’ you will make it (ë™ì‚¬ì›í˜• ì‚¬ìš©) Youve â†’ You've (ì˜¬ë°”ë¥¸ ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ì‚¬ìš©) ì–´ì¡° í†µì¼: wanna â†’ want to (ì¢€ ë” ê²©ì‹ìˆëŠ” í‘œí˜„) í•œì˜ í˜¼ìš©ì„ ì¢€ ë” ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ ê°œì„ : one step ahead â†’ one step at a time` (ë” ì¼ë°˜ì ì¸ í‘œí˜„) ìˆ˜ì • ì œì•ˆ ğŸ“ ì›ë¬¸ ìœ ì§€í•˜ë˜ ì´ë ‡ê²Œ ì“¸ ìˆ˜ë„ ìˆì–´ìš”: As I did yesterday, I want to talk to myself in English. Because I need some encouragement right now. I believe in you, MK. I believe in your ability to come out ahead. Even though it's tough right now, you will make it through. You've made it this far, and that's already incredible. Just focus on today, one step at a time. You're almost there. ì´í‰ ğŸ’­ ì˜ì–´ë¡œ ìì‹ ì—ê²Œ ê²©ë ¤í•˜ëŠ” ìŠµê´€ì´ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤! ë°°ìš´ í‘œí˜„ì„ ë°”ë¡œ ì‚¬ìš©í•˜ë ¤ëŠ” ì ê·¹ì„±ì´ ì¸ìƒì ì´ì—ìš”. ë¬¸ë²•ì ì¸ ë¶€ë¶„ë§Œ ì¡°ê¸ˆì”© ë‹¤ë“¬ì–´ê°€ë©´ ë”ìš± ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ í‘œí˜„ë ¥ì„ ê¸°ë¥¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê³„ì† ì´ëŸ° ì‹ìœ¼ë¡œ ì—°ìŠµí•˜ì„¸ìš”! ğŸŒŸ",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-10-lunit-cancer-screening",
    "title": "(Seoul) Research Engineer Â· AutoML ",
    "date": "2025-07-10",
    "excerpt": "ë£¨ë‹› ì±„ìš©ê³µê³  ë¶„ì„ê³¼ ì¤€ë¹„ê³¼ì •",
    "category": "Career",
    "tags": [
      "ë£¨ë‹› ì±„ìš©ê³µê³ "
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´</strong></summary> ë§í¬ Lunit, a portmanteau of â€˜Learning unit,' is a medical AI software company devoted to providing AI-powered total cancer care. Our AI solutions help discover cancer and predict cancer treatment outcomes, achieving timely and individually tailored cancer treatment. ğŸ—¨ï¸ About The Team AutoML team at Lunit automates AI product development processes to streamline time-consuming tasks and advance cutting-edge AutoML research. By combining engineering expertise with state-of-the-art deep learning techniques, our team plays an important role in accelerating product development for cancer detection and treatment. Join us in our mission to Conquer Cancer Through AI where your contributions will directly influence AI innovations that improve patient outcomes worldwide In short, we're a team that maximizes AI tech to create AI models with extreme efficiency ğŸ—¨ï¸ About The Position As a research engineer, you will work closely with the team to develop and optimize platforms for automating medical AI model research workflows Your responsibilities will involve implementing an automated product development framework for the medical domain, covering data ingestion, model training, packaging and inference You will collaborate with both research and engineering teams to bring cutting-edge AI research into real-world applications This position is ideal for someone with a strong research engineering background and deep understanding of the AI model development process ğŸš© Roles & Responsibilities Develop and implement tailored AutoML platforms to automate medical AI model research workflows Integrate AutoML pipelines into AI agent which includes data ingestion, model training, hyperparameter optimization and inference Enhance the maintainability, reliability, and efficiency of both new and existing frameworks Build robust AI infrastructure using Kubernetes, Docker, and cloud computing services (GCP, AWS, Azure) Keep up with the latest advancements in AI agents, LLMs, and AutoML research to drive innovation ğŸš© Tools Used Development Related: Django, Django REST Framework, React, Next.js, MySQL, PostgreSQL, Redis, Celery, Nginx, Go ML framework: PyTorch, Optuna Infrastructure: Google Cloud Platform, Kubernetes, Git, Docker, Helm, ArgoCD, Terraform General: Slack, Confluence, Jira Requirements ğŸ¯Qualifications 3+ years of experience in research engineering in the AI industry Master's, or Ph.D. in Computer Science or a related field Proficiency in Python, unit/integration testing, documentation, Git, collaborative code development, and Docker Solid knowledge of software design and system architecture, with a strong understanding of RESTful API design Hands-on experience with developing and operating scalable AI platforms and infrastructure, including DevOps Effective communication skills: Ability to clearly and efficiently convey information, thoughts, and ideas to other developers Proven ability to take ownership and drive projects from concept to deployment Highly responsible and detail-oriented, with a strong motivation to build high-quality, reliable solutions in line with current best practices ğŸ… Preferred Experiences Familiarity with web development, including both backend and frontend Deep insight into automating and orchestrating ML workflows Experience in designing or developing automated AI platforms Experience in designing or developing AI agents or LLM-based automation (e.g., fine-tuning, prompt engineering) Knowledge of cloud computing services (GCP, AWS, or Azure) Publications in AI or computer vision conferences, or active participation in the research community Ability to collaborate effectively as a team player in a cross-functional research-engineering environment Proactive in sharing knowledge, initiating collaborations, and promoting a positive research environment Passion for high-quality programming and software engineering to produce and maintain reliable code for the training and evaluation of models ğŸ“ How To Apply CV (resume, free format) ğŸƒâ€â™€ï¸ Hiring Process Document Screening â†’ Technical Interview(Teams) â†’ Assignment â†’ PT Interview â†’ Culture-fit Interview â†’ Onboarding After the final interview, we may proceed with reference checks if needed. ğŸ¤ Work Conditions and Environment Work type: Full Time Work location : Lunit HQ (5F, 374, Gangnam-daero, Gangnam-gu, Seoul) Salary: After Negotiation ğŸ¸ ETC If you misrepresent your experience or education or provide false or fraudulent information in or with your application, it may be grounds for cancellation of the employment Lunit is committed in providing the preferential processing to those eligible for employment protection (national merits and people with disabilities) relevant to related laws and regulations Benefits ğŸŒ» Benefits & Perks The office is at a very convenient location, just a minute away from Gangnam Station Exit 3 Meal Allowance is provided (up to 12,000 KRW per meal) when working at the office Up to 300,000 KRW is covered upon joining to decorate your personal workspace Latest computer models, such as Macs and 4K monitors are provided and can be renewed every three years Seminar registration fees and book purchases are covered Regular in-house AI and medical seminars are held Korean lessons are provided for Lunitians who do not speak Korean as their first language In-house English lessons (aka Luniversal) is provided for English development Access to high-quality AI learning resources & deep learning DevOps system Up to 1.2 million KRW worth of benefits points can be claimed annually Holiday Allowances are provided in the form of gifts or vouchers for Korean National holidays, Seollal and Chuseok Congratulatory and Condolence allowances, along with paid time off are provided Annual medical checkups and employee accident insurance are provided Expenses for monthly employees gatherings are partially covered </details> ì¤€ë¹„í•´ì•¼í•˜ëŠ” ê²ƒë“¤ ì¼ë‹¨ ë‚´ ë¶„ì•¼ì™€ ê²¹ì¹˜ëŠ” ì˜ë£Œìª½ ì±„ìš©ê³µê³ ê³ , ì—¬ê¸°ì„œ í–ˆë˜ MLê°œë°œë“¤ì— ëŒ€í•œ ê²½í—˜ì„ ì‚´ë¦´ ìˆ˜ ìˆê² ë‹¤. ì´ê²ƒë„ ì´ë²ˆì£¼ ì£¼ë§ì— ì´ê±°ì— ë§ê²Œ ì´ë ¥ì„œ ì“°ê¸°. ì—¬ê¸°ì— ë„ˆê°€ ìƒê°í•˜ëŠ” ì¤€ë¹„í•˜ë©´ ì¢‹ì€ ê²ƒë“¤ ì¶”ê°€í•´ì¤˜ ğŸ¯ í•µì‹¬ ì¤€ë¹„ì‚¬í•­ AutoML ì „ë¬¸ì„± ê°•í™” Optuna ì‹¤ìŠµ: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í”„ë ˆì„ì›Œí¬ ê²½í—˜ ìŒ“ê¸° AutoML íŒŒì´í”„ë¼ì¸: MLflow, DVC ë“±ì„ í™œìš©í•œ ML ì›Œí¬í”Œë¡œìš° ìë™í™” ê²½í—˜ Neural Architecture Search (NAS) ê´€ë ¨ ë…¼ë¬¸ ë° êµ¬í˜„ ê²½í—˜ Meta-learning ê¸°ì´ˆ ê°œë… í•™ìŠµ ì˜ë£Œ AI ë„ë©”ì¸ ì§€ì‹ ì˜ë£Œ ì˜ìƒ ë¶„ì„: DICOM í¬ë§·, ì˜ë£Œ ì˜ìƒ ì „ì²˜ë¦¬ ê¸°ë²• í•™ìŠµ Cancer detection papers: ë£¨ë‹›ì˜ ì£¼ìš” ë…¼ë¬¸ë“¤ ì½ì–´ë³´ê¸° (arXiv, ì˜ë£Œ AI ì»¨í¼ëŸ°ìŠ¤) FDA ìŠ¹ì¸ AI ì˜ë£Œê¸°ê¸°: ê·œì œ ìš”êµ¬ì‚¬í•­ ë° í’ˆì§ˆ ê´€ë¦¬ ê¸°ì¤€ ì´í•´ Medical imaging datasets: ChestX-ray, MIMIC ë“± ê³µê°œ ë°ì´í„°ì…‹ ê²½í—˜ ì¸í”„ë¼/DevOps ìŠ¤í‚¬ ì—…ê·¸ë ˆì´ë“œ Kubernetes ì‹¤ìŠµ: ë¡œì»¬ í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ë° ML ì›Œí¬ë¡œë“œ ë°°í¬ Docker ê³ ê¸‰ í™œìš©: Multi-stage builds, ìµœì í™” ê¸°ë²• Cloud ML ì„œë¹„ìŠ¤: GCP AI Platform, Vertex AI ì‹¤ìŠµ CI/CD for ML: GitHub Actionsë¡œ ëª¨ë¸ í•™ìŠµ/ë°°í¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì›¹ ê°œë°œ ìŠ¤í‚¬ (Preferred ìš”êµ¬ì‚¬í•­) Django REST Framework: API ê°œë°œ ì‹¤ìŠµ React/Next.js ê¸°ì´ˆ: ê°„ë‹¨í•œ ML ëª¨ë¸ ë°ëª¨ í˜ì´ì§€ êµ¬ì¶• ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL, Redis í™œìš© ê²½í—˜ í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ ì œì•ˆ ì˜ë£Œ ì˜ìƒ AutoML í”Œë«í¼: ê°„ë‹¨í•œ AutoML ì‹œìŠ¤í…œ êµ¬ì¶• MLOps íŒŒì´í”„ë¼ì¸: ëª¨ë¸ í•™ìŠµë¶€í„° ë°°í¬ê¹Œì§€ ìë™í™” API ì„œë²„: ML ëª¨ë¸ ì„œë¹™ REST API ê°œë°œ Kubernetes ë°°í¬: ìœ„ í”„ë¡œì íŠ¸ë“¤ì„ K8së¡œ ë°°í¬ ì´ë ¥ì„œ ì‘ì„± í¬ì¸íŠ¸ ì •ëŸ‰ì  ì„±ê³¼: ëª¨ë¸ ì„±ëŠ¥ ê°œì„ , ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•, ì¸í”„ë¼ ë¹„ìš© ì ˆê° ë“± ìˆ˜ì¹˜ë¡œ í‘œí˜„ ì˜ë£Œ AI ê´€ë ¨ ê²½í—˜: ì§ì ‘ì ì´ì§€ ì•Šë”ë¼ë„ í—¬ìŠ¤ì¼€ì–´ ë°ì´í„° ë‹¤ë£¬ ê²½í—˜ ê°•ì¡° AutoML/MLOps í‚¤ì›Œë“œ: íŒŒì´í”„ë¼ì¸ ìë™í™”, ëª¨ë¸ ìµœì í™” ê²½í—˜ ë¶€ê° í˜‘ì—… ê²½í—˜: ì—°êµ¬íŒ€ê³¼ ê°œë°œíŒ€ ê°„ ë¸Œë¦¿ì§€ ì—­í•  ê²½í—˜ ë©´ì ‘ ì¤€ë¹„ ê¸°ìˆ  ë©´ì ‘: AutoML ê°œë…, PyTorch ì‹¬í™”, ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì œ ê³¼ì œ: ê°„ë‹¨í•œ AutoML íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë˜ëŠ” ì˜ë£Œ ì˜ìƒ ë¶„ì„ íƒœìŠ¤í¬ ì˜ˆìƒ PT ë©´ì ‘: ë³¸ì¸ì˜ í”„ë¡œì íŠ¸ë¥¼ AutoML ê´€ì ì—ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„ ì»¬ì²˜í•: ì˜ë£Œ AIë¡œ í™˜ìì—ê²Œ ë„ì›€ì´ ë˜ê³  ì‹¶ë‹¤ëŠ” ë™ê¸°, ì—°êµ¬ì™€ ê°œë°œ ì‚¬ì´ì˜ ê· í˜•ê° ì¶”ê°€ í•™ìŠµ ìë£Œ ë…¼ë¬¸: \"Efficient Neural Architecture Search via Parameter Sharing\" (ENAS) ì±…: \"Hands-On Machine Learning\" (AutoML ì±•í„°) ê°•ì˜: CS330 (Stanford Meta-Learning), MLOps ê´€ë ¨ ì˜¨ë¼ì¸ ê°•ì˜ ë¸”ë¡œê·¸: Google AI Blogì˜ AutoML ê´€ë ¨ í¬ìŠ¤íŠ¸ë“¤ ë„¤íŠ¸ì›Œí‚¹ ì˜ë£Œ AI ì»¤ë®¤ë‹ˆí‹°: í•œêµ­ ì˜ë£Œ AI í•™íšŒ, ê´€ë ¨ ë°‹ì—… ì°¸ì—¬ ë£¨ë‹› ì§ì›: LinkedInìœ¼ë¡œ í˜„ì§ìì™€ ì»¤í”¼ì±— ìš”ì²­ ì»¨í¼ëŸ°ìŠ¤: MICCAI, SPIE Medical Imaging ë“± ì˜ë£Œ AI ì»¨í¼ëŸ°ìŠ¤ ë…¼ë¬¸ ì½ê¸° ğŸ“… ì£¼ì°¨ë³„ ì¤€ë¹„ ê³„íš (4ì£¼ ê°€ì •) 1ì£¼ì°¨: AutoML ê¸°ì´ˆ í•™ìŠµ + Optuna ì‹¤ìŠµ 2ì£¼ì°¨: ì˜ë£Œ AI ë…¼ë¬¸ ë¦¬ë·° + Docker/K8s ì‹¤ìŠµ 3ì£¼ì°¨: í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ êµ¬í˜„ 4ì£¼ì°¨: ì´ë ¥ì„œ ì™„ì„± + ë©´ì ‘ ì¤€ë¹„ ë‚˜ì˜ ê´€ë ¨ ì§ë¬´ê²½í—˜ê³¼ í•„ì‚´ê¸° ì¤€ë¹„í•˜ê¸°",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-10-hyundai-autoever",
    "title": "í˜„ëŒ€ ì˜¤í† ì—ë²„ MLOps / AI ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´",
    "date": "2025-07-10",
    "excerpt": "í˜„ëŒ€ ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  ë¶„ì„ê³¼ ì¤€ë¹„ê³¼ì •",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³ "
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´</strong></summary> [Tech] Machine Learning Engineer - MLOps / AI ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ ğŸ“ƒ ëˆ„êµ¬ë‚˜ â€‹ë§ˆìŒ â€‹ì†ì— â€‹ì´ë ¥ì„œ í•œ â€‹ì¥ì€ ìˆìœ¼ë‹ˆê¹Œ, âš¡ 1ë¶„ â€‹ì»· â€‹ì§€ì›ìœ¼ë¡œ í˜„ëŒ€ì˜¤í† ì—ë²„ë¡œì˜ â€‹ì—¬ì •ì„ ì‹œì‘í•˜ì„¸ìš”. (ì´ë ¥ì„œ â€‹ììœ¨ì–‘ì‹) âœ… MLOps â€‹/ â€‹AI ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ ğŸš€ â€‹í•©ë¥˜í•˜ì‹¤ â€‹íŒ€ì„ â€‹ì†Œê°œí•´ìš” ì–¸ì–´AIê¸°ìˆ íŒ€ ğŸ’» í•©ë¥˜í•˜ì‹œë©´ í•¨ê»˜ â€‹í•  â€‹ì—…ë¬´ì˜ˆìš” AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤ë¥¼ â€‹ìœ„í•œ â€‹ë°ì´í„°ë² ì´ìŠ¤ â€‹ì„¤ê³„ ë° ê°œë°œ AI â€‹ê²€ìƒ‰ ì—”ì§„ â€‹ê°œë°œ â€‹ë° ìš´ì˜ MLOps â€‹êµ¬ì¶• ë° â€‹ìš´ì˜ ğŸ” â€‹ì´ëŸ° ë¶„ê³¼ í•¨ê»˜ â€‹í•˜ê³  ì‹¶ì–´ìš” ê²€ìƒ‰ â€‹/ ì±—ë´‡ ê´€ë ¨ ëª¨ë“ˆ ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ Docker / Kubernetes í™œìš© ê°œë°œ ë° ë°°í¬ ê²½í—˜ Python / JAVA ê°œë°œ ê²½ë ¥ 3ë…„ ì´ìƒ í˜¹ì€ ê·¸ì— ì¤€í•˜ëŠ” ì‹¤ë ¥ ğŸ” ì´ëŸ° ë¶„ì´ë¼ë©´ ë”ìš± ì¢‹ì•„ìš” ì •ë³´ê²€ìƒ‰/SWê³µí•™ ê´€ë ¨ ì„ì‚¬ ì´ìƒ ELK(Elasticsearch, Kibana, Kubernetes, Kafka) êµ¬ì„± ì„¤ê³„ ë° ìš´ì˜ ê²½í—˜ì Neo4j / Redis ê¸°ë°˜ DB ì„¤ê³„ ë° êµ¬ì¶• ê²½í—˜ì ë²¡í„° DB (Milvus / qdrant / faiss ë“±) ê²½í—˜ì ES í”ŒëŸ¬ê·¸ì¸ ê°œë°œ ê²½í—˜ì âŒ› ì´ë ‡ê²Œ í•©ë¥˜í•´ìš” ì„œë¥˜ ì ‘ìˆ˜ â†’ ì„œë¥˜ ê²€í†  â†’ ì§ë¬´ì—­ëŸ‰í…ŒìŠ¤íŠ¸(ì½”ë”© ë˜ëŠ” ê³¼ì œí…ŒìŠ¤íŠ¸) ë° ì¸ì„±ê²€ì‚¬ â†’ 1ì°¨ë©´ì ‘ â†’ 2ì°¨ë©´ì ‘ â†’ ì²˜ìš°í˜‘ì˜ ë° ì±„ìš©ê²€ì§„ â†’ ìµœì¢… í•©ê²©ğŸ‰ ğŸ“ ë§Œë‚˜ê²Œ ë  ê·¼ë¬´ì§€ëŠ” ì—¬ê¸°ì˜ˆìš” ì„œìš¸ ê°•ë‚¨ ğŸ“Œ ì°¸ê³ í•´ ì£¼ì„¸ìš” ì±„ìš© ì‹œ ë§ˆê°ë˜ëŠ” ìƒì‹œ ì±„ìš© ê³µê³ ë¡œ ìš´ì˜ë˜ë©°, ì±„ìš© ì ˆì°¨ì™€ ì¼ì •ì€ ë³€ë™ë  ìˆ˜ ìˆì–´ìš”. ì‚¬íšŒì  ë°°ë ¤ ëŒ€ìƒì(ë³´í›ˆ ì·¨ì—…ì§€ì›ëŒ€ìƒì, ì¥ì• ì¸)ëŠ” ê´€ê³„ ë²•ë ¹ê³¼ ë‚´ê·œì— ë”°ë¼ ìš°ëŒ€í•´ìš”. ëª¨ì§‘ ë¶„ì•¼ ë° ë‹´ë‹¹ ì—…ë¬´ì— ë”°ë¼ ì˜ì–´ êµ¬ìˆ í‰ê°€, ë ˆí¼ëŸ°ìŠ¤ ì²´í¬, ë˜ëŠ” ê¸°íƒ€ ì „í˜•ì´ ì‹¤ì‹œë  ìˆ˜ ìˆì–´ìš”. ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ë¥¸ í¬ì§€ì…˜ì´ ë” ì í•©í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ê²½ìš° ì§€ì› ë¶„ì•¼ê°€ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ë°°ì¹˜ ë¶€ì„œ ë° ê·¼ë¬´ì§€ëŠ” íšŒì‚¬ ì‚¬ì •ì— ë”°ë¼ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ì•„ë˜ì˜ ê²½ìš°, í•©ê²©ì´ ì·¨ì†Œë˜ê±°ë‚˜ ì „í˜• ì§„í–‰ì—ì„œ ë¶ˆì´ìµì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì–´ìš”. ì§€ì›ì„œê°€ ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ê±°ë‚˜ ì¦ë¹™ì´ ë¶ˆê°€í•  ê²½ìš° í•´ì™¸ì—¬í–‰ ê²°ê²© ì‚¬ìœ ê°€ ìˆëŠ” ê²½ìš° (ë‚¨ì„±ì˜ ê²½ìš°, íšŒì‚¬ê°€ ì§€ì •í•œ ì…ì‚¬ì¼ê¹Œì§€ ë³‘ì—­ í•„ ë˜ëŠ” ë©´ì œ í•„ìš”) ìµœì¢… í•©ê²© í›„ íšŒì‚¬ê°€ ì§€ì •í•˜ëŠ” ì…ì‚¬ì¼ì— ì…ì‚¬ ë¶ˆê°€í•œ ê²½ìš° </details> ì¤€ë¹„í•´ì•¼í•˜ëŠ” ê²ƒë“¤ ê¸°ë³¸ ê³„íš âœ… ì´ë ¥ì„œ & í¬íŠ¸í´ë¦¬ì˜¤ ì¤€ë¹„ ë‚´ê°€ í•œ ê²½í—˜ë“¤ ì¤‘ì—ì„œ ì±„ìš©ê³µê³ ì— í•„ìš”í•œ ê²½í—˜ ë½‘ì•„ì„œ ì •ë¦¬ ì´ë ¥ì„œë‘ í¬íŠ¸í´ë¦¬ì˜¤ ë§Œë“¤ê¸° ì½”ë”©í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ê¸°ì¶œë¬¸ì œ ë½‘ì•„ì„œ ì •ë¦¬í•˜ê³  ëª¨ë¥´ëŠ” ê°œë… ì •ë¦¬í•˜ê¸° ë³´ì™„ ê³„íš ğŸ’¡ ê¸°ìˆ  ìŠ¤íƒ ì‹¤ë¬´ ì¤€ë¹„ MLOps ë„êµ¬ ì‹¤ìŠµ Docker/Kubernetes ì‹¤ìŠµ í”„ë¡œì íŠ¸ CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê²½í—˜ ê²€ìƒ‰ ì—”ì§„ í•™ìŠµ Elasticsearch í•™ìŠµ ë° ì‹¤ìŠµ ê²€ìƒ‰ ì—”ì§„ í† ì´ í”„ë¡œì íŠ¸ êµ¬ì¶• ë²¡í„° DB ê²½í—˜ Milvus, qdrant ë“± ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í•™ìŠµ êµ¬ì²´ì  ì‹¤í–‰ ê³„íš í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ ëª©ë¡ ì‘ì„± MLOps íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í”„ë¡œì íŠ¸ AI ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„ í”„ë¡œì íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ê²½í—˜ ì •ë¦¬ ì½”ë”©í…ŒìŠ¤íŠ¸ ì˜ì—­ ì„¸ë¶„í™” ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ (ì´ì§„íƒìƒ‰, ë¬¸ìì—´ ì²˜ë¦¬) ML ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì œ íƒ€ì„ë¼ì¸ ì„¤ì • 1ì£¼ì°¨: ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ ì´ˆì•ˆ ì‘ì„± 2-3ì£¼ì°¨: ê¸°ìˆ  ìŠ¤íƒ ì‹¤ìŠµ í”„ë¡œì íŠ¸ 4ì£¼ì°¨: ì½”ë”©í…ŒìŠ¤íŠ¸ ì§‘ì¤‘ ì¤€ë¹„ ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ ë¯¸ë‹ˆ MLOps íŒŒì´í”„ë¼ì¸: ê°„ë‹¨í•œ ML ëª¨ë¸ ë°°í¬ ìë™í™” ê²€ìƒ‰ ì—”ì§„ í”„ë¡œì íŠ¸: ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„ ë²¡í„° DB í™œìš©: ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ",
    "public": true
  },
  {
    "id": "Self_Development/Career/Graduate School/ëŒ€í•™ì›ì—ëŒ€í•œê³ ë¯¼",
    "title": "ëŒ€í•™ì›ì„ ì§„í•™í•´ì•¼í• ê¹Œ? í•˜ë©´ ì–´ë””ë¡œ?",
    "date": "2025-07-10",
    "excerpt": "ë‚˜ì˜ ë‹¤ìŒ ì„ íƒì§€ëŠ” ì–´ë””ë¡œ?",
    "category": "Career",
    "tags": [
      "ëŒ€í•™ì›"
    ],
    "content": "ì»¤ë¦¬ì–´ ì„ ë°°í˜•ì—ê²Œ ë¬¼ì–´ë³¸ ë‚´ìš©ë“¤ ã…‡ã…‡ ë‚´ê°€ ìì£¼ë³´ëŠ” ê´œì°®ì€ dsê´€ë ¨ ê³µê³ ì‚¬ì´íŠ¸ì¤„í…Œë‹ˆê¹Œ, ì¼ë‹¨ ìˆëŠ” ê³µê³ ë“¤ ì«˜-ì•… í›‘ì–´ë´ë´. ê·¸ë¦¬ê³  ê³µí†µì ì¸ í‚¤ì›Œë“œë“¤ì´ ìˆì„í…Œë‹ˆê¹Œ ê·¸ê±°ìœ„ì£¼ë¡œ ìƒê°í•´ë³´ë“ ê°€. (ex. LLM, RAG, agent, ML serving, triton ë“±) zighang offercent bzpp",
    "public": true
  },
  {
    "id": "Self_Development/Career/Graduate School/ê³ ë ¤ëŒ€ì•¼ê°„ëŒ€í•™ì›",
    "title": "ê³ ë ¤ëŒ€ ì•¼ê°„ëŒ€í•™ì› ê´€ë ¨ ì •ë¦¬",
    "date": "2025-07-10",
    "excerpt": "ê¸°íšŒë¹„ìš©ê³¼ ê¸°ëŒ€ë˜ëŠ” ê°€ì¹˜",
    "category": "Career",
    "tags": [
      "ëŒ€í•™ì›"
    ],
    "content": "ë°°ê²½ ì œê°€ ì›í•˜ëŠ” ì»¤ë¦¬ì–´ì˜ ë°©í–¥ì„ ìƒê°í•´ë³¼ ë•Œ ëŒ€í•™ì›ì— ëŒ€í•œ ì„ íƒì„ ëŠ˜ ê³ ë¯¼í•˜ê²Œ ë©ë‹ˆë‹¤. AI/ML ë¦¬ì„œì²˜ë‚˜ ê´€ë ¨ ì—°êµ¬ë¥¼ í•˜ëŠ” ì¼ë“¤ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ìµœì†Œ ì„ì‚¬ ì´ìƒì˜ í•™ë ¥ì„ ìš”êµ¬í•˜ëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëŠ¦ì—ˆë‚˜? ì§€ê¸ˆ ë‚˜ì´ê°€ ë§Œìœ¼ë¡œ 26ì •ë„, ë‚´ë…„ ì „ê¸°ì— ì‹œì‘í•´ë„ 29~30ì— ëë‚˜ë‹ˆê¹Œ ê·¸ë ‡ê²Œ ëŠ¦ì€ ì‹œê¸°ë„ ì•„ë‹ˆë¼ê³  ìƒê°ì´ ë“­ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì´ê°€ ë” ë§ì•˜ë”ë¼ë„ ì´ê±¸ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ”ê²Œ ë¶„ëª…í•˜ë‹¤ë©´, ê¸°íšŒë¹„ìš©ë³´ë‹¤ ë” í¬ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´ ëŠ¦ì€ ì‹œê¸°ë€ ì—†ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ê¸°íšŒë¹„ìš© (ì‹œê°„ê³¼ ê°€ê²©) í•™ë¹„ê°€ ëŒ€ë ¥ 4ì²œì •ë„. ì¼ë‹¨ ëˆì´ ì •ë§ ë§ì´ ë“­ë‹ˆë‹¤. ì•„ì§ 3ì²œë§Œì›ë„ ëª¨ì•„ë³¸ ì  ì—†ëŠ” ì œê°€ ì´ë ‡ê²Œ í° ë¹„ìš©ì„ ê°ë‹¹í•  ìˆ˜ ìˆì„ê¹Œ? ë¼ëŠ” ìƒê°ë„ ë“­ë‹ˆë‹¤. (í•™ìê¸ˆ ëŒ€ì¶œ ë°›ê³  ì¡°ê¸ˆì”© ê°šìœ¼ë©´ ì–¸ì  ê°„ ê°šê² ì£ ...?) ê·¸ë¦¬ê³  ì‹œê°„ë„ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. í‡´ê·¼ í›„ì—ëŠ” ì˜¨ì „íˆ ì‹œê°„ì„ ë‹¤ ìŸì•„ì•¼ í•˜ê³  ì¢‹ì€ ê²°ê³¼ë¥¼ ìœ„í•´ì„œëŠ” ì£¼ë§ì—ë„ ì´ ë¶€ë¶„ë§Œ ë³´ê³  ìˆì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. 2ë…„ ë°˜ì •ë„ì˜ ì‹œê°„ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ê¸°ëŒ€ë˜ëŠ” ê°€ì¹˜ ì œê°€ ì›í•˜ëŠ” ì»¤ë¦¬ì–´ì˜ ë°©í–¥ì— ë„ì›€ì´ ë©ë‹ˆë‹¤. í•™ë ¥ ì„ì‚¬ë¼ëŠ” í•™ë ¥ì€ ì‚¬ì‹¤ìƒ í•„ìˆ˜ì ì´ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ í•˜ë©´ì„œ ë§Œë‚˜ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì„ ë´ë„ ë‹¤ ì„ì‚¬ ì´ìƒì˜ í•™ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ ê·¸ë¦¬ê³  ì¢‹ì€ ë…¼ë¬¸ì„ ì‘ì„±í•  ìˆ˜ ìˆì–´ì•¼ë§Œ ë‹¨ìˆœ í•™ìœ„ì— ê·¸ì¹˜ì§€ ì•Šê³  ì§„ì§œ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì§€ì›ì‹œê¸°ì™€ ì¤€ë¹„í•  ê²ƒë“¤ ì˜¬í•´ í›„ê¸° ì§€ì›ì€ ë†“ì³¤ê³  ë‚´ë…„ ì „ê¸°ì— ì§€ì›í•˜ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¸”ë¡œê·¸ë“¤ì„ ì‚´í´ë³´ë‹ˆ ì¤€ë¹„í•´ì•¼í•˜ëŠ” ê±´ ê¸°ë³¸ì§€ì‹(í†µê³„, ì„ í˜•ëŒ€ìˆ˜ ë“±ë“±)ê³¼ ì—°êµ¬ê³„íšì„œ ì •ë„ì…ë‹ˆë‹¤. ê¸°ë³¸ì§€ì‹ í†µê³„, ì„ í˜•ëŒ€ìˆ˜ì™€ ê°™ì€ ì§ˆë¬¸ë“¤ì„ ë©´ì ‘ë•Œ ë°›ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤. ê²½ìŸë¥ ì´ ì•½ 6:1ì •ë„ ëœë‹¤ê³  í•˜ë‹ˆ, ì´ëŸ° ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ë¯¸ë¦¬ ì² ì €í•˜ê²Œ ì¤€ë¹„í•  í•„ìš”ê°€ ìˆì–´ë³´ì…ë‹ˆë‹¤. ì—°êµ¬ê³„íšì„œ ì´ ë¶€ë¶„ì— ëŒ€í•œ ê³ ë¯¼ì´ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. > ê·¸ë˜ì„œ ì–´ë–¤ ì—°êµ¬ë¥¼ í•˜ê³  ì‹¶ì€ê±°ì§€? ì´ ë¶€ë¶„ì´ ë§ì´ ë¹„ì–´ìˆì–´ì„œ ê¾¸ì¤€íˆ ì±„ì›Œë‚˜ê°€ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì°¸ê³  ìë£Œë“¤ Hello New World - ê³ ë ¤ëŒ€í•™êµ SW AI ëŒ€í•™ì› ë©´ì ‘ í›„ê¸° ê·€í‰ì´ ì„œì¬ - ê³ ë ¤ëŒ€í•™êµ AI ì•¼ê°„ëŒ€í•™ì› ë©´ì ‘ í›„ê¸° From data - ì¬ì§ì ë¹…ë°ì´í„°/ì¸ê³µì§€ëŠ¥ ëŒ€í•™ì› ì •ë¦¬",
    "public": true
  },
  {
    "id": "Self_Development/Career/25-07-10-visuworks-thoughts-of-my-career",
    "title": "ë‚˜ì˜ í˜„ì¬ ì»¤ë¦¬ì–´ ìƒí™©ì— ëŒ€í•œ ìƒê°",
    "date": "2025-07-10",
    "excerpt": "ë‚˜ì˜ í˜„ì¬ ì»¤ë¦¬ì–´ì— ëŒ€í•œ ê³ ì°°",
    "category": "Career",
    "tags": [
      "ë‚´ ì»¤ë¦¬ì–´ëŠ” ì–´ë””ë¡œ ê°€ì•¼í•˜ë‚˜"
    ],
    "content": "í˜„ì¬ ë‚˜ì˜ ìƒí™© ìš”ì¦˜ ì •ë§ ì»¤ë¦¬ì–´ì— ëŒ€í•œ ê±±ì •ê³¼ ê³ ë¯¼ì´ ë§ìŠµë‹ˆë‹¤... ã…ã… ì§€ê¸ˆ ë‹¤ë‹ˆê³  ìˆëŠ” íšŒì‚¬ì˜ ë„ë©”ì¸ì— ë¬¶ì—¬ìˆì§„ ì•Šì„ì§€. ì—¬ê¸°ì„œ ì´ë£¬ ì„±ê³¼ë“¤ì´ ë‚˜ì˜ ì»¤ë¦¬ì–´ì— ë„ì›€ì´ ë ì§€. ì‚¬ì‹¤ ìƒê°ë§Œí•˜ë©´ ì •ë¦¬ê°€ ì•ˆë˜ëŠ” ë¶€ë¶„ë“¤ì´ ë§ê¸° ë•Œë¬¸ì— ê¸€ë¡œ ì ì–´ë³´ë©´ì„œ ê³ ë¯¼ë“¤ì— ëŒ€í•œ ë‚˜ë¦„ì˜ ëŒ€ë‹µë“¤ì„ ì ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì €ëŠ” ì‚°ì—…ê³µí•™ì„ ì „ê³µí•˜ê³ , 6ê°œì›”ì •ë„ ë¶€íŠ¸ìº í”„ì—ì„œ ì»´í“¨í„° ë¹„ì „ ë¶€ë¶„ì„ ê³µë¶€í•˜ê³ , ì˜ë£Œ ë„ë©”ì¸ì—ì„œ ì¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë‹¤ë‹ˆê³  ìˆëŠ” íšŒì‚¬(visuworks)ëŠ” ì‹œë ¥êµì •ë³‘ì› (ë¹„ì•¤ë¹› ì•ˆê³¼)ë¥¼ ì£¼ëœ ê³ ê°ì‚¬ë¡œ ì‚¼ê³  ìˆê³  ì´ ë³‘ì›ì—ì„œ ë§Œë“¤ì–´ì¡Œë‹¤ê³ ë„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì§€ê¸ˆê¹Œì§€ í•œ í”„ë¡œì íŠ¸ëŠ” OCR pipeline ê°œë°œ ê³ ê° ìƒë‹´ìš© ì±—ë´‡ ê°œë°œ ë Œì¦ˆ ì‚¬ì´ì§• ì¶”ì²œ ì„œë¹„ìŠ¤ ê°œë°œ ì •ë„ê°€ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œ ìƒí™©ë“¤ ì œê°€ ìƒê°í•˜ëŠ” ë¬¸ì œë“¤ì„ ì •ë¦¬í•´ë³´ê³  ì–´ë–»ê²Œ í•´ê²°í•´ë³¼ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ìƒê°ë“¤ì„ ì •ë¦¬í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì—°ë´‰ì´ ì´ë ‡ê²Œ ì ë‚˜? ë˜ê²Œ ë‹¨ìˆœí•˜ê²Œ ì¼ì„ ì¼ë‹¨ ì‹œì‘í•˜ëŠ” ë§ˆìŒìœ¼ë¡œ ì´ íšŒì‚¬ì— ê³ ë¯¼ì—†ì´ ì…ì‚¬í–ˆëŠ”ë°, ì—°ë´‰ì´ ìƒê°ë³´ë‹¤ í˜„ì €íˆ ì ì–´ì„œ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ë” ì •í™•íˆëŠ” ì—°ë´‰ì´ ë‚®ì§€ë§Œ ì¼ë‹¨ ê²½ë ¥ì„ ìŒ“ìëŠ” ë§ˆìŒìœ¼ë¡œ ì…ì‚¬í–ˆìŠµë‹ˆë‹¤. 8ê°œì›”ì •ë„ ì¼í•˜ê³  ë‚˜ë¦„ 2ê°œì˜ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ëë‚¸ ì‹œì ì—ì„œ, ì—°ë´‰í˜‘ìƒì„ í–ˆì§€ë§Œ í° í­ìœ¼ë¡œ ì˜¤ë¥´ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤. > ì—°ë´‰ í…Œì´ë¸”ì´ ì¢‹ì€ íšŒì‚¬ë¡œ ë¹ ë¥´ê²Œ ì˜®ê²¨ì•¼ê² ë‹¤. í•˜ì§€ë§Œ ì‰½ì§€ ì•Šë„¤ìš”... ã…  ì§€ê¸ˆ 4~5ê°œì›”ì •ë„ ì´ì§ì„ ì¤€ë¹„í•˜ê³  ìˆëŠ”ë° ëŒ€ë¶€ë¶„ì˜ ê¸°ì—…ì—ì„œ ë–¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëƒ¥ ì´ì •ë„ ê°€ì¹˜ë¥¼ ê°€ì§„ ëŠ¥ë ¥ì¸ê°€?ë¼ëŠ” ì˜êµ¬ì‹¬ì´ ë“¤ì§€ë§Œ ë³´ì™„í•´ì•¼í•  ì ë“¤ì„ ë³´ì™„í•˜ë©´ì„œ ì´ì§ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ì‰¬ì›Œì•¼ë‹¬ë¼ì§„ë‹¤",
    "title": "ì‰¬ì›Œì•¼ ë‹¬ë¼ì§„ë‹¤",
    "date": "2025-07-10",
    "excerpt": "ì–´ë–»ê²Œ ì¢‹ì€ ìŠµê´€ì„ ë§Œë“¤ê³ , ë‚˜ìœ ìŠµê´€ì€ ë²„ë¦´ ê²ƒì¸ê°€",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "ìŠµê´€ì˜ ì¤‘ìš”ì„± ì¸ìƒì„ ì‚´ì•„ê°€ë©´ì„œ ì—¬ëŸ¬ ì¤‘ìš”í•œ ìš”ì†Œë“¤ì´ ìˆê² ì§€ë§Œ, ê°€ì¥ ì¤‘ìš”í•œ ê±´ ìŠµê´€ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë“€í¬ ëŒ€í•™ì˜ Wendy Wood êµìˆ˜ê°€ ìˆ˜í–‰í•œ ì—°êµ¬ì— ë”°ë¥´ë©´, ìš°ë¦¬ í–‰ë™ì˜ ì•½ 45%ê°€ ìŠµê´€ì— ì˜í•œ ê²ƒì´ë¼ê³  í•©ë‹ˆë‹¤. ì œê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë¬¸êµ¬ë„ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. > Habit is second nature ì´ë ‡ê²Œ ì¤‘ìš”í•œ ìŠµê´€ì„ ì–´ë–»ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆì„ê¹Œìš”? ì¢‹ì€ ìŠµê´€ì„ ë§ì´ ë§Œë“¤ê³ , ë‚˜ìœ ìŠµê´€ì„ ë²„ë ¤ ì¢‹ì€ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ ë‚˜ê°„ë‹¤ë©´ ì œê°€ ì›í•˜ëŠ” ì‚¶ì— ê°€ê¹Œì›Œì§ˆ ê²ƒì´ë¼ ìƒê°í•©ë‹ˆë‹¤. ì‰¬ì›Œì•¼ ë‹¬ë¼ì§„ë‹¤ 'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì´ë¼ëŠ” ì±…ì—ì„œëŠ” 2ë¶„ ë²•ì¹™ì„ ì œì‹œí•©ë‹ˆë‹¤. í•´ì•¼í•˜ëŠ” ì¼ì„ ì•„ì£¼ ì‘ê²Œ ìª¼ê°œë¼ë©° 2ë¶„ì•ˆì— ëë‚¼ ìˆ˜ ìˆëŠ” ì¼ë¡œ ë‚˜ëˆ„ë¼ê³  ë§í•©ë‹ˆë‹¤. 1~2ë‹¬ì •ë„ ì´ ë¶€ë¶„ì„ ì‚¶ì— ì ìš©í•´ë´¤ëŠ”ë° ì •ë§ íš¨ê³¼ê°€ ë§ì•˜ìŠµë‹ˆë‹¤. ê°€ì¥ í° ì´ìœ ëŠ” ì‹¬ë¦¬ì  ë¶€ë‹´ê°ì„ ì¤„ì—¬ì£¼ê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ë³´í†µ ì¼ì„ ë¯¸ë£¨ëŠ” ì´ìœ ë¥¼ ìƒê°í•´ë³´ë©´ ì‹¬ë¦¬ì  ë¶€ë‹´ê° ë•Œë¬¸ì…ë‹ˆë‹¤. íŠ¹íˆ ì €ëŠ” ì¼ì„ ì‹œì‘í•˜ê¸°ì— ë¶€ë‹´ìŠ¤ëŸ¬ì›Œì„œ ì‹œì‘ì¡°ì°¨ ëª»í•˜ê³  ë¯¸ë£¨ê¸°ë§Œ í–ˆë˜ ê²½í—˜ì´ ë§ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° í•´ì•¼í•  ì¼ì„ 2ë¶„ì•ˆì— ëë‚¼ ìˆ˜ ìˆëŠ” ì‘ì€ ì¼ë“¤ë¡œ ìª¼ê°œë‹ˆê¹Œ ì´ëŸ° ë¶€ë‹´ê°ì´ ê±°ì˜ ì—†ì–´ì¡Œê³ , ì‘ì€ ì¼ë“¤ì„ ì„±ì·¨í•˜ë©´ì„œ ê·¸ ì„±ì·¨ê°ì— ì ì  ë” ë§ì€ ì¼ë“¤ì„ í•  ìˆ˜ ìˆê²Œ ëìŠµë‹ˆë‹¤. > ì•„ì£¼ ì‘ì€ ì¼ë“¤ë¡œ ìª¼ê°œì. 2ë¶„ì•ˆì— ëë‚¼ ìˆ˜ ìˆëŠ” ì¼ë“¤ë¡œ. ì´ê±´ ì œ ì‚¶ì˜ í•˜ë‚˜ì˜ ì›ì¹™ì²˜ëŸ¼ ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ ë¯¸ë£¨ì§€ ë§ì ë¼ê³  ìƒê°í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ì™œ ë¯¸ë£¨ëŠ”ì§€ ì´í•´í•˜ê³ , ì‘ì€ ì¼ë“¤ë¡œ ìª¼ê°œê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ì‘ì€ ì„±ì·¨ë“¤ì´ ëª¨ì—¬ ë…¸ë ¥í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì‹¤í–‰ì´ ë˜ëŠ” ìŠµê´€ì´ ë˜ì–´ê°‘ë‹ˆë‹¤. ì–´ë–»ê²Œ ë‚˜ìœ ìŠµê´€ì„ ì—†ì•¨ê¹Œ ì¢‹ì€ ìŠµê´€ì„ ë§Œë“œëŠ” ê²ƒê³¼ ë°˜ëŒ€ë¡œ í•˜ê¸° ì–´ë µê²Œ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤. ì‚¬ëŒì˜ ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆê¸° ë•Œë¬¸ì— í™˜ê²½ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. Roy Baumeisterì˜ ì—°êµ¬ì— ì˜í•˜ë©´ ì˜ì§€ë ¥ì€ ì œí•œëœ ìì›ì´ë¼ê³  í•©ë‹ˆë‹¤. ì˜ì§€ë ¥ì€ ê·¼ìœ¡ì²˜ëŸ¼ ì‘ë™í•˜ë©°, ì‚¬ìš©í•˜ë©´ í”¼ë¡œí•´ì§€ê³  íœ´ì‹ì´ í•„ìš”í•´ì§€ê²Œ ë©ë‹ˆë‹¤. > ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆë‹¤. í™˜ê²½ì´ ì¤‘ìš”í•¨! Massachusetts General Hospital ì—°êµ¬ì—ì„œëŠ” ë³‘ì› ì¹´í˜í…Œë¦¬ì•„ì˜ ë¬¼ì˜ ìœ„ì¹˜ë§Œ ë°”ê¿¨ëŠ”ë°ë„ íƒ„ì‚°ìŒë£Œ íŒë§¤ 11.4% ê°ì†Œ, ìƒìˆ˜ íŒë§¤ 25.8% ì¦ê°€í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‚˜ìœ ìŠµê´€ì´ ë°œê²¬ëë‹¤ë©´, ì´ë¥¼ í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì„ ì–´ë–»ê²Œ ë§Œë“¤ì§€ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” ë°¤ì— íœ´ëŒ€í°ì„ ë³´ë©´ì„œ ëŠ¦ê²Œ ìëŠ” ìŠµê´€ì´ ìˆì–´ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìëª…ì¢…ì„ ìƒ€ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì•ŒëŒ ë•Œë¬¸ì— íœ´ëŒ€í°ì„ ì¹¨ëŒ€ì— ë“¤ê³  ê°€ì§€ ì•Šê³  ì§‘ì— ë“¤ì–´ì˜¤ë©´ ë°”ë¡œ êº¼ë²„ë¦½ë‹ˆë‹¤. > ë‚˜ìœ ìŠµê´€ì„ í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì„ ë§Œë“¤ì ì§‘ì—ì„œ ì•„ì— íœ´ëŒ€í°ì„ ë³´ì§€ ì•Šê²Œ ë˜ì—ˆê³ , ëŠ¦ê²Œ ìëŠ” ìŠµê´€ë„ í•´ê²°í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬ í•´ì•¼í•  ì¼ì„ ì•„ì£¼ ì‘ê²Œ ìª¼ê°œ ì‹¬ë¦¬ì  ë¶€ë‹´ê°ì„ ì—†ì• ì -> ì‘ì€ ì„±ì·¨ë“¤ì„ ë§›ë³´ë©° ê¾¸ì¤€íˆ ì´ì–´ê°€ì ì‹¤ì œë¡œ í•´ë³´ë‹ˆ ê·¸ë™ì•ˆ ë¯¸ë£¨ë˜ ë§ì€ ì¼ë“¤ì„ ì´ì–´ê°€ê³  ìˆë‹¤. ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆìœ¼ë‹ˆ, ë‚˜ìœ ìŠµê´€ì„ í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì„ ë§Œë“¤ì ì•ŒëŒì€ ìëª…ì¢…ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ , íœ´ëŒ€í°ì€ ì§‘ì— ì˜¤ë©´ ë°”ë¡œ êº¼ë²„ë¦¬ê¸°! -> ëŠ¦ê²Œê¹Œì§€ íœ´ëŒ€í° ë³´ëŠ” ìŠµê´€ ì œê±°ë¨",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-HETEROGENEOUS-TREATMENT-EFFECTS-ESTIMATION-WHEN-MACHINE-LEARNING-MEETS-MULTIPLE-TREATMENT-REGIME",
    "title": "[Paper Review] Heterogeneous Treatment Effects Estimation: When Machine Learning Meets Multiple Treatment Regime",
    "date": "2025-07-10",
    "excerpt": "HETEROGENEOUS TREATMENT EFFECTS ESTIMATION: WHEN MACHINE LEARNING MEETS MULTIPLE TREATMENT REGIME",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Data-Fusion-for-Partial-Identification-of-Causal-Effects",
    "title": "[Paper Review] Data Fusion for Partial Identification of Causal Effects",
    "date": "2025-07-10",
    "excerpt": "Data Fusion for Partial Identification of Causal Effects Paper review",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "Data Fusion for Partial Identification of Causal Effects > Data fusion techniques = ì—¬ëŸ¬ ë°ì´í„° ì†ŒìŠ¤ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í†µí•©í•˜ì—¬ ì‹ ë¢°ì„± ìˆê³  ì •í™•í•œ ê²°ë¡ ì„ ì´ëŒì–´ë‚´ëŠ” ë°©ë²•ë¡  Abstact > However, when both assumptions simultaneously failâ€”a common scenario in practiceâ€”current methods cannot identify or estimate causal effects.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Causal-Effect-Inference-for-Structured-Treatments",
    "title": "[Paper Review] Causal Effect Inference for Structured Treatments",
    "date": "2025-07-10",
    "excerpt": "Causal Effect Inference for Structured Treatments",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-09-ê²©ë ¤í•˜ëŠ”í‘œí˜„",
    "title": "ê²©ë ¤í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-09",
    "excerpt": "You did a great job on, I'm happy to see that",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ê±° You did a great job on the speaking practice. I'm happy to see that you're improving. I'm happy to see that you've overcome your fear of English. I can tell that she's sad. I could tell that she broke up the her boyfriend. I can tell that you put a lof of thought into this. ê°„ë‹¨í•œ ì‘ë¬¸ Today I wanna complement myself. You did a great job on the solving problem. Working on company 1ë…„ ì¡°ê¸ˆë„˜ê²Œ, I solved a lot of problems. On the OCR project, I solve communicate problem and make greak OCR pipeline. On the chatbot project, I made chatbot which ë§Œì¡±í•˜ë‹¤ ê³ ê°ì‚¬s needs. And then, I'm happy to see that youve overcome of ë¹„êµí•˜ë©´ì„œ ë’¤ì³ì§„ë‹¤ê³  ëŠë¼ëŠ” fear. When I was 21-25, I was just frozen because of fear that ë‚œ ë’¤ì³ì¡Œê³  ì¸ìƒì€ ë§í–ˆë‹¤. I can tell that your life is great. I ì¼ì¸ë¶„ì˜ ì‚¶ì€ ì‚´ì•„ê°„ë‹¤. And normal life is always wonderful. claud-4-sonnet í”¼ë“œë°± > gemini-2-proë³´ë‹¤ ë‚˜ì€ ë“¯ ìˆ˜ì •ëœ ë²„ì „: Today I want to compliment myself. You did a great job at problem-solving. Working at a company for a little over a year, I have solved many problems. On the OCR project, I solved communication issues and built a great OCR pipeline. On the chatbot project, I created a chatbot that satisfied the client's needs. I'm happy to see that you've overcome the fear of comparing yourself to others and feeling behind. When I was 21-25, I was frozen because of the fear that I was falling behind and my life was ruined. I can tell that your life is great. You're living a fulfilling life. And a normal life is always wonderful. í”¼ë“œë°±: ì˜í•œ ì : ë°°ìš´ í‘œí˜„ë“¤ (You did a great job on, I'm happy to see that, I can tell that)ì„ ì‹¤ì œ ìƒí™©ì— ì ìš©í•˜ë ¤ê³  ë…¸ë ¥í–ˆì–´ìš” ìì‹ ì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì§„ì •ì„± ìˆëŠ” ë‚´ìš©ì„ ì¼ì–´ìš” ìê¸° ê²©ë ¤ë¼ëŠ” ì£¼ì œë¥¼ ì˜ ì„ íƒí–ˆì–´ìš” ê°œì„ ì : ë‹¨ì–´ ì„ íƒ: \"complement\" â†’ \"compliment\" (ì¹­ì°¬í•˜ë‹¤) ë¬¸ë²•: \"on the solving problem\" â†’ \"at problem-solving\" ì‹œì œ ì¼ê´€ì„±: ê³¼ê±°ì™€ í˜„ì¬ì™„ë£Œ ì‹œì œë¥¼ ìƒí™©ì— ë§ê²Œ ì‚¬ìš© í•œì˜ í˜¼ìš©: í•œêµ­ì–´ ë¶€ë¶„ì„ ì˜ì–´ë¡œ ì™„ì „íˆ ë²ˆì—­í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ ì „ì¹˜ì‚¬: \"overcome of\" â†’ \"overcome\" (overcomeì€ ì „ì¹˜ì‚¬ ì—†ì´ ì‚¬ìš©) ë‹¤ìŒì— ì‹œë„í•´ë³¼ ê²ƒ: í•œ ë¬¸ì¥ ì•ˆì—ì„œëŠ” í•œ ì–¸ì–´ë¡œ í†µì¼í•˜ê¸° ë¬¸ì¥ì„ ì“°ê¸° ì „ì— ì „ì²´ êµ¬ì¡°ë¥¼ ì˜ì–´ë¡œ ë¨¼ì € ìƒê°í•´ë³´ê¸° ë°°ìš´ í‘œí˜„ì„ ë” ë‹¤ì–‘í•œ ìƒí™©ì— ì ìš©í•´ë³´ê¸°",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/what-is-statistical-bias",
    "title": "í†µê³„ì ì¸ í¸í–¥ê³¼ ì´ë¥¼ ì œê±°í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ",
    "date": "2025-07-09",
    "excerpt": "ì‹¤ì œ ë°ì´í„°ì—ì„œ í†µê³„ì  í¸í–¥ì„ í™•ì¸í•˜ê³  ì´ë¥¼ ì”ì°¨í™”ë¥¼ í†µí•´ ì œê±°í•˜ëŠ” ê³¼ì •",
    "category": "Causal Inference",
    "tags": [
      "í¸í–¥",
      "í†µê³„ì ",
      "bias"
    ],
    "content": "ë°°ê²½ --- ì¸ê³¼ì¶”ë¡ ì˜ ê¸°ë³¸ì ì¸ ê°œë…ì€ í†µê³„ì  í¸í–¥ (Statistical Bias)ì„ ì œê±°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì„ ë” ì˜ ì´í•´í•˜ê³  íš¨ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ”, ì´ í†µê³„ì  í¸í–¥ì„ ì´í•´í•˜ê³  í™•ì¸í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ì •ì˜ --- > í†µê³„ì  í¸í–¥(Statistical Bias)ì€ í†µê³„ ë¶„ì„ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¡œ, ì¸¡ì •ì´ë‚˜ ì¶”ì • ê³¼ì •ì—ì„œ ì‹¤ì œ ëª¨ì§‘ë‹¨ì˜ ëª¨ìˆ˜(parameter)ì™€ í‘œë³¸ í†µê³„ëŸ‰ ì‚¬ì´ì— ì¼ê´€ëœ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ” í˜„ìƒì„ ë§í•©ë‹ˆë‹¤. ì²´ê³„ì (systematic) ì˜¤ë¥˜ë€? ì¸¡ì • ë°©ë²•ì´ë‚˜ ë¶„ì„ ë°©ë²•ì— ë¬¸ì œê°€ ìˆì–´ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ë¬´ì‘ìœ„ê°€ ì•„ë‹Œ, ì¼ì •í•œ íŒ¨í„´ì´ë‚˜ ë°©í–¥ì„±ì„ ê°€ì§ ì²´ê³„ì  ì˜¤ë¥˜ (Systematic Error) vs ë¬´ì‘ìœ„ ì˜¤ë¥˜ (Random Error) ì²´ê³„ì  ì˜¤ë¥˜ (Systematic Error): í•­ìƒ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ë°œìƒ (ì˜ˆ: í•­ìƒ ì‹¤ì œê°’ë³´ë‹¤ ë†’ê²Œ ì¸¡ì •) ì¸¡ì • íšŸìˆ˜ë¥¼ ëŠ˜ë ¤ë„ ì¤„ì–´ë“¤ì§€ ì•ŠìŒ ì¸¡ì • ë„êµ¬ë‚˜ ë°©ë²•ì˜ ê²°í•¨ì—ì„œ ë¹„ë¡¯ë¨ ë¬´ì‘ìœ„ ì˜¤ë¥˜ (Random Error): ë°©í–¥ì´ ë¶ˆê·œì¹™í•¨ (ë•Œë¡œëŠ” ë†’ê²Œ, ë•Œë¡œëŠ” ë‚®ê²Œ) ì¸¡ì • íšŸìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ í‰ê· ì ìœ¼ë¡œ ìƒì‡„ë¨ ìš°ì—°í•œ ë³€ë™ì—ì„œ ë¹„ë¡¯ë¨ ì œê°€ ì´í•´í•œ ê°œë…ì€, ëœë¤ ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ êµ¬ì¡°ìƒì˜ ë¬¸ì œê°€ ìˆì–´ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ”ë° ì˜¤í•´ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë³´ë©´: êµ¬ì¡°ìƒ ë¬¸ì œ: ì˜¨ë¼ì¸ ì„¤ë¬¸ì¡°ì‚¬ë¡œë§Œ ì˜ê²¬ì„ ìˆ˜ì§‘ â†’ ë””ì§€í„¸ ê¸°ê¸° ì‚¬ìš©ì´ ì–´ë ¤ìš´ ê³ ë ¹ì¸µ ì˜ê²¬ ëˆ„ë½ë˜ê³  ì Šì€ ì¸µì˜ ì˜ê²¬ë§Œ ë°˜ì˜ë¨ ê²°ê³¼ì˜ ì˜¤í•´: \"ì Šì€ ì¸µì˜ ì˜ê²¬ì´ ì „ì²´ ì˜ê²¬\"ì´ë¼ê³  ì˜ëª» í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í†µê³„ì  í¸í–¥ì˜ ì¢…ë¥˜ì™€ ì„¤ëª… ì„ íƒ í¸í–¥ (Selection Bias) > ì—°êµ¬ ëŒ€ìƒì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” í¸í–¥ìœ¼ë¡œ, í‘œë³¸ì´ ëª¨ì§‘ë‹¨ì„ ì œëŒ€ë¡œ ëŒ€í‘œí•˜ì§€ ëª»í•  ë•Œ ë°œìƒí•©ë‹ˆë‹¤. ì˜ˆì‹œ: ë³‘ì› ë°ì´í„°ë§Œìœ¼ë¡œ ì§ˆë³‘ ì—°êµ¬ë¥¼ í•  ë•Œ â†’ ì¤‘ì¦ í™˜ìë§Œ í¬í•¨ë˜ì–´ ì§ˆë³‘ì˜ ì‹¬ê°ì„±ì´ ê³¼ëŒ€í‰ê°€ë¨ ìë°œì  ì°¸ì—¬ìë§Œìœ¼ë¡œ ì„¤ë¬¸ì¡°ì‚¬ â†’ íŠ¹ì • ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§„ ì‚¬ëŒë“¤ë§Œ ì°¸ì—¬í•˜ì—¬ ê²°ê³¼ê°€ í¸í–¥ë¨ ì œê°€ ì•ì„œ ì˜ˆì‹œë¥¼ ë“  ê²½ìš°ê°€ ì´ì— í•´ë‹¹í•©ë‹ˆë‹¤. í•œêµ­ ë‚¨ìì˜ í‰ê·  í‘¸ì‰¬ì—… ê°œìˆ˜ë¥¼ ì¸¡ì •í•œë‹¤ê³  í•  ë•Œ, í—¬ìŠ¤ì¥ì—ì„œ í‘¸ì‰¬ì—… ê°œìˆ˜ë¥¼ ì¸¡ì •í•œë‹¤ë©´ ì´ëŠ” í•œêµ­ ë‚¨ìë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆì„ê¹Œìš”? êµë€ë³€ìˆ˜ í¸í–¥ (Confounding Bias) > ì›ì¸ë³€ìˆ˜ì™€ ê²°ê³¼ë³€ìˆ˜ ëª¨ë‘ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì œ3ì˜ ë³€ìˆ˜(êµë€ë³€ìˆ˜)ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì»¤í”¼ ì†Œë¹„ì™€ ì‹¬ì¥ë³‘ì˜ ê´€ê³„ ì—°êµ¬ì—ì„œ í¡ì—°ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²½ìš° êµìœ¡ê³¼ ì†Œë“ì˜ ê´€ê³„ì—ì„œ ê°€ì • ë°°ê²½ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²½ìš° ì •ë³´ í¸í–¥ (Information Bias) > ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” í¸í–¥ìœ¼ë¡œ, ì¸¡ì • ì˜¤ë¥˜ë‚˜ ë¶„ë¥˜ ì˜¤ë¥˜ë¡œ ì¸í•´ ë°œìƒí•©ë‹ˆë‹¤. í•˜ìœ„ ìœ í˜•: íšŒìƒ í¸í–¥ (Recall Bias): ê³¼ê±° ì‚¬ê±´ì„ ê¸°ì–µí•˜ëŠ” ì •í™•ë„ ì°¨ì´ ê´€ì°°ì í¸í–¥ (Observer Bias): ì—°êµ¬ìì˜ ì£¼ê´€ì  íŒë‹¨ì´ ê°œì… ì¸¡ì • í¸í–¥ (Measurement Bias): ì¸¡ì • ë„êµ¬ë‚˜ ë°©ë²•ì˜ ê²°í•¨ ì˜ˆì‹œ: ì§ˆë³‘ í™˜ìê°€ ê±´ê°•í•œ ì‚¬ëŒë³´ë‹¤ ê³¼ê±° ë…¸ì¶œì„ ë” ì˜ ê¸°ì–µí•˜ëŠ” ê²½ìš° ì—°êµ¬ìê°€ ê°€ì„¤ì„ ì•Œê³  ìˆì–´ ê²°ê³¼ë¥¼ ì£¼ê´€ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²½ìš° ìƒì¡´ í¸í–¥ (Survivorship Bias) > ì„±ê³µí•˜ê±°ë‚˜ ì‚´ì•„ë‚¨ì€ ì‚¬ë¡€ì—ë§Œ ì§‘ì¤‘í•˜ê³ , ì‹¤íŒ¨í•˜ê±°ë‚˜ íƒˆë½í•œ ì‚¬ë¡€ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì„±ê³µí•œ ê¸°ì—…ê°€ë“¤ë§Œ ì¸í„°ë·°í•˜ì—¬ \"ì°½ì—… ì„±ê³µ ìš”ì¸\" ë¶„ì„ ì¡¸ì—…ìƒë§Œ ëŒ€ìƒìœ¼ë¡œ í•œ ëŒ€í•™ êµìœ¡ íš¨ê³¼ ì—°êµ¬ (ì¤‘ë„ íƒˆë½ì ì œì™¸) ë°œí‘œ í¸í–¥ (Publication Bias) > í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼ë‚˜ ê¸ì •ì ì¸ ê²°ê³¼ë§Œ ë°œí‘œë˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²°ê³¼ëŠ” ë°œí‘œë˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì•½ë¬¼ íš¨ê³¼ê°€ ìˆëŠ” ì—°êµ¬ë§Œ ë…¼ë¬¸ìœ¼ë¡œ ë°œí‘œë˜ê³ , íš¨ê³¼ê°€ ì—†ëŠ” ì—°êµ¬ëŠ” ë°œí‘œë˜ì§€ ì•ŠìŒ ì„±ê³µì ì¸ ì •ì±… ì‚¬ë¡€ë§Œ ë³´ê³ ë˜ê³ , ì‹¤íŒ¨í•œ ì •ì±…ì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ ì‹œê°„ ê´€ë ¨ í¸í–¥ (Time-related Bias) > ì‹œê°„ì˜ íë¦„ê³¼ ê´€ë ¨í•˜ì—¬ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. í•˜ìœ„ ìœ í˜•: ì‹œëŒ€ íš¨ê³¼ (Period Effect): íŠ¹ì • ì‹œì ì˜ í™˜ê²½ì  ìš”ì¸ ì½”í˜¸íŠ¸ íš¨ê³¼ (Cohort Effect): íŠ¹ì • ì„¸ëŒ€ì˜ ê³µí†µ ê²½í—˜ ì—°ë ¹ íš¨ê³¼ (Age Effect): ë‚˜ì´ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ë³€í™” íƒˆë½ í¸í–¥ (Attrition Bias) > ì—°êµ¬ ì§„í–‰ ì¤‘ ì°¸ì—¬ìê°€ íƒˆë½í•˜ì—¬ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì¥ê¸° ì¶”ì  ì—°êµ¬ì—ì„œ ê±´ê°•í•œ ì‚¬ëŒë“¤ë§Œ ê³„ì† ì°¸ì—¬í•˜ê³ , ì•„í”ˆ ì‚¬ëŒë“¤ì€ íƒˆë½ ì˜¨ë¼ì¸ êµìœ¡ íš¨ê³¼ ì—°êµ¬ì—ì„œ í•™ìŠµ ëŠ¥ë ¥ì´ ë†’ì€ ì‚¬ëŒë“¤ë§Œ ëê¹Œì§€ ì°¸ì—¬ ê²°ê³¼ë¥¼ í•´ì„í•  ë•Œ ì´ëŸ° í¸í–¥ë“¤ì€ ì˜¤í•´ë¥¼ ë§Œë“­ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ í¸í–¥ì„ ê³ ë ¤í•˜ê³  ê²°ê³¼ë¥¼ í•´ì„í•´ì•¼í•˜ë©°, ì´ëŸ° í¸í–¥ì„ ì°¾ê³  ì œê±°í•˜ëŠ” ë°©ë²•ì´ ì¸ê³¼ì¶”ë¡ ì˜ ì£¼ëœ ëª©ì ì…ë‹ˆë‹¤. ë‚´ê°€ ë‹¤ë£¨ê³  ìˆëŠ” ë°ì´í„°ì—ì„œì˜ í†µê³„ì  í¸í–¥ --- ì €ëŠ” ì‹œë ¥êµì •ë³‘ì›ì˜ ì˜ë£Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ì—°êµ¬ ëª©ì ì€ í™˜ìë“¤ì—ê²Œ ë‹¤ì–‘í•œ ìˆ˜ìˆ  ë°©ë²•ì„ ì ìš©í–ˆì„ ë•Œì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬, ìˆ˜ìˆ ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¹ì—°íˆ í™˜ìë§ˆë‹¤ ëˆˆì˜ ìƒíƒœê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë™ì¼í•œ ìˆ˜ìˆ ì„ í•´ë„ ê²°ê³¼ëŠ” ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œì¸ì°¨ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ìˆ˜ìˆ  íš¨ê³¼ë¥¼ í‰ê°€í•˜ë©´, ì•ì„œ ì„¤ëª…í•œ í¸í–¥ì´ ë°œìƒí•˜ì—¬ ì˜ëª»ëœ ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ìˆ ë§ˆë‹¤ í™˜ìì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê³  ì´ í™˜ìë“¤ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê±´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŠ” í˜¼ë™ í¸í–¥ (Confounding Bias)ì´ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™œ Confounding Biasì¸ê°€? êµ¬ì¡°: ì›ì¸ë³€ìˆ˜: ìˆ˜ìˆ  ë°©ë²• (Aìˆ˜ìˆ  vs Bìˆ˜ìˆ ) ê²°ê³¼ë³€ìˆ˜: ìˆ˜ìˆ  ê²°ê³¼ (ì‹œë ¥ ê°œì„  ì •ë„) í˜¼ë™ë³€ìˆ˜: í™˜ìì˜ ëˆˆ ìƒíƒœ (ê·¼ì‹œ ì •ë„, ê°ë§‰ ë‘ê»˜, ë‚˜ì´ ë“±) ë¬¸ì œì : ëˆˆ ìƒíƒœê°€ ìˆ˜ìˆ  ë°©ë²• ì„ íƒì— ì˜í–¥ì„ ë¯¸ì¹¨ ì˜ˆ: ê°ë§‰ì´ ì–‡ì€ í™˜ìëŠ” íŠ¹ì • ìˆ˜ìˆ ë§Œ ê°€ëŠ¥ ê·¼ì‹œê°€ ì‹¬í•œ í™˜ìëŠ” ë‹¤ë¥¸ ìˆ˜ìˆ  ë°©ë²• ì„ íƒ ëˆˆ ìƒíƒœê°€ ìˆ˜ìˆ  ê²°ê³¼ì—ë„ ì§ì ‘ ì˜í–¥ì„ ë¯¸ì¹¨ ì˜ˆ: ê°ë§‰ì´ ë‘êº¼ìš´ í™˜ìê°€ ì¼ë°˜ì ìœ¼ë¡œ ê²°ê³¼ê°€ ì¢‹ìŒ ì Šì€ í™˜ìê°€ íšŒë³µì´ ë¹ ë¦„ ê²°ê³¼: ëˆˆ ìƒíƒœë¥¼ í†µì œí•˜ì§€ ì•Šìœ¼ë©´, ìˆ˜ìˆ  ë°©ë²• ìì²´ì˜ íš¨ê³¼ê°€ ì•„ë‹Œ í™˜ì íŠ¹ì„±ì˜ ì°¨ì´ë¥¼ ìˆ˜ìˆ  íš¨ê³¼ë¡œ ì˜ëª» í•´ì„í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ ì–´ë–»ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì„ê¹Œ? --- ìš”ì¦˜ ê°€ì¥ ë§ì€ ê³ ë¯¼ì„ í•˜ê³  ìˆëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. > ë…¼ë¦¬ì ìœ¼ë¡œ í¸í–¥ì´ ë°œìƒí•˜ëŠ” êµ¬ì¡°ì´ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡ ì„ ë„ì…í–ˆë‹¤. > ê·¸ë˜ì„œ ê²°ê³¼ëŠ”? ì–´ëŠì •ë„ ì¢‹ì•„ì¡ŒëŠ”ë°? ì–´ë–¤ ë°©ë²•ì„ ë„ì…í•œ í›„ì˜ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤˜ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ì–´ë–»ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ê³ ë¯¼ì…ë‹ˆë‹¤. ì œê°€ ë³´ê³  ìˆëŠ” ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì„ ì¶”ê°€ì ìœ¼ë¡œ í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ìˆ˜ìˆ ë°©ë²•ì˜ ì°¨ì´ : ë Œì¦ˆì‚½ì…ìˆ ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë Œì¦ˆ í¬ê¸° [12.1, 12.6, 13.2] í™˜ì ëˆˆì˜ ì¡°ê±´ : [ë‚˜ì´, ATA(ëˆˆì˜ ê°€ë¡œê¸¸ì´), ACD(ëˆˆì˜ ì„¸ë¡œê¸¸ì´), CLR(ìˆ˜ì •ì²´ì˜ ìƒíƒœ)] ìˆ˜ìˆ ë°©ë²• ë³„ë¡œ ëˆˆì˜ ì¡°ê±´ ë¶„í¬ë¥¼ í™•ì¸í•˜ê¸° --- <small>ì²˜ì¹˜ : ì¸ê³¼ì¶”ë¡ ì—ì„œ ì—°êµ¬ìê°€ ì¡°ì‘í•˜ëŠ” ë³€ìˆ˜(ì˜ˆ: ìˆ˜ìˆ  ë°©ë²•, ì•½ë¬¼ íˆ¬ì—¬). ì²˜ì¹˜ ê·¸ë£¹ê³¼ ëŒ€ì¡° ê·¸ë£¹ì„ ë¹„êµí•˜ì—¬ íš¨ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.</small> > ì²˜ì¹˜(ìˆ˜ìˆ ë°©ë²•, ì‚½ì…í•˜ëŠ” ë Œì¦ˆí¬ê¸°)ë³„ë¡œ í™˜ì ëˆˆì˜ ë¶„í¬ê°€ ê°™ì€ê°€? <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„_Feature_DensityPlot.png\" alt=\"ì²˜ì¹˜ë³„ Feature Density Plot\"> <figcaption>ê·¸ë¦¼ 1: ì²˜ì¹˜ë³„ Feature ë¶„í¬ ë¹„êµ</figcaption> </figure> <figure> <img src=\"/post/what-is-statistical-bias/boxplot_grouped_by_size.png\" alt=\"Boxplot Grouped by Size\"> <figcaption>ê·¸ë¦¼ 2: í¬ê¸°ë³„ ê·¸ë£¹í™”ëœ ë°•ìŠ¤í”Œë¡¯</figcaption> </figure> ataì™€ clrì˜ ê²½ìš° ë¶„í¬ì˜ ì°¨ì´ê°€ ëª…í™•íˆ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. > ì²˜ì¹˜ë³„ í™˜ì ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤. ë”°ë¼ì„œ í™˜ì ëˆˆì˜ ì¡°ê±´ì„ í†µì œí•˜ì§€ ì•Šê³  ìˆ˜ìˆ  íš¨ê³¼ë¥¼ í‰ê°€í•˜ë©´ ì˜¤í•´ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì¦‰ ataì™€ clrì€ ì²˜ì¹˜ì™€ ê²°ê³¼ë³€ìˆ˜ ëª¨ë‘ì— ì˜í–¥ì„ ì£¼ëŠ” êµë€ë³€ìˆ˜ (confounding variable)ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì²˜ì¹˜ë³„ ê²°ê³¼ ì°¨ì´ëŠ” ì´ êµë€ë³€ìˆ˜ì— ì˜í•´ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„ê²°ê³¼ë¹„êµ.png\" alt=\"ì²˜ì¹˜ë³„ ê²°ê³¼ ë¹„êµ\"> <figcaption>ê·¸ë¦¼ 3: ì²˜ì¹˜ë³„ ê²°ê³¼ ë¶„í¬ ë¹„êµ</figcaption> </figure> > ë‹¨ìˆœíˆ ì²˜ì¹˜ë³„ ë¶„í¬ì˜ ì°¨ì´ë¥¼ í†µí•´ ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒë‹¨í•  ìˆ˜ ì—†ë‹¤. ê°™ì€ í¬ê¸°ì˜ ëˆˆì— 3ê°€ì§€ ì¢…ë¥˜ì˜ ë Œì¦ˆë¥¼ ëª¨ë‘ ë„£ì€ ê²°ê³¼ê°€ ìˆë‹¤ë©´, ì´ë¥¼ í†µí•´ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ë¦¼1ê³¼ 2ì—ì„œ í™•ì¸í•œ ê²ƒì²˜ëŸ¼ ë Œì¦ˆë³„ë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ ëˆˆì— ë Œì¦ˆë¥¼ ë„£ì–´ ê²°ê³¼ë¥¼ ì–»ì—ˆê¸° ë•Œë¬¸ì—, ì´ë¥¼ í†µí•´ì„œëŠ” ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤ì§ˆì ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì  --- ì˜ˆì¸¡ ëª¨ë¸ì€ ìƒê´€ê´€ê³„ì— ê¸°ë°˜í•´ íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì´ë€? ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ ì´ë•Œ confounding variableì€ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ ìƒê´€ê´€ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ataê°€ ì»¤ì§ˆìˆ˜ë¡ ë Œì¦ˆë¥¼ ë” í° ê±¸ ì‚¬ìš©í–ˆê³ , ë”°ë¼ì„œ ê²°ê³¼ (vaulting)ê°€ ë” ì»¤ì§„ ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ëª¨ë¸ì€ ataê°€ ì»¤ì§€ë©´ vaultingì´ ì»¤ì§„ë‹¤ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ì˜ˆì¸¡ ëª¨ë¸ì— ê¸°ë°˜í•´ ì²˜ì¹˜ë¥¼ ê²°ì •í•˜ëŠ” ê±´ êµ‰ì¥íˆ ìœ„í—˜í•©ë‹ˆë‹¤. > ì²˜ì¹˜ì™€ ê²°ê³¼ë³€ìˆ˜ ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª¨ë¸ë§í•  í•„ìš”ê°€ ìˆê³ , ì´ë¥¼ í†µí•´ ì²˜ì¹˜ë¥¼ ê²°ì •í•  ë•Œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²• --- > êµë€ë³€ìˆ˜ë“¤ì„ ì œê±°í•˜ê³  ì²˜ì¹˜ì™€ ê²°ê³¼ë³€ìˆ˜ ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª¨ë¸ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰ ì²˜ì¹˜(T)ì™€ ê²°ê³¼(Y)ì— ëª¨ë‘ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ë¥¼ $X$ë¼ê³  í–ˆì„ ë•Œ, $X$ê°€ $Y$ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œê±°í•˜ê³  $X$ê°€ $T$ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œê±°í•˜ê³  $Y$ì™€ $T$ ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª¨ë¸ë§í•´ì•¼ í•©ë‹ˆë‹¤. êµë€ë³€ìˆ˜ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ê·¸ ì¤‘ì— í•˜ë‚˜ê°€ ì”ì°¨í™” ë°©ë²•ì…ë‹ˆë‹¤. FWL ì •ë¦¬ ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ì”ì°¨í™” ë°©ë²•ì„ í†µí•´ êµë€ë³€ìˆ˜ë¥¼ í†µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì”ì°¨í™”ë¥¼ í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì‚¬ì´ì¦ˆë³„ì”ì°¨ë¶„í¬.png\" alt=\"ê²°ê³¼ì—ì„œ êµë€ë³€ìˆ˜ì— ëŒ€í•œ ì˜í–¥ ì œê±°í›„ ë¶„í¬\"> <figcaption>ê·¸ë¦¼ 4: ì”ì°¨í™”ë¥¼ í†µí•´ êµë€ë³€ìˆ˜ì— ëŒ€í•œ ì˜í–¥ì„ ì œê±°í•œ ê²°ê³¼ ë¶„í¬</figcaption> </figure> ê·¸ë¦¼3ê³¼ ë¹„êµí•˜ë©´, ì²˜ì¹˜(ë Œì¦ˆ í¬ê¸°)ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ êµë€ë³€ìˆ˜ë“¤ì˜ ì˜í–¥ì„ ì œê±°í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¼4ë¥¼ í†µí•´ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì”ì°¨í™”ê°€ ì˜ë˜ì—ˆëŠ”ì§€ëŠ” ì”ì°¨ê°€ ì •ê·œë¶„í¬ í˜•íƒœë¥¼ ë„ëŠ”ì§€ì™€ ì”ì°¨ê°€ êµë€ë³€ìˆ˜ì™€ ë…ë¦½ì¸ì§€ë¥¼ í™•ì¸í•˜ë©´ ë©ë‹ˆë‹¤. ì •ê·œë¶„í¬ í˜•íƒœë¥¼ ë„ê³  ìˆë‹¤ëŠ” ë¶€ë¶„ì€ ê·¸ë¦¼4ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆê³ , êµë€ë³€ìˆ˜ì™€ ë…ë¦½ì¸ì§€ëŠ” êµë€ë³€ìˆ˜ë¡œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì¡°ê±´ë¶€ë…ë¦½ì„±ê²€ì‚¬.png\" alt=\"êµë€ë³€ìˆ˜ë¡œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ê²°ê³¼\"> <figcaption>ê·¸ë¦¼ 5: êµë€ë³€ìˆ˜ë¡œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ê²°ê³¼</figcaption> </figure> ëª¨ë¸ì´ ì”ì°¨ë¥¼ ì „í˜€ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ëŠ” ê²ƒì„ í†µí•´ ë” ì´ìƒ Xì— ëŒ€í•œ ì •ë³´ê°€ Yì—ëŠ” í¬í•¨ë˜ì–´ìˆì§€ ì•Šë‹¤ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ì¢€ ë” ì§ê´€ì ì„ í•´ì„í•´ë³´ë©´ <figure> <img src=\"/post/what-is-statistical-bias/ê°™ì€ë Œì¦ˆí¬ê¸°_ê°™ì€ê²°ê³¼_ë‹¤ë¥¸ëˆˆ.png\" alt=\"ê°™ì€ë Œì¦ˆí¬ê¸°_ê°™ì€ê²°ê³¼_ë‹¤ë¥¸ëˆˆ\"> <figcaption>ê·¸ë¦¼ 6: ê°™ì€ ë Œì¦ˆ í¬ê¸°ì— ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆì§€ë§Œ ë‹¤ë¥¸ ëˆˆì„ ê°€ì§„ ê²½ìš°</figcaption> </figure> ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆê³  ê°™ì€ ì‚¬ì´ì¦ˆë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì—, ëˆˆì´ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©´ ë Œì¦ˆ í¬ê¸°ì˜ ì˜í–¥ì„ ì œëŒ€ë¡œ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. > ì§ê´€ì ìœ¼ë¡œ ë„ˆë¬´ë‚˜ë„ ë‹¹ì—°í•œ ë§ì´ì§€ë§Œ, ì•ì„œ ì„¤ëª…ë“œë¦° ê²ƒì²˜ëŸ¼ ì´ëŸ¬í•œ í†µê³„ì ì¸ í¸í–¥ë“¤ì„ ì œê±°í•˜ì§€ ì•Šê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê²Œ ë˜ë©´ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì”ì°¨í™”í•˜ëŠ” ì½”ë“œëŠ” ì”ì°¨í™” ì½”ë“œ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ë¦¬ ì²˜ì¹˜ (ìˆ˜ìˆ ë°©ë²•, ë Œì¦ˆí¬ê¸°) ë³„ë¡œ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ìˆ˜ìˆ ì„ ë°›ì€ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ìˆ˜ìˆ  ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì •í™•íˆ ìˆ˜ìˆ  ê²°ê³¼ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ì„œëŠ” ê²°ê³¼ì—ì„œ êµë€ë³€ìˆ˜ì˜ ì˜í–¥ (í†µê³„ì  í¸í–¥) ì„ ì œê±°í•œ í›„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-07-08-optimization-pitfall",
    "title": "ìµœì í™”ì˜ í•¨ì •ì— ëŒ€í•˜ì—¬",
    "date": "2025-07-08",
    "excerpt": "íŒ¨ë°°ì£¼ì˜ì—ì„œ ë²—ì–´ë‚˜ì",
    "category": "Reflections",
    "tags": [
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "ë°°ê²½ ì €ëŠ” ê½¤ë‚˜ ì˜¤ëœì‹œê°„ ì¼ì¢…ì˜ íŒ¨ë°°ì£¼ì˜ì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•˜ê³  ê³ í†µìŠ¤ëŸ¬ì›Œí•˜ë©° ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤. ì €ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì‚¬ëŒë“¤ì„ ë³´ë©° ë‚˜ëŠ” ì €ëŸ° ê¸¸ì„ ê±¸ì–´ì˜¤ì§€ ì•Šì•˜ìœ¼ë‹ˆê¹Œ ì•ˆë  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ë“¤ì— ì‚¬ë¡œì¡í˜€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì‚¬ê³ ë°©ì‹ì„ ì–´ë–»ê²Œ ì •ì˜í•  ìˆ˜ ìˆì„ì§€ ëª°ëëŠ”ë°, \"ë‰´ìš•í„¸ê²Œ\"ë‹˜ì˜ ì˜ìƒë“¤ì„ ë³´ë©´ì„œ \"ìµœì í™”ì˜ í•¨ì •\"ì— ë¹ ì¡Œë‹¤ê³  ì •ì˜í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ìµœì í™”ì˜ í•¨ì •ì´ë€? ì¸ìƒì—ëŠ” ì–´ë–¤ ìµœì í™”ëœ ê¸¸ì´ ìˆê³ , ì´ ê¸¸ì„ ë”°ë¼ê°€ì•¼í•˜ë©°, ë”°ë¼ê°€ì§€ ëª»í•œ ë‚˜ëŠ” íŒ¨ë°°ì¬ìë¼ê³  ìƒê°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ìµœì í™”ëœ ê¸¸ì„ ì°¾ìœ¼ë ¤ê³  í•˜ë©´ì„œ ì™„ë²½ì£¼ì˜ì— ë¹ ì§€ê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì™„ë²½í•œ ê³„íšì— ëŒ€í•´ ê³ ë¯¼í•˜ë©´ì„œ ì‹œê°„ì„ ë³´ë‚´ê³ , ë§‰ìƒ ì•ìœ¼ë¡œ í•œë°œìêµ­ë„ ë‚˜ì•„ê°€ì§€ ëª»í•˜ëŠ” ìƒí™©ì— ë¹ ì§€ê²Œ ë©ë‹ˆë‹¤. ë‰´ìš•í„¸ê²Œë‹˜ ì˜ìƒ ë§í¬ >ê³„íšì´ ì›ëŒ€í•´ì§€ë©´ ë‚´ í•˜ë£¨ëŠ” ë¹„ì°¸í•´ì§„ë‹¤. ê°€ì¥ ì¸ìƒê¹Šì€ ë§ì´ì, ì œ ê³¼ê±° ì‚¶ì„ ê´€í†µí•˜ëŠ” ë§ì…ë‹ˆë‹¤. ë‰´ìš•í„¸ê²Œë‹˜ì˜ ì‚¬ê³ ë°©ì‹ ë‰´ìš•í„¸ê²Œë‹˜ì´ ë§ì”€í•˜ì‹œëŠ” ë¶€ë¶„ë“¤ì„ ì œ ì‚¶ì— ì •ë§ ë§ì€ ë„ì›€ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ê°€ì§€ ê°œë…ë“¤ì„ ë§ì”€í•´ì£¼ì‹œëŠ”ë°, ê³µí†µì ì¸ ê°œë…ì€ \"ìˆ˜ìš©ì˜ ìì„¸\"ê°€ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì´ëŠ” ì œê°€ ì¢‹ì•„í•˜ëŠ” \"ë²•ë¥œìŠ¤ë‹˜\"ì˜ ë§ì”€ê³¼ ë¹„ìŠ·í•œ ì ì´ ë§ìŠµë‹ˆë‹¤. ê³¼ê±°, ì–´ì©Œë©´ ì§€ê¸ˆë„ ì œê°€ ê³ í†µë°›ëŠ” ì´ìœ ë¥¼ ìƒê°í•´ë³´ë©´ ì œê°€ ì›í•˜ëŠ” ì´ìƒì ì¸ ìƒì„ ê·¸ë ¤ë†“ê³  ì´ê²ƒê³¼ ë¹„êµí•˜ë©´ì„œ ì œ í•˜ë£¨í•˜ë£¨ë¥¼ ë¹„ì°¸í•˜ê²Œ ìƒê°í–ˆê¸° ë•Œë¬¸ì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì—­ì„¤ì ìœ¼ë¡œ ì œ ì‚¶ì„ ë¶€ì •í•˜ê³  ì›ëŒ€í•œ ê³„íšì„ ì„¸ìš¸ìˆ˜ë¡ ì‚¶ì€ ë‚˜ì•„ì§€ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê·¸ì € ë¹„ì°¸í•œ í•˜ë£¨ë¥¼ ë³´ë‚´ë©° ì‹œê°„ì„ ë³´ë‚´ê¸°ë§Œ í•  ë¿ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ ìˆ˜ìš©ì˜ ìì„¸ëŠ” ì‚¶ì„ ì‚´ì•„ê°€ëŠ”ë° êµ‰ì¥íˆ ì¤‘ìš”í•œ ìì„¸ê°€ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë‚´ ëª¨ìŠµì´ ë‚´ê°€ ìƒìƒí–ˆë˜ ëª¨ìŠµì€ ì•„ë‹ˆì§€ë§Œ, ì¶©ë¶„íˆ í›Œë¥­í•˜ë‹¤. 1ì¸ë¶„ì˜ ì‚¶ì„ ì˜¨ì „íˆ ì‚´ì•„ë‚˜ê°€ê³  ìˆê³  ì¡°ê¸ˆì”© ë‚˜ì•„ê°€ê³  ìˆë‹¤. ì´ë ‡ê²Œ ì§€ê¸ˆ ì‚¶ì„ ì˜¨ì „íˆ ìˆ˜ìš©í•  ë•Œ, ë¹„ë¡œì†Œ ë°œì „í•  ìˆ˜ ìˆëŠ” ë‚´ê°€ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë‚˜ì˜ ì‚¶ì— ì ìš©í•´ë³´ê¸° ì›í•˜ëŠ” íšŒì‚¬ì— ì§€ì›í•˜ê³  ìˆì§€ë§Œ ê³„ì†í•´ì„œ ë–¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë‚˜ì˜ ì´ìƒì ì¸ ìƒê³¼ ë¹„êµí•˜ë©° ë‚˜ëŠ” ì•ˆë˜ëŠ” ì¸ìƒì¸ê°€ë³´ë‹¤ í•˜ë©° ê³ ì •ì ì´ê³  íŒ¨ë°°ì ì¸ ì‚¬ê³ ë°©ì‹ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆì§€ë§Œ, ì§€ê¸ˆì˜ ìƒí™©ì„ ìˆ˜ìš©í•˜ê³  ì•ìœ¼ë¡œ ë‚˜ì•„ê°€ê¸° ìœ„í•œ ë°©ë²•ì„ ì°¾ì•„ë³´ëŠ”ê²Œ ë” ì¢‹ì•„ ë³´ì…ë‹ˆë‹¤. ì´ ìˆ˜ìš©ì˜ ìì„¸ë¥¼ ë– ì˜¬ë¦¬ë©° ì‚¶ì„ ì‚´ì•„ê°€ë³´ë ¤ í•©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-07-08-how-to-write-resume",
    "title": "ì´ë ¥ì„œëŠ” ì–´ë–»ê²Œ ì ì–´ì•¼í• ê¹Œ?",
    "date": "2025-07-08",
    "excerpt": "ì´ë ¥ì„œë¥¼ ì ìœ¼ë©° í–ˆë˜ ë‚˜ì˜ ê³ ë¯¼ë“¤",
    "category": "Reflections",
    "tags": [
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "ë‚˜ì˜ ë°°ê²½ ì¼í•œì§€ 1ë…„ì´ ì¡°ê¸ˆ ë„˜ëŠ” ì‹œì ì—ì„œ ì´ì§ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë ¥ì„œë¥¼ ì¤€ë¹„í•˜ë©° ë‚˜ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ë©´ ì¢‹ì„ì§€ì— ëŒ€í•œ ê³ ë¯¼ë“¤ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë¥´ëŠ” ë‚´ìš©ì„ ê³µë¶€í•˜ê³  ì¢‹ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ì¼ë“¤ì€ ì–´ë µì§€ë§Œ ê·¸ ê¸¸ì´ ë‚˜ë¦„ ëª…í™•í•©ë‹ˆë‹¤. ì´ëŸ° ê³¼ì •ì„ ì ì–´ë„ 5ë…„ì´ìƒ ê±°ì³ì™”ìœ¼ë‹ˆ ìì‹ ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ë¶€ë¶„ë“¤ì€ ì •ë§ ì–´ë µê²Œ ëŠê»´ì§‘ë‹ˆë‹¤. ë‚˜ëŠ” ì§„ì§œ ê²½ìŸë ¥ì´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì˜êµ¬ì‹¬ë¶€í„°, ë‚´ê°€ í–ˆë˜ ì¼ë“¤ì„ ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ ì„¤ëª…í•´ì•¼í•˜ëŠ”ì§€ ë“±ë“± ì´ëŸ° ê³¼ì •ì—ì„œ í–ˆë˜ ê³ ë¯¼ë“¤ì„ í•˜ë‚˜ì”© ì •ë¦¬í•´ë´…ë‹ˆë‹¤. ë‚´ ì´ë ¥ì„œì— ëŒ€í•œ í”¼ë“œë°± ì‚¬ì´ë“œí”„ë¡œì íŠ¸ë¥¼ ê°™ì´í•˜ëŠ” ê°œë°œìë¶„ > ì„íŒ©íŠ¸ê°€ ì—†ë‹¤. ì´ë ¥ì„œë¥¼ ë” ì§§ê²Œ ì“°ê³  í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í’ë¶€í•˜ê²Œ ê°€ì ¸ê°€ëŠ”ê²Œ ì¢‹ê² ë‹¤. ì´ í”¼ë“œë°±ì— ë™ì˜í•˜ëŠ” ë¶€ë¶„ì´ ë§ì•˜ìŠµë‹ˆë‹¤. ë‚´ê°€ í•œ í”„ë¡œì íŠ¸ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ”ë°ì—ë§Œ ì§‘ì¤‘í–ˆì§€, ì§„ì§œ ì–´ë–¤ ë¬¸ì œë¥¼ í’€ì–´ì„œ ì–´ë–¤ ì˜í–¥ì„ ì¤¬ëŠ”ì§€ì— ëŒ€í•œ ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²ƒ ê°™ë‹¤ê³  ëŠê¼ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë”°ë¡œ ì¤€ë¹„í•˜ì§€ ì•Šì•˜ë˜ ë¶€ë¶„ì€ ê¼­ ìˆ˜ì •ì´ í•—ìš”í•˜ë‹¤ê³  ëŠê»´ì§‘ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-08-ì¹­ì°¬í•˜ëŠ”í‘œí˜„",
    "title": "ì¹­ì°¬í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-08",
    "excerpt": "I was really impressed, It couldn`t be any better, I couldn`t have done it without you",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ê±° I was really impressed I was really impresed with your exhibit It couldnt be any better I couldnt have done it without you I couldnt have graduate without you ì´ê±° ì‚¬ìš©í•´ì„œ ê°„ë‹¨í•œ ì‘ë¬¸ Today, I attend side-project ëª¨ì„. We talked about recommmend system. The topic was, how to delete data noise, how to check 'ê°€ì •', distribute tasks for project. After that ë™ë£Œ ,who í”„ë¡œì íŠ¸ë¥¼ ê°™ì´í•˜ëŠ” ì‚¬ëŒ, give me advice. Because nowadays I fall ì„œë¥˜íƒˆë½, so I ask them how can I improve my ì´ë ¥ì„œ. I was really impressed with their advice. They ê³ ë¯¼í•˜ë‹¤ for me. How to improve my resume. It couldnt be any better. I was so ê°ë™ë°›ë‹¤. I always appreciate this project. They gave me so many ë„ì›€. I couldnt have be better person without them. ë¬¸ì¥ìˆ˜ì • (gemini-2.5-pro) Today, I attended a side-project meeting. We talked about recommendation systems. The main topics were how to remove noisy data, how to test our assumptions, and how to distribute tasks for the project. After the meeting, a teammate who is working on the project with me gave me some advice. Recently, I've been getting rejected at the resume screening stage, so I asked them for tips on how to improve my resume. I was really impressed with their advice. They thoughtfully considered how I could improve my resume. It couldnâ€™t have been any better. I was so moved. I always appreciate being a part of this project. They have been so helpful to me. I couldnâ€™t have become a better person without them. í”¼ë“œë°± (gemini-2.5-pro) ì „ë°˜ì ìœ¼ë¡œ ì „ë‹¬í•˜ë ¤ëŠ” ì˜ë¯¸ê°€ ëª…í™•í•´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤! ëª‡ ê°€ì§€ ë¬¸ë²•ì ì¸ ë¶€ë¶„ê³¼ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì¤‘ì‹¬ìœ¼ë¡œ í”¼ë“œë°±ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ì‹œì œ ì¼ì¹˜: ê³¼ê±°ì˜ ê²½í—˜ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë™ì‚¬ëŠ” ê³¼ê±°í˜•ìœ¼ë¡œ ì¼ì¹˜ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. (ì˜ˆ: attend â†’ attended) ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„: side-project ëª¨ì„ â†’ a side-project meeting ì²˜ëŸ¼ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ì˜ì–´ë¡œ ë°”ê¾¸ê³ , ëª…ì‚¬ ì•ì— ê´€ì‚¬(a/an/the)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ë©´ ë¬¸ì¥ì´ ë” ìì—°ìŠ¤ëŸ¬ì›Œì§‘ë‹ˆë‹¤. 'ì„œë¥˜ íƒˆë½í•˜ë‹¤'ëŠ” fail the resume screening ë˜ëŠ” get rejected at the resume screening stageì™€ ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ê³ ë¯¼í•˜ë‹¤'ëŠ” thoughtfully considered ë˜ëŠ” took the time to think about ë“±ìœ¼ë¡œ í‘œí˜„í•˜ë©´ 'ë‚˜ë¥¼ ìœ„í•´ ì‹ ì¤‘í•˜ê²Œ ìƒê°í•´ ì£¼ì—ˆë‹¤'ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ ì˜ ì‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ë²•: 'ë„ì›€(help)'ì€ ì…€ ìˆ˜ ì—†ëŠ” ëª…ì‚¬ë¼ many ëŒ€ì‹  muchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (so much help) I couldn't have be better person â†’ I couldn't have become a better person ì²˜ëŸ¼ ì¡°ë™ì‚¬ ë’¤ì— ë™ì‚¬ ì›í˜•ì˜ ê³¼ê±°ë¶„ì‚¬í˜•(p.p.)ì„ ì‚¬ìš©í•˜ê³ , ëª…ì‚¬ ì•ì— ê´€ì‚¬ë¥¼ ë¶™ì—¬ì£¼ë©´ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ì „í•œ ë¬¸ì¥ì´ ë©ë‹ˆë‹¤. ì˜¤íƒ€: recommmend â†’ recommendation`ê³¼ ê°™ì€ ì˜¤íƒ€ë¥¼ ì£¼ì˜í•˜ë©´ ë” ì™„ì„±ë„ ë†’ì€ ê¸€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-07-08-thoughts-on-review-quality",
    "title": "ë¦¬ë·° ë…¸ì´ì¦ˆì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2025-07-08",
    "excerpt": "ì¹´ì¹´ì˜¤ë§µ ë¦¬ë·° ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë“¤ì— ëŒ€í•˜ì—¬",
    "category": "Recommendation",
    "tags": [
      "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì •ë¦¬",
      "ì¶”ì²œì‹œìŠ¤í…œ",
      "ì¶”ì²œì‹œë‚˜ë¦¬ì˜¤"
    ],
    "content": "ë°°ê²½ ë°ì´í„°ê³¼í•™ìë¡œ 1ë…„ ì¡°ê¸ˆ ë„˜ê²Œ ì¼í•˜ë©´ì„œ ê°€ì¥ í¬ê¸° ëŠë¼ëŠ” ë¶€ë¶„ì€, ë°ì´í„°ì˜ ì¤‘ìš”ì„±ì…ë‹ˆë‹¤. ëª¨ë¸ì€ ë°ì´í„° ì•ˆì—ì„œ íŒ¨í„´ì„ ì°¾ì„ ë¿, ê·¸ ì•ˆì— íŒ¨í„´ì´ ì—†ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ í¬ë©´ ëª¨ë¸ì€ ì œ ì—­í• ì„ ëª»í•˜ê²Œ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë°ì´í„°ë¥¼ ëœ¯ì–´ë³´ê³  ê·¸ ì•ˆì˜ ë…¸ì´ì¦ˆë¥¼ ì‚´í´ë³´ëŠ” ì¼ì€ êµ‰ì¥íˆ ì¤‘ìš”í•œ ì¼ì…ë‹ˆë‹¤. > ë¦¬ë·° ë°ì´í„°ì—ëŠ” ì–´ë–¤ ë…¸ì´ì¦ˆê°€ ìˆëŠ”ì§€ ì‚´í´ë³´ê³  ì´ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤ <small>ê°™ì´ ì‚¬ì´ì¦ˆ í”„ë¡œì íŠ¸ë¥¼ í•˜ëŠ” 'ì‹ ë³´í˜„'ë‹˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ì •ë¦¬í—€ìŠµë‹ˆë‹¤.</small> ë¬¸ì œ ìƒí™©ë“¤ ì—¬ëŸ¬ ë…¸ì´ì¦ˆë“¤ì´ ì¡´ì¬í•˜ëŠ”ë° ê·¸ ì¤‘ ê°€ì¥ ì‹¬ê°í•˜ë‹¤ê³  ëŠë¼ëŠ” ë¶€ë¶„ë“¤ì— ëŒ€í•´ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ì¥ì†Œ(ìŒì‹ì )ê³¼ ìƒê´€ì—†ëŠ” ì´ìœ ì¸ ë¶€ì •ì /ê¸ì •ì ì¸ ë¦¬ë·°ë“¤ ê°€ì¥ ëˆˆì— ëˆ ë¶€ë¶„ì€ ê³„ì—„ê³¼ ê·¸ í›„ì˜ ì‹œìœ„ë“¤ê³¼ ê´€ë ¨ëœ ë¦¬ë·°ë“¤ì…ë‹ˆë‹¤. ê³„ì—„ì„ ì°¬ì„±í•˜ê±°ë‚˜ ë°˜ëŒ€í•  ê²½ìš° ê´€ë ¨ëœ ì‚¬ëŒë“¤ì´ 'ëŒ“ê¸€ í…ŒëŸ¬'ë¥¼ í•˜ëŠ” ê²½ìš°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ ì œê°€ ë‹¤ë‹ˆê³  ìˆëŠ” í—¬ìŠ¤ì¥ì— ì—¬ì íŠ¸ë ˆì´ë„ˆê°€ ëš±ëš±í•˜ë‹¤ëŠ” ì´ìœ ë¡œ ë¶€ì •ì ì¸ ë¦¬ë·°ë¥¼ ë‚¨ê¸°ëŠ” ê²½ìš°ë„ ìˆì—ˆìŠµë‹ˆë‹¤. ë§ˆì¼€íŒ… ëª©ì ìœ¼ë¡œ ì‘ì„±ëœ ë¦¬ë·°ë“¤ ë§ˆì¼€íŒ…ì„ ìœ„í•´ ì˜ë„ì ìœ¼ë¡œ ì‘ì„±ëœ ë¦¬ë·°ë“¤ì…ë‹ˆë‹¤. ì˜ˆì „ì— ì•Œë°”í–ˆë˜ ìŒì‹ì ì—ì„œë„ ì´ëŸ° ë§ˆì¼€íŒ…ì„ ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤. AIë¡œ ì‘ì„±í•´ì„œ ë¬¸ì²´ì— í‹°ê°€ ë‚˜ëŠ” ê²½ìš°ë¼ë©´ ë‹¤ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, ì‚¬ëŒì´ ì‘ì„±í•œ ë“¯í•œ ëŒ“ê¸€ì´ ë§ì•„ì„œ ì´ë¥¼ ì–´ë–»ê²Œ ê±°ë¥¼ ìˆ˜ ìˆëŠ”ì§€ë„ ê³ ë¯¼í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ? ì´ëŸ° ë¶€ë¶„ì„ ìƒì‡„í• ë§Œí¼ ë¦¬ë·°ê°€ ë§ìœ¼ë©´ ì´ê²Œ ìƒì‡„ê°€ ë˜ëŠ”ì§€, í˜¹ì€ ëª¨ë¸ë¡œ ì´ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆì„ì§€. ë°ì´í„°ë¥¼ í•„í„°ë§í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ë“¤ì„ ê³ ë¯¼í•´ì•¼í• ì§€ì— ëŒ€í•œ ë…¼ì˜ê°€ ì¶”ê°€ë¡œ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ë“¤ì— ëŒ€í•œ ë‚´ìš©ë„ ì¶”ê°€ë¡œ ì •ë¦¬í•  ì˜ˆì •ì…ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-07-08-thoughts-on-deciding-scenario",
    "title": "ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •í•˜ê¸° ìœ„í•œ ê³ ë¯¼",
    "date": "2025-07-08",
    "excerpt": "ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ëŠ” ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ì–´ë–»ê²Œ ì •í•˜ë©´ ì¢‹ì„ì§€ì— ëŒ€í•œ ë…¼ì˜ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.",
    "category": "Recommendation",
    "tags": [
      "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì •ë¦¬",
      "ì¶”ì²œì‹œìŠ¤í…œ",
      "ì¶”ì²œì‹œë‚˜ë¦¬ì˜¤"
    ],
    "content": "ë°°ê²½ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•´ë³´ê³  ìˆìŠµë‹ˆë‹¤. êµ¬í˜„í•˜ë©´ì„œ í•„ìš”í•œ ì—¬ëŸ¬ ê³ ë¯¼ ì¤‘ í•˜ë‚˜ëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì¶”ì²œì„ í•´ì¤„ ê²ƒì¸ì§€ì— ëŒ€í•œ ê³ ë¯¼ì…ë‹ˆë‹¤. ì´ë¥¼ ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤ë¼ê³  ì •ì˜í•˜ê³  ì´ë¥¼ êµ¬ì²´í™”í•˜ëŠ” ê³¼ì •ì— ìˆìŠµë‹ˆë‹¤. ì½œë“œ ìœ ì €ì— ëŒ€í•´ ì–´ë–»ê²Œ ì¶”ì²œí•  ê²ƒì¸ê°€ ì„œë¹„ìŠ¤ë¥¼ ìƒˆë¡­ê²Œ ë§Œë“¤ê²Œ ëœë‹¤ë©´ ì´ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ìœ ì €ëŠ” ì½œë“œ ìœ ì €ì¼ ê²ƒì…ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ì–´ë–»ê²Œ ëŒ€ì²˜í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ë…¼ì˜ê°€ ì£¼ëœ ë…¼ì˜ì˜€ìŠµë‹ˆë‹¤. ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ + í•„í„°ë§ ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì—¬ê¸°ì— ì¶”ê°€ë¡œ ë‚ ì”¨ë¼ë˜ì§€, ìœ„ì¹˜ë¼ë˜ì§€, íŠ¹ì • í•„í„°ë§ì„ ë¶™ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ìœ ì € í”„ë¡œí•„ ì™„ì„± (ì˜¨ë³´ë”© ê¸°ë°˜ ì¶”ì²œ) ìœ ì €ì—ê²Œ ì •ë³´ë¥¼ ì™„ì„±í•˜ë„ë¡ ìš”ì²­í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì„ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ì™“ì± ë‚˜ ë„·í”Œë¦­ìŠ¤ì˜ ê²½ìš°, ì²˜ìŒì— ë³¸ì¸ì´ ì¢‹ì•„í•˜ëŠ” ì˜í™”ë¥¼ ì„ íƒí•˜ë„ë¡ êµ¬ì„±í•˜ê³  ê·¸ í›„ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì„ í•´ì¤ë‹ˆë‹¤. ì´ëŸ°ì‹ìœ¼ë¡œ ìœ ì € ì •ë³´ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ì— ëŒ€í•œ ë…¼ì˜ë¥¼ í–ˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-07-08-cold-start-solution",
    "title": "Cold Start í•´ê²° ë°©ë²•ì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2025-07-08",
    "excerpt": "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì—ì„œ ì§„í–‰í•œ cold start í•´ê²°ë°©ì•ˆê³¼ ë°ì´í„°ì˜ í•œê³„ì  ê·¹ë³µ ê³ ë¯¼",
    "category": "Recommendation",
    "tags": [
      "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì •ë¦¬",
      "ì¶”ì²œì‹œìŠ¤í…œ"
    ],
    "content": "ë°°ê²½ ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ \"ì©ì©LAB\"ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì§„í–‰í•œ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì •ë¦¬ì…ë‹ˆë‹¤. ë§›ì§‘ ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ê³¼ì œ ì¤‘ Cold Startë¥¼ ì–´ë–»ê²Œ í•´ê²°í• ì§€ì— ëŒ€í•´ ë…¼ì˜í•œ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤. Cold Start Problemì´ë€? > ì‚¬ìš©ìë‚˜ ì•„ì´í…œì— ëŒ€í•œ ì •ë³´ê°€ ì—†ê±°ë‚˜ í¬ì†Œí•œ ë¬¸ì œ ì´ ì¤‘ì—ì„œ ìœ ì €ì— ëŒ€í•œ cold start ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í’€ì§€ì— ëŒ€í•´ ê³ ë¯¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. <small>ê°™ì´ ì‚¬ì´ì¦ˆ í”„ë¡œì íŠ¸ë¥¼ í•˜ëŠ” 'ì´ìœ¤ì„ 'ë‹˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ì •ë¦¬í—€ìŠµë‹ˆë‹¤.</small> ê¸°ë³¸ì ì¸ ì¶”ì²œì˜ ì»¨ì…‰ > Popularity Modelì—ì„œ Contextë¥¼ ë°˜ì˜í•˜ì—¬, ìœ ì €ê°€ ë§Œì¡±í• ë§Œí•œ ìŒì‹ì„ ì¶”ì²œí•´ì£¼ê³  ì‹¶ë‹¤. ì½œë“œ ìœ ì €ì—ê²Œ ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œì„ ë‚´ì£¼ëŠ” ê²ƒì²˜ëŸ¼, ê³„ì ˆê³¼ ë‚ ì”¨ë¥¼ ê³ ë ¤í•´ ì¶”ì²œì„ ë‚´ì£¼ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ëŠ” ì•„ì´ë””ì–´ì…ë‹ˆë‹¤. (ì˜ˆì‹œ) ìƒí™© ìœ ì €ì˜ ìƒê° ì¶”ì²œ ê°€ëŠ¥ ìŒì‹ --- --- --- ë§‘ê³  ì²­ëª…í•œ ë‚  â€œë°–ì— ë‚˜ê°€ì„œ ë¨¹ê³  ì‹¶ì–´â€ ìƒŒë“œìœ„ì¹˜, ê¹€ë°¥, ë¶„ì‹ íë¦¬ê³  ìŠµí•œ ë‚  â€œë­”ê°€ ì‹œì›í•œ ê²Œ ë•¡ê¸´ë‹¤â€ ëƒ‰ë©´, ë¬¼íšŒ, ì•„ì´ìŠ¤ì»¤í”¼ ë¹„ ì˜¬ ê²ƒ ê°™ì€ ë‚  â€œì§‘ì— ì¼ì° ê°€ê³  ì‹¶ì–´â€ êµ­ë¬¼ ìš”ë¦¬, ì¹¼êµ­ìˆ˜, í•´ì¥êµ­ ê²¨ìš¸ì²  â€œëŒ€ê²Œê°€ ì œì² ì´ë„¤?â€ ëŒ€ê²Œì°œ, ì–´íƒ•êµ­ìˆ˜, ì „ê³¨ë¥˜ ê³„ì¡€ ë‚ ì”¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— í¬í•¨ì‹œì¼œì•¼ í•˜ëŠ” ì´ìœ ë¡œ 3ê°€ì§€ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì™œ ê³„ì ˆ ë‚ ì”¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— í¬í•¨í•´ì•¼ í• ê¹Œ? ê°œì¸í™” ì¶”ì²œ ê°•í™” â†’ ë™ì¼í•œ ìœ ì €ë„ ë‚ ì”¨ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§ â†’ â€˜ìœ ì € + ë‚ ì”¨â€™ ì¡°í•© ê¸°ë°˜ì˜ ë” ë˜‘ë˜‘í•œ ì¶”ì²œ ê°€ëŠ¥ ëª¨ë¸ì˜ ì •ë°€ë„ í–¥ìƒ â†’ ê¸°ì¡´ ëª¨ë¸ì— ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ Cold Start ìƒí™©ì—ì„œë„ ê°•ë ¥í•œ ë³´ì™„ â†’ ìœ ì € ì •ë³´ê°€ ì—†ì„ ë•Œë„, ê·¸ë‚ ì˜ ë‚ ì”¨ + ì¸ê¸° ë©”ë‰´ë¡œ í•©ë¦¬ì ì¸ ì¶”ì²œ ê°€ëŠ¥ ì—¬ê¸°ì— ì¶”ê°€ë¡œ ì €ëŠ” ì¶”ì²œì˜ ê·¼ê±°ë¥¼ ì œì‹œí•´ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì‚¬ìš©ìê°€ ì¶”ì²œì˜ ê²°ê³¼ë¥¼ ë” ì‹ ë¢°í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. êµ¬í˜„ìƒí™©ì—ì„œì˜ ë¬¸ì œì , ë°ì´í„°ì˜ í•œê³„ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë°ì´í„°ëŠ” ì¹´ì¹´ì˜¤ë§µì„ í¬ë¡¤ë§í•˜ì—¬ ë§Œë“  ë°ì´í„°ì…ë‹ˆë‹¤. ì¹´ì¹´ì˜¤ë§µì— ìˆëŠ” ë¦¬ë·°ëŠ” ì‚¬ìš©ìê°€ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•œ í›„ì— ë°”ë¡œ ì‘ì„±í•˜ëŠ”ê²Œ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ì‘ì„± ì‹œì ê³¼ ì‹¤ì œ ì‚¬ìš©í•œ ì‹œì ì´ ë‹¤ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‚¬ìš©í•œ ì‹œì ì˜ ë‚ ì”¨ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ EDA ê²°ê³¼ë¥¼ ë°œí‘œìë‹˜ê»˜ì„œ ë°œí‘œí•´ì£¼ì…¨ìŠµë‹ˆë‹¤. > ë¦¬ë·° ë‚ ì§œì™€ ë°©ë¬¸ë‚ ì§œê°€ ê°™ë‹¤ëŠ” ê°€ì •ì„ íƒ€ë‹¹í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ë°©ë²• ë¦¬ë·°ì–´ ì¤‘ì— ë§¤ì¼ ë¦¬ë·°ë¥¼ ì“°ëŠ” ì‚¬ëŒë§Œ ê³ ë¥´ì > ì‘ì„±í•œ ë¦¬ë·°ë“¤ì˜ ì¼ìì˜ ì°¨ì´ê°€ í‰ê· ì ìœ¼ë¡œ 1ì¼ì •ë„ ë‚˜ëŠ” ìœ ì €, ì¦‰ ë§¤ì¼ ë¦¬ë·°ë¥¼ ì‘ì„±í•˜ëŠ” ë¦¬ë·°ì–´ë“¤ì„ í†µí•´ ë°ì´í„°ë¥¼ í†µí•´ ì´ë¥¼ í•´ê²°í•˜ì ì¦‰ ë¦¬ë·° ë‚ ì§œì™€ ë°©ë¬¸ë‚ ì§œê°€ ê°™ë‹¤ê³  ì¶”ì •ë˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§í•˜ëŠ” ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ë°ì´í„°ì˜ í•œê³„ë¥¼ ì–´ëŠì •ë„ëŠ” ê·¹ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/ê³µë¶€ë°©í–¥",
    "title": "ì½”ë”©í…ŒìŠ¤íŠ¸ ê³µë¶€ ë°©í–¥",
    "date": "2025-07-08",
    "excerpt": "ì½”ë”©í…ŒìŠ¤íŠ¸ ê³µë¶€ ë°©í–¥ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Codility Study",
    "tags": [
      "ê³ ë¯¼ì •ë¦¬"
    ],
    "content": "ë°°ê²½ ì ê¹œ ê³µë¶€í•˜ë‹¤ê°€ ë‚´ë ¤ë†“ê³  ë‹¤ì‹œ ê³µë¶€ë¥¼ ì‹œì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì£¼ë‹ˆì–´, í˜¹ì€ ì‹œë‹ˆì–´ë”ë¼ë„ ì½”ë”©í…ŒìŠ¤íŠ¸ëŠ” ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ê°€ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. (ì ì–´ë„ 10ë…„ ì´í•˜ê¹Œì§€ëŠ”?) ê·¸ë˜ì„œ ê¾¸ì¤€íˆ ê³µë¶€í•´ë³´ë ¤ê³  í•˜ëŠ”ë° ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì¡ì„ì§€ì— ëŒ€í•œ ì •ë¦¬ë¥¼ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ë˜ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ê°™ì´ í•˜ëŠ” 'ì‹ ë³´í˜„'ë‹˜ì˜ ê²½í—˜ì„ ë“¤ì–´ë³´ë‹ˆ, ë¼ì´ë¸Œì½”í…ŒëŠ” í•„ìˆ˜ì ìœ¼ë¡œ ë³´ê³  ë‚œì´ë„ê°€ ê½¤ ìˆì–´ì„œ ê¾¸ì¤€í•œ ì¤€ë¹„ê°€ í•„ìš”í•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì§€ê¸ˆë¶€í„°ë¼ë„ ê¾¸ì¤€íˆ ê³µë¶€í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì½”í…Œ ê³µë¶€í•˜ê¸° ì¢‹ì€ ê³³ (GPT ì •ë¦¬) > ì™€ ì—„ì²­ ì •ë¦¬ ì˜í•´ì£¼ë„¤ìš”... ì½”ë”© í…ŒìŠ¤íŠ¸(ì½”í…Œ) ê³µë¶€ì— ë„ì›€ì´ ë˜ëŠ” ì¢‹ì€ ì‚¬ì´íŠ¸ë“¤ì„ ì•„ë˜ì— ìš©ë„ë³„ë¡œ ì •ë¦¬í•´ë“œë¦´ê²Œìš”. ê° ì‚¬ì´íŠ¸ë§ˆë‹¤ íŠ¹ì„±ì´ ë‹¤ë¥´ë‹ˆ, ë³¸ì¸ì˜ í•™ìŠµ ëª©ì (ê¸°ì´ˆ ë‹¤ì§€ê¸°, ë¬¸ì œ í’€ì´, ì‹¤ì „ ëŒ€ë¹„ ë“±)ì— ë§ì¶° ì„ íƒí•˜ì‹œë©´ ë©ë‹ˆë‹¤. --- âœ… 1. ë¬¸ì œ í’€ì´ ì¤‘ì‹¬ ë°±ì¤€ ì˜¨ë¼ì¸ ì €ì§€ (BOJ) URL: https://www.acmicpc.net/ íŠ¹ì§•: êµ­ë‚´ ì½”í…Œ ì¤€ë¹„ìë“¤ì´ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì‚¬ì´íŠ¸. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜, ë‚œì´ë„ë³„ ë¬¸ì œ ì œê³µ. ë‹¨ê³„ë³„ë¡œ í’€ì–´ë³´ê¸°, ë¬¸ì œì§‘ ê¸°ëŠ¥ìœ¼ë¡œ ì²´ê³„ì ì¸ í•™ìŠµ ê°€ëŠ¥. C++, Python, Java ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›. í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ (Programmers) URL: https://programmers.co.kr/learn/challenges íŠ¹ì§•: ì‹¤ì œ ê¸°ì—… ì½”ë”©í…ŒìŠ¤íŠ¸ ë¬¸ì œ ìˆ˜ë¡ (ì¹´ì¹´ì˜¤, ë„¤ì´ë²„ ë“±). ì‹¤ì „ ê°ê°ì„ ê¸°ë¥´ê¸°ì— ì í•©. ë ˆë²¨ë³„ë¡œ ë¶„ë¥˜ë˜ì–´ ìˆì–´ì„œ ì…ë¬¸ìë¶€í„° ê³ ê¸‰ìê¹Œì§€ í™œìš© ê°€ëŠ¥. Python, JavaScript ë“± ì‹¤ë¬´ì— ë§ì´ ì“°ëŠ” ì–¸ì–´ì— ìµœì í™”. LeetCode URL: https://leetcode.com/ íŠ¹ì§•: ê¸€ë¡œë²Œ IT ê¸°ì—…(FAANG) ì½”í…Œ ë¬¸ì œ ë‹¤ìˆ˜ ìˆ˜ë¡. ë‚œì´ë„ ë‹¤ì–‘ (Easy, Medium, Hard). ì˜ì–´ ê¸°ë°˜ì´ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ ì‹¤ë ¥ì„ ì‹¤ì§ˆì ìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ê¸°ì— íš¨ê³¼ì . ë¬¸ì œë§ˆë‹¤ ë””ìŠ¤ì»¤ì…˜ê³¼ ì†”ë£¨ì…˜ì´ ì˜ ë˜ì–´ ìˆìŒ. --- âœ… 2. ì•Œê³ ë¦¬ì¦˜ ê°œë…/ì´ë¡  í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ë¹„ì£¼ì–¼ë¼ì´ì € URL: https://visualgo.net/ íŠ¹ì§•: ìë£Œêµ¬ì¡° ë° ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ì›ë¦¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ. DFS, BFS, ì •ë ¬, í™, íŠ¸ë¦¬ ë“± ê¸°ì´ˆ ê°œë… ì •ë¦½ì— íƒì›”. Tistory / velog ë¸”ë¡œê·¸ ê²€ìƒ‰ í™œìš© ë°©ë²•: \"[ë¬¸ì œ ì´ë¦„] + ì•Œê³ ë¦¬ì¦˜ í’€ì´\"ë¡œ ê²€ìƒ‰í•˜ë©´, í’€ì´ ê³¼ì • + ì‹œê°„ ë³µì¡ë„ ë¶„ì„ê¹Œì§€ ì„¤ëª…ëœ ë¸”ë¡œê·¸ ê¸€ì´ ë§ìŒ. ì´ë¡  + êµ¬í˜„ì„ ì—°ê²°ì§€ì–´ í•™ìŠµí•˜ê¸° ì¢‹ìŒ. --- âœ… 3. ëª¨ì˜ ì½”ë”© í…ŒìŠ¤íŠ¸ / ì‹¤ì „ ì—°ìŠµ ì½”ë“œì—… (CodeUp) URL: https://codeup.kr/ íŠ¹ì§•: ì…ë¬¸ìì—ê²Œ ì í•©í•œ ê¸°ì´ˆ ë¬¸ì œë¶€í„° ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œê¹Œì§€ ì œê³µ. ë‹¨ê³„ë³„ í•™ìŠµìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ ì‹¤ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ëŒì–´ì˜¬ë¦¼. Codeforces URL: https://codeforces.com/ íŠ¹ì§•: ì£¼ê¸°ì ì¸ ì˜¨ë¼ì¸ ëŒ€íšŒ ìš´ì˜. ì‹¤ì „ ê°ê° ë° ì†ë„, êµ¬í˜„ ëŠ¥ë ¥ í–¥ìƒì— ë§¤ìš° íš¨ê³¼ì . ëŸ¬ì‹œì•„ ê¸°ë°˜ ì»¤ë®¤ë‹ˆí‹°ì´ë‚˜ ì˜ì–´ ì§€ì›. --- âœ… 4. ë¬¸ì œ ì¶”ì²œ/ê°€ì´ë“œ ìë£Œ GitHub ì €ì¥ì†Œ ì˜ˆì‹œ: https://github.com/tony9402/baekjoon ë°±ì¤€ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜ ê¸°ë°˜ ì¶”ì²œ ë¬¸ì œ ëª¨ìŒ https://github.com/ellynhan/boj-algorithm-practice ê¸°ì´ˆ â†’ ê³ ê¸‰ ìˆœì„œë¡œ ì •ë¦¬ëœ ë°±ì¤€ ë¬¸ì œì§‘ --- ğŸ” ì½”í…Œ ì¤€ë¹„ ë°©ë²• ìš”ì•½ (ì´ë¡ ì  ë°°ê²½) ê¸°ì´ˆ ê°œë… ìŠµë“: ì‹œê°„ ë³µì¡ë„, ì •ë ¬, íƒìƒ‰, ì¬ê·€, DP, ê·¸ë¦¬ë”” ë“± ì´ë¡  í•™ìŠµ. ë¬¸ì œ ìœ í˜•ë³„ í•™ìŠµ: êµ¬í˜„ â†’ ì •ë ¬ â†’ íƒìƒ‰(BFS/DFS) â†’ ìŠ¤íƒ/í â†’ DP â†’ ê·¸ë˜í”„ ìˆœì„œ ì¶”ì²œ. ë¬¸ì œ í’€ì´ + ë³µìŠµ: ì˜¤ë‹µ ë…¸íŠ¸ í•„ìˆ˜. ê°™ì€ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì„œ í’€ì–´ë³´ëŠ” ê²ƒì´ ì¤‘ìš”. ì‹¤ì „ ëª¨ì˜ í…ŒìŠ¤íŠ¸: ì‹¤ì œ ì‹œí—˜ ì‹œê°„ì²˜ëŸ¼ ì œí•œ ì‹œê°„ ì•ˆì— í‘¸ëŠ” ì—°ìŠµì´ íš¨ê³¼ì . ë‚´ ìƒê°ì€? ê¸°ì´ˆë¶€í„° í•˜ë‚˜ì”© ìŠµë“í•˜ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ì›í•˜ëŠ” íšŒì‚¬ ê¸°ì¶œë¬¸ì œë¶€í„° í’€ì–´ë³´ë©´ì„œ ì±„ì›Œë‚˜ê°€ëŠ” ê²ƒë„ ì¢‹ì€ ë°©í–¥ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¹ì¥ 7ì›”ì— ìˆëŠ” í˜„ëŒ€ ì˜¤í† ì—ë²„ ì§€ì›í•˜ê³  ì—¬ê¸° ì½”í…Œ ê¸°ì¶œë¬¸ì œë¶€í„° í’€ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš©ë“¤ì„ í•˜ë‚˜ì”© ì •ë¦¬í•´ë³´ê³˜ìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-07-03-importance-of-numbers",
    "title": "ìˆ˜ì¹˜ì˜ ì¤‘ìš”ì„±",
    "date": "2025-07-03",
    "excerpt": "ìˆ˜ì¹˜ë¡œ í‘œí˜„í•˜ëŠ”ê²Œ ì™œ ì¤‘ìš”í•œì§€ì— ëŒ€í•œ ìƒê° ì •ë¦¬",
    "category": "Reflections",
    "tags": [
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-fwl",
    "title": "FWL(Frisch-Waugh-Lovell) ì •ë¦¬ë€?",
    "date": "2025-07-02",
    "excerpt": "í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "ì°¸ê³ ìë£Œ ì‹¤ë¬´ë¡œ í†µí•˜ëŠ” ì¸ê³¼ì¶”ë¡  with python ë„¤ì´ë²„ë¸”ë¡œê·¸_ (ì„ í˜•ëª¨í˜•) 04 Frisch-Waugh-Lovell ì •ë¦¬, ìˆ˜ì‹ í’€ì´ê°€ ì˜ë˜ì–´ ìˆìŒ ë°°ê²½ ì„¤ëª… --- ì¸ê³¼ì¶”ë¡ ì´ë€ \"ì›ì¸ -> ê²°ê³¼\"ì˜ íŒ¨í„´ì¸ \"ì¸ê³¼ê´€ê³„\"ë¥¼ ì°¾ëŠ”ë°ì— ëª©ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ íŒ¨í„´ì„ ì°¾ëŠ”ê±´ ì–´ë ¤ìš´ ì¼ì¸ë°, ê·¸ ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ì›ì¸ê³¼ ê²°ê³¼ì— ëª¨ë‘ ì˜í–¥ì„ ì£¼ëŠ” êµë€ë³€ìˆ˜ ë•Œë¬¸ì…ë‹ˆë‹¤. êµë€ë³€ìˆ˜(confounding variable)ë€ ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ì— ëª¨ë‘ ì˜í–¥ì„ ì¤˜ ì˜ëª»ëœ ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ë„ë¡ ë§Œë“œëŠ” ë³€ìˆ˜ë¥¼ ë§í•©ë‹ˆë‹¤. ì¦‰ êµë€ë³€ìˆ˜ê°€ ì˜ëª»ëœ íŒ¨í„´ì„ ì°¾ë„ë¡ ìœ ë„í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. !confounding variable ì˜ˆì‹œê·¸ë¦¼ ê°€ì¥ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ê³¼ \"ìƒì–´ ì–´íƒíšŸìˆ˜\"ë¥¼ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"ê¸°ì˜¨\"ì´ë¼ëŠ” ë³€ìˆ˜ëŠ” \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ê³¼ \"ìƒì–´ì˜ ê³µê²©íšŸìˆ˜\"ì— ëª¨ë‘ ì˜í–¥ì„ ì¤ë‹ˆë‹¤. ê·¸ë˜ì„œ \"ê¸°ì˜¨\"ì´ ì˜¬ë¼ê°€ì„œ \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ì´ ì¦ê°€í•˜ê³  \"ìƒì–´ì˜ ê³µê²©íšŸìˆ˜\"ê°€ ì¦ê°€í•œ ê²ƒì¸ë°, \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ì´ ì¦ê°€í•´ì„œ \"ìƒì–´ì˜ ê³µê²©íšŸìˆ˜\"ê°€ ì¦ê°€í–ˆë‹¤ëŠ” ìƒê°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰ \"êµë€ ë³€ìˆ˜\"ëŠ” ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ì˜ëª» í•´ì„í•  ê°€ëŠ¥ì„±ì„ ë§Œë“¤ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ë³¼ ë• êµë€ë³€ìˆ˜ë¥¼ ì œê±°í•´ì£¼ëŠ”ê²Œ êµ‰ì¥íˆ ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. > êµë€ë³€ìˆ˜ë¥¼ ì—†ì• ì•¼ í•œë‹¤ëŠ” ê±´ ì´í•´í–ˆì–´, ê·¸ëŸ¬ë©´ ì´ê±¸ ì–´ë–»ê²Œ ì—†ì•¨ ê±´ë°? ê°€ì¥ ì¢‹ì€ ê±´ êµë€ë³€ìˆ˜ë¥¼ íŒŒì•…í•˜ê³  ì‹¤í—˜ ì„¤ê³„ë¥¼ í†µí•´ ì—†ì• ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \"ê¸°ì˜¨\"ì´ë¼ëŠ” êµë€í¸ìˆ˜ë¥¼ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µì œí•œ í›„ì— ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ êµë€ë³€ìˆ˜ë¥¼ íŒŒì•…í•˜ë”ë¼ë„ ì´ë¥¼ í†µì œí•  ìˆ˜ ì—†ì„ ë•Œê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìˆ˜ìˆ ë°©ë²•(ë…ë¦½ë³€ìˆ˜)ê³¼ ìˆ˜ìˆ  í›„ ê²°ê³¼(ì¢…ì†ë³€ìˆ˜)ì˜ ê´€ê³„ì—ì„œëŠ” ë‚˜ì´, ìƒí™œìŠµê´€ ë“±ì˜ êµë€ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. êµë€ë³€ìˆ˜ë¥¼ íŒŒì•…í•˜ì§€ë§Œ ì´ë¥¼ í†µì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. > ì´ë¥¼ ìœ„í•´ êµë€ë³€ìˆ˜ì˜ ì˜í–¥ì„ \"í†µê³„ì \"ìœ¼ë¡œ ì œê±°í•´ì£¼ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. FWL Theorem ì´ë€? --- > ì•ì„œ ì„¤ëª…í•œ êµë€ë³€ìˆ˜ì˜ ì˜í–¥, ì¦‰ í¸í–¥ì„ ë³´ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. FWL ì •ë¦¬ì˜ ì´ë¡ ì  ë°°ê²½ì€ ë‹¤ì¤‘íšŒê·€ì˜ ê°•ë ¥í•¨ì— ìˆìŠµë‹ˆë‹¤. > ë‹¤ì¤‘ íšŒê·€ ë¶„ì„ì—ì„œ íšŒê·€ ê³„ìˆ˜ì˜ ì˜ë¯¸ëŠ” ëª¨ë¸ì˜ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ íš¨ê³¼ë¥¼ ê³ ë ¤í•œ í›„ ì–»ì€ ë™ì¼ ì„¤ëª…ë³€ìˆ˜ì˜ ì´ë³€ëŸ‰ê³„ìˆ˜ë‹¤. ì´ëŠ” ë‹¤ì¤‘íšŒê·€ëª¨í˜•ì—ì„œ íŠ¹ì • ê´€ì‹¬ ë³€ìˆ˜ì˜ OLS ì¶”ì •ê³„ìˆ˜ëŠ”, (1) ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ë¥¼ í•œêº¼ë²ˆì— í¬í•¨í•˜ì—¬ íšŒê·€ë¶„ì„í•œ ê²°ê³¼ì™€, (2) í†µì œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ì¢…ì†ë³€ìˆ˜ì™€ ê´€ì‹¬ ë³€ìˆ˜ ê°ê°ì˜ ì”ì°¨ë¥¼ êµ¬í•œ ë’¤ ì´ ì”ì°¨ë“¤ ê°„ì— íšŒê·€ë¶„ì„í•œ ê²°ê³¼ê°€ ë™ì¼í•˜ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ìˆ˜í•™ì  í‘œí˜„ ë‹¤ì¤‘íšŒê·€ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k + \\epsilon$$ ì—¬ê¸°ì„œ $Y$ëŠ” ì¢…ì†ë³€ìˆ˜, $X_1$ì€ ê´€ì‹¬ ë³€ìˆ˜, $X_2, \\ldots, X_k$ëŠ” í†µì œ ë³€ìˆ˜ë“¤ì…ë‹ˆë‹¤. FWL ì •ë¦¬ì— ë”°ë¥´ë©´, $\\beta_1$ì˜ OLS ì¶”ì •ì¹˜ëŠ” ë‹¤ìŒ ë‘ ë°©ë²•ìœ¼ë¡œ ë™ì¼í•˜ê²Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ë°©ë²• 1: ì „ì²´ ëª¨ë¸ì—ì„œ ì§ì ‘ ì¶”ì • $$\\hat{\\beta}_1 = \\frac{\\text{Cov}(X_1, Y)}{\\text{Var}(X_1)}$$ ë°©ë²• 2: ì”ì°¨í™”ë¥¼ í†µí•œ ì¶”ì • $Y$ë¥¼ $X_2, \\ldots, X_k$ì— ëŒ€í•´ íšŒê·€í•˜ì—¬ ì”ì°¨ $e_Y$ ê³„ì‚° $X_1$ì„ $X_2, \\ldots, X_k$ì— ëŒ€í•´ íšŒê·€í•˜ì—¬ ì”ì°¨ $e_{X_1}$ ê³„ì‚° $e_Y$ë¥¼ $e_{X_1}$ì— ëŒ€í•´ íšŒê·€í•˜ì—¬ $\\hat{\\beta}_1$ ì¶”ì • $$e_Y = Y - \\hat{Y}_{X_2,\\ldots,X_k}$$ $$e_{X_1} = X_1 - \\hat{X_1}_{X_2,\\ldots,X_k}$$ $$\\hat{\\beta}_1 = \\frac{\\text{Cov}(e_{X_1}, e_Y)}{\\text{Var}(e_{X_1})}$$ ì‹¤ì œë¡œ ìˆ˜ì‹ì ìœ¼ë¡œë„ ê°™ê³  (ì°¸ê³ ìë£Œ 2 ì°¸ê³ ) í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´ë„ ê°™ìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì—ì„œ ë‹¤ì¤‘íšŒê·€ëŠ” ì˜ˆì¸¡ ëª¨ë¸ì´ ì•„ë‹Œ ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ëŠ” ëª¨ë¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. R-learner, ì¸ê³¼ì¶”ë¡  ëª¨ë¸ë¡œ êµ¬í˜„í•˜ê¸° --- ì •ë¦¬í•˜ë©´ FWLì´ë€ ì”ì°¨í™”ë¥¼ í†µí•´ êµë€ë³€ìˆ˜ë¥¼ ì§êµí™” ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. FWLì„ meta learner í˜•íƒœë¡œ êµ¬í˜„í•œê²Œ R-learnerì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì”ì°¨í™”ë¥¼ ìœ„í•œ 2ê°œì˜ nuisance functionì´ í•„ìš”í•˜ê³ , CATEë¥¼ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. R-learnerì˜ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: $$\\mathcal{L}(\\tau) = \\mathbb{E}\\left[\\left(Y - m(X) - \\tau(X)(T - e(X))\\right)^2\\right]$$ ì—¬ê¸°ì„œ: $m(X) = \\mathbb{E}[Y X]$: ê²°ê³¼ ëª¨ë¸ (outcome model) $e(X) = \\mathbb{E}[T X]$: ì²˜ë¦¬ ëª¨ë¸ (treatment model) $\\tau(X)$: ì¡°ê±´ë¶€ í‰ê·  ì²˜ë¦¬ íš¨ê³¼ (CATE) ë‹¤ì¤‘íšŒê·€ì™€ ë¹„êµí•´ì„œ FWLì€ meta leanerë¡œ êµ¬í˜„í•  ë•Œì˜ ì¥ì ì€ 'ë¹„ì„ í˜•ì ì¸ íŒ¨í„´'ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ êµ¬í˜„ëœ R-learnerë¥¼ í†µí•´ CATE(Conditional Average Treatment Effect)ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/correlation-vs-causation",
    "title": "ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ì˜ ì´í•´",
    "date": "2025-06-28",
    "excerpt": "ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ì˜ ì°¨ì´ì ì„ ì´í•´í•˜ê³ , ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì™€ ì¸ê³¼ì¶”ë¡ ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•œ ì´ì•¼ê¸°",
    "category": "Causal Inference",
    "tags": [
      "ìƒê´€ê´€ê³„",
      "ì¸ê³¼ê´€ê³„",
      "ì¸ê³¼ì¶”ë¡ "
    ],
    "content": "> ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ ì´í•´í•˜ê³  êµ¬ë¶„í•˜ëŠ” ê±´ ê²°ê³¼ë¥¼ í•´ì„í•  ë•Œ ì¤‘ìš”í•œ ì ì…ë‹ˆë‹¤. ìƒê´€ê´€ê³„ë€, ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ì¼ì •í•œ íŒ¨í„´ì´ë‚˜ ë™ë°˜ ë³€ë™ì´ ì¡´ì¬í•¨ì„ ì˜ë¯¸í•˜ë©°, í•œ ë³€ìˆ˜ê°€ ì»¤ì§ˆ ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ì¼ì •í•˜ê²Œ ì»¤ì§€ê±°ë‚˜(ì–‘ì˜ ìƒê´€), ì‘ì•„ì§€ëŠ”(ìŒì˜ ìƒê´€) ê²½í–¥ì„ ë³´ì´ëŠ” í†µê³„ì  ê´€ê³„ë¥¼ ë§í•©ë‹ˆë‹¤. ì¸ê³¼ê´€ê³„ë€, í•œ ë³€ìˆ˜ê°€ ë³€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ê·¸ ë³€í™”ì— ì˜í•´ ì§ì ‘ì Â·ì²´ê³„ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ì•„ ë³€í™”í•˜ëŠ” ê´€ê³„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤ ë³´í†µ ë‘ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ì‚´í´ë³¼ ë•Œ ìƒê´€ê´€ê³„ë¥¼ ì‚´í´ë³´ê³¤ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìƒê´€ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ í•´ì„í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ê³¼ ì¼ì¼ ê¸°ì˜¨ ì‚¬ì´ì—ëŠ” ë†’ì€ ê¸°ì˜¨ì¼ìˆ˜ë¡ íŒë§¤ëŸ‰ì´ ì¦ê°€í•˜ëŠ” ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ê´€ì°°ë˜ì§€ë§Œ, ì´ëŠ” ê¸°ì˜¨ì´ ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ì„ ì§ì ‘ â€œì›ì¸â€í•œë‹¤ê³  ë‹¨ì •í•  ìˆ˜ ì—†ìœ¼ë©°, ì‚¬ëŒë“¤ì´ ë”ìš´ ë‚ ì”¨ì— ë°–ì— ë‚˜ì™€ íŒë§¤ëŸ‰ì´ ëŠ˜ì–´ë‚˜ëŠ” ë“± ë‹¤ì–‘í•œ ì œ3ì˜ ìš”ì¸ì´ ë™ì‹œì— ì‘ìš©í–ˆì„ ê°€ëŠ¥ì„±ì„ ë°°ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. > ì˜ˆì¸¡ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ê°„ì˜ ë™ì‹œ ë³€ë™ íŒ¨í„´, ì¦‰ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•´ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê²°ê³¼ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ë³´ì¥í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ê°€ê²© ë§¤ì¶œ --- --- --- 1 100 1000 2 150 900 3 600 10000 4 700 12000 ì˜ˆë¥¼ ë“¤ì–´ ìˆ™ë°• ê°€ê²©ê³¼ ë§¤ì¶œì´ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ê°€ê²©ì´ ë†’ì„ìˆ˜ë¡ ë§¤ì¶œì´ ë†’ì•„ ë³´ì¸ë‹¤ë©´, ëª¨ë¸ì€ â€œê°€ê²©ì´ ì˜¤ë¥´ë©´ ë§¤ì¶œì´ ì˜¤ë¥¸ë‹¤â€ê³  ì˜ˆì¸¡í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ íŒ¨í„´ ë’¤ì—ëŠ” â€œì„±ìˆ˜ê¸°/ë¹„ìˆ˜ê¸° ì—¬ë¶€â€ë¼ëŠ” ìˆ¨ê²¨ì§„ ì™¸ë¶€ ìš”ì¸ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ê²©ì´ ë¹„ìˆ˜ê¸°ì— ë‚®ì•„ì§€ê³  ì„±ìˆ˜ê¸°ì— ì˜¬ë¼ê°€ëŠ” ë™ì‹œì— ë§¤ì¶œë„ í•¨ê»˜ ë³€í–ˆë‹¤ë©´, ì‹¤ì œë¡œëŠ” ê³„ì ˆì„±ì´ ë§¤ì¶œì„ ê²°ì •ì§“ëŠ” ì§„ì§œ ì›ì¸ì¸ë°, ë‹¨ìˆœ ì˜ˆì¸¡ ëª¨ë¸ì€ ì´ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìƒê´€ê´€ê³„ì— ê¸°ë°˜í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ì˜¤í•´í•˜ë©´ â€œë§¤ì¶œì„ ì˜¬ë¦¬ë ¤ë©´ ê°€ê²©ì„ ì˜¬ë ¤ì•¼ í•œë‹¤â€ëŠ” ì˜ëª»ëœ ê²°ë¡ ì— ì´ë¥¼ ìˆ˜ ìˆê³ , ì‹¤ì œ íš¨ê³¼ê°€ ìˆëŠ” ê°œì…(ì˜ˆ: ë¹„ìˆ˜ê¸° í”„ë¡œëª¨ì…˜) ëŒ€ì‹  ë¶€ì ì ˆí•œ ê°€ê²© ì •ì±…ì„ ì„ íƒí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì´ëŸ¬í•œ ìˆ¨ê²¨ì§„ êµë€ ìš”ì¸ê¹Œì§€ ê³ ë ¤í•´, ì¡°ì‘ ê°€ëŠ¥í•œ ì›ì¸ì´ ì‹¤ì œë¡œ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í†µê³„ì ìœ¼ë¡œ ë¶„ë¦¬Â·ì¶”ì •í•¨ìœ¼ë¡œì¨ ì˜¬ë°”ë¥¸ ì˜ì‚¬ê²°ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-causal-inference",
    "title": "ì¸ê³¼ì¶”ë¡ ì´ë€?",
    "date": "2025-06-20",
    "excerpt": "ì¸ê³¼ì¶”ë¡ ì˜ ê°œë…ì— ëŒ€í•´ ì†Œê°œí•˜ëŠ” ì´ì•¼ê¸°",
    "category": "Causal Inference",
    "tags": [
      "ìƒê´€ê´€ê³„",
      "ì¸ê³¼ê´€ê³„",
      "ì¸ê³¼ì¶”ë¡ "
    ],
    "content": "â€œì¸ê³¼ì¶”ë¡ â€ (Causal Inference)ì€ ìµœê·¼ì— ì˜ë£Œ ë¶„ì•¼, ë§ˆì¼€íŒ… ë¶„ì•¼ ë“± ì„ íƒì— ëŒ€í•œ ë¶„ì„ì´ í•„ìš”í•œ ê³³ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ê°œë…ì…ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì— ë‹µì„ í•©ë‹ˆë‹¤. â€œë§ˆì¼€íŒ… ë¹„ìš©ì„ ëŠ˜ë ¸ë”ë‹ˆ ë§¤ì¶œì´ ì¦ê°€í–ˆë„¤, ë§ˆì¼€íŒ… ë¹„ìš©ì„ ëŠ˜ë ¤ì„œ ê·¸ëŸ°ê±´ê°€?â€ . â€œí™˜ìì—ê²Œ Aë¼ëŠ” ì•½ë¬¼ì„ ì²˜ë°©í–ˆë”ë‹ˆ ìƒíƒœê°€ ê´œì°®ì•„ì¡Œì–´. A ë•ë¶„ì¸ê°€?â€ ì´ëŸ¬í•œ ì§ˆë¬¸ë“¤ì€ ì „í†µì ìœ¼ë¡œ ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ í†µí•´ ì¦ëª…ë˜ì–´ì™”ìŠµë‹ˆë‹¤. ì‹¤í—˜í•˜ë ¤ëŠ” ì¡°ê±´ ì™¸ì— ë‹¤ë¥¸ ì¡°ê±´ì€ ëª¨ë‘ í†µì œí•œ í›„ì— ì‹¤í—˜ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìƒˆë¡œ ê°œë°œí•œ ë¹„ë£Œì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ê°™ì€ í’ˆì¢…ì˜ ì˜¥ìˆ˜ìˆ˜ ë¬˜ëª© 100í¬ê¸°ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´, ì‹¤í—˜êµ°ì—ëŠ” ìƒˆ ë¹„ë£Œë¥¼ í‘œì¤€ëŸ‰ë§Œí¼ íˆ¬ì…í•˜ê³  ëŒ€ì¡°êµ°ì—ëŠ” ê¸°ì¡´ ë¹„ë£Œ(í˜¹ì€ ë¹„ë£Œë¥¼ ì „í˜€ ì£¼ì§€ ì•ŠìŒ)ë¥¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë•Œ ë¬¼ì£¼ê¸°, ì¼ì¡°ëŸ‰, í† ì–‘ ì„±ë¶„, ì˜¨ë„ ë“± ë‚˜ë¨¸ì§€ ì¬ë°° ì¡°ê±´ì€ ë‘ ê·¸ë£¹ì—ì„œ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤. ì¼ì • ê¸°ê°„ í›„ ë‘ ê·¸ë£¹ì˜ ìƒì¥ ì†ë„, ìˆ˜í™•ëŸ‰, ì‘ë¬¼ ê±´ê°• ìƒíƒœë¥¼ ë¹„êµí•˜ë©´, ì˜¤ë¡œì§€ â€œë¹„ë£Œ ì¢…ë¥˜â€ì˜ ì°¨ì´ë§Œì´ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ê°€ì¥ í•©ë¦¬ì ì´ì§€ë§Œ, í˜„ì‹¤ ë¬¸ì œì— ì ìš©í•˜ê¸°ì—” ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ë§ˆë¸”ì˜ ë©€í‹°ë²„ìŠ¤ ì„¸ê³„ê´€ì´ ì•„ë‹ˆë¼ë©´, 2025ë…„ 7ì›” 1ì¼ì— ë§ˆì¼€íŒ… ë¹„ìš©ì„ 100ë§Œì›ê³¼ 1000ë§Œì›ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•œ ëª…ì˜ í™˜ìì—ê²Œ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ 12.1ë¥¼ ì‚½ì…í•œ í›„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³ , ë Œì¦ˆë¥¼ ëº€ í›„ 12.6ë¥¼ ì‚½ì…í•´ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦‰, ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ ì„¤ì •í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì´ë¥¼ í†µê³„ì ìœ¼ë¡œ ë³´ì™„í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œ Aë¼ëŠ” ê´‘ê³  ìº í˜ì¸ì´ íŒë§¤ëŸ‰ì— ë¯¸ì¹œ ì˜í–¥ì„ ì•Œê³  ì‹¶ì„ ë•Œ, ì‹¤ì œë¡œëŠ” ë™ì¼í•œ ê³ ê°ì—ê²Œ ê´‘ê³ ë¥¼ ë³´ì—¬ì£¼ì§€ ì•Šì€ ìƒíƒœì™€ ë³´ì—¬ì¤€ ìƒíƒœë¥¼ ëª¨ë‘ ê²½í—˜ì‹œí‚¬ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¸ê³¼ì¶”ë¡  ê¸°ë²•ì„ í™œìš©í•©ë‹ˆë‹¤. ì´ë•Œ ê³ ê°ì˜ ì—°ë ¹, ì„±ë³„, ê³¼ê±° êµ¬ë§¤ ì´ë ¥ ë“± ë‹¤ì–‘í•œ ê³ ê° íŠ¹ì„±ì„ ë³´ê³ , ê´‘ê³ ë¥¼ ë³¸ ê·¸ë£¹ê³¼ ë³´ì§€ ì•Šì€ ê·¸ë£¹ ê°„ì— ì´ íŠ¹ì„± ë¶„í¬ë¥¼ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°(ì˜ˆ: ì„±í–¥ì ìˆ˜ ë§¤ì¹­) ê°€ìƒì˜ ëŒ€ì¡°êµ°ì„ ë§Œë“­ë‹ˆë‹¤. ì´í›„ ë‘ ê·¸ë£¹ì˜ í‰ê·  êµ¬ë§¤ì•¡ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ë©´, â€œê´‘ê³  íš¨ê³¼â€ë§Œì„ í†µê³„ì ìœ¼ë¡œ ë¶„ë¦¬í•´ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì‹¤ì œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ ë§ˆì¹˜ ë¬´ì‘ìœ„ëŒ€ì¡°ì‹œí—˜ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì¸ê³¼ì  í•´ì„ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•  ë•Œë„, ë§ˆì¹˜ ê´‘ê³  íš¨ê³¼ë¥¼ ë¶„ì„í•˜ë“¯ì´ í™˜ìë§ˆë‹¤ ë‚˜ì´Â·êµ´ì ˆë„Â·ê°ë§‰ ë‘ê»˜ ê°™ì€ íŠ¹ì§•ë“¤ì„ ê³ ë ¤í•´ â€œ12.1, 12.6, 13.2 í¬ê¸° ë Œì¦ˆâ€ë¥¼ ì“´ í™˜ìë“¤ê³¼ ë¹„ìŠ·í•œ ì¡°ê±´ì˜ ê°€ìƒì˜ ëŒ€ì¡°êµ°ì„ ë§Œë“¤ì–´ ë¹„êµí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 30ëŒ€ ê·¼ì‹œ í™˜ì ì¤‘ì— 12.6mm ë Œì¦ˆë¥¼ ì‚¬ìš©í•œ ê·¸ë£¹ê³¼ 12.1mm ë Œì¦ˆë¥¼ ì‚¬ìš©í•œ ê·¸ë£¹ì´ ë¹„ìŠ·í•œ ëˆˆ êµ¬ì¡°ì™€ ì‹œë ¥ì¹˜ë¥¼ ê°–ê³  ìˆë„ë¡ í†µê³„ì ìœ¼ë¡œ ì¡°ì •í•œ ë’¤, ë‘ ê·¸ë£¹ì˜ ìˆ˜ìˆ  í›„ ì‹œë ¥ ê°œì„  ì •ë„ ì°¨ì´ë¥¼ ë¶„ì„í•˜ë©´ â€œ12.6mm ë Œì¦ˆë¥¼ ì“°ë©´ ì‹œë ¥ì´ ì–¼ë§ˆë‚˜ ë” ì¢‹ì•„ì§€ëŠ”ì§€â€ë¥¼ ì¸ê³¼ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì–»ì€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‹¤ì œë¡œëŠ” ì¨ë³´ì§€ ì•Šì€ 12.3mm ê°™ì€ ì¤‘ê°„ í¬ê¸°ì˜ ë Œì¦ˆê°€ ê°€ì ¸ì˜¬ íš¨ê³¼ë„ ì˜ˆì¸¡í•  ìˆ˜ ìˆê³ , ìµœì¢…ì ìœ¼ë¡œëŠ” ê° í™˜ìì—ê²Œ ê°€ì¥ íš¨ê³¼ì ì¸ ë Œì¦ˆ í¬ê¸°ë¥¼ ê³¼í•™ì ìœ¼ë¡œ ì¶”ì²œí•´ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Multi-Study-R-Learner-for-Estimating-Heterogeneous-Treatment-Effects-Across-Studies-Using-Statistical-Machine-Learning",
    "title": "[Paper Review] Multi-Study R-Learner for Estimating Heterogeneous Treatment Effects Across Studies Using Statistical Machine Learning",
    "date": "2025-01-16",
    "excerpt": "Multi-Study R-Learner ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·° ë° ë¶„ì„",
    "category": "Causal Inference",
    "tags": [
      "R-Learner",
      "Heterogeneous Treatment Effects",
      "Multi-Study",
      "Statistical Machine Learning",
      "Paper Review"
    ],
    "content": "paper link",
    "public": true
  },
  {
    "id": "Self_Development/Career/Graduate School/OMSCSê´€ë ¨",
    "title": "OMSCS(Georgia Tech ì˜¨ë¼ì¸ CS ì„ì‚¬) ì§€ì› ì¤€ë¹„ ê³„íš",
    "date": "2025-01-15",
    "excerpt": "ì„¸ê³„ ìµœê³  ê°€ì„±ë¹„ CS ì„ì‚¬ í”„ë¡œê·¸ë¨ ë¶„ì„ê³¼ ì§€ì› ì „ëµ",
    "category": "Career",
    "tags": [
      "ëŒ€í•™ì›",
      "OMSCS",
      "Georgia Tech",
      "ì»´í“¨í„°ì‚¬ì´ì–¸ìŠ¤"
    ],
    "content": "ë°°ê²½ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ì—ì„œ ë” ê¹Šì´ ìˆëŠ” ì»¤ë¦¬ì–´ë¥¼ ìŒ“ê¸° ìœ„í•´ ì„ì‚¬ ê³¼ì •ì„ ê³ ë ¤í•˜ë˜ ì¤‘, OMSCS(Georgia Tech Online Master of Science in Computer Science)ë¼ëŠ” í˜ì‹ ì ì¸ í”„ë¡œê·¸ë¨ì„ ë°œê²¬í–ˆë‹¤. ì„¸ê³„ í†±ê¸‰ CS í”„ë¡œê·¸ë¨ì„ ì˜¨ë¼ì¸ìœ¼ë¡œ, ê·¸ê²ƒë„ ê·¹ë„ë¡œ ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ ì´ìˆ˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ë§¤ë ¥ì ì´ë‹¤. OMSCSë€? ğŸ“Š ê¸°ë³¸ ì •ë³´ ì •ì‹ëª…ì¹­: Georgia Institute of Technology Online Master of Science in Computer Science ê°œì„¤ì—°ë„: 2014ë…„ (Udacity, AT&Tì™€ í˜‘ë ¥) ì´ ë¹„ìš©: ì•½ $7,000-$8,500 (í•œí™” 900ë§Œ-1,100ë§Œì›) í•™ìœ„: ìº í¼ìŠ¤ì™€ ë™ì¼í•œ Georgia Tech MS in Computer Science (ì˜¨ë¼ì¸ í‘œê¸° ì—†ìŒ) ê¸°ê°„: í‰ê·  2-3ë…„ (íŒŒíŠ¸íƒ€ì„ìœ¼ë¡œ ì§„í–‰ ê°€ëŠ¥) ê³¼ì • êµ¬ì„±: 10ê°œ ê³¼ì • (30í•™ì ) í•©ê²©ë¥ : ì•½ 70% âœ… ì£¼ìš” ì¥ì  ì••ë„ì ì¸ ê°€ì„±ë¹„ ì „ì²´ í”„ë¡œê·¸ë¨ ë¹„ìš©ì´ $7,000-$8,500 (í•œí™” 900ë§Œ-1,100ë§Œì›) íƒ€ ëª…ë¬¸ëŒ€ ì˜¨ìº í¼ìŠ¤ í”„ë¡œê·¸ë¨ ëŒ€ë¹„ 1/5~1/10 ìˆ˜ì¤€ ëª…ë¬¸ëŒ€ ë¸Œëœë“œ + ë™ë“±í•œ í•™ìœ„ Georgia TechëŠ” CS ë¶„ì•¼ ì„¸ê³„ 8ìœ„ (US News 2024) ì¡¸ì—…ì¥ì— \"ì˜¨ë¼ì¸\" í‘œê¸° ì—†ìŒ ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì—ì„œ ì¸ì •ë°›ëŠ” ë¸Œëœë“œ ë‹¤ë¥¸ ë¸”ë¡œê·¸ ê¸€ë“¤ì´ë‚˜ ë§í¬ë“œì¸ ë´¤ì„ ë•Œ job interview ê¸°íšŒë„ í™•ì‹¤íˆ ë” ì–»ì„ ìˆ˜ ìˆë‹¤. í•´ì™¸ ì´ì§ë„ ê³ ë ¤í•˜ê³  ìˆìœ¼ë‹ˆê¹Œ ì§€ê¸ˆ ìƒí™©ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì¢‹ì€ ì„ íƒì´ ì•„ë‹ê¹Œ. ìœ ì—°ì„± í’€íƒ€ì„ ì§ì¥ê³¼ ë³‘í–‰ ê°€ëŠ¥ ìì‹ ì˜ í˜ì´ìŠ¤ë¡œ ì§„ë„ ì¡°ì ˆ ì „ ì„¸ê³„ ì–´ë””ì„œë‚˜ ìˆ˜ê°• ê°€ëŠ¥ ë³´í†µ 1ê³¼ëª©ë‹¹ ì£¼ë‹¹ 20~40ì‹œê°„ì •ë„ í•„ìš”í•˜ë‹¤ê³  í•¨. ì‹¤ë¬´ ì¤‘ì‹¬ ì»¤ë¦¬í˜ëŸ¼ Machine Learning, AI, Computer Vision ë“± í˜„ì—… ì—”ì§€ë‹ˆì–´ë“¤ì´ ì§ì ‘ í™œìš©í•  ìˆ˜ ìˆëŠ” ìŠ¤í‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜ í•™ìŠµ âš ï¸ ë‹¨ì  ë° ê³ ë ¤ì‚¬í•­ ë†’ì€ ìê¸°ê´€ë¦¬ ìš”êµ¬ ì˜¨ë¼ì¸ íŠ¹ì„±ìƒ ê°•í•œ ìê¸° ë™ê¸°ë¶€ì—¬ í•„ìš” ì‹œê°„ ê´€ë¦¬ì™€ í•™ìŠµ ê³„íš ìˆ˜ë¦½ ëŠ¥ë ¥ ì¤‘ìš” ì¡¸ì—…ì´ ì–´ë ¤ì›€ ë„¤íŠ¸ì›Œí‚¹ ì œí•œ êµìˆ˜ë‚˜ ë™ë£Œë“¤ê³¼ì˜ ì§ì ‘ì ì¸ ë§Œë‚¨ ë¶€ì¡± ì—°êµ¬ ê¸°íšŒë‚˜ ì¸í„´ì‹­ ì—°ê²° ì–´ë ¤ì›€ Redisì—ì„œ í™œë°œí•˜ê²Œ ì´ì•¼ê¸°ê°€ ë‚˜ì˜¤ê³  ìˆê¸´ í•¨. ê·¸ë˜ë„ ë„¤íŠ¸ì›Œí‚¹ì€ ë§ì´ ë¶€ì¡±í•¨. í•œêµ­ì—ì„œì˜ ì¸ì‹ ì•„ì§ êµ­ë‚´ì—ì„œëŠ” ì¸ì§€ë„ê°€ ë‚®ìŒ ì „í†µì ì¸ í•œêµ­ ê¸°ì—…ì—ì„œëŠ” í‰ê°€ ë¶ˆë¶„ëª… ê·¸ë˜ë„ ì•Œê³  ìˆëŠ” ì‚¬ëŒë“¤ì€ ì¶©ë¶„íˆ ì¢‹ê²Œ ë´ì£¼ëŠ” ê±¸ë¡œ ì´í•´ ì¤‘ ì–¸ì–´ ì¥ë²½ ëª¨ë“  ìˆ˜ì—…ì´ ì˜ì–´ë¡œ ì§„í–‰ TOEFL 100ì  ì´ìƒ í•„ìš” í† í”Œ ì¤€ë¹„ê°€ ê°€ì¥ ê¸‰í•˜ë‹¤. ì§€ì› ì¼ì • ë° ì „ëµ ğŸ“… ì§€ì› ì¼ì • Spring í•™ê¸° ì§€ì› ë§ˆê°ì¼: 8ì›” 15ì¼ Fall í•™ê¸° ì§€ì› ë§ˆê°ì¼: 3ì›” 1ì¼ Priority Decision: ë§ˆê°ì¼ ì´ì „ë¶€í„° ìˆœì°¨ì  í•©ê²© ë°œí‘œ ë‚´ë…„ ê°€ì„ í•™ê¸° ëª©í‘œë¡œ 3ì›”ê¹Œì§€ ì¤€ë¹„ ì„œë¥˜ ê°–ì¶”ê¸° ì¤€ë¹„í•´ì•¼ í•˜ëŠ” ê²ƒë“¤ [x] OMSCS í”„ë¡œê·¸ë¨ ìƒì„¸ ì¡°ì‚¬ ì™„ë£Œ [ ] TOEFL ê¸°ì¡´ ì ìˆ˜ í™•ì¸ ë˜ëŠ” ì‘ì‹œ ì¼ì • ì¡ê¸° [ ] ì¶”ì²œì¸ 3ëª…ì—ê²Œ ì—°ë½ (8ì›” 10ì¼ê¹Œì§€ ì œì¶œ ìš”ì²­) ì¶”ì²œì¸ë“¤ì—ê²Œ ë¦¬ë§ˆì¸ë” ë©”ì‹œì§€ í•„ìš”í•œ ì •ë³´ ì¦‰ì‹œ ì œê³µ [ ] ëŒ€í•™êµ ì˜ë¬¸ ì„±ì ì¦ëª…ì„œ ë°œê¸‰ ì‹ ì²­ [ ] Personal Statement ì´ˆì•ˆ ì‘ì„± ì‹œì‘ [ ] ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ê²½í—˜ + OMSCS ì§€ì› ë™ê¸° ëª…í™•íˆ [ ] ì˜¨ë¼ì¸ ìƒ˜í”Œë“¤ ì°¸ê³ í•˜ì—¬ ë¹ ë¥´ê²Œ ì‘ì„± [ ] ì´ë ¥ì„œ OMSCS ë§ì¶¤ ì—…ë°ì´íŠ¸ ê¸°ìˆ  í”„ë¡œì íŠ¸, í”„ë¡œê·¸ë˜ë° ê²½í—˜ ê°•ì¡° ğŸ“… í•©ê²© í›„ ê³„íš (2025.9 ~ 2026.1) [ ] TOEFL ì ìˆ˜ í–¥ìƒ (í•©ê²© í›„ì—ë„ ê³„ì†) ì‹¤ì œ ìˆ˜ì—…ì—ì„œ í•„ìš”í•œ Speaking/Writing ì‹¤ë ¥ [ ] CS ê¸°ì´ˆ ì§€ì‹ ë³´ê°• ìë£Œêµ¬ì¡°/ì•Œê³ ë¦¬ì¦˜ ì²´ê³„ì  ë³µìŠµ Python, Java ë“± í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ìˆ™ë ¨ë„ í–¥ìƒ [ ] OMSCS ì»¤ë®¤ë‹ˆí‹° í™œë™ í•©ê²©ì ê·¸ë£¹ ì°¸ì—¬ ì²« í•™ê¸° ìˆ˜ê°• ê³¼ëª© ë¦¬ì„œì¹˜ [ ] ìˆ˜í•™ ê¸°ì´ˆ ê°•í™” ì„ í˜•ëŒ€ìˆ˜, í™•ë¥ í†µê³„, ì´ì‚°ìˆ˜í•™ ë³µìŠµ [ ] Foundational Course ì„ íƒ ì²˜ìŒì—ëŠ” 1ê°œ ê³¼ëª©ë§Œ ìˆ˜ê°• ê¶Œì¥ CS 6515 (Algorithms) ë˜ëŠ” CS 6601 (AI) ê³ ë ¤ [ ] í•™ìŠµ í™˜ê²½ êµ¬ì¶• ì˜¨ë¼ì¸ í•™ìŠµì— ìµœì í™”ëœ í™˜ê²½ ì„¤ì • ì‹œê°„ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶• --- ğŸ¯ í•µì‹¬ ìš°ì„ ìˆœìœ„ ğŸ”¥ HIGH (í•„ìˆ˜) TOEFL 100ì  ì´ìƒ - ê°€ì¥ ì¤‘ìš”í•œ ì •ëŸ‰ì  ì§€í‘œ Personal Statement - í•©ê²©ì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ìš”ì†Œ ì¶”ì²œì„œ 3ê°œ - ì œ3ì ê´€ì ì—ì„œì˜ ê²€ì¦ ğŸ“ˆ MEDIUM (ì¤‘ìš”) CS ê¸°ì´ˆ ì§€ì‹ - ë©´ì ‘ì´ë‚˜ ì¶”ê°€ ì§ˆë¬¸ ëŒ€ë¹„ í”„ë¡œê·¸ë˜ë° í”„ë¡œì íŠ¸ - ì‹¤ë¬´ ì—­ëŸ‰ ì¦ëª… ì„±ì ì¦ëª…ì„œ - ê¸°ë³¸ ìê²© ìš”ê±´ ğŸ’¡ LOW (í”ŒëŸ¬ìŠ¤ ìš”ì†Œ) OMSCS ì»¤ë®¤ë‹ˆí‹° í™œë™ - ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ ê¸°ìˆ  ë¸”ë¡œê·¸ - í•™ìŠµ ì˜ì§€ í‘œí˜„ --- â° ì£¼ê°„ í•™ìŠµ ê³„íš (ì˜ˆì‹œ) í‰ì¼ (ì›”-ê¸ˆ) ì•„ì¹¨ 1ì‹œê°„: ì˜ì–´ ê³µë¶€ (TOEFL ì¤€ë¹„) ì €ë… 1ì‹œê°„: CS ê¸°ì´ˆ ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì£¼ë§ í† ìš”ì¼ 3-4ì‹œê°„: ì§€ì›ì„œ ì‘ì„± ë° í”„ë¡œì íŠ¸ ì¼ìš”ì¼ 2-3ì‹œê°„: ë³µìŠµ ë° ë‹¤ìŒ ì£¼ ê³„íš ì´ ì£¼ë‹¹ ì•½ 10-12ì‹œê°„ íˆ¬ì ì»¤ë¦¬ì–´ì— ë¯¸ì¹  ì˜í–¥ ê¸ì •ì  ì˜í–¥ ê¸°ìˆ ì  ê¹Šì´: í˜„ì¬ ì‹¤ë¬´ ê²½í—˜ì— ì´ë¡ ì  ë°±ê·¸ë¼ìš´ë“œ ì¶”ê°€ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬: ì „ ì„¸ê³„ OMSCS ë™ë¬¸ë“¤ê³¼ì˜ ì—°ê²° ê²½ë ¥ ë°œì „: ML ì—”ì§€ë‹ˆì–´, ë¦¬ì„œì¹˜ ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ í¬ì§€ì…˜ ì§„ì¶œ ê°€ëŠ¥ ë¸Œëœë“œ ê°€ì¹˜: Georgia Tech í•™ìœ„ì˜ ì‹ ë¢°ë„ í˜„ì‹¤ì  ê³ ë ¤ì‚¬í•­ ì‹œê°„ íˆ¬ì: ì£¼ë‹¹ 15-20ì‹œê°„ í•™ìŠµ ì‹œê°„ í•„ìš” ì—…ë¬´ ë³‘í–‰: í˜„ì¬ ì§ì¥ê³¼ì˜ ê· í˜• ìœ ì§€ ì „ëµ í•„ìš” ROI ê´€ì : í•™ìœ„ ì·¨ë“ í›„ ì‹¤ì œ ì—°ë´‰ ìƒìŠ¹ ë° ì»¤ë¦¬ì–´ ê¸°íšŒ ì¦ê°€ ì—¬ë¶€ ê²°ë¡  OMSCSëŠ” í˜„ì¬ ë‚´ ìƒí™©ì— ë§¤ìš° ì í•©í•œ ì„ íƒì§€ë¡œ ë³´ì¸ë‹¤. íŠ¹íˆ: ê²½ì œì  ë¶€ë‹´ ìµœì†Œí™”: ê¸°ì¡´ ì§ì¥ ìœ ì§€í•˜ë©´ì„œ ì§„í–‰ ê°€ëŠ¥ ì‹¤ë¬´ ì—°ê³„: í˜„ì¬ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì—…ë¬´ì™€ ì§ì ‘ì  ì—°ê´€ ë¯¸ë˜ ê°€ì¹˜: AI/ML ë¶„ì•¼ ì „ë¬¸ì„± ê°•í™”ë¡œ ì»¤ë¦¬ì–´ í™•ì¥ ë‹¤ë§Œ ì„±ê³µì ì¸ ì™„ì£¼ë¥¼ ìœ„í•´ì„œëŠ” ì² ì €í•œ ì‹œê°„ ê´€ë¦¬ì™€ í•™ìŠµ ê³„íšì´ í•„ìˆ˜ì ì´ë‹¤. ë¬´ì—‡ë³´ë‹¤ ì™œ ì´ í”„ë¡œê·¸ë¨ì„ í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ë™ê¸°ë¶€ì—¬ê°€ ê°€ì¥ ì¤‘ìš”í•  ê²ƒ ê°™ë‹¤. Spring 2026 ì…í•™ì„ ëª©í‘œë¡œ ì²´ê³„ì ì¸ ì¤€ë¹„ë¥¼ ì‹œì‘í•´ë³´ì! ğŸ¯ --- ì°¸ê³  ìë£Œ OMSCS ê³µì‹ ì›¹ì‚¬ì´íŠ¸ OMSCS ì§€ì› ê°€ì´ë“œë¼ì¸ OMSCS ì»¤ë®¤ë‹ˆí‹° ë° í›„ê¸°",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/test-private-manski",
    "title": "[Private] Test Manski Bounds Analysis",
    "date": "2025-01-01",
    "excerpt": "ë¹„ê³µê°œ í…ŒìŠ¤íŠ¸ í¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. ê´€ë¦¬ì ëª¨ë“œì—ì„œë§Œ ë³´ì…ë‹ˆë‹¤.",
    "category": "Causal Inference",
    "tags": [
      "Test",
      "Private",
      "Manski"
    ],
    "content": "ë¹„ê³µê°œ í…ŒìŠ¤íŠ¸ í¬ìŠ¤íŠ¸ ì´ í¬ìŠ¤íŠ¸ëŠ” public: false ì„¤ì •ìœ¼ë¡œ ì¸í•´ ê´€ë¦¬ì ëª¨ë“œì—ì„œë§Œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚´ìš© URLì— ?admin=true ë˜ëŠ” ?admin=Trueë¥¼ ì¶”ê°€í•˜ë©´ ì´ í¬ìŠ¤íŠ¸ë¥¼ ë³¼ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ í•­ëª© í¬ìŠ¤íŠ¸ ëª©ë¡ì—ì„œ í‘œì‹œ ì—¬ë¶€ ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í‘œì‹œ ì—¬ë¶€ ì§ì ‘ URL ì ‘ê·¼ ì‹œ ì°¨ë‹¨ ì—¬ë¶€ ì‹œê°ì  í‘œì‹œ (ğŸ”’ ì•„ì´ì½˜, ë¹¨ê°„ í…Œë‘ë¦¬) ì´ ë‚´ìš©ì´ ë³´ì¸ë‹¤ë©´ ê´€ë¦¬ì ëª¨ë“œê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤!",
    "public": false
  },
  {
    "id": "Self_Development/Career/Portfolio/chatbot",
    "title": "AI Chatbot Project",
    "date": "2024-12-01",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "AI Chatbot Project í”„ë¡œì íŠ¸ ê°œìš” ë³‘ì› ìƒë‹´ ì—…ë¬´ë¥¼ ìë™í™”í•˜ê¸° ìœ„í•œ RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ì§€ëŠ¥í˜• ì±—ë´‡ ì‹œìŠ¤í…œì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì¹´ì¹´ì˜¤í†¡ ì±„ë„ì„ í†µí•´ ê³ ê°ì—ê²Œ ì œê³µë˜ë©°, ë°˜ë³µì ì¸ ìƒë‹´ ì§ˆë¬¸ì— ëŒ€í•œ ìë™ ì‘ë‹µì„ í†µí•´ ìƒë‹´ì‚¬ì˜ ì—…ë¬´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±: ë¬¸ì„œ ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•œ ì •í™•í•œ ë‹µë³€ ì œê³µ ë§¥ë½ ì¸ì‹ ëŒ€í™”: ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•œ ì—°ì†ì ì¸ ìƒë‹´ ì§€ì› ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´ ë° ì˜ì–´ ì§€ì› ì‹¤ì‹œê°„ ì‘ë‹µ: ë¹ ë¥¸ ì‘ë‹µ ì†ë„ë¡œ ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ ìë™ ëª¨ë‹ˆí„°ë§: LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ ìë™ í‰ê°€ ì‹œìŠ¤í…œ ê¸°ìˆ  ìŠ¤íƒ Backend: Python, FastAPI NLP: Transformers, spaCy, OpenAI GPT Vector Database: Qdrant, ChromaDB Embedding: BGE-m3 Deployment: Docker, AWS Monitoring: Slack API í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±… ë¬¸ì„œ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ë¬¸ì œ: ì˜ë¯¸ì  ê²€ìƒ‰ì˜ í•œê³„ë¡œ ì¸í•œ ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„ í•´ê²°ì±…: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•: Qdrantë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ë©”íƒ€ë°ì´í„° í•„í„°ë§: ì˜ì‚¬ë³„, ì§„ë£Œê³¼ë³„ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì •ë°€ ê²€ìƒ‰ êµ¬í˜„ ì„ë² ë”© ëª¨ë¸ ì„ ì •: BGE-m3 ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì˜µì…˜ê³¼ ë¹„êµí•˜ì—¬ ìµœì  ì„±ëŠ¥ í™•ì¸ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§: ê³ ìœ ëª…ì‚¬(ì˜ì‚¬ ì´ë¦„ ë“±) ì²˜ë¦¬ ê°œì„ ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° í•„í„°ë§ ì§ˆë¬¸ ì„¸ë¶„í™”: ë³µí•© ì§ˆë¬¸ì„ ë‹¨ìˆœ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ ëŒ€í™” ë§¥ë½ ë°˜ì˜: Chat historyë¥¼ í™œìš©í•œ ì§ˆë¬¸ ì¬ì •ì˜ ê²°ê³¼: ê²€ìƒ‰ ì •í™•ë„ 95% ì´ìƒ ë‹¬ì„± ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ ë¬¸ì œ: ì±—ë´‡ì´ ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë°˜ë³µ ì‘ë‹µìœ¼ë¡œ ì¸í•œ ì‚¬ìš©ì ë¶ˆë§Œ í•´ê²°ì±…: ì§ˆë¬¸ ë¶„ë¥˜ ëª¨ë¸: ë‹µë³€ ê°€ëŠ¥/ë¶ˆê°€ëŠ¥ ì§ˆë¬¸ì„ ì‚¬ì „ íŒë³„í•˜ëŠ” ë¶„ë¥˜ ëª¨ë¸ ì¶”ê°€ ìƒë‹´ì‚¬ ì—°ê²°: ë‹µë³€ ë¶ˆê°€ ì§ˆë¬¸ ì‹œ ì¦‰ì‹œ ì‹¤ì œ ìƒë‹´ì‚¬ë¡œ ì—°ê²°í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„ ê²°ê³¼: ì‚¬ìš©ì ë§Œì¡±ë„ 90% ì´ìƒ ë‹¬ì„± ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ìµœì í™” ë¬¸ì œ: ìˆ˜ë™ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì¸í•œ ë†’ì€ ìš´ì˜ ë¹„ìš© (í•˜ë£¨ 30ë¶„ ì´ìƒ ì†Œìš”) í•´ê²°ì±…: ìë™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ: LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ í‰ê°€ ìë™í™” í•µì‹¬ ì§€í‘œ ì¶”ì : ë‹µë³€ ì¶œì²˜ ì í•©ì„±: ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í‰ê°€ ì§ˆë¬¸-ë¬¸ì„œ ì—°ê´€ì„±: ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ ê°„ì˜ ê´€ë ¨ì„± í‰ê°€ ì„ê³„ì¹˜ ê¸°ë°˜ ì•Œë¦¼: í’ˆì§ˆ ì ìˆ˜ê°€ ì„ê³„ì¹˜ ì´í•˜ì¸ ëŒ€í™”ë§Œ ì•Œë¦¼ ì „ì†¡ ê²°ê³¼: ëª¨ë‹ˆí„°ë§ ë¹„ìš© 80% ê°ì†Œ ë‹µë³€ ìƒì„± ìµœì í™” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë‹¤ìŒ 3ê°€ì§€ ê´€ì ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í–ˆìŠµë‹ˆë‹¤: ë¬¸ì„œ ì‚¬ìš©ì„±: ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë¬¸ì„œì— ê¸°ë°˜í–ˆëŠ”ì§€ ì¹œì ˆì„±: ì–¼ë§ˆë‚˜ ì¹œì ˆí•˜ê³  ìƒëƒ¥í•˜ê²Œ ì‘ë‹µí•˜ëŠ”ì§€ ì •í™•ë„: ì–¼ë§ˆë‚˜ ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ì„ í•˜ëŠ”ì§€ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì˜ë£Œ ì •ë³´ì˜ ì •í™•ì„±ì€ ë§¤ìš° ì¤‘ìš”í•˜ë¯€ë¡œ: ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œë§Œ ë‹µë³€ ìƒì„±í•˜ë„ë¡ ê°•ì œ RAGAS ì»¨ì…‰ì„ í™œìš©í•œ ë‹µë³€ ê¸°ë°˜ì„± ìë™ í‰ê°€ LLMì´ ì¶”ì¶œí•œ ëª…ì œ/ê°œë…ì˜ ë¬¸ì„œ ê¸°ë°˜ì„± ê²€ì¦ ê²°ê³¼ ë° ì„±ê³¼ ë‹µë³€ ì •í™•ë„: 95% ì´ìƒ ë‹¬ì„± ì‚¬ìš©ì ë§Œì¡±ë„: 90% ì´ìƒ ë‹¬ì„± ëª¨ë‹ˆí„°ë§ ìë™í™”: ìœ ì§€ë³´ìˆ˜ ë¹„ìš© 80% ê°ì†Œ ì—…ë¬´ íš¨ìœ¨ì„±: ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ì±—ë´‡ìœ¼ë¡œ ëŒ€ì²´ ì‘ë‹µ ì†ë„: í‰ê·  0.5ì´ˆ ì´ë‚´ í•™ìŠµí•œ ì  RAG ì‹œìŠ¤í…œì˜ ì‹¤ì œ ì ìš© ë°©ë²•ê³¼ í•œê³„ì  ì‚¬ìš©ì ê²½í—˜ì„ ê³ ë ¤í•œ ì±—ë´‡ ì„¤ê³„ì˜ ì¤‘ìš”ì„± ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ êµ¬ì¶• ë°©ë²• ì˜ë£Œ ë¶„ì•¼ì—ì„œì˜ AI ì‹œìŠ¤í…œ ì‹ ë¢°ì„± í™•ë³´ ë°©ë²•",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/lenze_size_rec",
    "title": "Lens Size Recommendation System",
    "date": "2024-11-15",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "ì •ì˜í–ˆë˜ ë¬¸ì œë“¤, í•´ê²°í•˜ëŠ” ê³¼ì •, í•´ê²°í•œ ê²°ê³¼ í”„ë¡œì íŠ¸ ì„¤ëª… í”„ë¡œì íŠ¸ ê°œìš” ì‹œë ¥êµì •ì„¼í„°ì—ì„œ ë Œì¦ˆ ì‚½ì…ìˆ ì„ ìœ„í•œ ìµœì ì˜ ë Œì¦ˆ í¬ê¸°ë¥¼ ì¶”ì²œí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹œìŠ¤í…œì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì˜ì‚¬ì˜ ê²½í—˜ê³¼ ë…¸í•˜ìš°ì— ì˜ì¡´í•˜ë˜ ë Œì¦ˆ ì„ íƒ ê³¼ì •ì„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°ê´€í™”í•˜ì—¬, í™˜ìë³„ ë§ì¶¤í˜• ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸: í™˜ì ì •ë³´ì™€ ë Œì¦ˆ í¬ê¸°ë¥¼ ì…ë ¥ë°›ì•„ ìˆ˜ìˆ  ê²°ê³¼(Vault) ì˜ˆì¸¡ ì¸ê³¼ì¶”ë¡  ê¸°ë°˜: í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ì œê³µ ì˜í•™ì  ì§ê´€ ë°˜ì˜: ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ ì„¤ê³„ í•´ì„ ê°€ëŠ¥ì„±: ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ê·¼ê±°ì™€ ì„¤ëª… ì œê³µ ì‹¤ì‹œê°„ ì¶”ë¡ : BentoML ê¸°ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸ ì„œë¹™ ê¸°ìˆ  ìŠ¤íƒ Backend: Python, Flask, FastAPI Machine Learning: Scikit-learn, LightGBM, XGBoost Causal Inference: Orthogonalization, Double ML Model Serving: BentoML, Docker Database: PostgreSQL Monitoring: Airflow, Flask Dashboard í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ë¬¸ì œ: ë‹¨ìˆœí•œ ì˜ˆì¸¡ ì„±ëŠ¥ë§Œìœ¼ë¡œëŠ” ì„ íƒì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•  ìˆ˜ ì—†ìŒ í•´ê²°ì±…: ì¸ê³¼ì¶”ë¡  ì ìš©: êµë€í¸í–¥ ë“± í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ì—¬ ì¸ê³¼ì  ê´€ê³„ ê¸°ë°˜ ì˜ˆì¸¡ ì§êµí™”(Orthogonalization): ì˜ˆì¸¡ ë³€ìˆ˜ì™€ êµë€ ë³€ìˆ˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ í¸í–¥ ì œê±° ì˜í•™ì  ì§ê´€ ë°˜ì˜: ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ ì„¤ê³„ ê²°ê³¼: ê²°ì •ê³„ìˆ˜(RÂ²) 0.7 ë‹¬ì„±ìœ¼ë¡œ ì˜ˆì¸¡ ì‹ ë¢°ë„ í™•ë³´ ì‚¬ìš©ì ì‹ ë¢°ë„ ë° ë§Œì¡±ë„ í–¥ìƒ ë¬¸ì œ: ë†’ì€ í‰ê°€ì§€í‘œê°€ ì‚¬ìš©ì ì‹ ë¢°ì™€ ë§Œì¡±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŒ í•´ê²°ì±…: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê·¼ê±° ì œì‹œ: AI ì˜ˆì¸¡ ê²°ê³¼ì™€ í•¨ê»˜ ì˜í•™ì  ê·¼ê±° ì œê³µ ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜: ì£¼ê¸°ì ì¸ í”¼ë“œë°± ìˆ˜ì§‘ ë° UI/ë¦¬í¬íŒ… ê¸°ëŠ¥ ê°œì„  ì°¨ë³„í™” ì¦ëª…: ì—°êµ¬ ë‚´ìš©ì„ í•™ìˆ  ë…¼ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì—¬ ì œí’ˆ ì°¨ë³„ì  ì…ì¦ ê²°ê³¼: ì‚¬ìš©ì ì¹œí™”ì ì¸ ì„œë¹„ìŠ¤ ì„¤ê³„ë¡œ ì‹ ë¢°ë„ í–¥ìƒ ë°ì´í„° í’ˆì§ˆ ë° íŠ¹ì„± ë¶„ì„ ë¬¸ì œ: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„° í’ˆì§ˆê³¼ ì¼ê´€ì„± í™•ë³´ í•´ê²°ì±…: ì¢…í•©ì  EDA: ì¢Œì•ˆ/ìš°ì•ˆ ì°¨ì´, ê²€ì‚¬ ì¥ë¹„ ì˜¤ë¥˜, ì˜ì‚¬ë³„ ì°¨ì´ ë“± ë¶„ì„ ë°ì´í„° ê²€ì¦: OCR ê²°ê³¼, ìˆ˜ê¸° ì…ë ¥ ë°ì´í„°ì˜ ì™„ì „ì„± ê²€ì¦ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: ë Œì¦ˆ íšŒì „ê°ë„, ì¢…ë¥˜, ë‘ê»˜ ë“± ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ ì¶”ê°€ ê²°ê³¼: ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë° ì˜ˆì¸¡ ì„±ëŠ¥ ê°œì„  ëª¨ë¸ë§ ì ‘ê·¼ë²• ëª¨ë¸ë§ ì¡°ê±´ ì„¤ì • ì„œë¹„ìŠ¤ ëª©í‘œì— ë§ëŠ” ëª¨ë¸ ì¡°ê±´ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í–ˆìŠµë‹ˆë‹¤: ë‹¨ì¡°ì„± ì¡°ê±´: íŠ¹ì • ë³€ìˆ˜ì™€ ì˜ˆì¸¡ê°’ ê°„ì˜ ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ê´€ê³„ ìœ ì§€ í•´ì„ ê°€ëŠ¥ì„±: ëª¨ë¸ì´ í•´ì„ ê°€ëŠ¥í•˜ê±°ë‚˜ ê²°ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆì–´ì•¼ í•¨ ëª¨ë¸ ì„ íƒ ë° ìµœì í™” Linear Regression ê³„ì—´: Lasso, Ridge, ElasticNet (íšŒê·€ ê³„ìˆ˜ë¡œ ë‹¨ì¡°ì„± í™•ë³´) GBDT ê³„ì—´: LightGBM, XGBoost, CatBoost (monotonic constraint ì ìš©) ì•™ìƒë¸”: Votingê³¼ Stackingì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: Bayesian optimization í™œìš© ì¼ë°˜í™” ì„±ëŠ¥ í™•ë³´ Data Leakage ë°©ì§€: ì‹œê°„ ê¸°ë°˜ test set êµ¬ì„± ë° unknown ì²˜ë¦¬ ì˜í•™ì  ë…¼ë¦¬ ê²€ì¦: íšŒê·€ ê³„ìˆ˜, feature importance, SHAP ë¶„ì„ì„ í†µí•œ ì •ì„±í‰ê°€ ì •ê¸°ì  ì„±ëŠ¥ í™•ì¸: íŠ¹ì • ì£¼ê¸°ë¡œë§Œ test set í™•ì¸í•˜ì—¬ ê³¼ì í•© ë°©ì§€ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ëª¨ë¸ ì„œë¹™ BentoML í™œìš©: Docker ì´ë¯¸ì§€ë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ ë²„ì „ ê´€ë¦¬ ì˜¨í”„ë ˆë¯¸ìŠ¤ ë°°í¬: ê¸°ì¡´ ì„œë²„ ë¦¬ì†ŒìŠ¤ í™œìš©í•œ ì¶”ë¡  ì„œë²„ êµ¬ì¶• ë°ì´í„° ì¼ê´€ì„± í™•ë³´ Feature Store êµ¬ì¶•: í•™ìŠµê³¼ ì¶”ë¡ ì—ì„œ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ FastAPI ê¸°ë°˜: ORMì„ í™œìš©í•œ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ Flask Dashboard: ëª¨ë¸ ì„±ëŠ¥ ë° ë°ì´í„° ë¶„í¬ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ Airflow ìŠ¤ì¼€ì¤„ë§: ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì—…ë°ì´íŠ¸ í˜„ì¬ ì—°êµ¬ ë° ê°œì„  ë°©í–¥ í•œê³„ì  ì¸ì‹ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„í–ˆìŠµë‹ˆë‹¤: ì¸¡ì • í•œê³„: ê²€ì‚¬ ì¥ë¹„ëŠ” í™ì±„ ìœ—ë¶€ë¶„ë§Œ ì¸¡ì • ê°€ëŠ¥ êµ¬ì¡°ì  ìš”ì†Œ: ì‹¤ì œ ë Œì¦ˆ ì‚½ì… ìœ„ì¹˜(ì„¬ëª¨ì²´ ë‚­) ì •ë³´ ë¶€ì¡± ëª¨ë¸ í•œê³„: Linear/GBDT ëª¨ë¸ë¡œëŠ” latent space êµ¬ì„± ì–´ë ¤ì›€ í–¥í›„ ì—°êµ¬ ë°©í–¥ êµ¬ì¡°ì  ìš”ì†Œ ì¶”ì •: ë³´ì´ì§€ ì•ŠëŠ” í•´ë¶€í•™ì  êµ¬ì¡°ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ê°œë°œ Latent Space ëª¨ë¸ë§: ë³µì¡í•œ êµ¬ì¡°ì  ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ ëª¨ë¸ ì—°êµ¬ ê²°ê³¼ ë° ì„±ê³¼ ì˜ˆì¸¡ ì‹ ë¢°ë„: ê²°ì •ê³„ìˆ˜(RÂ²) 0.7 ë‹¬ì„± í†µê³„ì  í¸í–¥ ì œê±°: ì¸ê³¼ì¶”ë¡  ê¸°ë°˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ì œê³µ ì‚¬ìš©ì ë§Œì¡±ë„: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê·¼ê±° ì œì‹œë¡œ ì‹ ë¢°ë„ í–¥ìƒ ì‹œìŠ¤í…œ ì•ˆì •ì„±: ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜ í•™ìŠµí•œ ì  ì¸ê³¼ì¶”ë¡ ì„ í†µí•œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì‹ ë¢°ì„± í™•ë³´ ë°©ë²• ì˜ë£Œ ë¶„ì•¼ì—ì„œì˜ AI ì‹œìŠ¤í…œ ì„¤ê³„ ë° ê²€ì¦ ë°©ë²• ì‚¬ìš©ì ì¤‘ì‹¬ì˜ ì„œë¹„ìŠ¤ ì„¤ê³„ ë° í”¼ë“œë°± ë°˜ì˜ ê³¼ì • ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œ êµ¬ì¶•",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/segmentation",
    "title": "Segmentation Model",
    "date": "2024-10-20",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ íƒì§€ ëª¨ë¸ í”„ë¡œì íŠ¸ ê°œìš” ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ì€ ë‹¹ë‡¨ë³‘ í™˜ìì—ê²Œ ìì£¼ ë°œìƒí•˜ëŠ” í•©ë³‘ì¦ìœ¼ë¡œ, í˜ˆê´€ ì†ìƒê³¼ ìƒˆë¡œìš´ í˜ˆê´€ ìƒì„±ìœ¼ë¡œ ì¸í•´ ì‹¤ëª…ê¹Œì§€ ìœ ë°œí•  ìˆ˜ ìˆëŠ” ì‹¬ê°í•œ ì§ˆí™˜ì…ë‹ˆë‹¤. ì´ˆê¸° ì§„ë‹¨ì´ ë§¤ìš° ì¤‘ìš”í•œ ì´ ë³‘ì¦ì„ ì•ˆì €ì‚¬ì§„ì„ í†µí•´ íƒì§€í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì•ˆì €ì‚¬ì§„ì€ ëˆˆì˜ ì•ˆìª½(ë§ë§‰ ë¶€ë¶„)ì„ ì´¬ì˜í•˜ì—¬ í˜ˆê´€ ìƒíƒœë¥¼ ê´€ì°°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜ë£Œ ì˜ìƒì…ë‹ˆë‹¤. !ì•ˆì €ì‚¬ì§„ ì´ í”„ë¡œì íŠ¸ëŠ” ì˜ì‚¬ë“¤ì˜ ì§„ë‹¨ì„ ì§€ì›í•˜ëŠ” ëª©ì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°, ì…ì‚¬ ì „ ë¶€íŠ¸ìº í”„ ê¸°ê°„ ì¤‘ ê¸°ì—…ì—°ê³„ í”„ë¡œì íŠ¸ë¡œ ì‹œì‘í•˜ì—¬ ì…ì‚¬ í›„ì—ë„ ì§€ì†ì ìœ¼ë¡œ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤. ê´€ë ¨ ë§í¬: í”„ë¡œì íŠ¸ Github ë°œí‘œì˜ìƒ ìœ íŠœë¸Œ ë°ì´í„° í˜„í™© ë° ë„ì „ê³¼ì œ í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ ê°€ì¥ í° ì´ìŠˆëŠ” ë°ì´í„° ë¶€ì¡±ì´ì—ˆìŠµë‹ˆë‹¤. ì´ˆê¸° ë°ì´í„° ìƒí™© ì˜¤í”ˆ ë°ì´í„°ì…‹ í™œìš©: ì•½ 1,500ê°œ ì´ë¯¸ì§€ ë ˆì´ë¸” ë³´ìœ ìœ¨: ì „ì²´ì˜ 80% ì •ë„ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ: ë¶€ì ì ˆí•œ ë ˆì´ë¸” ë‹¤ìˆ˜ ì¡´ì¬ ë°ì´í„° í™•ë³´ ê³¼ì • ì…ì‚¬ í›„ ì„ìƒíŒ€ê³¼ í˜‘ì—…í•˜ì—¬ ë°ì´í„° ê²€ìˆ˜ ë° ì¶”ê°€ í™•ë³´ë¥¼ ì§„í–‰í–ˆì§€ë§Œ, ì—¬ì „íˆ ì œí•œëœ ë°ì´í„° í™˜ê²½ì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ë§ ì „ëµ ë° ì„±ëŠ¥ ê°œì„  ëª¨ë¸ ì„ íƒ: U-Net ê¸°ë°˜ ë°ì´í„°ì˜ íŠ¹ì§•ìƒ ì „ì²´ ì´ë¯¸ì§€ ì¤‘ ë ˆì´ë¸”(ë§ˆìŠ¤í¬)ì´ í¬í•¨ëœ ë¶€ë¶„ì´ ë§¤ìš° ì‘ì€ ì˜ë£Œ ë„ë©”ì¸ì˜ íŠ¹ì„±ìƒ, U-Net ê³„ì—´ ëª¨ë¸ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì—°êµ¬ ê²°ê³¼ì™€ í”„ë¡œì íŠ¸ ë‚´ ë¹„êµ ì‹¤í—˜ì—ì„œë„ U-Netì´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ê¸°ë²•ë“¤ ì „ì²˜ë¦¬: CLAHE ì ìš© ëª…ì•”ëŒ€ë¹„ í–¥ìƒ ê¸°ë²•ì¸ CLAHEë¥¼ ì ìš©í•˜ì—¬ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ë” ëª…í™•í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. !CLAHE ì ìš© ê²°ê³¼ ìœ¡ì•ˆìœ¼ë¡œë„ íŠ¹ì§•ì´ ë” ëª…í™•í•˜ê²Œ ë³´ì´ë©°, ì‹¤ì œ ì„±ëŠ¥ë„ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ê³¼ë„í•œ ëª…ì•”ëŒ€ë¹„ í–¥ìƒì€ false positiveë¥¼ ì¦ê°€ì‹œí‚¤ë¯€ë¡œ ì‹¤í—˜ì„ í†µí•´ ì ì • ìˆ˜ì¤€ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. Contrastive Learning ê¸°ë°˜ ì‚¬ì „í•™ìŠµ ImageNet ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ê°€ ì•ˆì €ì‚¬ì§„ê³¼ ê°™ì€ íŠ¹ìˆ˜í•œ ì˜ë£Œ ì˜ìƒì—ëŠ” íš¨ê³¼ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì„ í†µí•´ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ëŒ€ì‹  íšŒì‚¬ ë‚´ ë³´ìœ í•œ ëŒ€ëŸ‰ì˜ ì•ˆì €ì‚¬ì§„(ë ˆì´ë¸” ì—†ìŒ)ì„ í™œìš©í•˜ì—¬ contrastive learning ê¸°ë°˜ ì‚¬ì „í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼: ì„±ëŠ¥ 10-15% í–¥ìƒ ë‹¬ì„± Multi-Task Learning U-Netì˜ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ segmentation ì™¸ì— reconstruction taskë¥¼ ì¶”ê°€ë¡œ ìˆ˜í–‰í•˜ëŠ” multi-task learningì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì„œë¹™ ë° ë°°í¬ ë°°í¬ ì „ëµ ì„œë¹„ìŠ¤ ì´ˆê¸° ë‹¨ê³„ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ë¹ ë¥¸ ëª¨ë¸ ë°°í¬ê°€ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ê¸°ìˆ  ìŠ¤íƒ ëª¨ë¸ ìµœì í™”: TensorRT engine ë³€í™˜ ì„œë¹™ í”Œë«í¼: Triton Server ì¥ì : ëª¨ë¸ ì¶”ë¡  ìµœì í™” ìš©ì´ GPU ìì› ê´€ë¦¬ ìë™í™” ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬ ê°„ì†Œí™” í”„ë¡œì íŠ¸ ì„±ê³¼ í•­ëª© ë‚´ìš© ------ ------ í”„ë¡œì íŠ¸ëª… ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ ë³‘ë³€ íƒì§€ ëª¨ë¸ë§ ë° ì„œë¹„ìŠ¤ ê°œë°œ ê¸°ê°„ 2024ë…„ 4ì›” ~ 2025ë…„ 6ì›” (3ê°œì›”) ì£¼ìš” ì„±ê³¼ â€¢ Contrastive Learning ì ìš©ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ<br>â€¢ ì œí•œëœ ë°ì´í„°ì—ì„œë„ ëª¨ë¸ íŒë³„ë ¥ ê°œì„ <br>â€¢ Multi-Task Learningìœ¼ë¡œ ë³‘ë³€ íƒì§€ ì„±ëŠ¥ ìµœì í™”<br>â€¢ Segmentation ì„±ëŠ¥: IoU ê°œì„ , Dice Score 0.8 ë‹¬ì„± ì—­í•  â€¢ ìµœì‹  ì—°êµ¬ ë¶„ì„ ë° ëª¨ë¸ ì„¤ê³„ ì ìš©<br>â€¢ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê¸°ë²• ì ìš©<br>â€¢ ë°ì´í„° í’ˆì§ˆ ê°œì„  ë° ì •ì œ<br>â€¢ ëª¨ë¸ ìµœì í™” ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¸°ìˆ  ìŠ¤íƒ PyTorch, Triton-Server, gRPC, Docker",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/ocr_pipeline",
    "title": "OCR Pipeline System",
    "date": "2024-10-20",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "ì •ì˜í–ˆë˜ ë¬¸ì œë“¤, í•´ê²°í•˜ëŠ” ê³¼ì •, í•´ê²°í•œ ê²°ê³¼ DB insert error (deadlock) ``text bnviit-ocr-server ì•± ì˜¤ì „ 9:51 [ERROR] pentacam_4_2025-08-09 09:51:50 : Fail to insert ocr result to database, Error message : Traceback (most recent call last): File \"/app/assets/ocr/models.py\", line 202, in insert_database cursor.execute(query) pyodbc.ProgrammingError: ('42000', '[42000] [FreeTDS][SQL Server]Transaction (Process ID 727) was deadlocked on lock communication buffer resources with another process and has been chosen as the deadlock victim. Rerun the transaction. (1205) (SQLExecDirectW)') , Status : None ` í•œë²ˆì”© ì´ëŸ° ë¬¸ì œê°€ ë°œìƒí•œë‹¤. ë¬´ìŠ¨ ë¬¸ì œì§€? GPT ì„¤ëª… > ë©”ì‹œì§€ì˜ í•µì‹¬ì€ â€œTransaction (Process ID 727) was deadlocked â€¦ and has been chosen as the deadlock victimâ€ ìœ¼ë¡œ, ë™ì‹œì— ì‹¤í–‰ ì¤‘ì¸ ë‘(ë˜ëŠ” ê·¸ ì´ìƒ) íŠ¸ëœì­ì…˜ì´ ì„œë¡œê°€ ë³´ìœ í•œ ë¦¬ì†ŒìŠ¤ë¥¼ ê¸°ë‹¤ë¦¬ë©° êµì°© ìƒíƒœì— ë¹ ì¡Œê³ , SQL Serverê°€ ê·¸ ì¤‘ í•˜ë‚˜(ë„¤ íŠ¸ëœì­ì…˜)ë¥¼ í¬ìƒ(victim) ìœ¼ë¡œ ê³¨ë¼ ë¡¤ë°±í–ˆë‹¤ëŠ” ëœ»ì´ì•¼. ê·¸ë˜ì„œ ê°„í—ì ìœ¼ë¡œ ì‹¤íŒ¨ê°€ ë‚˜ê³ , ë³´í†µ ì¬ì‹¤í–‰í•˜ë©´ ì„±ê³µí•´. > ë©”ì‹œì§€ì— ë³´ì´ëŠ” lock communication bufferëŠ” ë­˜ ì˜ë¯¸í•˜ë‚˜? > - lockì€ ìš°ë¦¬ê°€ ìµìˆ™í•œ í…Œì´ë¸”/ì¸ë±ìŠ¤/í‚¤ ë½ ê°™ì€ ì¼ë°˜ ì ê¸ˆì„ ëœ»í•˜ê³ , > - communication bufferëŠ” ë³‘ë ¬ ì‹¤í–‰(Parallelism) ë•Œ ìŠ¤ë ˆë“œ ê°„ ë°ì´í„°ë¥¼ ì£¼ê³ ë°›ëŠ” ìµìŠ¤ì²´ì¸ì§€(exchange) ë²„í¼ ê°™ì€ ë‚´ë¶€ í†µì‹  ìì›ì„ ëœ»í•´. ë“œë¬¼ì§€ë§Œ ë³‘ë ¬ ê³„íšì—ì„œ ì´ ìì› ë•Œë¬¸ì— ë°ë“œë½ì´ ë‚  ìˆ˜ ìˆì–´. > > ì •ë¦¬: ë„¤ ì—ëŸ¬ëŠ” â€œì¼ë°˜ ì ê¸ˆ(lock)â€ í˜¹ì€ â€œë³‘ë ¬ í†µì‹  ë²„í¼(communication buffer)â€ ìˆ˜ì¤€ì—ì„œ ë‹¤ë¥¸ ì„¸ì…˜ê³¼ ë°ë“œë½ì´ ë‚¬ê³ , ê·¸ ê²°ê³¼ 1205ë¡œ ì‹¤íŒ¨í–ˆë‹¤ëŠ” ì˜ë¯¸. ì½”ë“œì—ì„œ ë°œê²¬í•œ ë¬¸ì œì  INSERTê°€ ë™ì‹œì— ì‹¤í–‰ë  ë•Œ deadlockì´ ë°œìƒí•  ìˆ˜ ìˆìŒ ê³µí†µ ë² ì´ìŠ¤: assets/ocr/models.py (ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤ì™€ ì¼ì¹˜) `173:213:assets/ocr/models.py def insert_database( self, logger: logging.Logger, db_name: str, data_object: BaseDataBinocular BaseDataMonocular, server_info: ServerInfo ) -> FuncResult[str]: try: conn = get_conn(db_info=server_info.crm_db_info, deploy_env=server_info.deploy_env) cursor = conn.cursor() cust_num = data_object.CUST_NUM[\"ocr_value\"] exam_date = data_object.Exam_Date[\"ocr_value\"] # cust_num, exam_dateê°€ ì´ë¯¸ ìˆìœ¼ë©´ update, ì•„ë‹ˆë©´ insert cursor.execute(self.select_query(cust_num, exam_date, db_name)) rows = cursor.fetchall() if len(rows) == 0: keys, values = data_object.get_insert_key_value() query = self.insert_query(keys, values, db_name) else: keys, values = data_object.get_update_key_value() set_query = \",\".join([f\"{key} = {value}\" for key, value in zip(keys, values)]) query = self.update_query(set_query, cust_num, exam_date, db_name) logger.info(f\"Query : {query}\") cursor.execute(query) conn.close() ` ë‘ ì›Œì»¤ê°€ ê±°ì˜ ë™ì‹œì— ì‹¤í–‰ë˜ì§€ëŠ” ì˜ˆì‹œ ` ì‹œê°„ì¶•: 0ms 1ms 2ms 3ms 4ms ì›Œì»¤A: SELECT â†’ INSERT ì‹œì‘ â†’ ê³ ìœ ì¸ë±ìŠ¤ ì ê¸ˆ íšë“ â†’ í´ëŸ¬ìŠ¤í„°ì¸ë±ìŠ¤ ì ê¸ˆ ëŒ€ê¸° ì›Œì»¤B: SELECT â†’ INSERT ì‹œì‘ â†’ í´ëŸ¬ìŠ¤í„°ì¸ë±ìŠ¤ ì ê¸ˆ íšë“ â†’ ê³ ìœ ì¸ë±ìŠ¤ ì ê¸ˆ ëŒ€ê¸° ` SQL Serverì˜ ë‚´ë¶€ ì²˜ë¦¬ ë°©ì‹ : SQL ServerëŠ” INSERT ì‘ì—…ì„ ì²˜ë¦¬í•  ë•Œ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬í•©ë‹ˆë‹¤: ë‚´ë¶€ ì²˜ë¦¬ ê³¼ì •: ê³ ìœ ì„± ê²€ì¦: ê³ ìœ  ì¸ë±ìŠ¤ì— ì¤‘ë³µ ê°’ì´ ìˆëŠ”ì§€ í™•ì¸ ê³µê°„ í• ë‹¹: í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤ì—ì„œ ì‹¤ì œ ë°ì´í„° í˜ì´ì§€ í• ë‹¹ ë°ì´í„° ì‚½ì…: ì‹¤ì œ ë°ì´í„°ë¥¼ í˜ì´ì§€ì— ê¸°ë¡ ì ê¸ˆ íšë“ ìˆœì„œì˜ ì°¨ì´ ì›Œì»¤ Aì˜ ê²½ìš°: ê³ ìœ  ì¸ë±ìŠ¤ ì ê¸ˆì„ ë¨¼ì € íšë“ (ì¤‘ë³µ ê²€ì‚¬) í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤ ì ê¸ˆì„ ê¸°ë‹¤ë¦¼ (ë°ì´í„° í˜ì´ì§€ í• ë‹¹) ì›Œì»¤ Bì˜ ê²½ìš°: í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤ ì ê¸ˆì„ ë¨¼ì € íšë“ (ë°ì´í„° í˜ì´ì§€ í• ë‹¹) ê³ ìœ  ì¸ë±ìŠ¤ ì ê¸ˆì„ ê¸°ë‹¤ë¦¼ (ì¤‘ë³µ ê²€ì‚¬) ì‹¤ì œ ì˜ˆì‹œë¡œ ì„¤ëª… `sql -- ì›Œì»¤ A: ê³ ìœ  ì¸ë±ìŠ¤ ë¨¼ì € ì ê¸ˆ BEGIN TRANSACTION; -- 1. ê³ ìœ  ì¸ë±ìŠ¤ ì ê¸ˆ íšë“ (ì¤‘ë³µ ê²€ì‚¬) -- 2. í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤ ì ê¸ˆ ëŒ€ê¸° (ë°ì´í„° í˜ì´ì§€ í• ë‹¹) INSERT INTO PENTACAM_DATA (CUST_NUM, Exam_Date, ...) VALUES ('1234', '20240820', ...); COMMIT; -- ì›Œì»¤ B: í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤ ë¨¼ì € ì ê¸ˆ (ë™ì‹œ ì‹¤í–‰) BEGIN TRANSACTION; -- 1. í´ëŸ¬ìŠ¤í„° ì¸ë±ìŠ¤ ì ê¸ˆ íšë“ (ë°ì´í„° í˜ì´ì§€ í• ë‹¹) -- 2. ê³ ìœ  ì¸ë±ìŠ¤ ì ê¸ˆ ëŒ€ê¸° (ì¤‘ë³µ ê²€ì‚¬) INSERT INTO PENTACAM_DATA (CUST_NUM, Exam_Date, ...) VALUES ('1234', '20240820', ...); COMMIT; `` ì™œ ì´ëŸ° ìˆœì„œê°€ ë°œìƒí•˜ëŠ”ê°€? SQL Serverì˜ ì¿¼ë¦¬ ìµœì í™”ê¸°(Query Optimizer)ê°€ ê° INSERT ì‘ì—…ì˜ ì‹¤í–‰ ê³„íšì„ ë…ë¦½ì ìœ¼ë¡œ ê²°ì •í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤: ì›Œì»¤ A: ê³ ìœ ì„± ê²€ì¦ì„ ìš°ì„ ì ìœ¼ë¡œ ì²˜ë¦¬ ì›Œì»¤ B: ë°ì´í„° í˜ì´ì§€ í• ë‹¹ì„ ìš°ì„ ì ìœ¼ë¡œ ì²˜ë¦¬ ì´ë ‡ê²Œ ì„œë¡œ ë‹¤ë¥¸ ì‹¤í–‰ ê²½ë¡œë¥¼ íƒí•˜ë©´ì„œ ì„œë¡œ ë‹¤ë¥¸ ì ê¸ˆ ìˆœì„œë¡œ ì§„í–‰ë˜ì–´ Deadlockì´ ë°œìƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ì„¤ëª… í”„ë¡œì íŠ¸ ê°œìš” ë³‘ì› ê²€ì‚¬ ê²°ê³¼ ì´ë¯¸ì§€ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ìë™ ì ì¬í•˜ëŠ” OCR íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì§„í–‰ë˜ë˜ ë°ì´í„° ì…ë ¥ ê³¼ì •ì„ ìë™í™”í•˜ì—¬ ê²€ì•ˆì‚¬ì˜ ì—…ë¬´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ ì‹¤ì‹œê°„ OCR ì²˜ë¦¬: ê²€ì‚¬ ì™„ë£Œ ì¦‰ì‹œ ì´ë¯¸ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ ê³ ì • ì´ë¯¸ì§€ ìµœì í™”: ê²€ì‚¬ ì¥ë¹„ë³„ ê³ ì •ëœ ì´ë¯¸ì§€ êµ¬ì¡° í™œìš© ë‹¤ì¤‘ OCR ì—”ì§„: TrOCR ê¸°ë°˜ ê³ ì •ë°€ í…ìŠ¤íŠ¸ ì¸ì‹ ë°ì´í„° í’ˆì§ˆ ìë™í™”: ë¶„í¬ ë¶„ì„ ë° ì´ìƒì¹˜ íƒì§€ ìë™í™” ì‹œìŠ¤í…œ ì¼ê´€ì„± ê²€ì¦: ê³ ê°ë²ˆí˜¸ ë“± ê³ ì •ê°’ë“¤ì˜ ë¬´ê²°ì„± ê²€ì¦ ìë™ ëª¨ë‹ˆí„°ë§: Slack ê¸°ë°˜ ì‹¤ì‹œê°„ ì—ëŸ¬ ì•Œë¦¼ ë° ì‘ì—… ìš”ì•½ ì•ˆì •ì  íŒŒì´í”„ë¼ì¸: 1% ë¯¸ë§Œ ì—ëŸ¬ìœ¨ì˜ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜ ê¸°ìˆ  ìŠ¤íƒ OCR Engine: TrOCR (Microsoft) Image Processing: OpenCV, PIL Backend: Python, gRPC, FastAPI Model Serving: Triton Server Containerization: Docker Monitoring: Slack API Database: PostgreSQL í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±… OCR ì •í™•ë„ 99% ë‹¬ì„± ë¬¸ì œ: ê²€ì‚¬ ê²°ê³¼ì˜ ì •í™•ì„±ì´ ë§¤ìš° ì¤‘ìš”í•˜ë¯€ë¡œ ë†’ì€ OCR ì •í™•ë„ í•„ìš” í•´ê²°ì±…: ê³ ì • ì´ë¯¸ì§€ êµ¬ì¡° í™œìš©: ê²€ì‚¬ ì¥ë¹„ë³„ ê³ ì •ëœ ì´ë¯¸ì§€ ë ˆì´ì•„ì›ƒì„ í™œìš©í•˜ì—¬ Text Detection ë‹¨ê³„ ìƒëµ TrOCR ëª¨ë¸ ì ìš©: Text Recognitionì— íŠ¹í™”ëœ TrOCR ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬: ë…¸ì´ì¦ˆ ì œê±°, ëŒ€ë¹„ ê°œì„  ë“± ì „ì²˜ë¦¬ ê³¼ì • ìµœì í™” ê²°ê³¼ í›„ì²˜ë¦¬: OCR ê²°ê³¼ ì •ì œ ë° ê²€ì¦ ë¡œì§ êµ¬í˜„ ë°ì´í„° í’ˆì§ˆ ìë™í™”: ê° ê°’ì˜ ë¶„í¬ ë¶„ì„ ë° ì´ìƒì¹˜ íƒì§€ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶• ì¼ê´€ì„± ê²€ì¦: ê³ ê°ë²ˆí˜¸ ë“± ê³ ì •ê°’ë“¤ì˜ ì¼ê´€ì„± ê²€ì¦ ë¡œì§ ê°œë°œ ê²°ê³¼: OCR ì •í™•ë„ 99% ë‹¬ì„± ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë¬¸ì œ: ì‹¤ì‹œê°„ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜ í•„ìš” í•´ê²°ì±…: ê°ì²´ ì§€í–¥ ì„¤ê³„: ê° ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” ê°ì²´ë“¤ë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬: ì´ë¯¸ì§€ ì†¡ìˆ˜ì‹ , ì „ì²˜ë¦¬, OCR, í›„ì²˜ë¦¬, DB ì ì¬ ê° ë‹¨ê³„ë³„ ëª…í™•í•œ ì—ëŸ¬ ì²˜ë¦¬ ë¹„ë™ê¸° ì²˜ë¦¬: 1ì´ˆ ì´ë‚´ ì‘ë‹µì„ ìœ„í•œ ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬ì¡° êµ¬í˜„ ìë™ ëª¨ë‹ˆí„°ë§: ê° ê°ì²´ë³„ ëª¨ë‹ˆí„°ë§ ìë™í™” ê²°ê³¼: ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œì˜ ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë°©í•´ ìµœì†Œí™” ë¬¸ì œ: ê²€ì•ˆì‚¬ì˜ ê¸°ì¡´ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ì— ë³€í™”ë¥¼ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ OCR ì„œë¹„ìŠ¤ ì¶”ê°€ í•´ê²°ì±…: Client-Server êµ¬ì¡°: ë³‘ì› ì»´í“¨í„°ì˜ clientê°€ ì´ë¯¸ì§€ë¥¼ ì „ì†¡ í›„ ê¸°ì¡´ \"ì•„ì´ë¦¬ë”\" í”„ë¡œê·¸ë¨ì— ìë™ ì ì¬ gRPC í†µì‹ : ë¹ ë¥¸ ì´ë¯¸ì§€ ì „ì†¡ì„ ìœ„í•œ gRPC í”„ë¡œí† ì½œ ì‚¬ìš© ê¶Œí•œ ë¬¸ì œ í•´ê²°: ë‹¤ì–‘í•œ ë³‘ì› í™˜ê²½ì— ë§ëŠ” ë°°í¬ ë°©ì‹ (Python/Go) ì‘ë‹µ ì†ë„ ìµœì í™”: 1ì´ˆ ì´ë‚´ ì‘ë‹µìœ¼ë¡œ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì§€ì—° ìµœì†Œí™” ê²°ê³¼: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ì— ë°©í•´ ì—†ì´ OCR ì„œë¹„ìŠ¤ ì„±ê³µì  í†µí•© ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ Client í”„ë¡œê·¸ë¨ ëª©ì : ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë°©í•´ ì—†ì´ OCR ì„œë²„ë¡œ ì´ë¯¸ì§€ ì „ì†¡ ê¸°ìˆ : gRPCë¥¼ í†µí•œ ê³ ì† ì´ë¯¸ì§€ ì „ì†¡ í™˜ê²½ ëŒ€ì‘: ë‹¤ì–‘í•œ ë³‘ì› ì»´í“¨í„° í™˜ê²½ì— ë§ëŠ” ë°°í¬ (Python/Go) ìë™í™”: ì´ë¯¸ì§€ ì „ì†¡ í›„ ê¸°ì¡´ \"ì•„ì´ë¦¬ë”\" í”„ë¡œê·¸ë¨ì— ìë™ ì ì¬ OCR Server ë¹„ë™ê¸° ì²˜ë¦¬: ì´ë¯¸ì§€ ìˆ˜ì‹  ì¦‰ì‹œ ì‘ë‹µ í›„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ OCR ì²˜ë¦¬ ëª¨ë¸ ì„œë¹™: Triton Serverë¥¼ í†µí•œ TrOCR ëª¨ë¸ ê³ ì„±ëŠ¥ ì„œë¹™ GPU í™œìš©: ì˜¨í”„ë ˆë¯¸ìŠ¤ GPU ì„œë²„ë¥¼ í†µí•œ ê³ ì† ì¶”ë¡  ê³ ì • ì´ë¯¸ì§€ ì²˜ë¦¬: ê²€ì‚¬ ì¥ë¹„ë³„ pixel ê¸°ë°˜ ì¡°ê±´ë¬¸ìœ¼ë¡œ Text Detection ìƒëµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ Slack ì•Œë¦¼: ì—ëŸ¬ ë°œìƒ ì‹œ ì‹¤ì‹œê°„ ì•Œë¦¼ ì‘ì—… ìš”ì•½: í•˜ë£¨ ì‘ì—…ëŸ‰ ë° ì„±ê³µë¥  ìë™ ë³´ê³  ê°ì²´ë³„ ëª¨ë‹ˆí„°ë§: ê° ê¸°ëŠ¥ë³„ ë…ë¦½ì ì¸ ëª¨ë‹ˆí„°ë§ ë¡œê·¸ ê´€ë¦¬: ê¹”ë”í•œ ë¡œê·¸ êµ¬ì¡°ë¡œ ì‰¬ìš´ ë¬¸ì œ ì§„ë‹¨ í•µì‹¬ ìµœì í™” í¬ì¸íŠ¸ Text Detection ìƒëµ ê²€ì‚¬ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ê³ ì •ëœ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” íŠ¹ì„±ì„ í™œìš©: ê²€ì‚¬ ì¥ë¹„ë³„ ì´ë¯¸ì§€ ë²„ì „ ë° í˜ì´ì§€ êµ¬ë¶„ Pixel ê°’ ê¸°ë°˜ ì¡°ê±´ë¬¸ìœ¼ë¡œ ì˜ì—­ ë¶„í•  Text Recognition ë‹¨ê³„ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì²˜ë¦¬ ì†ë„ í–¥ìƒ TrOCR ëª¨ë¸ ì„ íƒ ë‹¤ì–‘í•œ OCR ëª¨ë¸ ë¹„êµ í›„ TrOCR ì„ íƒ: ì •í™•ë„: ë²¤ì¹˜ë§ˆí¬ì—ì„œ 98% ì •í™•ë„ ì‹¤ì œ ì„±ëŠ¥: ìˆ«ì ì¸ì‹ 100% ì •í™•ë„ í•œêµ­ì–´ ì§€ì›: ë¬¸ì ì¸ì‹ì€ ë‹¨ìœ„ ì •ë³´ë¡œ ê³ ì • ìœ„ì¹˜ í™œìš© ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬ì¡° ì‚¬ìš©ì ê²½í—˜ì„ ìœ„í•œ ì‘ë‹µ ì†ë„ ìµœì í™”: ì´ë¯¸ì§€ ìˆ˜ì‹  ì¦‰ì‹œ ì‘ë‹µ (1ì´ˆ ì´ë‚´) ë°±ê·¸ë¼ìš´ë“œì—ì„œ OCR ì²˜ë¦¬ ìƒë‹´ ì‹œì ê¹Œì§€ ì¶©ë¶„í•œ ì²˜ë¦¬ ì‹œê°„ í™•ë³´ ê²°ê³¼ ë° ì„±ê³¼ OCR ì •í™•ë„: 99% ë‹¬ì„± ì—ëŸ¬ìœ¨: 1% ë¯¸ë§Œì˜ ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì²˜ë¦¬ ì†ë„: 1ì´ˆ ì´ë‚´ ì‘ë‹µìœ¼ë¡œ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì§€ì—° ìµœì†Œí™” ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨: ì½”ë“œ êµ¬ì¡° ê°œì„ ìœ¼ë¡œ ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨ 10ë°° ì¦ê°€ ëª¨ë‹ˆí„°ë§ ë¹„ìš©: ìë™í™”ë¥¼ í†µí•œ ëª¨ë‹ˆí„°ë§ ì‹œê°„ 90% ê°ì†Œ í•™ìŠµí•œ ì  ê³ ì • ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ í™œìš©í•œ OCR ìµœì í™” ë°©ë²• ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì•ˆì •ì„± í™•ë³´ ë°©ë²• ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ì™€ì˜ í†µí•© ì‹œ ì‚¬ìš©ì ê²½í—˜ ê³ ë ¤ì˜ ì¤‘ìš”ì„± ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ êµ¬ì¶• ë° ìš´ì˜ ë°©ë²•",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2024-07-05-recommendation-system-interest",
    "title": "ì¶”ì²œì‹œìŠ¤í…œì— ê´€ì‹¬ìˆëŠ” ì´ìœ ",
    "date": "2024-07-04 23:30:00 +0900",
    "excerpt": "",
    "category": "Recommendation",
    "tags": [
      "Recommendation"
    ],
    "content": "ì ê¹ ì¶”ì²œì‹œìŠ¤í…œì„ ê³µë¶€í–ˆì—ˆëŠ”ë°, ë‹¤ì‹œ ê³µë¶€ë¥¼ ì‹œì‘í•˜ë©´ì„œ í”„ë¡œì íŠ¸ë¥¼ í•˜ë‚˜ì”© ìŒ“ì•„ê°€ë ¤ê³  í•œë‹¤. ê·¸ ì „ì— ë‚´ê°€ ì™œ ê´€ì‹¬ì´ ìˆëŠ”ì§€, ë˜ ì–´ë–¤ í”„ë¡œì íŠ¸ë¥¼ í•´ë³´ê³  ì‹¶ì€ì§€ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤. ì¶”ì²œì‹œìŠ¤í…œì´ë€? ì¶”ì²œì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ë¹„ìŠ·í•œ ëª©ì ì„ ê°€ì§€ê³  ìˆë‹¤. > ë§ì€ ì •ë³´ ì†ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ í•„í„°ë§í•˜ëŠ” ê²ƒ \"ì§„ì§œ ë§ì€ ì •ë³´ë“¤, ì»¨í…ì¸ ë“¤ì´ ìˆëŠ”ë° ìœ ì €ì—ê²Œ ì–´ë–¤ ê²ƒì„ ë³´ì—¬ì¤˜ì•¼í• ê¹Œ?\"ì— ëŒ€í•œ ë‹µì„ í•˜ëŠ” ê²ƒì´ë‹¤. ì™œ ì¤‘ìš”í• ê¹Œ? ì¶”ì²œì‹œìŠ¤í…œì´ í•„ìš”í•œ ê³³ì€ ëŒ€í‘œì ìœ¼ë¡œ OTT íšŒì‚¬ë“¤ì´ ë– ì˜¤ë¥¸ë‹¤. ì†Œë¹„ìëŠ” ì–¸ì œ êµ¬ë…ì„ ê·¸ë§Œë‘˜ê¹Œ? > ë‹¹ì—°í•˜ê²Œë„ ë” ì´ìƒ ë³¼ê²Œ ì—†ë‹¤ê³  ëŠê»´ì§ˆë•Œ ê·¸ë§Œë‘˜ ê²ƒì´ë‹¤. ì „ì²´ ì»¨í…ì¸ ì— ë¹„í•´ ì†Œë¹„í•œ ì»¨í…ì¸ ëŠ” ì†Œìˆ˜ì¼í…ë° ì™œ ë³¼ê²Œ ì—†ë‹¤ê³  ëŠê»´ì§ˆê¹Œ? > ë­˜ ë´ì•¼í• ì§€ ëª¨ë¥´ê² ì–´ì„œ, ì–´ë–¤ê²Œ ë‚´ ì¬ë°Œì„ì§€ ëª°ë¼ì„œ. ê·¸ë˜ì„œ ì†Œë¹„ìì—ê²Œ ì·¨í–¥ì— ë§ëŠ” ì»¨í…ì¸ ë¥¼ ê¾¸ì¤€íˆ ë…¸ì¶œí•´ì¤˜ì•¼í•˜ê³ , ê·¸ë˜ì•¼ ì†Œë¹„ìê°€ ë– ë‚˜ì§€ ì•Šë„ë¡ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì–´ë ¤ìš´ ì  ë‚´ê°€ ìƒê°í–ˆì„ ë•Œ ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ëŠ”ë° ê°€ì¥ ì–´ë ¤ìš´ ê²ƒì€ ì†Œë¹„ìì˜ í‰ê°€ ë°ì´í„°ê°€ ì ê¸° ë•Œë¬¸ì¸ ê²ƒ ê°™ë‹¤. ì–´ë–¤ ì‚¬ëŒì´ í‰ê°€í•œê²Œ ì ìœ¼ë©´ ë‹¹ì—°íˆ ì·¨í–¥ì„ ë¶„ì„í•˜ê¸° ì–´ë µê³ , ê·¸ë˜ì„œ ì¶”ì²œí•´ì£¼ê¸°ë„ ì–´ë µë‹¤. í‰ê°€ ë°ì´í„°ê°€ ë§ì€ ì‚¬ëŒì—ê²Œ ì˜ ì¶”ì²œí•´ì£¼ëŠ” ê²ƒë„ êµ‰ì¥íˆ ì¤‘ìš”í•˜ì§€ë§Œ, í‰ê°€ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‚¬ëŒì—ê²Œ ì–´ë–»ê²Œ ì¶”ì²œí•´ì¤„ì§€ì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ ê°™ë‹¤. í•´ë³´ê³  ì‹¶ì€ í”„ë¡œì íŠ¸ ìš”ì¦˜ í•´ë³´ë ¤ëŠ” í”„ë¡œì íŠ¸ëŠ” LLMì„ ì´ìš©í•´ì„œ ì†Œë¹„ìì˜ ì·¨í–¥ì„ ì•Œì•„ë‚´ëŠ” ê²ƒì´ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” Multi-Agent-Systemìœ¼ë¡œ ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ë ¤ê³  í•˜ê³ , ë§¨ ì•ë‹¨ì— ì·¨í–¥ì„ ë¶„ì„í•˜ëŠ” Agentë¥¼ êµ¬ì¶•í•´ ì‚¬ìš©í•´ë³´ë ¤ê³  í•œë‹¤. ë¨¼ì € ì˜í™”ë¡œ ì‹œì‘í•´ì„œ ì±…ê³¼ ê°™ì€ ë¶„ì•¼ë¡œë„ í™•ì¥í•´ë³´ë ¤ê³  í•œë‹¤. ì·¨í–¥ì„ ë¶„ì„í•œë‹¤ëŠ”ê²Œ êµ‰ì¥íˆ ì• ë§¤í•œ ë¶€ë¶„ì¸ ê²ƒ ê°™ì•„ì„œ ê³ ë¯¼ì´ ë§ì´ í•„ìš”í•  ê²ƒ ê°™ë‹¤. ì´ê²Œ ê°€ëŠ¥í•´ì§„ë‹¤ë©´ ë§ì€ ì‚°ì—…ì—ì„œ ì •ë§ ë§¤ë ¥ì ìœ¼ë¡œ ëŠë¼ëŠ” ê¸°ìˆ ì´ ì•„ë‹ê¹Œ? ì´ 3ëª…ì´ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ê²ƒ ê°™ì€ë° ê·¸ ê³¼ì •ì„ ë¸”ë¡œê·¸ì— ë‚¨ê²¨ë³´ë ¤ê³  í•œë‹¤.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2024-07-04-interest-concerns",
    "title": "ê´€ì‹¬ì‚¬ì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2024-07-04 19:49:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "Interest"
    ],
    "content": "í˜„ì¬ ë‚˜ì˜ ìƒí™© ì´ì œ ë°ì´í„°ê³¼í•™ìë¡œ ì¼í•œì§€ 3ê°œì›”ì •ë„ ë˜ì—ˆë‹¤. ì•„ì´í ì´ë¼ëŠ” ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¶€íŠ¸ìº í”„ë¥¼ ì§„í–‰í–ˆì—ˆëŠ”ë°, ì´ë•Œ ê¸°ì—…ì—°ê³„í”„ë¡œì íŠ¸ë¡œ ì§„í–‰í–ˆë˜ ê¸°ì—…ì— ì·¨ì—…ì„ í–ˆë‹¤. ë¶„ì•¼ëŠ” ì•ˆê³¼ì˜ë£Œë¶„ì•¼ì´ë©°, ë¹„ì•¤ë¹› ë°ì€ì„¸ìƒ ì•ˆê³¼ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ ìŠ¤íƒ€íŠ¸ì—…ì´ë‹¤. ì´ë•Œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ëŠ” ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ì˜ ë³‘ë³€ë“¤ì„ íƒì§€í•´ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ì—ˆë‹¤. ì´ì— ëŒ€í•œ ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ë°œí‘œìë£Œ ë°œí‘œë§í¬ ê²°ê³¼ì ìœ¼ë¡œëŠ” ì˜ë£Œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  Segmentation ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ë°œì „ì‹œì¼œë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤. ë˜í•œ ì´ê²Œ ì–´ë–»ê²Œ ì“°ì¼ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ê³ ë¯¼ë„ í•´ë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤. ê·¸ë ‡ê²Œ ì¸ê³µì§€ëŠ¥ì„ í†µí•´ ê±´ê°•ì— ê¸°ì—¬í•  ìˆ˜ ìˆê² ë‹¤ëŠ” ê¿ˆì„ ê°€ì§€ê³  ì…ì‚¬ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ìŠ¤í† ë¦¬ê°€ ê·¸ëŸ¬í•˜ë“¯, ë‚˜ ì—­ì‹œ ê¸°ëŒ€ì™€ ë§ì´ ë‹¤ë¥¸ íšŒì‚¬ìƒí™œì„ í•˜ê²Œ ë˜ì—ˆë‹¤. ì•ˆê³¼ ë¶„ì•¼ì—ì„œì˜ ì¸ê³µì§€ëŠ¥ ê°œë°œí˜„í™© ì˜ë£Œ ë¶„ì•¼ ì¤‘ì—ì„œ íŠ¹íˆ ì•ˆê³¼ì— ëŒ€í•´ì„œë§Œ ì•Œê³ , ê·¸ë˜ì„œ ì•ˆê³¼ì— ëŒ€í•´ì„œë§Œ í•œì •í•´ì„œ ì´ì•¼ê¸°í•˜ë ¤ê³  í•œë‹¤. ë¹„ì¥¬ì›ìŠ¤ ë‚´ê°€ ì¼í•˜ê³  ìˆëŠ” ë¹„ì¥¬ì›ìŠ¤ë¼ëŠ” ê¸°ì—…ì€ ë‚´ê°€ ëŠë¼ê¸°ì— ê°€ì¥ ì„±ì·¨ê°€ ì ë‹¤. ì˜ë£Œì¸ì¦ì„ ë°›ì€ ëª¨ë¸ë„ ì—†ê³ , ë°›ìœ¼ë ¤ë©´ ì•„ì§ ë©€ì—ˆë‹¤. ìì„¸í•˜ê²Œ ì´ì•¼ê¸°í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, íšŒì‚¬ì˜ ë°©í–¥ë„ ë‹¹ì¥ì˜ ìˆ˜ìµì„ ë§Œë“œëŠ” ê²ƒì— ì§‘ì¤‘í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ ë‚´ê°€ ì›í–ˆë˜ ë¶€ë¶„ê³¼ëŠ” ë§ì´ ë‹¤ë¥´ê²Œ ì¼ì„ í•˜ê³  ìˆë‹¤. ë‹¤ë¥¸ íšŒì‚¬ë¡œ ì´ì§í•˜ë©´ ë˜ëŠ” ê±¸ê¹Œ? ì„ ë‘ê¸°ì—…ë“¤ì€ ì–´ë–»ê²Œ í•˜ê³  ìˆì„ê¹Œ? ë·°ë…¸ ë·°ë…¸ì—ì„œëŠ” ì•ˆì €ì‚¬ì§„ì„ í†µí•œ ì§ˆë³‘ ë¶„ë¥˜ ëª¨ë¸ì„ ì¶œì‹œí•˜ê³ , ê²°ê³¼ì— CAMë„ ê°™ì´ ë³´ì—¬ì¤€ë‹¤. (CAMì„ í†µí•´ ëª¨ë¸ì´ ì–´ëŠ ë¶€ë¶„ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ë³´ê³  ìˆëŠ”ì§€ 'ì¶”ì •'í•  ìˆ˜ ìˆìŒ) ë³‘ì›ì— ìˆëŠ” ì˜ì‚¬ë¶„ê»˜ ë“£ê¸°ë¡œëŠ” ì „í˜€ ì•ˆì“°ì¸ë‹¤ê³  í•œë‹¤. ì •í™•ë„ê°€ ë‚®ê³ , ëª‡ë²ˆ ì •í™•ë„ê°€ ë‚®ë‹¤ëŠ” ìƒê°ì´ ë“¤ë©´ ë°”ë¡œ ì•ˆì“°ê²Œ ëœë‹¤ê³  í•œë‹¤. êµ³ì´? ì´ëŸ° ë§ˆìŒì´ í° ê²ƒ ê°™ë‹¤. ë©”ë””ë¹„ì „ ê°€ì¥ ì¬ë¯¸ìˆì–´ ë³´ì´ëŠ” ê¸°ì—…ì´ë‹¤. ì•ˆì €ì‚¬ì§„ì„ í†µí•´ ì‹¬ì§ˆí™˜ ìœ„í—˜ë„ ì˜ˆì¸¡, ì½©íŒ¥ ìœ„í—˜ë„ ì˜ˆì¸¡ì„ ì§„í–‰í•œë‹¤. ì‹œë¦¬ì¦ˆB(100ì–µ)íˆ¬ìë¥¼ ë°›ê³ , ìŸìŸí•œ ì´ì‚¬ì§„ë“¤ê³¼ í•¨ê»˜ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ì˜ë£Œì¸ì¦ì„ ì§„í–‰í•˜ê³  ìˆë‹¤. ì•„ë§ˆë„ êµ­ë‚´ëŠ” ìˆ˜ê°€ê°€ ë‚®ê³  ë³´ìˆ˜ì ì´ë¼ëŠ” ì´ìœ  ë•Œë¬¸ì— ë¯¸êµ­ FDA ìŠ¹ì¸ì„ ì§„í–‰ ì¤‘ì´ë©°, ì½©íŒ¥ ëª¨ë¸ì˜ ê²½ìš°ì—ëŠ” ìŠ¹ì¸ì„ ë°›ì€ ê²ƒìœ¼ë¡œ ì•Œê³  ìˆë‹¤. ë§í¬ë“œì¸ì„ íŒ”ë¡œì‰í•˜ë©° ë³´ê³  ìˆëŠ”ë°, í•™ë¬¸ì ì¸ ë°œí‘œë„ ë§ì´ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í™ˆí˜ì´ì§€ì—ì„œë„ ë°œí–‰í•œ ë…¼ë¬¸ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ê³¼ì—° ìˆ˜ìµì´ ë‚ ê¹Œ? ì— ëŒ€í•´ì„œëŠ” ê¶ê¸ˆí•˜ê³  ë‚¬ìœ¼ë©´ ì¢‹ê² ë‹¤. ë‚˜ì˜ ì§„ë¡œì— ëŒ€í•œ ê³ ë¯¼ ì´ëŸ° ìƒí™©ì†ì—ì„œ ë‚˜ëŠ” ë‚˜ì˜ í¥ë¯¸ì— ëŒ€í•´ í•œë²ˆ ë” ìƒê°í•´ë³´ê²Œ ëë‹¤. ê·¸ë¦¬ê³  ì´ëŸ° í¥ë¯¸ì— ë”°ë¼ ì§„ë¡œë¥¼ ê²°ì •í•˜ê²Œ ë˜ì—ˆë‹¤. ê±´ê°•ì— ê¸°ì—¬í•˜ê³  ì‹¶ì€ê°€? ê³¼ì—° ë‚˜ëŠ” ì§„ì§œ ê±´ê°•ì— ê¸°ì—¬í•˜ê³  ì‹¶ì„ê¹Œ? ë‹¹ì¥ì— ë‚˜ì˜ ì˜í–¥ì´ ì „í˜€ ëŠê»´ì§€ì§€ ì•ŠëŠ” ìƒí™©ì—ì„œ, ì´ëŸ° ê¿ˆì„ ê°€ì§€ê³  ê°œë°œì„ ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆì„ê¹Œ? ë‚˜ì˜ ëŒ€ë‹µì€ \"ì•„ë‹ˆ, ì „í˜€\" . í•­ìƒ ëŠë¼ì§€ë§Œ ë‚˜ëŠ” ì†Œë¹„ìì™€ ë°€ì ‘í•œ ë¶„ì•¼ì—ì„œ ê°œë°œì„ í•˜ê³  ì‹¶ë‹¤. ë‚´ê°€ ë§Œë“  ê²ƒë“¤ì´ ë°”ë¡œ ì‚¬ëŒë“¤ì—ê²Œ ì˜í–¥ì„ ë¯¸ì³¤ìœ¼ë©´ ì¢‹ê² ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ëŠ ë¶„ì•¼? ê³ 3ë•Œ ë‚´ ê¿ˆì€ PD ì˜€ë‹¤. ë¬¼ë¡  ì´ê³¼ì˜€ì§€ë§Œ, ì»¨í…ì¸ ë“¤ì„ ì†Œë¹„í•˜ë©° ì‚¶ì„ ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆì—ˆê³  ë§ì€ ì˜ê°ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë‚˜ë„ ì¢‹ì€ ì»¨í…ì¸ ë¥¼ ë§Œë“¤ì–´ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ê¸ì •ì ì¸ ì˜í–¥ì„ ì£¼ê³  ì‹¶ì—ˆë‹¤. í•‘ê³„ì§€ë§Œ ì´ëŸ° ìƒê°ì„ ë°œì „ì‹œí‚¬ ìƒí™©ì´ ì•„ë‹ˆì—ˆê³  ìì—°ìŠ¤ëŸ½ê²Œ í˜ëŸ¬ê°”ë‹¤. ì‹œê°„ì´ ì§€ë‚˜ ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì»¨í…ì¸ ë¥¼ ë§Œë“œëŠ” ì„¸ìƒì´ ì™”ë‹¤. ì§ì ‘ì ìœ¼ë¡œ ì»¨í…ì¸ ë¥¼ ìƒì„±í•˜ëŠ” ì¼ì„ í•˜ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ë‚´ê°€ ì›í•˜ëŠ” ì»¨í…ì¸ ë“¤ì„ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ê°œë°œí•˜ëŠ” ê²ƒë„ ì¬ë°Œì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë§ì´ ë“ ë‹¤. ì¤€ë¹„í•  ê²ƒë“¤ ì •ë¦¬í•˜ìë©´ ì˜ë£Œë¶„ì•¼ ë°ì´í„° ê³¼í•™ìì—ì„œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ì„ ê°œë°œí•˜ëŠ” ìª½ìœ¼ë¡œ ì»¤ë¦¬ì–´ë¥¼ ì´ì–´ë‚˜ê°€ê³  ì‹¶ë‹¤. í˜¹ì€ ì»¨í…ì¸ ë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì—…ì˜ ë°ì´í„° ê³¼í•™ìë„ ê´œì°®ì„ ê²ƒ ê°™ë‹¤. ì–´ì¨Œë“  ì»¨í…ì¸ ë¥¼ ë‹¤ë£¨ëŠ” ìª½ìœ¼ë¡œ ì»¤ë¦¬ì–´ë¥¼ ê°€ì§€ê³  ì‹¶ë‹¤. ì´ë¥¼ ìœ„í•´ í¬ê²Œ 2ê°€ì§€ ë°©í–¥ìœ¼ë¡œ ì¤€ë¹„í•˜ë ¤ê³  í•œë‹¤. ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ ì¶”ì²œì‹œìŠ¤í…œ ì´ë¥¼ ìœ„í•´ ì§€ê¸ˆ í•˜ê³  ìˆëŠ” ê²ƒì€ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ ìŠ¤í„°ë”” (ëª¨ë‘ì˜ ì—°êµ¬ì†Œ í’€ììŠ¤ì¿¨) ì¶”ì²œì‹œìŠ¤í…œ (í”„ë¡œì íŠ¸, ê°œì¸ê³µë¶€) ì •ë¦¬ ì•ìœ¼ë¡œ ê³µë¶€í•´ì•¼í•  ê²ƒë“¤ì´ ë§ë‹¤. íƒ‘ì„ ìŒ“ì•„ê°€ëŠ” ê³¼ì •ì„ ê¾¸ì¤€íˆ ë¸”ë¡œê·¸ì— ë‚¨ê¸°ë ¤ê³  í•œë‹¤. í•˜ë‚˜ì”© í•˜ë‹¤ë³´ë©´ ëª»í•˜ëŠ” ì¼ì€ ê±°ì˜ ì—†ë‹¤ê³  ìƒê°í•œë‹¤. ê¾¸ì¤€íˆ ì¤€ë¹„í•´ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì»¤ë¦¬ì–´ë¥¼ ë§Œë“¤ì–´ ë‚˜ê°€ì!",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2024-04-10-job-hunting-important-things",
    "title": "ì·¨ì¤€í•  ë•Œ ì¤‘ìš”í•œ ê²ƒ",
    "date": "2024-04-10 12:00:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "ì·¨ì¤€"
    ],
    "content": "ì €ëŠ” í˜„ì¬ ì˜ë£Œì¸ê³µì§€ëŠ¥ íšŒì‚¬ì—ì„œ ë°ì´í„°ê³¼í•™ìë¡œ ì¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì œê°€ ë°ì´í„°ê³¼í•™ìë¥¼ ì¤€ë¹„í•˜ë©´ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í–ˆê³ , ë„ì›€ì´ ëë˜ ìƒê°ì„ ê³µìœ í•´ë³´ë ¤ í•©ë‹ˆë‹¤. > íšŒì‚¬ì™€ ë¹„ìŠ·í•œ ê³ ë¯¼ì„ í•˜ê³  ë‚´ê°€ ê°€ì§„ ìì›ìœ¼ë¡œ í•´ê²°í•´ë³´ëŠ” ê²ƒ ë°ì´í„°ê³¼í•™ì ë°ì´í„°ê³¼í•™ìë€ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ì´ê±¸ ë°ì´í„°ë¡œ í•´ê²°í•´ë‚˜ê°€ëŠ” ì‚¬ëŒì„ ëœ»í•©ë‹ˆë‹¤. ì´ëŠ” í¬ê²Œ 2ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ë‚˜ëˆ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ ì •ì˜ ë‚´ê°€ ê°€ì§„ ìì›ê³¼ ê¸°ìˆ ë¡œ í•´ê²° í”„ë¡œì íŠ¸ë¥¼ í•˜ê³  ì´ë¥¼ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë§Œë“¤ ë•ŒëŠ” ì´ 2ê°€ì§€ ê³¼ì •ì´ ê¼­ ì˜ ë‹´ê²¨ìˆì–´ì•¼ í•©ë‹ˆë‹¤. > ì–´ë–»ê²Œ ë¬¸ì œë¥¼ ì •ì˜í–ˆìœ¼ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê³¼ì •ì—ì„œ ì–´ë–¤ ê³ ë¯¼ë“¤ì„ í—€ëŠ”ì§€. íšŒì‚¬ íšŒì‚¬ëŠ” ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ê·¸ê±¸ í•´ê²°í•´ë‚˜ê°€ë©° ìˆ˜ìµì„ ë‚´ëŠ” ì§‘ë‹¨ì…ë‹ˆë‹¤. íšŒì‚¬ì—ì„œ ì¼í•˜ëŠ” ë°ì´í„°ê³¼í•™ìë“¤ì€ ë³¸ì¸ì´ ì •ì˜í•˜ê±°ë‚˜ ìš´ì˜ì§„ì—ì„œ ì •ì˜í•œ ë¬¸ì œë¥¼ íšŒì‚¬ì˜ ìì›ê³¼ ë³¸ì¸ì˜ ê¸°ìˆ ë“¤ë¡œ í•´ê²°í•´ë‚˜ê°‘ë‹ˆë‹¤. > ë°ì´í„°ê³¼í•™ìë¥¼ ì¤€ë¹„í•˜ëŠ” ì‚¬ëŒê³¼ íšŒì‚¬ì—ì„œ ë°ì´í„°ê³¼í•™ìë¡œ ì¼í•˜ëŠ” ì‚¬ëŒì˜ ê³¼ì •ì€ ì™„ì „íˆ ë™ì¼í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ë¶€ë¶„ ê²°êµ­ ì¤‘ìš”í•œ ê²ƒì€ ë‚˜ì˜ ê³¼ì •ë“¤ê³¼ íšŒì‚¬ì˜ ê³¼ì •ì´ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ ì…ë‹ˆë‹¤. ì´ êµì§‘í•©ì´ í´ìˆ˜ë¡ ë³¸ì¸ì˜ í”„ë¡œì íŠ¸ í˜¹ì€ í¬íŠ¸í´ë¦¬ì˜¤ëŠ” ë§¤ë ¥ì ìœ¼ë¡œ ë³´ì´ê²Œ ë©ë‹ˆë‹¤. ì¦‰, ë‚´ê°€ ìƒê°í–ˆì„ ë•Œ ì´ëŸ° ë¬¸ì œê°€ ìˆê³  ë˜ëŠ” ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ì´ëŸ° ë¬¸ì œë¥¼ ì´ë ‡ê²Œ í’€ì–´ë´¤ë‹¤. í‘¸ëŠ” ê³¼ì •ì—ì„œëŠ” ì´ëŸ° ì–´ë ¤ì›€ì´ ìˆì—ˆê³  ì´ë ‡ê²Œ í•´ê²°í•´ë‚˜ê°”ë‹¤. ì´ê²Œ í¬íŠ¸í´ë¦¬ì˜¤ì— ì˜ ë‹´ê²¨ìˆì–´ì•¼í•˜ë©°, ì·¨ì—…ì„ ì¤€ë¹„í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ì ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ ì €ë„ ì œê°€ ì •ì˜í•˜ê³  í•´ê²°í–ˆë˜ ë¬¸ì œê°€ í˜„ì¬ ì œê°€ íšŒì‚¬ì—ì„œ í•´ê²°í•˜ê³  ìˆëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ë©´ì ‘ì—ì„œë„ ì´ê±¸ ê°€ì¥ ì¢‹ê²Œ ë´ì£¼ì…¨ê³  ì·¨ì—…ì— ê°€ì¥ ë„ì›€ì´ ë§ì´ ëë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë§Œì•½ ì œê°€ ë‹¤ë¥¸ ë¶„ì•¼ë¡œ ì´ì§ì„ ì¤€ë¹„í•œë‹¤ë©´, ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì¤€ë¹„í•  ê²ƒì…ë‹ˆë‹¤. ë‹¹ì—°í•´ë³´ì´ëŠ” ìƒê°ì´ì§€ë§Œ, ìƒê°ë³´ë‹¤ ë§ì´ ë†“ì¹  ìˆ˜ ìˆì–´ í•œë²ˆ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤. ì·¨ì—…ì„ ì¤€ë¹„í•˜ì‹œëŠ” ë¶„ë“¤ì´ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤!",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2024-03-12-aifell-review",
    "title": "ì•„ì´í  í›„ê¸°",
    "date": "2024-03-12",
    "excerpt": "6ê°œì›” ë™ì•ˆì˜ ì•„ì´í  í›„ê¸°",
    "category": "Reflections",
    "tags": [
      "ì•„ì´í ",
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "ì €ëŠ” 23.9 ~ 24.2 ë™ì•ˆ ì•„ì´í  6ê¸° ë¦¬ì„œì¹˜ ê³¼ì •ì„ ìˆ˜ë£Œí–ˆê³ , íšŒê³  ê²¸ í›„ê¸°ë¥¼ ì ì–´ë³´ë ¤ í•©ë‹ˆë‹¤. !ì¡¸ì—…ì¦ ì‹ ì²­í–ˆì„ ë•Œì˜ ë‚˜ì˜ ìƒí™© ì¼ë‹¨ ì œê°€ ì–´ë–¤ ìƒíƒœì—ì„œ ì•„ì´í ì„ ë“¤ì—ˆëŠ”ì§€ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì¼ë‹¨ ì €ëŠ” ì‚°ì—…ê³µí•™ì„ ì „ê³µí–ˆê³ , í†µê³„í•™ì„ ë¶€ì „ê³µí–ˆìŠµë‹ˆë‹¤. í•™ë¶€ìƒë•Œ MLì— ê´€ì‹¬ì´ ë§ì•„ì„œ í˜¼ìì„œ ê³µë¶€í–ˆì—ˆê³ , ê´€ë ¨ í”„ë¡œì íŠ¸ë„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. DL ë¶€ë¶„ì€ ì˜ ëª°ëëŠ”ë°, ì‹œì‘í•˜ê¸° 1ë‹¬ì „ì— \"ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹\" 2íšŒë…ì •ë„ í–ˆìŠµë‹ˆë‹¤. ì œ ë°°ê²½ì§€ì‹ì€ í†µê³„í•™ + ì»´í“¨í„°ê³µí•™ + DL ì¡°ê¸ˆ. ì •ë„ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ë‚´ê°€ ìƒê°í•˜ëŠ” ë‚œì´ë„ ì²˜ìŒ 1~2ë‹¬ ì •ë„ëŠ” ê·¸ ì „ì— ê³µë¶€í–ˆë˜ ê²ƒë“¤ì´ë¼ ë³µìŠµí•˜ëŠ” ëŠë‚Œìœ¼ë¡œ ê³µë¶€í–ˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê°œë…ì •ë„ë§Œ ì•„ëŠ” ìƒíƒœì—ì„œ ê³µë¶€ë¥¼ í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë”°ë¼ê°€ëŠ”ë° ì–´ë ¤ì›€ì€ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì–‘ì´ ì›Œë‚™ ë§ì•„ì„œ í‰ê· ì ìœ¼ë¡œ ì¶”ê°€ê³µë¶€ë¥¼ 3~4ì‹œê°„, ì ì–´ë„ 1ì‹œê°„ì”©ì€ í–ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë°°ê²½ì§€ì‹ì´ ì „í˜€ ì—†ëŠ” ë¶„ì´ ë¦¬ì„œì¹˜ ê³¼ì •ì„ ìˆ˜ë£Œí•˜ë ¤ë©´ ì ì–´ë„ ë§¤ì¼ 6ì‹œê°„ì”©ì€ ì¶”ê°€ê³µë¶€í•´ì•¼í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê³µë¶€ë°©ì‹ ì„ ìƒë‹˜ì´ ìˆëŠ”ê²Œ ì•„ë‹Œ, ê³µë¶€ìë£Œê°€ ì£¼ì–´ì§€ê³  ê°™ì´ ê³µë¶€í•˜ëŠ” ì‚¬ëŒë“¤ê³¼ í•¨ê»˜ ì•Œì•„ê°€ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê³µë¶€ì˜ ë°©í–¥ì„ \"í¼ì‹¤\" ë¶„ë“¤ì´ ì¡ì•„ì£¼ê³ , ëª¨ë¥´ëŠ” ë¶€ë¶„ì„ í•´ì†Œí•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ë„ì›€ì„ ì£¼ì‹­ë‹ˆë‹¤. ì´ê²Œ ë³´í†µ ìŠ¤í„°ë”” ê°™ì€ ê²½ìš° ë‚˜ë³´ë‹¤ ë” ì˜ì•„ëŠ” ì‚¬ëŒê³¼ í•´ì•¼ì§€ ì–»ëŠ”ê²Œ ë§ì„ ê²ƒì´ë¼ ìƒê°í•˜ëŠ”ë°, ê²½í—˜í•´ë³´ë‹ˆ ë°˜ëŒ€ì˜€ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. > ì™œ ì„ ìƒë‹˜ì´ ê°€ì¥ ë§ì´ ë°°ìš´ë‹¤ê³  í•˜ì–ì•„ìš”, ì €ëŠ” ë°°ê²½ì§€ì‹ì´ ìˆì–´ \"ì„ ìƒë‹˜\" ì—­í• ì„ ë§¡ì„ ë•Œê°€ ë§ì•˜ëŠ”ë° ì •ë§ ë§ì´ ë°°ì› ìŠµë‹ˆë‹¤. ì–´ëŠì •ë„ ì•Œë˜ ê°œë…ë“¤ì´ ì„¤ëª…ì„ í•˜ë©´ì„œ ì™„ì„±ë˜ê³¤ í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì•„ì— ëª¨ë¥´ë˜ ê°œë…ë“¤ë„ ì–´ë–»ê²Œ í•´ì•¼ ë¹ ë¥´ê²Œ ìŠµë“í•˜ê³  í•˜ë‚˜ì˜ ì§€ì‹ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ ë§ì´ ë°°ì› ìŠµë‹ˆë‹¤. ë˜ í† ì˜, í† ë¡ ì„ ì •ë§ ë§ì´ í–ˆëŠ”ë°, ì´ë¥¼ í†µí•´ ì œ ì§€ì‹ì„ ì ê²€í•  ìˆ˜ ìˆì—ˆê³  ì˜¤ë¥˜ê°€ ë‚œ ë¶€ë¶„ì„ ë§ì´ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì–´ìƒ‰í–ˆë˜ ë°©ë²•ì¸ë°, ì •ë§ ë§Œì¡±í–ˆê³  ë§ì´ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ 6ê°œì›” ê³¼ì • ì¤‘ ë§ˆì§€ë§‰ 1~2ê°œì›”ì— ìµœì¢… í”„ë¡œì íŠ¸(ì•„ì´í í†¤)ë¥¼ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ ì „ 4ê°œì›”ë™ì•ˆ ì§§ê²Œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆëŠ”ë° (í•˜ë£¨ or 3ì¼), ì´ ë•Œ ì—°ìŠµí•œê²Œ ìµœì¢… í”„ë¡œì íŠ¸ì— ë„ì›€ì´ ë§ì´ ëìŠµë‹ˆë‹¤. ì•„ì´í í†¤ì—ëŠ” ê¸°ì—…ì´ ì°¸ì—¬í•˜ê¸°ë„ í•˜ë©°, ì°¸ì—¬ê¸°ì—…ì´ ìˆì„ ê²½ìš° ì£¼ì œë¥¼ ë“¤ì–´ë³´ê³  ê´œì°®ë‹¤ê³  íŒë‹¨ë˜ë©´ ì°¸ì—¬í•´ ê¸°ì—…í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ì €ëŠ” í•´ë³´ê³  ì‹¶ì—ˆë˜ ì•„ì´ë””ì–´ê°€ ìˆì–´ì„œ ê·¸ê±¸ ë°œì œí–ˆê³ , ì„ ì •ì´ ëì§€ë§Œ ê¸°ì—…í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ëª…í™•í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ì‹¶ì—ˆê³ , ê¸°ì—…ì˜ ê´€ì ì´ ê¶ê¸ˆí–ˆìŠµë‹ˆë‹¤. ë˜ ì±„ìš© ê°€ëŠ¥ì„±ë„ ìˆì–´ì„œ ë§¤ë ¥ì ìœ¼ë¡œ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. ì±„ìš©ì— ëŒ€í•´ì„œëŠ” ê¸°ì—…ì†Œê°œë•Œ ê°™ì´ ì–¸ê¸‰í•´ì£¼ì‹­ë‹ˆë‹¤. (ë³´í†µ ë§˜ì— ë“¤ë©´ ì±„ìš©í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ì‹ì…ë‹ˆë‹¤) ì•„ì´í ì—ì„œëŠ” ë©˜í† ë¥¼ ê° í”„ë¡œì íŠ¸ì— ë°°ì •í•´ì£¼ëŠ”ë°, ì´ ë¶€ë¶„ì´ ë„ì›€ì´ ë§ì´ ëìŠµë‹ˆë‹¤. ì„ íƒì˜ ê°ˆë¦¼ê¸¸ì—ì„œ ì¡°ì–¸ì„ ë§ì´ êµ¬í–ˆê³ , í”¼ë“œë°±ì„ ë§ì´ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë©˜í† ë‹˜ì´ ëª¨ë‘ ì¢‹ì§€ëŠ” ì•Šì•˜ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒ€ì—ì„œëŠ” ë˜ê²Œ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤ê³  ì´ì•¼ê¸°í•˜ë”ë¼êµ¬ìš”. ì•½ 2ê°œì›”ë™ì•ˆ ì£¼ì œë¥¼ ì •í•˜ê³ , ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ë©´ì„œ ê²°ê³¼ë¥¼ ì™„ì„±í–ˆìŠµë‹ˆë‹¤. ì™œ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ì¤‘ìš”í•˜ë‹¤ê³  ì´ì•¼ê¸°í•˜ëŠ”ì§€ ì •ë§ ë§ì´ ëŠê¼ˆê³ , í¬íŠ¸í´ë¦¬ì˜¤ì— í•˜ë‚˜ê°€ ì¶”ê°€ë˜ì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤. íšŒê³  ì‹œê°„ì´ ì°¸ ë¹ ë¥¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 6ê°œì›”ì´ ì§„ì§œ ë¹¨ë¦¬ ì§€ë‚˜ê°”ê³ , ê·¸ ê³¼ì •ì—ì„œ ë§ì´ ë°°ìš°ê³  ì¢‹ì€ ì¸ì—°ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ì•„ì´í ì— ë§Œì¡±í•˜ëƒê³  ë¬»ëŠ”ë‹¤ë©´, ì €ëŠ” ì§„ì§œ ëŒ€ë§Œì¡±ì…ë‹ˆë‹¤. ì •ë§ ì¢‹ì€ ë¶„ë“¤ì´ ê³„ì‹œê³ , ë§ì€ ë„ì›€ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì•„ì´í ì˜ ì²˜ìŒ ëª©ì ì€ \"ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê²ƒ\" ì´ì—ˆëŠ”ë°, ëª©ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ì½”ë“œ ì¤‘ì‹¬ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë‚´ëŠ”ê²Œ ì•„ë‹Œ, ê°œë…ì— ëŒ€í•´ í† ë¡ í•˜ë©´ì„œ ê°œë…ì„ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 6ê°œì›”ì´ ì§€ë‚¬ê³ , ì´ ê³¼ì •ì—ì„œ ë°°ìš´ ê²ƒë“¤ì„ ì´ìš©í•´ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  í•´ê²°í•˜ë©´ì„œ ê¾¸ì¤€íˆ ì„±ì¥í•˜ë ¤ê³  í•©ë‹ˆë‹¤.",
    "public": true
  }
]