[
  {
    "id": "Self_Development/English Study/SPEAK/가정_상상/25-09-20",
    "title": "나라면~~하겠다.",
    "date": "2025-09-20",
    "excerpt": "나~하기로 했어. 나라면 ~하겠어.",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "I've decided to continue working here. pursue a career in tech. I've decided to pursue a career in English Teaching. If I were you, I'd do the same thing. Would you do that too? continue If I were you, I'd shoot for the stars. If I were you, I'd be more realistic. 대화 시뮬레이션 상황 1: 친구의 진로 고민 A: I'm thinking about quitting my job and starting a business. B: Really? That's a big decision. If I were you, I'd think about it carefully first. A: What would you do in my situation? B: If I were you, I'd save up some money first and create a solid business plan. A: That makes sense. I've decided to take some business courses before making the leap. B: That's smart! Would you do that too if you were in my shoes? 상황 2: 여행 계획 A: I've decided to travel to Europe next summer. B: That sounds amazing! If I were you, I'd visit Italy first - the food is incredible. A: What about you? What would you do if you had a month off? B: If I were you, I'd explore different countries but not try to see everything. Quality over quantity! A: Good advice. I'm thinking about staying in each city for at least a week. B: Perfect! If I were you, I'd book accommodations early - summer gets busy. 상황 3: 새로운 도전 A: I'm considering learning a new language. B: That's exciting! If I were you, I'd start with Spanish - it's widely spoken. A: What would you choose if you had to pick one? B: If I were you, I'd go with something that interests you personally, not just what's practical. A: I've decided to learn Korean because I love K-dramas. B: Great choice! If I were you, I'd also try watching shows without subtitles to practice listening.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/visuworks_segmentation",
    "title": "[Visuworks] Segmentation Model",
    "date": "2025-09-20",
    "excerpt": "출혈관련 병변 탐지 모델 개발",
    "category": "Career",
    "tags": [
      "Computer Vision",
      "Segmentation"
    ],
    "content": "기간 : 2024.04 ~ 2024.06 (3개월) 역할 : 출혈 관련 병변 탐지를 위한 segmentation model 개발 및 모델 서빙, 모델 추론 관련 API 개발 --- 프로젝트 개요 눈의 안쪽 사진인 안저사진을 통해 눈의 질병들을 진단할 수 있습니다. 이때 질병들의 증상인 병변들을 눈으로 확인하는 과정이 꽤나 번거로웠기 때문에, 이를 모델을 통해 찾아서 보여주고자 했습니다. 여러가지 탐지 중에 출혈과 관련된 병변을 탐지하는 모델을 맡아 개발을 진행했습니다. Aiffel에서 했던 프로젝트의 결과(기본적인 Segmentation 모델 개발, 전처리 방법 고도화, MTL)를 고도화하는 방향으로 진행했습니다. 모델 개발 후에는 온프레미스로 GPU에 모델을 서빙하고 관련된 API를 개발하는 부분도 같이 진행했습니다. 프로젝트를 진행하면서 contrastive learning을 통한 pretraining 방법을 이해할 수 있었고, Triton Server와 같은 모델 서빙 도구를 활용하여 실제 서빙 환경에서의 경험을 쌓을 수 있었습니다. 하지만 한계도 분명한 프로젝트였습니다. 일반화하긴 어렵겠지만 의료분야에 기술 도입의 어려움을 알 수 있었고, 진로에 대한 생각이 바뀌게 된 계기이기도 합니다. --- 문제해결과정 프로젝트를 진행하면서 고민했던 부분은 2가지인데요. 모델의 성능을 어떻게 올릴까? 모델 서빙은 어떻게 하는걸까? 각각 어떻게 해결해나갔는지 간단하게 정리해봤습니다. 그리고 프로젝트가 끝나면서 어떤 한계점을 마주했는지도 정리했습니다. --- 모델 성능을 올리기 위해서 시도했던 것들 모델의 성능을 올리기 위해 시도한 방향은 비지도 학습을 통해 사전학습을 시키는 것입니다. 데이터셋이 약 1500개정도로 적었고, 문제의 특성상 데이터셋 안에서 레이블이 있는 경우가 80%, 있어도 이미지에서 차지하는 비율은 매우 작았습니다. 하지만 안저사진 자체는 1만개 이상으로 굉장히 많았고, 이를 활용해 성능을 향상시키고자 했습니다. CLIP과 같은 모델이 contrastive learning을 통해 좋은 결과를 보이고 있었고, 이를 이미지 분야에서도 적용하는 SimCLR 같은 컨셉이 있었습니다. 또한 병변탐지 모델에도 이를 적용한 논문들이 있었기 때문에 이를 적용해 성능을 향상시키고자 했습니다. 1년이 지난 시점에서 적어서 구체적인 수치가 기억은 안나지만, 결과적으로 성능향상이 되었던 걸로 기억합니다. 그 과정에서 시행착오가 많았지만, 논문을 보고 구현했던 경험이 실력을 향상시키는데 정말 많은 도움이 되었습니다. --- 모델 선택: U-Net 기반 모델은 Aiffel 프로젝트를 진행하면서 얻은 결과를 바탕으로 사용했습니다. 이때 실험결과를 통해, 많은 의료 논문에서 Unet이 작은 병변을 탐지하는데 유리하다고 했던 부분을 확인할 수 있었고, CLAHE와 같은 이미지 전처리 방법을 고도화했습니다. Unet 구조로 모델을 설계하고, Encoder를 풍부하게 하기 위한 방법들을 적용하려고 했습니다. 이를 위해 Contrastive Learning (이후 CL)이나 Multi-Task Learning(이후 MTL) 등을 적용했습니다. --- Contrastive Learning 기반 사전학습 성능을 향상시키기 위해 일반화된 패턴을 먼저 학습시키는, pretraining 방법을 사용하려고 했습니다. 먼저 많이 사용되는 ImageNet기반 사전학습 가중치를 사용했습니다. 하지만 실험을 해보니 성능향상이 크게 되지 않았고, 그 이유로 안저사진이 ImageNet의 사진들과 많이 다르기 때문이라고 이해할 수 있었습니다. 따라서 안저사진 자체로 사전학습을 시도했습니다. 이미지분야에서 주로 사용되는 contrastive learning (대조학습) 방법을 적용했습니다. 대조학습의 기본적인 개념은 비슷한 건 비슷한 벡터로, 다른 건 다른 벡터를 가지도록 학습해나가는 것입니다. 이미지에서는 이미지의 원본과 이미지의 변형본이 비슷한 벡터를 가지도록, 다른 이미지와는 다른 벡터를 가지도록 학습을 하게 됩니다. 이는 SimCLR과 같은 방법들의 컨셉입니다. SimCLR과 같은 개념을 병변 탐지 task에도 적용한 논문들을 찾을 수 있었습니다. 구체적인 구현과정이나 코드는 공개되지 않았지만, 이 개념을 제가 푸는 문제에도 적용하고자 했습니다. SimCLR의 경량화 버전이라고도 이해할 수 있는 MoCo를 통해 대조학습을 적용했습니다. 구현할 때 가장 어려웠던 점은 성능에 해상도와 batch size가 가장 중요한 역할을 하는데, 이를 위해서는 multi-gpu를 사용해야했습니다. 예를 들어 배치가 128이라면 4개의 gpu에 32개씩 나눠서 학습을 진행하도록 해야했습니다. 즉 nvidia 4090 이 4개가 있는 실험환경에서, 이를 전부 사용해야 이상적인 결과(논문과 비슷한 성능)가 나온다는 것입니다. 다중 GPU 분산 학습을 해본 경험이 없었기 때문에 이 부분에서 시행착오가 정말 많았습니다. 또한 GPU 사용률을 높이기 위해 병목 지점이 되는 이미지 augmentation과 같은 CPU 작업들을 최적화하는 작업도 필요했습니다. 결과적으로 성능 향상이 눈에 띄게 나타났습니다. 사실 구체적인 수치로 표시해야 하는데, 1년이라는 시간이 지나면서 정확한 수치는 기억나지 않습니다. 다만 random vs ImageNet vs CL 기반으로 비교했을 때, 분명히 CL이 가장 좋은 성능을 보였습니다. --- Multi-Task Learning 다음으로 Unet의 Encoder 부분을 풍부하게 학습시키기 위해 MTL을 적용했습니다. MTL은 하나의 모델이 여러 작업을 동시에 학습하는 방법으로, 각 작업이 서로 다른 관점에서 특징을 학습하도록 도와줍니다. 구체적으로는 Encoder에 Segmentation을 위한 Decoder 외에 reconstruction을 위한 decoder도 추가했습니다. 이는 일반화 성능을 향상시켰는데, 그 이유를 분석해보면 $p(X)$ 자체를 추정하는 task가 추가되면서 encoder가 근본적인 패턴을 찾는 방향으로 학습이 유도되었기 때문입니다. reconstruction task는 입력 이미지를 그대로 복원하는 작업으로, encoder가 이미지의 전반적인 구조와 특징을 더 잘 이해하도록 도와줍니다. --- 모델을 서빙할 때 했던 고민들 모델을 개발한 후에는 실제 서비스 환경에서 사용할 수 있도록 모델 서빙 작업을 진행했습니다. 추론에 최적화된 그래프 형태로 모델을 변환하고, Triton Server를 이용해 dynamic batching과 같은 고급 기능들을 쉽게 구현할 수 있었습니다. 다만 온프레미스 환경에 설정하면서 CUDA, PyTorch, Triton Server 버전들을 맞추는 과정에서 정말 많은 시행착오가 있었습니다. 의존성 충돌과 버전 호환성 문제로 인해 고통스러웠던 경험이지만, 이를 통해 실제 프로덕션 환경에서의 모델 서빙에 대한 깊은 이해를 얻을 수 있었습니다. 추론 성능은 서비스 상황에서의 예상 요청량을 분석하고, 이때 추론에 걸리는 시간(latency)을 계산하는 방식으로 평가했습니다. 제약조건은 대략 10장의 이미지를 처리하는데 1초 이내가 걸려야 했고, 이를 충분히 만족시킬 수 있었습니다. 돌이켜보면 단순히 PyTorch로 모델을 올려도 만족할 수 있는 수준의 제약조건이었기 때문에, 빠르게 서비스를 시작해야 했다면 Triton Server 없이도 충분했을 것 같습니다. 하지만 Triton Server를 도입한 것은 향후 확장성과 성능 최적화를 위한 좋은 경험이 되었습니다. --- 프로젝트의 한계점 이 프로젝트는 실패로 끝이 났습니다. 모델의 성능이 충분하지 않아서가 아니라, 사용자가 필요성을 느끼지 못했기 때문입니다. 더 자세히 설명하면, 소프트웨어 의료기기를 출시하는 데에는 많은 어려움이 있습니다. 그 중 하나는 의료기기 허가를 받는 데까지 많은 시간이 걸린다는 점입니다 (보통 2~4년 정도). 따라서 의료기기 허가를 받기 전에는 법을 피해서 서비스를 제공해야 합니다. 이때 \"진단\"을 해서는 안 되며, 진단을 위한 보조 정도만이 가능합니다. 따라서 이 프로젝트는 \"보조\"에 목적을 두고 시작되었습니다. 하지만 실제 사용자의 이야기를 들어보면 병변을 찾는 것이 그렇게 어려운 일이 아니고, 굳이 돈을 내고 보조 프로그램을 쓸 필요를 느끼지 못하고 있었습니다. 또한 불편한 UI를 감수하면서까지 기존의 방법을 벗어나 새로운 프로그램을 쓰고 싶지도 않았습니다. 조금 허무한 결론이지만, 열심히 모델을 만들고 서빙했지만 사용자가 필요로 하지 않았기 때문에 실패로 끝이 나게 되었습니다. 소프트웨어 의료기기를 개발할 때의 한계점과 실제 사용자에게 맞는 문제 정의가 필요하다는 것을 많이 느낄 수 있었습니다.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/visuworks_ocr_pipeline",
    "title": "OCR Pipeline System",
    "date": "2025-09-20",
    "excerpt": "문서 이미지에서 텍스트를 추출하는 자동화된 OCR 파이프라인",
    "category": "Career",
    "tags": [
      "Python",
      "OCR",
      "Computer Vision",
      "Docker",
      "gRPC"
    ],
    "content": "기간 : 2024.07 ~ 2024.10 (4개월) 역할 : 시스템 기획부터 구현까지 모든 과정 결과 (성과) : 정확도 99%, 에러율 1% 미만의 안정적인 OCR 파이프라인 구축 비즈니스에 주는 영향 : 안정적인 데이터 수집 기반을 구축하여 서비스 개발 및 운영의 안정성 확보 프로젝트 개요 : 병원 검사 결과 이미지에서 실시간으로 검사 결과 데이터를 추출해 데이터베이스에 적재하는 OCR 파이프라인 시스템을 설계하고 구현하는 프로젝트를 진행했습니다. 환자의 검사 결과를 바탕으로 \"시력 교정 수술 추천 서비스\"나 \"렌즈 사이즈 추천 서비스\"를 운영하고 있었기 떄문에, 안정적인 파이프라인을 구축해야만 안정적인 서비스를 운영할 수 있는 상황이었습니다. <figure> <img src=\"/post/Portfolio/OCR_pipeline.png\" alt=\"OCR Pipeline\" width=\"100%\" /> <figcaption>그림1. OCR Pipeline 구조</figcaption> </figure> 제가 구성한 파이프라인 구조는 그림1과 같습니다. 6가지 종류, 총 40~50대의 검사장비에서 검사를 진행하고 이를 실시간으로 OCR하고 추출된 데이터를 DB에 적재하게 됩니다. 파이프라인을 설계하면서 정의했던 목표는 크게 2가지입니다. 에러가 발생하지 않고 안정적인 상태 안정적인 서비스 개발 및 운영을 위해서 99% 이상의 OCR 정확도 \"검사 결과\"이기 때문에 OCR 오류가 치명적일 수 있음 프로젝트가 끝나고 결과 혹은 성과는 다음과 같습니다. 에러율 1% 미만의 안정적인 파이프라인 구조 기능과 역할이 명확하게 구분된 객체지향적인 코드 구조 문제 이유와 범위가 명확하게 보이는 에러 로그 지속적인 모니터링 시스템 비동기구조 적용으로 속도 향상 유닛 테스트와 타입 검증 99% 이상의 OCR 정확도 이미지 전처리를 통해 정확도 향상 테스크의 특성을 활용해 최적화된 모델 사용 및 후처리 로직으로 목표 정확도 달성 이러한 결과를 얻기 위해 어떤 선택들을 했는지에 대해 정리했습니다. 프로젝트 시작할 때의 상황 : 처음 프로젝트를 시작할 때의 상황은 전임자가 프로젝트르 진행하다가 중간에 떠난 상황이었습니다. 알 수 없는 에러로그가 쌓이고 있었고, OCR 모델의 정확도는 알 수 없었습니다. 어떤 문제를 해결하려고 해도 어디서 발생한 에러인지, 그 범위는 어디까지인지 알 수 없었습니다. 또 절차지향적으로, 정확히는 하나의 .py에 구조없이 구현된 코드들은 기능과 역할이 모호했습니다. 목표를 달성하기 위해서는 코드 구조를 개선하며 에러를 잡아나가야 했고, OCR 모델 개선해야 했습니다. 맞으면서 배우다, 객체지향 설계의 중요성 : 무자비하게 쌓인 코드들을 보면서, 머리로 이해했던 1기능 1함수 원칙이나 디자인 패턴 등 코드를 잘 짜기 위한 방법들을 마음으로 이해할 수 있었습니다. 여러 기능이 하나의 함수에 작성되어있고, 타입힌트도 전혀 작성되어있지 않은 코드를 이해하고 디버깅하는 과정은 굉장히 어려웠습니다. 하지만 이런 코드를 이해하고 고쳐나가는 과정은 코드를 작성하는 능력을 키우는데 굉장히 큰 도움이 됐습니다. 이해하기 쉽고 디버깅이 쉬운 코드를 작성하기 위한 나름의 규칙들을 세울 수 있었습니다. 함수에는 꼭 하나의 기능만, 객체에는 하나의 역할만 부여하기 객체 지향의 꽃은 추상화, 입력과 출력만 신경쓸 수 있도록 이를 고려해 코드를 작성했고 문제를 하나씩 고쳐나갈 수 있었습니다. 에러의 범위와 원인을 파악하기 쉬워졌고, 전체 문제를 작은 문제들로 쪼갤 수 있었습니다. 문제를 정확히 이해하기, 비동기 구조로 속도 향상 : <figure> <img src=\"/post/Portfolio/OCR_server_threadpool.png\" alt=\"OCR Server 구조\" width=\"80%\" /> <figcaption>그림2. OCR Server 구조</figcaption> </figure> client와 server에서 실행되는 로직은 그림2와 같습니다. 처음 프로젝트를 인수인계 받을 때 동시에 여러 이미지를 처리하더라도 OCR 속도가 1초 미만이어야 한다고 했습니다. 그 이유는 Client 프로그램에서 Server로부터 응답이 늦게 오면, 그 다음에 진행되는 작업도 늦어지게 되고, 이는 고객에게 불편함을 주기 때문입니다. Client에서 OCR 성공 여부를 확인할 필요가 있나? 에 대한 의문에서 시작했습니다. 이미지 송수신 여부만 확인하면 되기 때문에 주고 받을 성공여부에 대한 의미를 재정의했습니다. 그리고 Server에서의 역할을 구분했고, OCR 과정이 속도가 오래걸리더라도 Client에는 영향을 미치지 않는 구조로 재정의할 수 있었습니다. 런타임 에러 줄이기, pytest를 활용한 unit test와 mypy를 활용한 타입 검증 : 안정적인 파이프라인을 운영하기 위해 런타임 에러를 줄일 수 있는 방법에 대해 고민했습니다. 런타임 에러를 줄이기 위해 사용한 방법은, pytest를 통한 unitest와 mypy를 이용한 타입 검증입니다. 작게 쪼개놓은 기능이나 역할들이 잘 수행되는지 unit test를 통해 미리 확인하고, 주고 받는 값들의 타입들이 맞는지 미리 확인하는 작업을 CI과정에서 진행했습니다. 이를 통해 어떤 기능을 추가하거나 수정했을 때 발생할 수 있는 런타임 에러들을 미리 검증하고 사전에 파악할 수 있었습니다. 문제에 특화된 모델 사용하기, OCR 정확도 확보 : <figure> <img src=\"/post/Portfolio/OCR_tritonserver.png\" alt=\"OCR Model\" width=\"50%\" /> <figcaption>그림3. OCR Model</figcaption> </figure> 안정적인 파이프라인이 어느정도 완성된 다음에는 99%이상의 OCR 정확도를 어떻게 달성할 수 있을지에 대한 고민을 했습니다. 이를 위해서 문제의 특징을 사용했습니다. OCR은 크게 2단계로 구분됩니다. 위치를 찾는 Text Detection과 텍스트를 인식하는 Text Recognition입니다. 검사장비에서 생성되는 검사 이미지는 고정되니까 위치 정보를 사전에 정의해놓을 수 있었습니다. 따라서 텍스트를 인식하는 부분에 특화된 모델만이 필요했습니다. 여러 모델을 찾던 중 TrOCR이라는 오픈소스 모델을 발견했고, 이 모델이 개발된 동기가 Text Detection은 추후로미루고 Text Recognition에 특화된 모델 먼저 만들겠다는 것입니다. 이미지 전처리와 결과 후처리 작업을 추가하니, 200건정도로 구성한 테스트 셋에서 99%이상, 거의 100%에 가까운 정확도를 확인할 수 있었습니다. 모델 배포는 온프레미스로 진행했고, TritonServer를 사용했습니다. TritonServer를 선택한 주요 이유는 동적 배치 처리와 동시성 관리를 쉽게 구현할 수 있고, 이를 통해 처리량을 달성할 수 있기 때문입니다.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/visuworks_lenze_size_rec",
    "title": "Lens Size Recommendation System",
    "date": "2025-09-20",
    "excerpt": "머신러닝을 활용한 맞춤형 렌즈 사이즈 추천 시스템",
    "category": "Career",
    "tags": [
      "Python",
      "Machine Learning",
      "Causal Inference"
    ],
    "content": "기간 : 2025.03 ~ 현재 (6개월) 역할 : 문제정의, 프로젝트 설계, 구현까지 모든 과정 --- 프로젝트 개요 렌즈 삽입을 통한 시력교정 수술시 \"렌즈의 크기\"를 결정하는건 의사의 노하우와 감에 의존하는 일이라고 합니다. 따라서 경험이 적은 의사는 선택에 어려움을 느끼고, 경험이 많더라도 판단에 어려움이 있다고 합니다. <figure> <img src=\"/post/Portfolio/Lenze_size_reco.png\" alt=\"lenze size reco\" width=\"100%\" /> <figcaption>그림1. 렌즈 사이즈 추천 서비스</figcaption> </figure> 이러한 선택을 도와주기 위해 렌즈 삽입 후 결과 (렌즈 후면과 수정체 전면까지의 거리) 를 예측해 렌즈 사이즈를 추천하는 서비스를 운영하고 있습니다. 기존에 진행되던 프로젝트를 이어서, 전반적인 부분에 대한 문제를 정의하고 해결하는 프로젝트를 진행했습니다. 데이터 수집부터 모델 개발, 서빙, 모니터링까지 ML 서비스를 위한 모든 부분에 대해 문제를 정의하고 해결했습니다. 이 과정에서 전체적인 흐름에 대해 배울 수 있었고, 문제를 해결해 나가는 과정에서 각 부분에서 중요하게 생각해야 하는 점들을 배울 수 있었습니다. 문제를 정의하고 해결한 과정을 아래에 정리했습니다. 모델에 대한 연구는 계속 진행 중이며, 관련한 내용을 논문으로 작성 중입니다. 연구 방향에 대해 정리한 글 - 외삽 문제를 어떻게 다뤄야할까? --- 문제 해결 과정 크게 데이터 관련 / 서빙과 모니터링 관련 / 모델링 관련 으로 문제 해결과정을 구분해서 정리했습니다. 전체적인 ML 파이프라인은 다음과 같습니다. <figure> <img src=\"/post/Portfolio/lenze_size_ml_system.png\" alt=\"data pipeline\" width=\"100%\" /> <figcaption>그림2. ML pipeline</figcaption> </figure> 1번 OCR 파이프라인에 대한 내용은 다음 포스트해서 확인할 수 있습니다. portfolio - OCR pipeline 구현 --- 데이터 일관성을 확보하자 : data pipeline, feature store 구현 모델을 개발하면서 첫번째로 마주한 문제는 데이터 일관성 문제입니다. 기존에 회사에서 실험, 모델을 만들고 서빙할 때 개별적으로 DW (데이터웨어하우스)에서 데이터를 불러왔습니다. 이는 학습과 추론 상황에서의 데이터 일관성을 보장하지 못하며, 실제로 코드를 뜯어봤을 때 서로 다른 전처리 로직을 사용하고 있었습니다. 이렇게 사용했을 때 학습 상황에서 확인한 성능과 추론 성능이 굉장히 달라질 수 있습니다. 또한 실험하는 사람마다 다른 데이터 전처리 로직을 사용했기에 일관된 모델 성능을 파악하기도 어려웠습니다. 이를 해결하기 위해 데이터 파이프라인을 통해 데이터 전처리 과정을 통일했고, 피처 스토어를 통해 학습과 추론 상황에서의 데이터 일관성을 보장했습니다. 구현할 때는 유명한 프레임워크나 툴을 사용하기보다 현재 필요한 기능에 맞도록 구현했습니다. 필요없는 기술을 위해 복잡성을 증가시키지 않고, 해결하고자 하는 문제에 집중했습니다. --- 모델 서빙은 어떻게 하면 좋을까? 모델을 서빙할 때 고려했던 핵심 요소는 배포 환경의 일관성과 모델 업데이트의 용이성이었습니다. 컨테이너화를 통해 개발 환경과 운영 환경 간의 차이를 최소화하고, FastAPI를 사용하여 간단하면서도 효율적인 API 서버를 구축했습니다. Docker를 활용한 배포로 인프라 관리의 복잡성을 줄이고, S3를 통해 모델 파일을 중앙 집중식으로 관리하여 새로운 모델 버전을 쉽게 배포할 수 있도록 했습니다. 이는 특히모델 성능 개선 시 빠른 반영이 가능하게 해주었습니다. --- 추론 상황에서의 성능은? : Monitoring 구현 모델이 실제 서비스 상황에서 어떤 성능을 보이는지, 데이터 분포 변화가 있는지 등을 지속적으로 모니터링하는 것이 중요했습니다. 데이터는 시간이 지남에 따라 계속 변화하기 때문에, 학습 시점의 성능과 현재 성능 간의 차이가 발생할 수 있습니다. 이러한 변화를 놓치면 모델이 조용히 성능이 저하되는 silent failure* 상황이 발생할 수 있어, 지속적인 모니터링과 필요시 재학습 과정이 필수적이었습니다. 구현한 모니터링 시스템은 크게 세 가지였습니다. 먼저 성능 모니터링은 서비스 로그에 저장된 모델 추론값과 수술 후 실제 측정 결과값을 비교해서 평가지표들을 주기적으로 계산했습니다. 두 번째로 Data Drift 감지는 입력 데이터의 분포 변화를 감지해서 모델 성능 저하의 조기 경고 신호를 포착했습니다. 특히 피처별 통계량 변화와 분포 변화를 학습 시점과 비교했습니다. 마지막으로 자동 알림 시스템을 통해 성능지표와 data drift에 대한 지표들을 지속적으로 확인했습니다. 결과적으로 모델의 성능 변화를 사전에 감지할 수 있게 되어서, 성능 저하 시 즉시 대응할 수 있었습니다. 데이터 drift가 발생했을 때도 원인을 빠르게 파악하고 적절한 조치를 취할 수 있어서 모델의 안정성과 신뢰성을 크게 향상시킬 수 있었습니다. 개념에 대해서 제가 이해한 부분들을 정리한 내용입니다. data drfit란? silent failure에 대해서 --- 모델링 : 선택의 문제에서 예측 모델이 가지는 한계점 모델링 부분이 가장 어려웠습니다. 답을 알려줄 사람이 없다보니, 문제를 풀기 위해 이런저런 논문이나 내용도 찾아보고 세미나에 가서 조언도 구했습니다. 그러다가 \"인과추론\"이라는 키워드를 알게되고 이를 통해 문제를 정의하고 하나씩 해결해나갈 수 있었습니다. 이 과정을 요약하면, 선택의 문제를 예측의 문제로 정의했을 때의 한계점을 확인하고 이를 해결하기 위한 방안을 제시했습니다. 어떤 문제인지, 어떻게 해결방안을 정의했는지 정리했습니다. --- 측정하지 않은 값에 대한 예측 성능을 어떻게 평가할 수 있을까? 이 프로젝트는 그림1처럼 최적의 렌즈 사이즈 선택하는 문제를 풀고 있습니다. 선택의 문제에서 예측 평가지표만을 통해 모델을 신뢰하기엔 한계가 있습니다. <figure> <img src=\"/post/Portfolio/Lenze_size_evaluate.png\" alt=\"how to evalute\" width=\"100%\" /> <figcaption>그림3. 고민했던 문제 : 어떻게 평가할 것인가</figcaption> </figure> 그림 3은 문제 상황을 그린 것입니다. 선택할 수 있는 \"렌즈 사이즈\"는 3가지가 있고, 의사는 3가지 중 하나를 선택해 수술을 진행하게 됩니다. 3가지에 대한 예측값을 모두 생성하지만, 실제 결과는 1개만 알 수 있습니다. 이때 실제 결과가 없는 나머지 예측들은 어떻게 평가할 수 있을지에 대한 의문이 들었습니다. > 즉, 학습 데이터셋에는 고객별로 1개의 렌즈 사이즈에 대한 결과만 존재하는데, 나머지 사이즈에 대한 예측값은 어떻게 신뢰할 수 있지? 라는 의문이 들었습니다. 이에 대한 답을 \"양수성\" 혹은 \"분포의 겹침\"에서 찾을 수 있었습니다. 양수성에 대해 정리한 글 양수성은 모든 개체가 모든 처치(treatment)를 받을 수 있는 가능성이 있어야 한다는 가정입니다. 렌즈 사이즈 선택 문제에서 각 환자가 각 렌즈 사이즈를 받을 수 있는 가능성 이 있어야 한다는 의미입니다. 만약 특정 환자에게는 절대 사용하지 않은 렌즈 사이즈가 있다면, 예측값을 신뢰할 수 없을 것입니다. 양수성을 만족한다면 관찰되지 않은 사이즈에 대한 예측도 신뢰할 수 있다 고 이야기할 수 있습니다. --- 추론 로직 구조 <figure> <img src=\"/post/Portfolio/Lenze_size_inference1.png\" alt=\"Positivity\" width=\"80%\" /> <figcaption>그림4. 추론할 때의 로직</figcaption> </figure> 현재 가져가고 있는 추론 로직은 그림 4와 같습니다. propensity score를 추정해, 고객에 대해 신뢰할 수 있는 처치범위와 그렇지 않은 범위를 정하고, 각각에 따라 다른 예측 모델을 사용하고 있습니다. <figure> <img src=\"/post/Portfolio/Lenze_size_PI.png\" alt=\"Prediction Interval\" width=\"80%\" /> <figcaption>그림5. 양수성 조건에 따른 예측</figcaption> </figure> 양수성 혹은 분포의 겹침이 보장되어 신뢰할 수 있는 처치의 범위는 CQR을 통해 예측 범위를 추정하고, 그렇지 않은 곳은 partial identification의 아이디어를 차용해 예측 범위를 추정했습니다. 이를 통해 예측의 불확실성을 구간으로 표현할 수 있습니다. --- 신뢰할 수 있는 처치의 범위는 어떻게 구할까 : Propensity Estimator 가장 먼저 예측 평가지표 (MSE, MAE)로 성능을 확인한 예측의 범위를 정해야 합니다. 이는 성향 점수 (propensity score)를 추정함으로써 정할 수 있습니다. 성향 점수는 처치를 선택할 확률을 의미하며, $P(T X)$을 추정하는 문제입니다. 성햠 점수 추정 모델로 고객별로 데이터에서 처치의 분포가 겹치는 범위를 정할 수 있습니다. (인과추론의 가장 기본적인 가정인 양수성을 만족하는 범위를 찾는다고도 할 수 있습니다) <figure> <img src=\"/post/Portfolio/Lenze_size_propensity.png\" alt=\"Positivity\" width=\"70%\" /> <figcaption>그림6. 양수성 조건 만족하는 범위</figcaption> </figure> 그림6은 고객별로 성향점수를 추정하고, 이를 통해 양수성이 보장되는 처치의 범위를 정한 예시입니다. 고객 A는 12.1, 12.6, 13.2에 대한 예측 성능을 신뢰할 수 있고, 고객 B는 12.6에 대한 예측값을 신뢰할 수 있다고 판단할 수 있습니다. --- 신뢰할 수 있는 범위의 예측은 어떻게 할까 : CQR 신뢰할 수 있는 범위는 예측 지표를 통해 고도화한 예측 모델을 사용하면 됩니다. 예측 구간 (Prediction Interval)을 그려주기 위해 Quantile Regressor를 사용했습니다. 그 후에 Conformal prediction의 개념을 도입해 원하는 신뢰구간을 포함하는 예측구간을 추정할 수 있도록 했습니다. 개념에 대해서 제가 이해한 부분들을 정리한 내용입니다. Quantile Regression이란? Conformal prediciton이란? --- 신뢰할 수 없는 범위의 예측은 어떻게 할까 : Partial Identification 신뢰할 수 있는 정도를 예측 범위로 나타낼 수 있고, 따라서 양수성이 보장되지 않는 예측 범위의 예측 범위를 넓게 그려주고자 했습니다. 양수성이 보장되는 처치의 예측값은 신뢰구간을 예측해 보여주고, 나머지 처치에 대해서는 \"partial identification\"과 같은 개념을 사용해 예측 범위를 추정했습니다. 개념에 대해서 제가 이해한 부분들을 정리한 내용입니다. Partial Identification이란? Manski Bounds란?",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/visuworks_chatbot",
    "title": "AI Chatbot Project",
    "date": "2025-09-20",
    "excerpt": "고객상담용 챗봇 개발",
    "category": "Career",
    "tags": [
      "RAG"
    ],
    "content": "기간 : 2024.11 ~ 2025.02 (4개월) 역할 : 문제정의, RAG pipeline 구현 및 배포, 모니터링 시스템 구축 결과 (성과) : 일평균 100~200건정도의 상담을 자동화해 상담사의 상담 효율성 향상 --- 프로젝트 개요 반복되는 고객상담업무는 상담사가 필요한 상담업무를 하지 못하게 만드는 요인 중 하나입니다. 이를 대체하기 위해 챗봇을 개발하는 프로젝트를 진행했습니다. FAQ (자주 묻는 질문)에 대한 데이터셋을 제공 받았고, 이를 통해 답변을 생성하려고 했습니다. 이를 위해 RAG pipeline을 구축해 문서를 참조해 답변을 생성하도록 했습니다. 답변 후에 만족/불만족 여부를 선택할 수 있도록 해 사용자의 피드백을 수집했고, 이를 통해 만족도를 높여나가는 방식으로 개발을 진행했습니다. 특히 불만족도가 가장 높은 부분을 분류 모델을 앞단에 붙여 해결할 수 있었고, 불만족도를 50%이상 감소시킬 수 있었습니다. RAG pipeline을 완성한 후에는 모니터링 시스템을 구현해 silent failure를 확인하고 유지비용을 최소화할 수 있었습니다. 프로젝트를 통해 RAG에 대한 이해와 사용자의 피드백을 받으며 만족도를 높여나가는 경험을 할 수 있었습니다. 자세한 문제 해결과정은 아래에 정리했습니다. --- 문제 해결 과정 RAG 파이프라인은 다음과 같이 구성되어있습니다. <figure> <img src=\"/post/Portfolio/chatbot_pipeline1.png\" alt=\"RAG pipeline\" width=\"50%\" /> </figure> <figure> <img src=\"/post/Portfolio/chatbot_pipeline2.png\" alt=\"RAG pipeline\" width=\"100%\" /> <figcaption>그림1. RAG pipeline</figcaption> </figure> 각각의 구성 요소들을 어떻게 구성했으며, 어떻게 최적화했는지 설명드리겠습니다. --- 사용자 만족도를 높이기 위한 방법 : 분류 모델 서비스를 운영하면서 만족도를 높여나가는 방향으로 프로젝트를 진행했습니다. 만족도는 대답 후에 \"만족\" , \"불만족\" 버튼을 통해 수집했습니다. 대답에 대한 피드백은 보통 불만족하는 경우에 많이 누르기 때문에, 만족도를 높이기 위해서 \"불만족\"하는 포인트들을 없애려고 노력했습니다. 사용자들이 가장 불만족스러워했던 부분은 LLM이 명확히 답변할 수 없는 질문에 대해 모호한 답변을 생성하는 것이었습니다. 특히, 답변할 수 없는 내용임에도 불구하고 계속해서 재질문만 하며 대화를 이어가는 불필요한 상황이 빈번하게 발생했습니다. 주민번호와 전화번호를 입력하며 예약에 대한 문의를 하는 것과 같은 상황입니다. 또한 고객사에서 원하는 메뉴얼이 있지만, 이를 LLM에게 명령하기에는 쉽지 않았습니다. 이러한 부분들을 해결하기 위해 파이프라인의 맨 앞단에 분류모델을 배치해, 질문의 종류를 분류하여 적절한 대응을 할 수 있도록 했습니다. 분류 모델은 LLM에 분류 프롬프트를 적용해 구현했습니다. 이를 통해 \"불만족\" 피드백을 50%이상 감소시킬 수 있었습니다. --- 대화의 맥락을 고려해 문서를 검색하자 : 질문 재정의 대화의 흐름을 고려해 답변을 생성해야 했습니다. 이때 맥락을 고려해 문서를 검색하는 부분이 중요했습니다. 예를 들어 Q: 라식 가격은 얼마야? A: 라식 가격은 ~~ 이후에 Q: 라섹은? 이라고 묻는다면 \"라섹의 가격은 얼마야?\"에 대한 문서를 검색해 대답을 해야합니다. 이를 위해 질문을 재정의하는 부분이 필요했습니다. redis관리하는 짧은 기간의 대화기록을 불러와 대화 맥락에 맞는 질문으로 재정의하도록 구현했습니다. --- 검색 성능 향상을 위해 : 질문 쪼개기 + 키워드 추출 하나의 질문에 2개의 의미가 담겨있는 경우가 있습니다. \"라식과 라섹의 가격 알려줘\" 라는 질문을 답변하기 위해서는 \"라식의 가격\"과 \"라섹이 가격\"이라는 2개의 질문으로 쪼개서, 각각에 대한 문서를 검색할 필요가 있습니다. 또한 시멘틱 서치의 한계와 사용하는 embedding model이 원장이름과 같은 단어를 모르기 때문에 발생하는 한계점 등을 극복하기 위해, 질문에서 키워드를 추출한 후 이를 사용해 필터링을 진행했습니다. \"김민규 원장님에 대한 정보 알려줘\"와 같은 질문에는 임베딩 벡터로는 적절한 문서를 찾기 어려웠지만, \"김민규\"라는 단어를 추출해 문서를 필터링하게 된다면 관련된 정보를 찾을 수 있게 됩니다. 또한 사용하는 데이터셋이 FAQ 이기 때문에, FAQ의 장점을 살릴 수도 있습니다. --- 문서 검색 문서 검색을 잘하기 위해서는 2, 3번의 과정도 중요했지만, 결국 시멘틱 서치에 사용될 임베딩 벡터를 잘 만드는게 중요했습니다. 따라서 FAQ를 바탕으로 테스트셋을 구성하고 이를 통해 모델들을 비교해 선택했습니다. 테스트셋을 만드는 과정은 LLM을 통해 생성했고, 답변에 적절한 질문을 생성하도록 설정했습니다. VectorDB는 Qdrant로 구성했고, 사용자가 문서를 수정하거나 추가했을 때 바로 반영되도록 구현해 유지비용을 최소화했습니다. 또한 참조 문서 버전별로 명확히 구분되게 하기 위해서 멀티테넌시 기능을 이용했습니다. --- 답변 생성 답변 생성 시 중점을 둔 부분은 두 가지였습니다. 첫째, 참조된 문서의 내용만을 바탕으로 답변하도록 제한한 것이고, 둘째, 사용자가 친근하고 이해하기 쉽다고 느끼는 답변을 생성하도록 한 것입니다. 프롬프트 엔지니어링을 통해 LLM이 참조 문서의 내용만을 바탕으로 답변하도록 제한하고, 동시에 친근하고 이해하기 쉬운 톤으로 응답하도록 설계했습니다. 이를 통해 정확성과 사용자 경험을 모두 향상시킬 수 있었습니다. --- 모니터링 사용자 채팅 로그를 자동으로 모니터링해, silent failure 감지하고 유지보수 비용을 최소화하기 위해 모니터링 기능을 구현했습니다. 오픈소스인 RAGAS 프레임워크의 프롬프트를 뜯어보면서 구현했고, 답변이 검색된 문서에 기반한 것인지에 대한 점수를 냈습니다. Airflow를 통해 로그가 쌓이는 스프레드 시트에서 점수를 냈고, 일별 혹은 주별 점수와 점수가 높은 대화를 슬랙을 통해 확인했습니다.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/pnu_cloud_funding_prediction",
    "title": "[PNU] Cloud Funding Success prediction model",
    "date": "2025-09-20",
    "excerpt": "부산대학교 산업공학과 졸업 프로젝트",
    "category": "Career",
    "tags": [
      "ML",
      "Crawling",
      "Prediction"
    ],
    "content": "기간 : 2022.03 ~ 2022.06 (4개월) 역할 : 데이터 수집 및 정제 (1만건), 모델 개발 관련 자료들 내용을 정리한 블로그 링크 발표자료 대학교 4학년때 대학교 졸업과제로 진행한 프로젝트입니다. \"와디즈\"라는 클라우드 펀딩 사이트에서 상품의 성공여부를 예측하는 모델을 개발하려고 했습니다. 사실 2년이 지난 시점이라 자세한 과정이 기억이 나진 않습니다. 하지만 실패했던 기억과 왜 실패했는지에 대해 복기하기 위해서 정리를 했습니다. 프로젝트 개요 클라우드 펀딩(Crowdfunding)은 인터넷을 통해 다수의 사람들로부터 소액의 자금을 모아 특정 프로젝트나 사업을 진행하는 방식입니다. 즉 상품을 먼저 만드는게 아닌 후원을 받고, 후원 금액이 일정금액을 넘기면 제작에 들어가는 방식입니다. 이때 사용자나 판매자에게 \"성공 여부\"를 예측해서 알려줘 도움을 주고자 했습니다. 이를 위해 크롤링을 통해 1만건이 넘는 데이터를 수집헀고, 이를 정제해 예측 모델을 만들었습니다. 결과 모델 성능은 90%이상으로 기억하고 성능을 높이기 위해 데이터 불균형을 SMOTE와 같은 오버 샘플링을 통해 해결했던 기억이 있습니다. 데이터를 수집하고 정제하는 과정, 모델링 과정, 높은 성능 등을 근거로 발표를 했을 때 긍정적인 피드백을 받을 것이라고 생각했었습니다. 하지만 교수님들의 피드백은 부정적이었습니다. 모델의 성능이나 결과를 어떻게 신뢰할 수 있지?에 대한 내용이 주를 이뤘습니다. 부끄럽지만 당시에는 ML을 모르는 교수님들의 한계다. 성능이 이렇게 좋은데 이걸 모른다고? 이런식의 불만을 가졌던 기억이 있습니다. 돌이켜보니 아쉬운 부분들이 보입니다. 결과에 대한 회고 사용한 피처로는 [가격, 상품의 수, 좋아요 수, 기간] 등이 있고, 이를 통해 성공 여부를 예측했습니다. 돌이켜보면 성공여부나 확률을 예측한 결과에 대한 해석이 아쉽습니다. 예를 들어 실패할 확률이 더 높게 나왔다면, 왜 그런지에 대한 해석을 할 수가 없습니다. 좋아요 수를 늘리면 되는건가? 하나씩 값을 바꿔가면서 성공확률을 높여나가도, 그게 실제로 성공확률을 높여주진 않습니다. 즉 문제 정의가 명확하지 않았다는 점인데요, 특히 판매자에게 정보를 제공한다면 단순히 예측모델은 의미가 없다고 보입니다. 선택의 문제에서 예측 모델은 도움이 안되는 경우가 많기 때문입니다. 대신 결과에 어떤 원인이 있는지 파악하는 인과추론적인 컨셉을 가져간다면 좋을 것 같습니다. 어떤 이유 때문에 성공 또는 실패했는지를 파악할 수 있다면, 판매자에게 더 좋은 선택을 위한 근거를 제시할 수 있을 것입니다. 실제로 ML 서비스를 운영해보면서 느끼는 점은 단순히 예측 모델 성능을 높인다고 좋은 서비스가 되는게 아니라는 점입니다. 사용자에게 맞는 문제를 명확하게 정의해야하는게 가장 중요하고, 이를 해결해나가는 과정에서 설득력이 있어야 합니다. 이런 부분들을 고려해서 프로젝트를 진행했다면, 교수님들에게도 더 설득력있는 프로젝트가 되었을 것 같습니다.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Portfolio/aiffel_segmentation",
    "title": "Segmentation Model",
    "date": "2025-09-20",
    "excerpt": "눈의 안쪽 부분을 찍는 안저사진에서 망막병증을 탐지하는 모델",
    "category": "Career",
    "tags": [
      "Segmentation"
    ],
    "content": "기간 : 2023.12 ~ 2024.03 (4개월) 역할 : 문제정의, 프로젝트 설계, 구현까지 모든 과정 --- 프로젝트 개요 아이펠이라는 AI 엔지니어 양성과정 부트캠프를 들으면서 진행했던 기업 연계 프로젝트입니다. 사람들의 건강에 기여하고 싶었고, 이를 위해 의료 도메인과 관련된 프로젝트를 진행했습니다. 그 중에서 \"안과\" 도메인인 회사와 기업 연계 프로젝트를 진행하게 되었습니다. 여러가지 주제 중 \"당뇨병성 망막병증 병변 탐지\"라는 문제를 정의해 프로젝트를 진행했습니다. 그 이유는 실명률이 가장 높은 질환이었고, 당뇨병 환자에게 자주 발생하는 합병증이기 때문이었습니다. 아버지가 당뇨가 있으시기 때문에, 가족의 건강에 직접적으로 기여할 수 있다는 생각에 관련 문제를 풀어보고 싶었습니다. <figure> <img src=\"/post/Portfolio/aiffel_presentaion_1.png\" alt=\"aiffel_presentaion_1\" /><width 80%/> <figcaption>그림1. 왜 당뇨병성 망막병증으로 주제를 정했는지</figcaption> </figure> <small>발표자료 링크</small> 눈의 안쪽 사진인 \"안저사진\"에 당뇨병성 망막병증 관련 병변들이 보이고, 이를 통해 초기 진단을 할 수 있습니다. 이를 돕기 위해 안저사진에서 병변들을 탐지하는, Segmentation 모델을 개발하는 프로젝트를 진행했습니다. 프로젝트를 진행하면서, 논문을 읽고 Segmenation 모델들 구현하고 고도화하는 과정을 통해 개발 능력을 키울 수 있었고, 데이터를 뜯어보면서 이미지 전처리를 고도화하는 과정에서 데이터 전처리에 대한 감을 키울 수 있었습니다. --- 문제 해결 과정 어떤 문제들이 있었고, 이를 해결하는 과정을 정리했습니다. --- 데이터셋은? > 데이터셋이 없다... 가장 어려웠던 부분입니다. 제공되는 데이터셋이 없었기 때문에 데이터 수집부터 해야했습니다. 하지만 데이터셋이 없으니 거의 1~2주정도를 데이터를 구하기 위한 구글링에 시간을 썼던 것 같습니다. 그러던 중 FGADR 이라는 데이터셋을 찾았고 대략 1500건정도의 데이터를 확보할 수 있었습니다. --- 모델링 > Unet vs DeepLab Segmentation은 크게 Unet 계열과 DeepLab 계열로 구분할 수 있었고, 각각 다양한 모델들이 존재했습니다. 여러 논문에서 전체 이미지 중 레이블(마스크)이 포함된 부분이 매우 작은 의료 도메인의 특성상, U-Net 계열 모델이 가장 높은 성능을 보였고, 실제로 가지고 있는 데이터셋안에서도 확인할 수 있었습니다. 초기 실험을 통해 Unet을 베이스라인으로 잡았고, 이를 고도화시켜나가는 방식으로 실험읋 진행했습니다. --- 이미지 전처리: CLAHE 적용 <figure> <img src=\"/post/Portfolio/aiffel_presentation_clahe.png\" alt=\"aiffel_presentaion_clahe\" /><width 80%/> <figcaption>그림2. 이미지 전처리 방법 적용 : CLAHE</figcaption> </figure> 가장 성능을 높일 수 있었던 부분입니다. 명암대비를 향상시킬 수 있는 방법으로, 병변의 특징을 더 명확하게 보이게 만들 수 있었습니다. 실제로 모델이 그림2의 사진처럼 작은 병변들을 더 잘 찾는 것을 확인할 수 있었습니다. 다만 과도한 명암대비 향상은 false positive를 증가시키므로 실험을 통해 적정 수준을 설정했습니다. --- multi-task learning <figure> <img src=\"/post/Portfolio/aiffel_presentation_mtl.png\" alt=\"aiffel_presentaion_mtl\" /><width 80%/> <figcaption>그림3. 인코더 풍부하게 하기 : MTL</figcaption> </figure> 다음으로는 Unet의 Encoder를 풍부하게 학습시키기 위해서 multi-task learning을 적용했습니다. 이는 관련 논문들에서 제시한 아이디어들을 참고해 구현한 것입니다. 논문들에서 구현 코드가 공개되어 있지 않아 실제로 구현해보면서 많은 시행착오를 겪어야 했습니다. 특히 Encoder의 어느 부분까지 pretraining 시켜야하며, segmentaion task를 학습할 때는 다른 task의 가중치를 얼마나 줘야하는지 등을 실험을 통해 확인할 수 있었습니다. MTL을 적용하고 많은 성능은 아니지만 향상된 것을 확인할 수 있었습니다.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-simpsons-paradox",
    "title": "Simson's Paradox란?",
    "date": "2025-09-18",
    "excerpt": "심슨의 역설에 대한 개념 정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1 : 구체적인 사례가 잘 나와있음 --- 통계는 결과를 해석하는데 중요한 도구이지만, 사용할 때 굉장히 주의해야한다. 잘못 사용되기 쉽고, 이는 잘못된 해석과 결과를 가져온다. 특히 어떤 연구를 할 때 무지성 t-test를 남발하는 경우가 많다. t-test의 의미가 뭔지, t-test의 유의성을 보장하기 위해 주의해야하는 건 뭔지, t-test의 결과가 진실을 보장하는 것은 아닌 이유 등을 이해하고 비판적인 시선이 필요하다. 심슨의 역설을 통해 이런 문제들을 이해할 수 있다. --- 심슨의 역설(Simpson's Paradox)이란? > 심슨의 역설은 통계학에서 매우 흥미로운 현상으로, 전체 데이터를 보았을 때와 부분 집단별로 나누어 보았을 때 완전히 반대되는 결과가 나타나는 현상 또는 > 특정 집단에서의 추세가 전체적으로 발견되는 추세와 다른 현상 정의 여러 그룹으로 나누어 분석했을 때는 각 그룹에서 A가 B보다 우세하지만, 전체 데이터를 합쳐서 보면 오히려 B가 A보다 우세해 보이는 상황이 발생합니다. 이는 데이터의 구조적 특성과 변수들 간의 상호작용 때문에 생기는 현상. 유명한 예시: 버클리 대학교 입학 성별 차별 사건 1973년 버클리 대학교의 대학원 입학에서 발생한 사건이 가장 유명한 예시. 전체 데이터 남성 지원자: 8,442명 중 3,738명 합격 (44.3%) 여성 지원자: 4,321명 중 1,511명 합격 (35.0%) 전체적으로는 남성의 합격률이 여성보다 9.3%포인트 높았다. 여자가 차별받고 있다. 학과별 데이터 (일부 예시) 학과 남성 지원자 남성 합격자 남성 합격률 여성 지원자 여성 합격자 여성 합격률 ------ ------------- ------------- ------------- ------------- ------------- ------------- A 825 512 62.1% 108 89 82.4% B 560 353 63.0% 25 17 68.0% C 325 120 36.9% 593 202 34.1% D 417 138 33.1% 375 131 34.9% 대부분의 학과에서 여성의 합격률이 더 높았다. > 남/여의 편견으로 인해 남성의 합격률이 높다는 해석은 잘못된 해석. 역설이 발생하는 원인 혼동 변수(Confounding Variable) 숨겨진 제3의 변수가 결과에 영향을 미치는 경우입니다. 버클리 사건에서는 \"학과\"가 혼동 변수였다. 데이터의 구조적 편향 여성이 주로 지원하는 학과들의 전체적인 합격률이 낮았음 남성이 주로 지원하는 학과들의 전체적인 합격률이 높았음 이로 인해 전체적으로는 남성의 합격률이 높아 보였음 변수들 간의 상호작용 독립변수들(성별과 학과)이 서로 영향을 미치는 경우입니다. 다른 예시들 의료 치료 효과 전체적으로는 치료 A가 치료 B보다 효과가 좋아 보임 하지만 환자 그룹별로 나누어 보면 치료 B가 더 효과적 이는 치료 A를 받은 그룹에 더 심각한 환자들이 많았기 때문 스포츠 성적 전체 시즌에서 선수 A가 선수 B보다 평균 성적이 좋음 하지만 월별로 나누어 보면 선수 B가 더 일관된 성적을 기록 이는 선수 A가 특정 월에만 뛰어난 성과를 냈기 때문 데이터 분석에서의 교훈 > 단순히 통계적인 수치만을 신뢰하는게 아닌, 문제에 대해 깊게 고민하고 데이터를 뜯어봐야 한다. 계층적 분석의 중요성 단순히 전체 통계만 보는 것이 아니라, 데이터를 적절한 그룹으로 나누어 분석해야함. 혼동 변수 고려 인과관계를 추론할 때는 반드시 혼동 변수를 고려해야 함. 데이터의 구조와 맥락 이해 데이터의 구조와 맥락을 충분히 이해한 후에 결론을 내려야 함. 심슨의 역설 탐지 방법 계층적 분석 데이터를 여러 그룹으로 나누어 각각 분석해보기 혼동 변수 식별 결과에 영향을 줄 수 있는 숨겨진 변수들을 찾아보기 시각화 그룹별 데이터를 시각화하여 패턴을 확인해보기 통계적 검정 적절한 통계적 검정을 통해 유의성을 확인해보기 결론 심슨의 역설은 데이터 분석에서 매우 중요한 교훈을 제공합니다. 단순한 전체 통계만으로는 진실을 파악하기 어려울 수 있으며, 데이터를 적절한 그룹으로 나누어 분석하는 것이 얼마나 중요한지 보여준다. 특히 인과관계를 추론할 때는 반드시 혼동 변수를 고려하고, 데이터의 구조와 맥락을 충분히 이해한 후에 결론을 내려야 한다. 이러한 역설을 이해하고 적절히 대응하는 것은 데이터 사이언티스트에게 필수적인 역량이라고 생각함.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/트리/what-is-tree",
    "title": "트리 개념정리",
    "date": "2025-09-18",
    "excerpt": "트리 개념정리",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/트리/notebook",
    "title": "트리 파이썬 구현",
    "date": "2025-09-18",
    "excerpt": "트리 탐색 파이썬 구현",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "트리"
    ],
    "content": "--- title: \"트리 파이썬 구현\" date: \"2025-09-18\" category: \"Codility Study\" tags: [\"코딩 테스트 합격자 되기\", \"Codility Study\", \"트리\"] excerpt: \"트리 탐색 파이썬 구현\" --- # 문제 26 : 트리 순회 > 전위 탐색, 중위 탐색, 후위 탐색 구현하는 문제 > 책에서 구현한게 훨~~씬 간단하고, 직관에 가깝다. 이해하기 # 입력되는 배열 트리 tree = [1, 2, 3, 4, 5, 6, 7] 트리 구조 1 / \\ 2 3 / \\ / \\ # 전위 순회 : 부모 -> 왼쪽 -> 오른쪽 visited = [False] len(tree) result = [] 재귀로 짜보기 방문과 탐색을 구분해야 함. def preorder(node): print('node :', node) if visited[node] == False: # 방문하지 않았던 곳이라면 탐색하기 visited[node] = True result.append(tree[node]) # 왼쪽 방문하기 if node 2 + 1 < len(tree) and visited[node 2 + 1] == False: preorder(node 2 + 1) # 왼쪽 방문했으면 오른쪽 방문하기 elif node 2 + 2 < len(tree) and visited[node 2 + 2] == False: preorder(node 2 + 2) # 왼쪽 오른쪽 모두 방문했으면 위로 가기 elif (node - 1) // 2 >= 0: preorder((node - 1)// 2) else: return preorder(0) print(result) print(visited) # 중위 순회 : 왼쪽 -> 부모 > 오른쪽 visited = [False] len(tree) result = [] def inorder(node): print('node :', node) # 왼쪽 방문하기 if node 2 + 1 < len(tree) and visited[node 2 + 1] == False: inorder(node 2 + 1) # 왼쪽 없거나 갔던 거면 본인 방문하기 elif node < len(tree) and visited[node] == False: print(\"본인 방문 : \", node) visited[node] = True result.append(tree[node]) inorder(node) # 왼쪽 방문, 본인 방문했으면, 오른쪽 방문하기 elif node 2 + 2 < len(tree) and visited[node 2 + 2] == False: inorder(node 2 + 2) # 왼쪽 오른쪽 모두 방문했으면 위로 가기 elif (node - 1) // 2 >= 0: inorder((node - 1)// 2) else: return inorder(0) print(result) print(visited) # 후위순회 : 왼쪽 -> 오른쪽 -> 본인 visited = [False] len(tree) result = [] def postorder(node): print(node) # 왼쪽 방문하기 if node 2 + 1 < len(tree) and visited[node 2 + 1] == False: postorder(node 2 + 1) # 오른쪽 방문하기 elif node 2 + 2 < len(tree) and visited[node 2 + 2] == False: postorder(node 2 + 2) # 본인 방문하기 elif node < len(tree) and visited[node] == False: print(\"본인 방문 : \", node) visited[node] = True result.append(tree[node]) postorder(node) # 왼쪽 오른쪽 모두 방문했으면 위로 가기 elif (node - 1) // 2 >= 0: postorder((node - 1)// 2) else: return postorder(0) print(result) print(visited) # 책 풀이 입력되는 배열 트리 nodes = [1, 2, 3, 4, 5, 6, 7] def preorder(nodes:list, current_node:int): if current_node < len(nodes): ret: str = str(nodes[current_node]) + \" \" # 부모 ret += preorder(nodes, current_node2 + 1) # 왼쪽 ret += preorder(nodes, current_node2 + 2) # 오른쪽 return ret else: return \"\" def inorder(nodes:list, cur_node:int): if cur_node < len(nodes): ret: str = inorder(nodes, cur_node2 + 1) # 왼쪽 ret += str(nodes[cur_node]) + \" \" # 부모 ret += inorder(nodes, cur_node2 + 2) # 오른쪽 return ret else: return \"\" def postorder(nodes:list, current_node:int): if current_node < len(nodes): ret :str = postorder(nodes, current_node 2 + 1) # 왼쪽 ret += postorder(nodes, current_node 2 + 2) # 오른쪽 ret += str(nodes[current_node]) + \" \" return ret else: return \"\" print(preorder(nodes, 0)) print(inorder(nodes, 0)) print(postorder(nodes, 0)) # 문제 27 : 이진 탐색 트리 구현 class Node: def __init__(self, value): self.value = value self.left = None # point self.right = None # point class BTS: def __init__(self): self.root_node = None def insert(self, tree: list): for value in tree: if self.root_node is None: self.root_node = Node(value) else: # 비교하면서 넣기 compare_node = self.root_node while compare_node is not None: if compare_node.value > value: if compare_node.left is None: compare_node.left = Node(value) break else: compare_node = compare_node.left else: if compare_node.right is None: compare_node.right = Node(value) break else: compare_node = compare_node.right def search(self, value): if self.root_node is None: raise compare_node = self.root_node while compare_node is not None: if compare_node.value == value: return True elif compare_node.value > value: compare_node = compare_node.left else: compare_node = compare_node.right return False tree = [5, 3, 8, 4, 2, 1, 7, 10] 5 3 8 2 4 7 10 1 bts = BTS() bts.insert(tree) print(\"root_node: \", bts.root_node.value) print(\"root_node.left: \", bts.root_node.left.value) print(\"root_node.left.left: \", bts.root_node.left.left.value) print(\"root_node.left.right: \", bts.root_node.left.right.value) print(\"root_node.right: \", bts.root_node.right.value) print(\"root_node.right.left: \", bts.root_node.right.left.value) print(\"root_node.right.right: \", bts.root_node.right.right.value) search_list = [1, 2, 5, 6] for search in search_list: print(search) print(bts.search(search)) # 문제 28 : 예상 대진표 > 문제를 이해하면 쉽게 풀 수 있는 문제 N, A, B, answer = [8, 4, 7, 3] def search(N:int, A:int, B:int): answer = 0 while True: A = A // 2 + A % 2 B = B // 2 + B % 2 answer += 1 if A == B: return answer # 문제 29 : 다단계 칫솔 판매 > 문제 제대로 이해하기 enroll, referral, seller, amount = [[\"john\", \"mary\", \"edward\", \"sam\", \"emily\", \"jaimie\", \"tod\", \"young\"], [\"-\", \"-\", \"mary\", \"edward\", \"mary\", \"mary\", \"jaimie\", \"edward\"], [\"young\", \"john\", \"tod\", \"emily\", \"mary\"], [12, 4, 2, 5, 10]] result = [360, 958, 108, 0, 450, 18, 180, 1080] def solution(enroll, referral, seller, amount): amount = [a 100 for a in amount] income_dict = dict(zip(seller, amount)) tree_dict = {} result_dict = {} for e, r in zip(enroll, referral): if r == '-': if 'center' not in tree_dict.keys(): tree_dict['center'] = [e] else: tree_dict['center'].append(e) else: if r not in tree_dict.keys(): tree_dict[r] = [e] else: tree_dict[r].append(e) def get_income(e): r_income = 0 for r in tree_dict.get(e, []): tmp = int(get_income(r) 0.1) if tmp > 1: r_income += tmp income = income_dict.get(e, 0) + r_income result_dict[e] = income return income get_income('center') for e in enroll: print(e, end=' ') print(result_dict[e] - int(result_dict[e] 0.1), end=' ') print(result) solution(enroll, referral, seller, amount) enroll, referral, seller, amount = [[\"john\", \"mary\", \"edward\", \"sam\", \"emily\", \"jaimie\", \"tod\", \"young\"], [\"-\", \"-\", \"mary\", \"edward\", \"mary\", \"mary\", \"jaimie\", \"edward\"], [\"sam\", \"emily\", \"jaimie\", \"edward\"], [2, 3, 5, 4]] result = [0, 110, 378, 180, 270, 450, 0, ] print(result) solution(enroll, referral, seller, amount) # 문제 30 : 미로 탈출 > 최소거리 -> BFS, DFS, 다익스트라 등을 떠올려야 함 > 리스트 객체 생성할 때 을 사용하면 같은 객체로 참조하도록 되어버림 maps = [\"SOOOL\", \"XXXXO\", \"OOOOO\", \"OXXXX\", \"OOOOE\"] result = 16 def find_str_loc(target:str): for i in range(len(maps)): for j in range(len(maps[0])): if maps[i][j] == target: return (i, j) return False S_loc = find_str_loc('S') L_loc = find_str_loc('L') min_x, max_x = -1, len(maps[0]) min_y, max_y = -1, len(maps) def get_distance(curr_loc: tuple, target: str, visited: list, distance:int): print(curr_loc) curr_x, curr_y = curr_loc if maps[curr_x][curr_y] == target: distances.append(distance) return visited[curr_x][curr_y] = True dx = [-1, 0, 1, 0] dy = [0, 1, 0, -1] for dx, dy in zip(dx, dy): next_x = curr_x + dx next_y = curr_y + dy cond_x = min_x < next_x < max_x cond_y = min_y < next_y < max_y if cond_x and cond_y and maps[next_x][next_y] != 'X' and visited[next_x][next_y] == False: get_distance((next_x, next_y), target, visited, distance + 1) distances = [] visited = [[False for _ in range(max_x)] for _ in range(max_y)] S_to_L = get_distance(S_loc, \"L\", visited.copy(), 0) if len(distances) == 0: print(-1) else: print(min(distances)) distances = [] visited = [[False for _ in range(max_x)] for _ in range(max_y)] L_to_X = get_distance(L_loc, \"E\", visited, 0) if len(distances) == 0: print(-1) else: print(min(distances)) # 원래 코드 visited = [[False] max_x] max_y 연산자는 리스트를 복사할 때 같은 객체를 참조하게 만듬 visited[0][0] = True # 첫 번째 셀 방문 처리 결과: 모든 행의 첫 번째 열이 True가 됨 visited = [[True, False, False], [True, False, False], [True, False, False]] # 올바른 visited 배열 생성 visited = [[False for _ in range(max_x)] for _ in range(max_y)] 또는 더 간단하게 visited = [[False] * max_x for _ in range(max_y)] # 문제 31 : 양과 늑대",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/계획과실행/나너한테전화하려고했는데일이생겼어",
    "title": "계획이 바뀌었을 때 쓰는 표현들",
    "date": "2025-09-17",
    "excerpt": "나 너한테 전화하려고 했는데, 일이 좀 생겼어",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "I was gonna (going to) 말할 때 쓰는 표현들. I was gonna go to the movies. I was gonna cook dinner. I was gonna cook dinner, but I just ordered takeout instead. I was goona call my grandma, but someting came up. it slipped my mind : 까먹었어 대화 시뮬레이션, 예시 상황 1: 친구와의 약속 A: Hey, are you coming to the party tonight? B: Oh, I was gonna come, but something came up at work. I have to stay late. A: That's too bad. Maybe next time! B: Yeah, I was really looking forward to it too. 상황 2: 가족과의 통화 A: Mom, I was gonna call you yesterday, but it slipped my mind. B: That's okay, honey. What were you going to tell me? A: I was gonna ask about the recipe for your apple pie. B: Oh, I can tell you right now! 상황 3: 운동 계획 변경 A: I was gonna go to the gym this morning, but I overslept. B: That happens to everyone. Are you going to go later? A: I was gonna, but now I have a meeting. Maybe tomorrow. 상황 4: 쇼핑 계획 A: I was gonna buy groceries today, but I just ordered takeout instead. B: What made you change your mind? A: I was too tired to cook, and I was gonna make something simple anyway.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-09-17-리맴버-ml-engineer",
    "title": "리맴버 - ML Engineer",
    "date": "2025-09-17",
    "excerpt": "채용공고 정리 -> 지원전략 정리",
    "category": "Career",
    "tags": [
      "ML Engineer",
      "채용공고"
    ],
    "content": "채용공고 원티드 링크 리멤버 페이지 링크 포지션 상세 [프로페셔널 네트워크 서비스로, 일하는 세상을 혁신합니다] 리멤버는 기회를 연결합니다. 일하는 사람과 기회를 연결하여 개인의 성공, 회사의 성장, 세상의 발전을 만들 수 있다고 믿습니다. [리멤버의 폭발적 성장은 지금부터입니다] 리멤버가 쌓아온 플랫폼의 기반 위에 Data-tech와 AI 역량이 더해진 솔루션을 제공함으로써 비즈니스 세계의 변화들을 주도하고 있습니다. • 최고의 인재를 연결하는 HR Network Service • 산업 전문가들을 연결하는 Expert Network Service • 잠재 고객을 연결하는 Business Network Service 월간 흑자 전환을 지나 이제 본격적인 이익 창출의 phase로 접어든 리멤버에서 리멤버의 자랑인 훌륭한 동료와 함께, 리멤버가 만들어 나갈 세상의 변화와 폭발적 성장 여정에 함께하세요! ---------------------------------------------------------------------------------------- [우리 팀을 소개할게요!] AI Lab은 리멤버의 명함·채용·광고·커뮤니티 등 서비스 전반에서 ML·LLM·Agent로 실제 비즈니스 문제를 해결합니다. 4억 장 이상의 명함 데이터와 대규모 사용자 행동 데이터, H100/A100 등의 GPU 인프라를 바탕으로 빠르게 실험하고 제품화합니다. 최신 기술을 무조건 적용하지 않고, 문제에 가장 적합한 솔루션을 선택합니다. 수평적 피드백 문화와 원팀 협업으로 빠른 의사결정과 책임 있는 실행을 지향합니다. [Team Talk (동료의 한 마디)] 👨🏻: AI Lab은 우리 서비스를 사용하는 유저가 만족할 수 있는 모델을 만들자는 목표를 위해 One Team Spirit 으로 동료들 모두 서로를 도와 부족함을 보완하며 함께 성장하고 있어요. 👩🏻: 틀에 갇히지 않은, 혁신적인 사고를 가지고 뛰어난 역량을 발휘할 수 있는 분과 리멤버의 미래를 그려나가고 싶어요. AI Lab에 대해 더 알고싶다면 아래 링크를 참고해주세요 :) AI Lab 인터뷰: 리멤버 AI Lab “LLM·캐싱 기술로 직장인 네트워크 혁신 선도” 리멤버 tech 블로그: remember-tech ✋ 담당하시게 될 업무 LLM, AI Agent를 활용하여 비즈니스 문제를 해결하고, 업무 효율화를 진행합니다. 최신 LLM 프레임워크, Retrieval-Augmented Generation(RAG) 등 관련 기술을 연구하고 적용하여 서비스 품질을 고도화합니다. AI 기반의 추천/검색 알고리즘을 연구-개발-배포까지 수행합니다. ML 모델링에 필요한 데이터를 효과적이고 효율적으로 관리합니다. 🤗 함께하고 싶은 분 AI 연구 개발 경험 혹은 유관 경험이 2년 이상이신 분 Senior의 경우, 5년 이상의 경험과 LLM/AI Agent 서비스 적용 경험 최소 1년 이상 보유하신 분 Python(Pytorch)을 활용한 딥러닝·머신러닝 모델 구현 및 실험 경험이 풍부하신 분 데이터 가공, 분석을 통해 유의미한 insight 도출 경험이 있는 분 최신 딥러닝 논문을 빠르게 이해하고 정확히 구현할 수 있는 분 👍 우대사항 LLM, Agent 관련 프레임워크를 활용하여 문제를 해결해본 경험이 있으신 분 관련 전공(컴퓨터공학, 데이터사이언스, 통계학, 산업공학 등)의 석사 이상의 학위를 보유하신 분 AI를 실제 서비스 or 제품에 적용하여 괄목할만한 비즈니스적 성과를 만든 경험이 있으신 분 Github, Docker 등 개발 관련 기술 stack을 보유하신 분 ⏱ 채용 프로세스 서류 전형 > 과제 전형 > 1차 면접 > 2차 면접 > 입사 과제 제출해주심에 감사의 마음을 담아 소정의 과제비를 드립니다. 📂 제출 서류 자유양식의 이력서, 지원동기를 포함한 지원서 지원서에는 'Why Remember' 즉, 왜 수많은 회사 중 이 곳에 지원했는지를 꼭 포함해주세요. ※ 채용서류 반환에 관한 고지(채용절차의 공정화에 관한 법률 제11조) 최종합격자를 제외한 지원자는 기 제출한 채용 서류를 반환받을 수 있으며, 최종 발표일로부터 14일 이내에 기제출한 채용서류 반환을 청구할 수 있습니다. (당사의 요구 없이 자발적으로 제출한 경우 제외) 이메일(recruit@remember.co.kr)로 요청주시면 됩니다. 서면 제출: 반환 받으실 주소와 함께 요청주시면 14일 이내 등기 우편 발송(등기우편 요금 수신자 부담) 온라인 제출: 즉시 파기 --- 지원동기",
    "public": false
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch6_모델개발과오프라인평가",
    "title": "피처 엔지니어링에 대해서",
    "date": "2025-09-17",
    "excerpt": "feature 를 다룰 때 생각하면 좋은 내용들",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Data Science/Computer Science/what-is-GIL",
    "title": "python의 GIL에 대해서",
    "date": "2025-09-17",
    "excerpt": "Global Interpreter Lock에 대해 정리",
    "category": "Computer Science",
    "tags": [
      "python",
      "GIL"
    ],
    "content": "참고자료 1 2 3 --- 파이썬에서 비동기, 멀티쓰레드 등을 구현하다보면 GIL이라는 키워드를 마주하게 된다. GIL이란 뭘까? GIL (1) ? > GIL은 파이썬에만 존재하는 독특한 개념으로 파이썬에서 멀티스레딩을 할 때 다수의 스레드가 동시에 파이썬 바이트 코드를 실행하지 못하게 막는 일종의 뮤텍스(Mutex)이다. Global Interpreter Lock > GIL은 문자 그대로 인터프리터에 대한 Lock이다. <figure> <img src=\"/post/CS/GIL_그림1.png\" alt=\"GIL\" /><width=\"80%\" /> <figcaption>GIL 예시 (1)</figcaption> </figure> Lock은 인터프리터가 여러 스레드를 병렬적으로 실행하지 않도록 막는다. 따라서 파이썬에서는 멀티 쓰레딩은 병렬적(Parallel)이 아닌 동시적(Concurrent)으로 실행됨. 인터프리터와 GIL의 관계 인터프리터는 프로그래밍 언어의 소스 코드를 한 줄씩 읽어서 즉시 실행하는 프로그램. 파이썬의 경우, 소스 코드를 바이트코드로 변환한 후 Python Virtual Machine(PVM)에서 실행하는 방식으로 작동. 여기서 문제가 발생. 여러 스레드가 동시에 파이썬 바이트코드를 실행하려고 하면, 메모리 관리나 객체 참조 카운팅 같은 중요한 작업들이 동시에 일어나면서 데이터 무결성 문제가 생길 수 있다. 예를 들어, 한 스레드가 객체를 삭제하려고 하는 순간 다른 스레드가 그 객체를 참조하려고 하면 예상치 못한 오류가 발생할 수 있다. 이런 문제를 방지하기 위해 파이썬은 GIL을 도입. GIL은 마치 도서관에서 한 번에 한 사람만 책을 빌릴 수 있도록 하는 것과 같다. 여러 사람(스레드)이 도서관(파이썬 인터프리터)에 와도, 한 번에 한 사람만 책(파이썬 바이트코드)을 읽을 수 있도록 제한하는 것. 이렇게 하면 데이터 무결성은 보장되지만, 여러 스레드가 있어도 실제로는 한 번에 하나의 스레드만 파이썬 코드를 실행하게 됨. GIL이 필요한 이유 <figure> <img src=\"/post/CS/ThreadMemory.png\" alt=\"ThreadMemory\" /><width=\"80%\" /> <figcaption>ThreadMemory</figcaption> </figure> > 파이썬이 GIL를 도입한 이유는 멀티 스레드 환경에서 발생할 수 있는 Race Condition(경쟁 상태)를 방지하기 위함이다. > 그래서 GIL은 경쟁 상태를 방지하기 위해 도입된 Lock이라고 생각하면 된다. > (참조 1) 메모리 저장 위치: 파이썬에서 생성되는 모든 객체들은 Heap 영역에 저장된다. Heap은 프로그램 실행 중에 동적으로 할당되는 메모리 영역으로, 여러 스레드가 공유할 수 있는 공간이다. 반면 Stack 영역은 각 스레드마다 독립적으로 할당되는 메모리 공간이다. 하지만 파이썬의 객체들은 모두 Heap에 저장되기 때문에, 여러 스레드가 같은 객체에 접근할 수 있어 Race Condition이 발생할 수 있다. ( 파이썬의 메모리 관리 방법인 RC의 한계) 파이썬 메모리 관리 방법 : Reference Counting (RC) 파이썬은 메모리 관리를 위해 Reference Counting 방식을 사용. 이는 각 객체가 몇 개의 변수나 다른 객체에 의해 참조되고 있는지를 카운트하는 방식. <figure> <img src=\"/post/CS/ReferenceCounting.png\" alt=\"ReferenceCounting\" /><width=\"80%\" /> <figcaption>Reference Counting (RC)</figcaption> </figure> Reference Counting의 작동 원리: 객체가 생성되면 참조 카운트가 1이 됨 다른 변수가 그 객체를 참조하면 카운트가 증가 참조가 해제되면 카운트가 감소 참조 카운트가 0이 되면 자동으로 메모리에서 해제 (garbage collection) GIL과 Reference Counting의 관계: 여러 스레드가 동시에 같은 객체의 참조 카운트를 수정하려고 하면 Race Condition이 발생할 수 있음. 예를 들어, 스레드 A가 참조 카운트를 읽고 증가시키려는 순간, 스레드 B가 같은 카운트를 읽고 감소시키면 데이터 불일치가 생긴다. GIL은 이런 문제를 방지하기 위해 한 번에 하나의 스레드만 파이썬 객체의 참조 카운트를 수정할 수 있도록 보장한다. 이렇게 하면 Reference Counting의 안전성이 보장되지만, 동시에 멀티스레딩의 병렬 실행도 제한되는 것이다. > 문제를 방지하려면, 자원 공유가 불가능 하도록 Mutex(뮤텍스)와 같은 Lock을 도입해야 한다. > 하지만 파이썬의 모든 것은 객체(Heap 영역이라 공유 됨)이기에, 모두 Lock 하여 관리하는 것은 굉장히 비효율적이다. > 그래서 파이썬은 객체 단위 Lock 대신, 인터프리터 단위 Lock을 도입했고 그것이 바로 GIL이다. > (참조 1) GIL이 없는 파이썬의 미래 (참조 2, 3 정리) Python 3.14부터 도입된 free-threaded 빌드는 GIL을 제거하여 진정한 병렬 실행을 가능하게 했다. 이는 파이썬의 멀티스레딩 성능을 획기적으로 개선하는 중요한 변화다. GIL과 asyncio의 한계 기존 GIL 환경에서 asyncio는 다음과 같은 제약이 있었다: 단일 이벤트 루프 제한: 각 스레드당 하나의 이벤트 루프만 실행 가능했고, GIL 때문에 여러 이벤트 루프를 병렬로 실행할 수 없었다. CPU 집약적 작업의 병목: CPU 집약적 작업을 별도 스레드로 분리해도 여전히 GIL을 획득하기 위해 경쟁해야 했다. 확장성 제한: 멀티코어 환경에서 asyncio 애플리케이션의 성능 확장이 제한되었다. free-threaded 빌드의 핵심 개선사항 스레드별 상태 관리 기존 방식: 전역 데이터 구조에 의존하여 GIL이 스레드 안전성을 보장 새로운 방식: 스레드별 독립적인 상태 관리로 잠금 없는 접근 가능 작업 관리 최적화 스레드별 원형 이중 연결 리스트: 전역 WeakSet 대신 각 스레드가 독립적인 작업 리스트를 유지 약한 참조(weak reference) 제거로 성능 향상 잠금 경쟁 없이 작업 추가/제거 가능 참조 카운팅 오버헤드 완전 제거 현재 작업 상태 개선 스레드 상태에 저장: \"현재 작업\"을 스레드 상태 구조체에 저장하여 빠른 접근 가능 딕셔너리 조회 없이 직접 접근 작업 전환 시 성능 향상 성능 벤치마크 결과 TCP 성능 테스트 <figure> <img src=\"/post/CS/benchmark_1.png\" alt=\"benchmark_1\" /><width=\"80%\" /> <figcaption>tcp benchmark</figcaption> </figure> 단일 작업자: 276 MB/s 6개 작업자: GIL 빌드 532 MB/s → free-threaded 빌드 1455 MB/s (약 2.7배 향상) 12개 작업자: GIL 빌드 698 MB/s → free-threaded 빌드 1924 MB/s (약 2.8배 향상) 웹 스크래핑 성능 (aiohttp 사용) <figure> <img src=\"/post/CS/benchmark_2.png\" alt=\"benchmark_2\" /><width=\"80%\" /> <figcaption>webscraping benchmark_2</figcaption> </figure> 단일 작업자: 12 stories/sec 12개 작업자: GIL 빌드 35 stories/sec → free-threaded 빌드 80 stories/sec (약 2.3배 향상) 단일 스레드 성능도 개선 pyperformance 벤치마크에서 10-20% 성능 향상 메모리 사용량 감소 미래 전망 이러한 변화로 인해 다음과 같은 새로운 가능성이 열렸다: 병렬 이벤트 루프: 여러 이벤트 루프를 동시에 실행하여 멀티코어 활용 극대화 고성능 웹 서버: FastAPI, aiohttp 등이 더욱 높은 성능으로 확장 가능 데이터 처리 파이프라인: CPU 집약적 작업의 진정한 병렬 처리 분산 작업 큐: 멀티스레드 환경에서 효율적인 작업 분산 free-threaded 파이썬은 asyncio의 새로운 시대를 열어주는 혁신적인 변화다. free-threaded가 가능해진 원리 GIL을 제거할 수 있게 된 핵심은 Reference Counting의 문제를 해결한 것이다. 기존에는 GIL이 Reference Counting의 Race Condition을 방지하는 역할을 했지만, 이제는 다른 방식으로 해결했다. 기존 GIL의 역할 GIL이 존재했던 이유는 파이썬의 Reference Counting 메모리 관리 방식 때문이었다: ``python 예시: 참조 카운트 증가/감소 obj = SomeObject() # ref_count = 1 other = obj # ref_count = 2 (GIL 보호 필요) del obj # ref_count = 1 (GIL 보호 필요) del other # ref_count = 0 → 메모리 해제 (GIL 보호 필요) ` 여러 스레드가 동시에 같은 객체의 참조 카운트를 수정하면 Race Condition이 발생할 수 있었다. free-threaded에서의 해결책 Per-Object Locking (객체별 잠금) 각 객체마다 개별적인 잠금을 사용하는 방식: `python 각 객체가 자신만의 뮤텍스를 가짐 class PyObject: def __init__(self): self.ref_count = 1 self.mutex = threading.Lock() # 객체별 잠금 def incref(self): with self.mutex: self.ref_count += 1 def decref(self): with self.mutex: self.ref_count -= 1 if self.ref_count == 0: self.dealloc() ` Lock-Free Reference Counting 원자적 연산(atomic operations)을 사용하여 잠금 없이 참조 카운트를 관리: `python import threading class PyObject: def __init__(self): self.ref_count = threading.AtomicInteger(1) # 원자적 정수 def incref(self): self.ref_count.fetch_add(1) # 원자적 증가 def decref(self): if self.ref_count.fetch_sub(1) == 1: # 원자적 감소 self.dealloc() ` Biased Reference Counting 대부분의 참조가 단일 스레드에서 발생한다는 특성을 활용: 로컬 참조: 스레드별 로컬 카운터 사용 글로벌 참조: 여러 스레드에서 공유되는 경우에만 원자적 연산 사용 기술적 구현 세부사항 메모리 순서 보장 `c // C 레벨에서의 원자적 연산 Py_ssize_t Py_INCREF(PyObject op) { Py_ssize_t new_refcount = atomic_fetch_add(&op->ob_refcnt, 1) + 1; return new_refcount; } ` 스레드 로컬 저장소 활용 `c // 스레드별 상태 관리 typedef struct { PyObject current_task; PyListObject *task_list; } _PyThreadState; `` 성능 최적화 Lock Contention 감소 객체별 잠금으로 경쟁 범위 축소 핫스팟(hotspot) 객체에 대한 특별 처리 메모리 오버헤드 최소화 잠금 구조체의 메모리 사용량 최적화 캐시 친화적인 데이터 구조 사용 단일 스레드 성능 유지 단일 스레드 환경에서는 기존과 동일한 성능 멀티 스레드에서만 추가 오버헤드 발생 왜 이제 가능해졌나? 하드웨어 발전: 원자적 연산의 성능 향상 컴파일러 최적화: 더 효율적인 원자적 연산 코드 생성 메모리 모델 개선: 현대 CPU의 메모리 일관성 보장 연구 성숙도: Lock-free 프로그래밍 기법의 발전 이러한 기술적 발전으로 인해 GIL 없이도 안전하고 효율적인 멀티스레딩이 가능해졌다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/계획과실행/경험관련된표현들",
    "title": "과거 경험 묻고 답하기",
    "date": "2025-09-16",
    "excerpt": "경험에 대해 이야기 나눌 때 사용하는 표현",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "How was your weekend? I spent most of the weekend sleeping in. 경험을 물을 때 쓸 수 있는 표현들. 잘 익혀놓으면 스몰톡하기 좋겠다. 대화 연습 상황들 상황 1: 직장 동료와의 점심시간 대화 A: How was your weekend? B: It was pretty relaxing. I spent most of the weekend sleeping in and catching up on some Netflix shows. How about you? A: I had a great time! I went hiking with some friends on Saturday. The weather was perfect for it. B: That sounds amazing! I've been meaning to get back into hiking. Where did you go? A: We went to Bukhansan National Park. The views were incredible from the top. You should definitely try it sometime! 상황 2: 새로운 사람과의 첫 만남 A: So, tell me about yourself. What do you like to do in your free time? B: Well, I'm really into photography. I spend a lot of time exploring different neighborhoods in Seoul, trying to capture the city's unique atmosphere. A: That's so cool! I've always wanted to learn photography. Do you have any tips for beginners? B: Absolutely! I'd recommend starting with your smartphone camera and just practicing composition. That's how I got started, actually. 상황 3: 친구와의 저녁 식사 A: I heard you went to Japan last month. How was the trip? B: It was absolutely incredible! I had the most amazing time exploring Tokyo and Kyoto. The food was out of this world. A: I'm so jealous! What was your favorite part? B: Definitely the traditional tea ceremony in Kyoto. It was such a peaceful and meditative experience. I felt like I was transported to another time. 상황 4: 학원에서의 수업 후 대화 A: How did you find today's lesson? B: It was really challenging but in a good way. I struggled with the pronunciation exercises at first, but I think I'm getting the hang of it. A: I know what you mean. I had the same experience last week. Practice makes perfect, right? B: Exactly! I've been practicing at home every day, and I can already feel the improvement. 상황 5: 취미 동호회에서의 대화 A: How long have you been playing guitar? B: I've been playing for about three years now. I started learning during the pandemic when I had more free time at home. A: That's impressive! Do you play in a band or just for yourself? B: Mostly for myself, but I've been thinking about joining a local music group. I think it would be a great way to meet people with similar interests. 유용한 표현들 경험 묻기 How was your weekend/holiday/vacation? What did you do last night/weekend? How did you spend your time off? Tell me about your trip/experience. What was it like? 경험 답하기 It was amazing/incredible/fantastic! I had a great/wonderful/terrible time. I spent most of the time + -ing I had the most amazing experience... It was challenging but rewarding. I learned a lot from the experience. 감정 표현 I was so excited/nervous/surprised! I felt really relaxed/stressed/confident. It made me feel like... I couldn't believe how... I was blown away by... 시간 표현 I've been doing this for + 기간 I started + -ing when... I've been meaning to + 동사원형 I've always wanted to + 동사원형 I've been thinking about + -ing",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch5_피처엔지니어링",
    "title": "피처 엔지니어링에 대해서",
    "date": "2025-09-16",
    "excerpt": "feature 를 다룰 때 생각하면 좋은 내용들",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> 오늘날 ML 시스템의 성공은 여전히 피처에 달려 있습니다. > 따라서 프로덕션 환경에서 ML을 사용하려면 피처 엔지니어링에 시간과 노력을 투자해야 하죠 ML서비스를 개발하고 운영하면서 ML 시스템을 더 잘 구축하기 위해 공부하고 반영하고 있습니다. 그 중에서 feature store를 구축할 때 생각하면 좋은 내용들을 정리합니다. 데이터는 무작위로 분할하는 대신 시간별로 학습, 검증, 테스트 분할로 분할합니다. train/valid/test로 데이터셋을 나눠 모델의 일반화 성능을 확인합니다. 이때 가장 중요한 부분은 모델이 실제 추론 상황에서도 좋은 성능을 보일 것인지에 대한 것입니다. 이러기 위해서는 실제 추론 상황과 같은 test set을 구축해서, 일반화 성능을 확인헤야합니다. 이때 가장 좋은 방법이 시간 순대로 나누는 것입니다. 일종의 시뮬레이션처럼, 미래 (추론 상황)의 데이터를 모른다고 생각하고 모델을 만들고 검증하는 것입니다. 이를 통해 추론상황에서 발생할 수 있는 시간에 따른 data drift와 같은 문제들을 확인해볼 수도 있습니다. data leakage를 조심하자! 오버 셈플링은 분할 후에 수행합니다 데이터 누수를 방지하기 위해 스케일링과 정규화는 데이터 분할 후에 수행합니다. 피처 스케일링하고 결측값을 처리할 때는 전체 데이터 대신 훈련 분할의 통계치만 사용합니다. 특정 레이블의 개수가 부족할 때 데이터의 개수를 늘리는 오버샘플링 방법을 사용하곤 합니다. 이때 가장 주의해야하는 부분이 역시나 \"data leakage\"입니다. 일반화 성능을 정확히 확인하지 못하게 만드는 문제입니다. 따라서 오버샘플링은 학습 상황에서 진행해야하며, 성능은 실제 추론상황이라고 가정한 test set에서 온전히 수행해야 합니다. 데이터 생성, 수집, 처리 방식을 이해합니다. 가능하면 도메인 전문가를 참여시키세요. 데이터 계보 (lineage)를 추적합니다. 전체 과정 중 데이터를 이해하는게 가장 중요하다고 생각합니다. 마치 형사가 범인을 쫒듯이, 하나씩 단서를 모아가며 데이터를 온전히 이해하려는 노력이 필요합니다. 값이 튀는게 있다면 어디서 잘못된 건지, 실제로 유의미한 범위의 데이터는 어떤건지, 어떻게 생성되고 수집되는 건지 등을 알아야 데이터를 올바르게 처리할 수 있습니다. 적절한, 좋은 피처만을 사용하기! 모델에 대한 피처 중요도를 이해합니다. 잘 일반화되는 피처를 사용합니다. : 이 부분을 가장 깊게 고민해보면 좋을 것 같습니다. 모델에서 더 이상 유용하지 않은 피처를 제거합니다. 데이터로부터 피처를 만들고 이를 통해 정답을 예측하는 모델을 만들게 됩니다. 이때 피처를 어떻게 구성해야할지 고민하게 됩니다. 기본적인 피처를 만들다가 하나씩 추가하게 됩니다. 성능을 높이기 위해 이런저런 피처를 추가해보면서 고도화해나갑니다. 이때 고민해보면 좋은 점이 있습니다. 피처가 많다고 좋은게 아니다 (당연하게도...) 피처가 많게 되면 latency가 증가하게 됩니다. 또한 일반화되지 않는 피처라면 일반화 성능을 해치게 됩니다. 예를 들면 \"요일\"에 대한 피처가 있을 때 학습셋에서는 \"월~금\"만 있고, 테스트셋에는 \"토일\"만 있다면 이 피처는 일반화 성능을 해치게 될 것입니다. 불필요한 피처들은 제거하자 SHAP이나 feature importance score를 통해 중요한 피처를 유추해볼 수 있습니다. 중요도가 낮은 피처는 제거하는 것이 latency를 줄이고 일반화 성능에 도움이 될 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/tuning/what-is-grid-search",
    "title": "그리드 서치란(Grid Search)?",
    "date": "2025-09-15",
    "excerpt": "하이퍼 파라미터를 찾는 가장 기본적인 컨셉",
    "category": "Engineering",
    "tags": [
      "Tuning",
      "Grid Search"
    ],
    "content": "Grid Search란? <figure> <img src=\"/post/DataScience/GridSearch.png\" alt=\"Grid Search\" /> <figcaption>그림1. Grid Search</figcaption> </figure> 하이퍼파라미터를 찾는 가장 기본적인 방법. Grid Search는 말 그대로 격자(grid) 형태로 하이퍼파라미터의 조합을 체계적으로 탐색하는 방법이다. 예를 들어 learning rate를 [0.001, 0.01, 0.1]로, batch size를 [32, 64, 128]로 설정했다면, 총 3 × 3 = 9가지 조합을 모두 시도해보는 것이다. 장점은 모든 조합을 체계적으로 탐색하기 때문에 최적값을 놓칠 가능성이 낮다는 것이다. 하지만 하이퍼파라미터가 많아질수록 조합의 수가 기하급수적으로 증가해서 계산 비용이 매우 커진다. 실제로는 random search나 bayesian optimization 같은 더 효율적인 방법들을 많이 사용하지만, Grid Search는 여전히 하이퍼파라미터가 적고 탐색 공간이 작을 때 유용한 기본적인 방법이다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/tuning/how-to-tune-model",
    "title": "내가 모델을 튜닝해나가는 방식",
    "date": "2025-09-15",
    "excerpt": "모델을 튜닝하는 방법에 대한 생각 정리",
    "category": "Engineering",
    "tags": [
      "Tuning"
    ],
    "content": "모델 튜닝을 위한 체계적인 접근법 tabular data, image 등의 데이터에 대한 모델들을 만들 때 내가 가져가는 사고흐름을 정리해보자. 나의 사고방식에 가장 영향을 많이 준 책은 흔히 \"케창딥\"이라고 부르는 책이다. 기본 원칙: 과적합 우선, 그 다음 일반화 모델 튜닝에서 내가 따르는 핵심 원칙은 간단하다. 첫째, 과적합을 먼저 시킨다. 둘째, 규제를 가하면서 일반화 성능을 확보한다. 과적합을 먼저 시키는 이유는 학습할 수 있는 패턴 자체가 존재하는지 확인하기 위해서다. 모델이 학습 데이터조차 제대로 못 배운다면, 이건 모델의 capacity가 부족하거나 데이터에 패턴이 없을 가능성이 높다. 그래서 먼저 모델이 학습 데이터를 완벽하게 학습할 수 있도록 과적합 상태를 만든 다음, 그 다음에 regularization, dropout, subsampling 같은 규제 기법을 써서 학습 셋과 테스트 셋 사이의 차이를 줄여나간다. 결국 가장 기본적인 개념인 bias-variance tradeoff 사이에서 중간지점을 찾아나가는 과정이다. ML이나 DL의 목표는 일반화된 패턴을 찾아서 일반화된 성능을 확보하는 것이다. 학습하지 않은 데이터셋에서의 성능을 일반화된 성능이라고 표현할 수 있겠다. 데이터 중심 접근법의 중요성 프로젝트하면서 느끼는 건 새로운 모델 구조를 제시하는 정도의 수준이 아닌 이상, 대부분 데이터가 95% 이상 역할을 한다는 것이다. 모델의 성능이 일정 이상 값보다 안 줄어들 때, 일반화된 패턴이 없는, 데이터가 가지고 있는 노이즈 자체가 굉장히 크다는 의심이 들 때가 있다. 이런저런 것들을 시도할 때 일정 이상 값보다 안 줄어들면 데이터를 뜯어보면서 노이즈가 발생하는 부분이 있는지, 패턴 자체가 없는지를 뜯어봐야 한다. 이번에 렌즈 추천 프로젝트도 데이터를 뜯어보면서 고쳤던 부분이 많았다. 측정 기기의 오류로 임상적 기준을 벗어나는 값들, 전처리 과정에서의 이슈로 사람이 수작업으로 입력하는 값들, 더러운 데이터 처리할 때의 문제들, 의학적 지식에 기반해서 새로운 피처를 찾아낸 적도 있다(나이 - 렌즈 두께). 렌즈 자체가 가지고 있는 특성, 두께 등도 고려했다. 모델 튜닝, 데이터 전처리 등을 거쳐서 기존에 사용하던 모델보다 약 20% 성능향상을 보였다. (MAE 기준) Data Leakage 방지의 중요성 튜닝할 때 가장 주의해야 할 점이 data leakage다. 모델이 테스트 데이터셋을 보진 않지만, 결국 테스트 셋을 통해 하이퍼파라미터를 정하게 되기 때문에 과적합이 될 수 있다. 이를 방지하려면 반드시 validation set을 별도로 분리해서 하이퍼파라미터 튜닝에만 사용하고, 테스트 셋은 최종 평가할 때만 써야 한다. cross-validation을 적절히 활용해서 모델의 일반화 성능을 더 정확하게 평가하고, early stopping 같은 기법으로 과적합을 방지하는 것도 중요하다. 이런 체계적인 접근을 통해 데이터의 본질적인 패턴을 발견하고, 실제 운영 환경에서도 안정적인 성능을 보이는 모델을 개발할 수 있다.",
    "public": true
  },
  {
    "id": "Self_Development/Book/Factfulness/why-I-read-it",
    "title": "책을 읽게 된 이유",
    "date": "2025-09-14",
    "excerpt": "가짜뉴스가 판을 치는 세상에서 올바른 세계관을 성립하기 위해서",
    "category": "Book",
    "tags": [
      "Factfulness"
    ],
    "content": "성별간, 나이간에 증오와 분노가 가득찬 세계를 살아가는 것 같다. 양극단에서 대부분의 컨텐츠를 생산하고 의견을 낸다. 그렇게 생산된 컨텐츠를 소비하는 사람들은 그렇게 생각해 서로를 증오하고 이해하지 못하게 되는 것 같다. 특히 정치적인 이야기에서는 사실보다는 감정적으로 받아들이는 것 같다. 나또한 이로부터 자유롭지 않다. 친구랑 정치적인 이야기를 나누다보면, 근거에 기반한 이야기보다는 서로 주장을 하다가 끝났던 경험을 하곤 한다. 사람은 본래 인지적인 부하를 줄이기 위해, 추상적으로 개념을 받아들이곤 한다. 그래서 구체적인 사실보다는 그냥 감정적으로, 추상적인 개념적으로만 이해하고 판단한다. 이런 태도는 \"선동\"당하기 너무 쉽다는 문제가 있다. \"광우병\", \"세월호\", \"일본 방사선 처리수\" 등등 사실보다는 감정이 우선시 되었던 사건들이 있다. 어떤 사건을 이해하기 보다는 주위의 반응에 그저 동조했던 사건들이다. 다른 걸 다 떠나서, 나는 이렇게 행동하기가 싫다. 또한 내가 아끼는 사람들이라도 그렇게 행동하지 않았으면 좋겠다. 선동하는 사람들의 원하는대로 행동하지 않고, 사실을 판단하고 올바른 방향으로 행동했으면 좋겠다. 이렇게 행동하기 위해서는 어떻게 해야할까? Faithfulness, 사실충실성. 이 책을 통해 내가 원하는 태도, 시선을 가질 수 있을 것 같다. 책에서는 사람이 가지는 인지적 한계들을 이야기하고, 이에 대한 구체적인 사례들을 소개하고 있다. 이를 통해 사실에 근거한 세계관을 구축할 수 있겠다. 올바른 세계관을 가지려고 노력하자. 편한대로 살아가지 말고, 불편하더라도 세상을 올바르게 살아가보자.",
    "public": true
  },
  {
    "id": "Self_Development/Current Affairs/노란봉투법",
    "title": "노란봉투법",
    "date": "2025-09-12",
    "excerpt": "노란봉투법은 무엇일까",
    "category": "Current Affairs",
    "tags": [
      "노란봉투법"
    ],
    "content": "노란봉투법: 노동자 권익 보호를 위한 법안 노란봉투법이란? 노란봉투법은 노동조합의 정당한 쟁의행위(파업, 집회 등)로 인해 발생한 손해에 대해 사용자(기업)가 노동자나 노동조합에 과도한 손해배상을 청구하지 못하도록 제한하는 법안입니다. 정식 명칭은 '노동조합 및 노동관계조정법 일부개정법률안'입니다. 명칭의 유래 '노란봉투법'이라는 이름은 2014년 쌍용자동차 파업 당시의 사건에서 비롯되었습니다. 당시 파업으로 인한 손해배상 청구를 받은 노동자들을 돕기 위해 시민들이 노란 봉투에 성금을 담아 전달한 데서 유래되었습니다. 이는 시민들의 연대와 지지가 법안의 이름에 반영된 것으로, 노동자 보호에 대한 사회적 공감대를 보여주는 상징적 의미를 담고 있습니다. 제정 배경 3.1 기존 문제점 과도한 손해배상 청구: 노동조합의 합법적인 쟁의행위에도 불구하고 사용자 측이 거액의 손해배상 청구를 통해 노동자와 노동조합의 활동을 위축시키는 사례가 빈번했습니다. 노동권 위축: 이러한 소송은 노동자들의 생계를 위협하고, 결과적으로 노동조합 활동을 위축시키는 효과를 가져왔습니다. 권력 불균형: 기업과 개별 노동자 간의 경제적 권력 불균형으로 인해 노동자들이 정당한 권리 행사를 주저하게 되는 상황이 발생했습니다. 3.2 사회적 필요성 노동자의 단체행동권은 헌법에서 보장하는 기본권이지만, 실제로는 경제적 부담으로 인해 제대로 행사되지 못하는 경우가 많았습니다. 노란봉투법은 이러한 문제를 해결하고 노동권을 실질적으로 보장하기 위해 제안되었습니다. 주요 내용 4.1 근로자 범위 확대 특수고용노동자 포함 : 자신이 아닌 다른 사람의 업무를 위하여 노무를 제공하고 해당 사업주 또는 노무수령자로부터 대가를 받아 생활하는 사람, 그 밖에 노무를 제공하는 사람으로서 보호의 필요성이 있다고 대통령령으로 정하는 사람을 근로자에 포함시킵니다. 이는 플랫폼 노동자, 프리랜서 등 특수고용형태 노동자들도 노동관계법의 보호를 받을 수 있도록 하는 조항입니다. 4.2 사용자 범위 확대 4.2.1 사용자의 기본 개념 노동법에서 \"사용자\"는 고용주, 사업주, 경영자를 의미하며, 노동자에게 일을 시키고 임금을 주는 쪽을 말합니다. 기존 사용자 정의: 사업주 (회사 대표, 개인 사업자) 사업의 경영담당자 (임원, 관리자) 사업주를 위하여 행동하는 자 (인사담당자 등) 핵심: 근로계약을 직접 체결한 상대방 4.2.2 노란봉투법의 사용자 범위 확대 실질적 지배력 기준 : 근로계약 체결 당사자가 아니더라도 근로자의 근로조건이나 수행업무에 대하여 사실상의 영향력 또는 지배력을 행사하는 자, 그 사업의 노동조합에 대하여 상대방으로서의 지위를 인정할 수 있는 자도 사용자에 해당합니다. 이는 원청-하청 관계에서 실질적 지배력을 가진 원청도 사용자로 인정하는 조항으로, 2007년 대법원 판례(2007두8881)에서 확립된 법리를 명문화한 것입니다. 4.2.3 구체적 사례 기존 방식: 하청업체 직원 → 하청업체만 사용자로 인정 원청업체는 노동조합과 교섭할 의무 없음 노란봉투법 적용 후: 하청업체 직원 → 하청업체 + 원청업체도 사용자로 인정 (실질적 지배력이 있다면) 원청업체도 노동조합과 교섭할 의무 부담 이렇게 하면 원청-하청 구조에서 원청이 하청 노동자들의 근로조건을 실질적으로 결정하고 있다면, 원청도 그 노동자들의 사용자로 인정받아 노동조합과 교섭할 의무를 지게 됩니다. 4.3 노동쟁의 범위 확대 4.3.1 노동쟁의의 기본 개념 노동쟁의는 노동조합과 사용자(또는 사용자단체) 간에 근로조건에 관한 주장이 맞지 않아서 발생한 분쟁상태를 말합니다. 노동쟁의의 구성 요소: 당사자: 노동조합 vs 사용자(또는 사용자단체) 대상: 근로조건 (임금, 근로시간, 복지, 해고, 기타 대우 등) 상태: 주장의 불일치로 인한 분쟁상태 조건: 당사자간 합의를 위한 노력을 계속하여도 더 이상 자주적 교섭에 의한 합의의 여지가 없는 경우 노동쟁의의 구체적 사례: 임금 인상 요구 vs 임금 동결 주장 근로시간 단축 요구 vs 근로시간 유지 주장 해고 철회 요구 vs 해고 유지 주장 단체협약 체결 요구 vs 교섭 거부 4.3.2 노란봉투법의 노동쟁의 범위 확대 기존: \"근로조건의 결정\"에 관한 주장의 불일치만 노동쟁의 대상 개정 후: \"근로조건\"에 관한 주장의 불일치로 확대 구체적 변화: 기존: 근로조건을 새로 정하는 과정에서의 분쟁만 쟁의 대상 개정 후: 이미 정해진 근로조건의 위반이나 변경에 대한 분쟁도 쟁의 대상 4.3.3 확대 효과 이 변경으로 인해 다음과 같은 경우에도 쟁의행위가 가능해집니다: 단체협약 위반: 이미 체결된 단체협약을 회사가 위반한 경우 부당노동행위: 노동조합 활동을 방해하는 행위 근로조건 변경: 기존 근로조건을 일방적으로 변경하려는 경우 사업경영상 결정: 근로조건에 영향을 미치는 경영 결정에 대한 분쟁 예시: 기존: \"임금을 얼마로 정할지\"에 대한 분쟁만 쟁의 가능 개정 후: \"이미 정해진 임금을 지급하지 않는 것\", \"임금을 임의로 깎는 것\" 등도 쟁의 가능 4.4 손해배상 청구 제한 강화 기본 원칙: 폭력이나 파괴로 인한 직접 손해를 제외하고는 단체교섭, 쟁의행위, 그 밖의 노동조합의 목적 달성을 위한 행위로 인한 손해에 대하여 노동조합 또는 근로자에게 손해배상을 청구할 수 없습니다. 개별 근로자 보호: 쟁의행위가 노동조합에 의하여 계획된 것이라면 개별 근로자에게는 손해배상을 청구할 수 없습니다. 4.5 신원보증인 면책 신원보증법 예외: 신원보증법에도 불구하고 쟁의행위로 인한 손해에 대해서는 신원보증인이 책임지지 않도록 합니다. 간접 손해 제외: 사용자의 영업손실, 사용자의 제3자에 대한 채무불이행으로 인한 손해 또는 그 밖의 근로자 또는 노동조합의 위법행위로 인하여 직접 발생한 것이 아닌 손해는 손해배상 대상에서 제외됩니다. 4.6 손해배상액 제한 조합 존립 보호: 손해배상으로 인하여 노동조합의 존립이 불가능하게 되는 경우에는 손해배상의 청구를 허용하지 않습니다. 상한 설정: 노동조합에 대한 손해배상액의 상한은 조합원 수, 조합비, 노동조합의 재정규모 등을 고려하여 대통령령으로 정합니다. 4.7 손해배상액 감면 감면 청구권: 손해의 배상의무자는 법원에 감면을 청구할 수 있습니다. 감면 기준: 법원은 쟁의행위의 원인과 경과, 사용자의 영업 규모, 시장 상황, 피해 확대 방지 노력, 배상의무자의 재정 상태 등을 고려하여 감면할 수 있습니다. 4.8 개별 책임 원칙 연대책임 제한: 노동쟁의로 인한 손해배상책임을 인정하는 경우 각 손해의 배상의무자별로 귀책사유와 기여도에 따라 개별적으로 책임 범위를 정하여야 합니다. 이는 기존의 부진정연대채무 원칙에서 벗어나 각자의 기여도에 따른 개별 책임을 지도록 하는 조항입니다. 찬반 논란 5.1 찬성 의견 5.1.1 노동권 보장 강화 단체행동권 실질화: 헌법에서 보장하는 노동자의 단체행동권을 실질적으로 보장하여 노동조합 활동의 자유를 확대합니다. 경제적 압박 해소: 과도한 손해배상 청구로 인한 노동자들의 경제적 부담을 해소하여 정당한 권리 행사를 보장합니다. 특수고용노동자 보호: 플랫폼 노동자, 프리랜서 등 특수고용형태 노동자들도 노동관계법의 보호를 받을 수 있게 됩니다. 5.1.2 공정한 노사관계 조성 권력 균형: 기업과 노동자 간의 권력 불균형을 완화하여 공정한 노사관계를 조성합니다. 실질적 사용자 개념: 원청-하청 관계에서 실질적 지배력을 가진 원청도 사용자로 인정하여 노동자 보호를 강화합니다. 신원보증 제도 개선: 신원보증 제도가 노동쟁의를 억제하는 도구로 악용되는 것을 방지합니다. 5.1.3 사회적 정의 실현 사회적 연대: 노동자 보호를 통한 사회적 연대와 정의 실현에 기여합니다. 약자 보호: 경제적 약자인 노동자들의 권익을 보호하여 사회적 형평성을 제고합니다. 5.2 반대 의견 5.2.1 법적 원칙 위반 우려 죄형법정주의 위반: 사용자 정의를 불명확한 개념으로 확대하는 것은 죄형법정주의의 명확성 원칙에 위반될 수 있습니다. 형평성 원칙 위반: 노동조합의 손해배상책임에만 부진정연대책임의 예외를 인정하는 것은 형평과 정의에 반할 수 있습니다. 법적 안정성 훼손: 기존에 확립된 법리와 판례를 무시하고 새로운 기준을 도입함으로써 법적 안정성을 훼손할 수 있습니다. 5.2.2 기업 경영권 침해 재산권 침해: 기업의 재산권을 침해할 수 있다는 우려가 제기됩니다. 경영 자율성 제한: 사용자 범위 확대로 인해 기업의 경영 자율성이 제한될 수 있습니다. 경제적 부담 증가: 기업의 경영 부담이 증가할 수 있다는 우려가 있습니다. 5.2.3 파업 남발 우려 노동쟁의 범위 확대: 노동쟁의 대상 확대로 인해 파업의 가능성이 증대하여 빈번한 파업이 발생할 수 있습니다. 국민 불편 초래: 사법적 해결보다는 파업 등 실력행사를 통한 문제해결 시도가 증가하여 국민 불편과 국가 경제의 어려움을 불러올 수 있습니다. 정당한 해고까지 문제: 법적 절차를 거쳐 정당한 해고로 확인된 경우까지 해고자 복직을 요구하는 파업이 가능해질 수 있습니다. 5.2.4 기술적 문제점 기여도 산정의 어려움: 공동불법행위에 가담한 조합원이 많을수록 그 기여도를 산정하기 어렵습니다. 불명확한 개념: \"상대방으로서의 지위를 인정할 수 있는 자\"와 같은 표현이 모호하고 포괄적이어서 기업운영을 어렵게 만들 수 있습니다. 산업현장 혼란: 사용자 정의 확대로 인해 산업현장에서 혼란이 초래될 수 있습니다. 5.3 대통령 거부권 행사 (2023년) 윤석열 대통령은 2023년 11월 노란봉투법에 대해 거부권을 행사하며 다음과 같은 이유를 제시했습니다: 사용자 정의 불명확: 사용자 정의를 불명확한 개념으로 확대하는 것은 산업현장의 혼란을 초래할 뿐만 아니라 죄형법정주의에도 위반된다. 노동쟁의 대상 확대: 노동쟁의 대상 확대로 사법적 해결보다는 파업 등 실력행사를 통한 문제해결 시도가 증가하여 국민 불편과 국가 경제의 어려움을 불러올 것이다. 형평성 원칙 위반: 노동조합의 손해배상책임에만 부진정연대책임의 예외를 인정하는 것은 형평과 정의에 반한다. 사회적 의미와 영향 6.1 노동권 신장 노란봉투법은 노동자의 단체행동권을 실질적으로 보장하고, 사용자 측의 부당한 압박을 방지하여 노동권 신장을 도모합니다. 이를 통해 노동조건 개선과 권익 보호가 가능해집니다. 6.2 사회적 정의 시민들의 연대와 지지에서 비롯된 이 법안은 사회적 정의 실현과 약자 보호라는 가치를 담고 있습니다. 노동자 보호를 통한 사회적 연대 강화에 중요한 의미를 지닙니다. 6.3 노사관계 개선 공정한 노사관계 조성을 통해 기업과 노동자 간의 상호 존중과 협력을 바탕으로 한 건전한 노사관계 발전에 기여할 것으로 기대됩니다. 현재 상황과 전망 7.1 입법 과정 2023년 11월: 국회 본회의를 통과하여 법률안이 성립되었습니다. 2023년 11월: 윤석열 대통령이 거부권을 행사하여 최종 부결되었습니다. 2025년 7월 28일: 국회 환경노동위원회 법안심사소위원회에서 노란봉투법이 통과되었습니다. 2025년 8월 5일: 국회 본회의에서 여당 주도로 법안이 최종 통과되었습니다. 현재: 법안이 통과되어 시행 준비 중입니다. 7.2 주요 지지 세력 노동계: 전국민주노동조합총연맹(민주노총), 한국노동조합총연맹(한국노총) 등 주요 노동조합들이 적극 지지하고 있습니다. 정치권: 정의당, 더불어민주당 일부 의원들이 지지하고 있습니다. 시민사회: 노동권 보호를 위한 시민단체들이 지지하고 있습니다. 7.3 주요 반대 세력 경영계: 한국경영자총협회, 전국경제인연합회 등 기업 단체들이 반대하고 있습니다. 정치권: 국민의힘 등 보수 정당들이 반대하고 있습니다. 법조계: 일부 법조인들이 법적 원칙 위반을 우려하며 반대하고 있습니다. 7.4 법안 통과 이후 상황 현대자동차 사례: 법안 통과 전망에 따라 현대자동차가 파업에 참여한 비정규직 노동자들에 대해 제기한 약 3억 원의 손해배상 소송 3건을 취하했습니다. 정부 반응: 이정식 고용노동부 장관은 법안 통과에 대해 \"국민의 어려움을 외면한 무책임한 결정\"이라며 우려를 표명했습니다. 경영계 반응: 경제 6단체는 법안 통과에 대해 유감을 표명하며, 노사 분쟁 증가와 투자·고용 위축을 우려하고 있습니다. 7.5 현대중공업 파업 사례 (2024-2025) 7.5.1 파업 배경 현대중공업 노동조합이 2024년 임금 및 단체협약(임단협) 협상에서 난항을 겪으며 '무기한 총파업'을 선언했습니다. 이는 노란봉투법 통과 이후의 대표적인 노사 분쟁 사례로 주목받고 있습니다. 주요 쟁점: 임금 인상 규모: 노조와 사측 간의 임금 인상 폭에 대한 입장 차이 HD현대미포 합병: 교섭 중 발표된 합병 소식으로 인한 고용 불안 투자 비용 분배: 합병 기업의 투자 비용이 노동자들에게 돌아갈 이익에 대한 요구 7.5.2 파업 진행 상황 2024년 7월: 임단협 교섭 관련 파업 찬반투표에서 65.1% 찬성률로 파업안 가결 2024년 12월: 잠정합의안이 조합원 총회에서 부결되어 교섭에 진전 없음 2025년 1월: 무기한 총파업 선언 및 고공 농성 시작 고공 농성: 50m 높이의 대형 크레인 위에서 농성 진행 '총파업 총투쟁' 현수막 게시 사측의 결단을 촉구하는 강경한 입장 표명 7.5.3 충돌 및 갈등 물리적 충돌: 노조원들이 천막을 설치하려다 사측 경비대와 충돌 여성 조합원 1명이 경비원에게 얼굴을 맞아 병원 이송 사측은 \"불필요한 갈등을 최소화하고 협상에 집중해 주길 바란다\"고 반응 연대 움직임: 대우조선해양 노조가 현대중공업 노조의 농성이 침탈될 경우 동반 총파업 선언 현대중공업의 물적 분할과 대우조선해양 인수에 대한 반대 의사 표명 7.5.4 노란봉투법과의 연관성 이번 현대중공업 파업은 노란봉투법 통과 이후의 노사 관계 변화를 보여주는 중요한 사례입니다: 법안의 실질적 효과: 노조가 더 적극적으로 쟁의행위를 전개할 수 있는 환경 조성 손해배상 우려 없이 파업을 진행할 수 있는 법적 보호 고공 농성 등 강경한 쟁의행위에 대한 법적 보호 우려사항 현실화: 경영계가 우려했던 노사 분쟁 증가가 실제로 발생 파업 장기화로 인한 생산 차질 우려 노사 간 갈등 심화로 인한 협상 어려움 7.5.5 향후 전망 협상 지속: 총파업 중에도 실무진 간의 임금 협상은 이어갈 예정 장기화 우려: 양측의 입장 차이가 커 협상 타결까지 상당한 시간 소요 예상 산업 영향: 조선업계 전체에 미칠 파급 효과 우려 7.6 향후 전망 시행 준비: 법안이 통과되어 현재 시행 준비 단계에 있습니다. 노사관계 변화: 노동자 권익 보호가 강화될 것으로 기대되지만, 노사 분쟁 증가 가능성도 우려되고 있습니다. 지속적 모니터링: 법안 시행 과정에서 노사 간의 갈등 조정과 사회적 합의 도출이 중요할 것으로 보입니다. 실제 효과 검증: 법안의 실제 효과와 부작용에 대한 지속적인 모니터링과 평가가 필요합니다. 결론 노란봉투법은 2014년 쌍용자동차 파업 당시 시민들의 노란 봉투 성금에서 시작된 노동자 보호에 대한 사회적 공감대와 연대의 의미를 담고 있는 중요한 법안입니다. 이 법안은 노동자의 정당한 권리 행사를 보장하고, 공정한 노사관계 조성에 기여할 것으로 기대되지만, 동시에 여러 논란과 우려도 함께 제기되고 있습니다. 8.1 법안의 의의 노란봉투법은 단순한 법적 조치를 넘어서 노동권 신장과 사회적 정의 실현이라는 근본적인 가치를 추구합니다. 특히 특수고용노동자 보호, 실질적 사용자 개념 도입, 손해배상 제한 등을 통해 노동자들의 경제적 부담을 해소하고 정당한 권리 행사를 보장하려는 의도는 높이 평가할 만합니다. 8.2 논란의 핵심 하지만 이 법안은 법적 원칙, 기업 경영권, 파업 남발 우려 등 여러 측면에서 논란이 지속되고 있습니다. 특히 사용자 정의 확대, 노동쟁의 범위 확대, 연대책임 제한 등은 기존 법리와 다른 새로운 접근으로 인해 법적 안정성과 형평성에 대한 우려가 제기되고 있습니다. 8.3 향후 과제 노란봉투법이 성공적으로 입법되기 위해서는 다음과 같은 과제들이 해결되어야 합니다: 법적 명확성 확보: 사용자 정의 등 불명확한 개념들을 구체적으로 명문화하여 법적 안정성을 확보해야 합니다. 균형점 모색: 노동자 권익 보호와 기업 경영권 보장 사이의 균형을 찾는 것이 중요합니다. 사회적 합의: 다양한 이해관계자들의 의견을 수렴하여 사회적 합의를 도출해야 합니다. 단계적 접근: 모든 조항을 한 번에 통과시키기보다는 단계적으로 접근하는 방안을 고려해야 합니다. 8.4 최종 평가 노란봉투법은 2025년 8월 5일 국회 본회의를 통과하여 결국 입법되었습니다. 2023년 대통령의 거부권 행사에도 불구하고, 여당 주도로 법안이 재통과된 것은 노동권 보호에 대한 사회적 요구가 지속적으로 존재했음을 보여줍니다. 법안 통과 이후 현대자동차가 손해배상 소송을 취하한 것은 법안의 실질적 효과가 이미 나타나고 있음을 보여주는 사례입니다. 하지만 정부와 경영계의 우려도 여전히 존재하며, 법안 시행 과정에서 이러한 우려들이 현실화될지 지켜봐야 할 상황입니다. 궁극적으로 노란봉투법은 노동자와 기업 모두에게 공정하고 건전한 노사관계를 조성하는 데 기여할 수 있는 법안으로 발전되어야 하며, 법안 시행 과정에서 지속적인 모니터링과 사회적 합의 과정이 필요할 것입니다. 특히 법안의 실제 효과와 부작용에 대한 객관적 평가를 통해 향후 개선 방향을 모색해야 할 것입니다. 전국금속노동조합 (전국금속노조) 9.1 개요 전국금속노동조합(전국금속노조)은 대한민국의 금속산업 분야 노동자들을 대표하는 산업별 노동조합으로, 금속노조라고도 불립니다. 2001년에 설립되어 금속산업에 종사하는 노동자들의 권익 보호와 근로 조건 개선을 목표로 활동하고 있습니다. 9.2 설립 배경과 역사 설립 시기: 2001년 설립 설립 배경: 1990년대 후반 IMF 경제 위기와 구조조정으로 인해 금속산업 노동자들의 권익 보호 필요성이 대두되면서 개별 기업별 노조들이 연합하여 산업별 노동조합을 형성 목적: 노동자들의 교섭력과 단결력을 강화하여 노동 조건 개선과 권익 보호 9.3 조직 구조 전국 단위: 본부를 중심으로 한 전국 조직 지역별 지부: 각 지역의 금속 산업 노동자들을 대표 기업별 지회: 대기업 및 중소기업의 개별 사업장 단위 조직 분회: 각 기업별로 구성되어 기업 특성에 맞는 활동 전개 9.4 주요 활동과 역할 9.4.1 노동 조건 개선 임금 협상: 금속산업 내 다양한 기업들과의 단체교섭을 통한 임금 인상 근로 시간 단축: 근로 시간 단축을 통한 노동자 복지 향상 안전한 작업 환경: 안전한 작업 환경 조성을 위한 교섭과 투쟁 9.4.2 노동자 권익 보호 부당 해고 대응: 부당 해고 사례에 대한 법적 지원과 상담 제공 차별 대우 방지: 노동자들의 차별 대우 등 권익 침해 사례 대응 법적 지원: 노동자들의 권익 보호를 위한 법률 지원 9.4.3 교육 및 연대 활동 노동자 교육: 노동자들의 역량 강화를 위한 교육 프로그램 운영 사회적 연대: 다른 산업의 노동조합, 시민사회 단체와의 연대 활동 사회 정의 실현: 민주주의 발전, 평등한 사회 구현을 위한 사회 운동 참여 9.5 주요 성과 및 역사적 사례 대규모 파업: 설립 이후 여러 차례의 대규모 파업과 교섭을 통한 노동 조건 개선 임금 인상: 대기업과의 교섭에서 중요한 역할을 수행하며 노동자들의 권익 신장에 기여 사회적 역할: 노동자 권익 보호뿐만 아니라 사회적 약자와의 연대 활동 9.6 노란봉투법과의 연관성 9.6.1 법안 지지 활동 적극적 지지: 노란봉투법의 통과를 환영하며 적극적인 활동 전개 법안 의의: 노동조합의 정당한 쟁의행위로 인한 손해배상 청구 제한을 통한 노동자 보호 강화 사용자 범위 확대: 하청 노동자들의 교섭권 강화에 대한 기대 9.6.2 현대중공업 파업 지원 연대 활동: 현대중공업 노동조합의 무기한 총파업 선언에 대한 지원 임금 협상 지원: 임금 협상과 관련된 노동자들의 요구가 관철될 수 있도록 연대 지회 지원: 해당 지회를 지원하며 노동자들의 권익 보호 9.7 현재 상황과 도전과제 9.7.1 새로운 도전 산업 구조 변화: 금속산업의 구조적 변화에 대한 대응 필요 노동 시장 유연화: 노동 시장의 유연화에 따른 새로운 대응 전략 수립 디지털 전환: 4차 산업혁명에 따른 노동 환경 변화 대응 9.7.2 대응 전략 조직 강화: 노동자들의 단결력과 교섭력 강화 조합원 교육: 새로운 노동 환경에 대한 교육 프로그램 강화 연대 확대: 다른 산업과의 연대를 통한 노동 운동 강화 9.8 부정적인 활동과 기업에 미친 악영향 9.8.1 장기 파업으로 인한 생산 차질 생산 일정 지연: 장기 파업으로 인해 기업들의 생산 일정이 지연되고 납품 기한을 맞추지 못하는 상황 발생 고객 신뢰 관계 타격: 납품 지연으로 인해 고객사와의 신뢰 관계에 심각한 타격 계약 취소 및 연기: 일부 계약이 취소되거나 연기되는 등 직접적인 매출 손실 발생 경쟁력 저하: 해외 경쟁사에 비해 신뢰도 하락으로 시장 점유율 감소 9.8.2 불법 점거 및 시설 피해 공장 및 사무실 불법 점거: 조합원들이 공장이나 사무실을 불법으로 점거하여 정상적인 업무 수행 불가 시설물 파손: 점거 과정에서 시설물이 파손되거나 생산 설비에 손상 발생 물리적 피해: 기업의 재정적 손실로 이어지는 직접적인 물리적 피해 복구 비용 부담: 파손된 시설의 복구를 위한 추가 비용 발생 9.8.3 대외 이미지 실추 및 신뢰도 하락 언론 부정적 보도: 노조의 강경한 투쟁 방식이 언론에 부정적으로 보도되면서 기업 이미지 실추 소비자 신뢰 하락: 소비자들의 기업에 대한 신뢰도 급격히 하락 주가 하락: 기업 이미지 실추로 인한 주가에 부정적인 영향 장기적 성장 저해: 기업의 장기적인 성장과 발전에 심각한 악영향 9.8.4 협상 결렬로 인한 추가 비용 발생 대체 인력 투입: 파업 장기화로 인한 대체 인력 투입으로 인한 추가 비용 생산 라인 재조정: 생산 라인을 재조정하는 등의 추가 비용 부담 수익성 악화: 비용 증가로 인한 기업의 수익성 급격히 악화 재무 건전성 위협: 장기적으로 기업의 재무 건전성에 심각한 위협 9.8.5 법적 분쟁으로 인한 시간 및 비용 소모 소송 비용 증가: 노조와의 갈등이 법적 분쟁으로 이어져 소송 비용 급증 경영진 시간 소모: 경영진과 법무팀의 시간과 자원을 과도하게 소모 장기화된 분쟁: 법적 분쟁의 장기화로 인한 지속적인 운영 부담 합의금 부담: 법적 분쟁 해결을 위한 합의금 등 추가 재정적 부담 9.8.6 현대중공업 파업 사례의 부정적 영향 고공 농성으로 인한 안전 위험: 50m 높이 크레인에서의 농성으로 인한 안전 사고 위험 경비대와의 충돌: 사측 경비대와 노조원 간의 물리적 충돌로 인한 부상 사고 생산 중단: 무기한 총파업으로 인한 조선소 생산 완전 중단 해외 주문 취소 위험: 납품 지연으로 인한 해외 고객사의 주문 취소 가능성 9.8.7 과도한 임금 요구로 인한 경쟁력 저하 인건비 급증: 과도한 임금 인상 요구로 인한 기업의 인건비 부담 급증 해외 이전 압력: 높은 인건비로 인한 해외 생산 기지 이전 압력 증가 투자 위축: 높은 인건비로 인한 신규 투자 위축 및 고용 감소 장기적 경쟁력 상실: 글로벌 경쟁에서의 경쟁력 상실로 인한 시장 점유율 감소 9.8.8 노사 관계 악화로 인한 조직 효율성 저하 조직 내 분위기 악화: 노조와 경영진 간의 갈등으로 인한 조직 내 분위기 급격히 악화 직원 사기 저하: 지속적인 갈등으로 인한 직원들의 사기 급격히 저하 팀워크 파괴: 갈등으로 인한 팀워크와 협업 능력 급격히 저하 업무 효율성 감소: 전반적인 업무 효율성과 생산성 급격히 감소 9.9 터무니없는 주장과 과도한 요구사항 9.9.1 현대중공업 노조의 비현실적 요구사항 기본급 월 14만1,300원 인상: 조선업계 불황과 회사 경영난을 고려할 때 과도한 임금 인상 요구 정년 연장 최대 65세: 고령화로 인한 인건비 상승과 신규 인력 채용 어려움을 초래하는 비현실적 요구 근속수당 대폭 인상: 이미 높은 수준의 근속수당을 더욱 인상하라는 무리한 요구 업계 평균 대비 과도한 수준: 다른 조선업체 대비 현저히 높은 임금 수준에서 추가 인상 요구 9.9.2 현대자동차 노조의 터무니없는 주장 퇴직자 전기차 25% 할인 요구: 퇴직자에게까지 전기차 구매 시 최대 25% 할인을 적용해달라는 무리한 요구 업계에서 무리한 주장으로 평가: 업계 관계자들도 비현실적이라고 평가하는 요구사항 노사 협상 난항 초래: 이러한 비현실적 요구로 인한 협상 결렬과 갈등 심화 9.9.3 과도한 휴일 보장 요구 무제한 휴일 보장: 기업 운영을 고려하지 않은 무제한 휴일 보장 요구 생산 일정 무시: 휴일 보장을 위해 생산 일정과 납기 준수를 무시하는 주장 고객 서비스 저해: 과도한 휴일로 인한 고객 서비스 품질 저하 가능성 경쟁력 상실: 해외 경쟁사 대비 생산성 저하로 인한 경쟁력 상실 9.9.4 비현실적인 복리후생 요구 과도한 복리후생 확대: 기업 재무 상황을 고려하지 않은 무제한 복리후생 확대 요구 의료비 전액 지원: 의료비 전액 지원 등 기업 부담이 과도한 복리후생 요구 교육비 무제한 지원: 자녀 교육비 무제한 지원 등 현실적으로 불가능한 요구 주거비 지원 확대: 주거비 지원을 무제한으로 확대하라는 비현실적 요구 9.9.5 근로시간 단축의 극단적 요구 주 4일 근무제: 조선업계 특성상 불가능한 주 4일 근무제 도입 요구 일일 근무시간 대폭 단축: 생산성 저하를 초래할 수 있는 극단적 근무시간 단축 요구 야간근무 완전 금지: 조선업계 특성상 불가피한 야간근무까지 금지하라는 무리한 요구 휴게시간 무제한 확대: 업무 효율성을 고려하지 않은 휴게시간 무제한 확대 요구 9.9.6 기업 경영에 대한 무리한 개입 요구 경영진 임명권 요구: 노조가 경영진 임명에 개입하겠다는 무리한 요구 투자 결정권 요구: 기업의 투자 결정에 노조가 개입하겠다는 비현실적 요구 사업 계획 승인권: 사업 계획에 대한 노조의 승인권을 요구하는 터무니없는 주장 재무 관리 개입: 기업의 재무 관리에 노조가 개입하겠다는 무리한 요구 9.9.7 사회적 비판과 우려 업계 평균 대비 과도한 수준: 이미 업계 최고 수준의 임금과 복리후생을 받는 상황에서 추가 요구 기업 경쟁력 저해: 과도한 요구로 인한 기업 경쟁력 저하와 해외 이전 압력 증가 일자리 감소 위험: 과도한 요구로 인한 고용 감소와 일자리 창출 저해 국가 경제에 악영향: 주요 기업들의 경쟁력 저하로 인한 국가 경제 전반에 미치는 부정적 영향 9.9.8 현실성 부족한 주장들 경영 상황 무시: 기업의 경영난과 업계 불황을 전혀 고려하지 않은 요구 경쟁 환경 무시: 글로벌 경쟁 환경과 해외 경쟁사 상황을 무시한 주장 시장 원리 무시: 시장 경제 원리와 기업의 수익성을 무시한 비현실적 요구 지속가능성 무시: 기업의 지속가능한 경영을 고려하지 않은 단기적 이익만 추구하는 주장 9.10 정치적 성향과 진보/좌파 연관성 9.10.1 민주노총과의 연계 민주노총 소속: 전국금속노조는 전국민주노동조합총연맹(민주노총)의 주요 구성원으로, 민주노총의 진보적 성향을 공유 진보 노동운동: 민주노총은 한국의 대표적인 진보 성향 노동조합 연맹으로, 사회주의적 이념과 밀접한 관련 정치적 활동: 민주노총은 노동운동을 넘어서 정치적 활동에도 적극적으로 참여하는 진보 성향의 조직 9.10.2 정의당과의 정치적 연계 정의당 지지: 전국금속노조는 정의당 등 진보 정당과 밀접한 정치적 연계를 유지 정치적 후원: 노조가 진보 정당의 정치적 활동을 후원하고 지지하는 구조 정치적 목표: 노동운동을 통한 사회주의적 사회 구현이라는 정치적 목표 추구 정치적 영향력: 노조의 조직력을 바탕으로 한 정치적 영향력 행사 9.10.3 사회주의적 이념과의 연관성 사회주의 역사: 노동조합은 역사적으로 사회주의 이념과 밀접한 관련을 가진 조직 생산수단 공공소유: 사회주의의 핵심 이념인 생산수단의 공공 소유와 평등한 분배 추구 자본주의 체제 부정: 일부 노조 지도자들이 자본주의 체제 자체를 부정하는 성향 계급투쟁론: 자본가와 노동자 간의 계급투쟁을 통한 사회 변혁 추구 9.10.4 과거 사노맹과의 연관성 사노맹 배경: 남한사회주의노동자동맹(사노맹)과 같은 극좌 조직과의 연관성 의혹 조국 사태: 2019년 조국 법무부 장관 후보자가 사노맹 기관지에 자본주의 체제 부정 논문 기고 극좌 성향: 일부 노조 지도자들의 극좌 성향과 사회주의적 사상 체제 전복 목표: 자본주의 체제 전복을 목표로 하는 극좌 성향의 존재 9.10.5 정치적 활동의 구체적 사례 정치 집회 참여: 진보 정당의 정치 집회와 시위에 적극적으로 참여 정치적 후보 지지: 진보 성향의 정치인과 정당 후보에 대한 조직적 지지 정치적 압력: 노조의 조직력을 통한 정치적 압력과 영향력 행사 사회운동 연계: 노동운동을 넘어선 다양한 사회운동과의 연계 9.10.6 빨갱이 논란의 근거 사회주의 성향: 명확한 사회주의적 이념과 목표를 가진 조직 정치적 목표: 노동운동을 통한 정치적 사회 변혁 추구 체제 부정: 자본주의 체제 자체에 대한 부정적 시각 극좌 연관성: 과거 극좌 조직과의 연관성 의혹 정치적 활동: 순수 노동운동을 넘어선 정치적 활동과 목표 9.10.7 사회적 우려와 비판 정치적 중립성 상실: 노조가 정치적 중립성을 상실하고 특정 정치 세력과 결탁 노동운동 정치화: 순수한 노동운동이 정치적 목적을 위한 도구로 전락 사회 분열 조장: 정치적 성향으로 인한 사회적 분열과 갈등 조장 민주주의 위협: 극좌 성향의 정치적 활동이 민주주의 체제에 위협 9.10.8 역사적 배경 일제강점기: 1931년 부여공산주의자협의회 등 공산주의 조직의 활동 해방 후: 좌익 노동운동의 역사적 전통과 연속성 현대적 연계: 과거 극좌 조직과 현대 노조의 이념적 연계성 정치적 계승: 사회주의적 정치 목표의 현대적 계승과 발전 9.11 향후 전망 지속적 교섭: 금속 산업 노동자들의 권익 보호와 향상을 위한 지속적인 교섭과 투쟁 노란봉투법 시행: 노란봉투법의 시행에 따른 노동 환경 변화에 대한 대응 권리 보장: 노동자들의 권리가 실질적으로 보장될 수 있도록 지속적인 노력 사회적 역할: 대한민국 노동 운동의 중요한 축으로서 사회 정의 실현을 위한 활동 지속 균형점 모색: 노동자 권익 보호와 기업 경영 안정 사이의 균형점 모색 필요 전국금속노조는 금속산업 노동자들의 권익을 대변하며, 노동 환경 개선과 사회적 정의 실현을 위해 지속적으로 활동하고 있는 중요한 노동조합입니다. 하지만 일부 활동이 기업에 부정적인 영향을 미치는 사례도 존재하므로, 노조와 기업 간의 상호 협력과 원활한 소통을 통한 균형 잡힌 노사관계 구축이 필요합니다. 특히 노란봉투법 통과 이후 노동자들의 권익 보호가 강화되는 환경에서 더욱 신중한 접근이 요구됩니다.",
    "public": true
  },
  {
    "id": "Self_Development/Current Affairs/삼권분립",
    "title": "삼권분립에 대하여",
    "date": "2025-09-11",
    "excerpt": "삼권분립이 무엇인지, 검찰청 폐지가 의미하는게 무엇인지 정리",
    "category": "Current Affairs",
    "tags": [
      "삼권분립"
    ],
    "content": "참고자료 1: 선관위 --- 삼권분립이란? <figure> <img src=\"/post/Self_Development/Current Affairs/삼권분립.png\" alt=\"삼권분립\" /> <figcaption>삼권분립</figcaption> </figure> 삼권분립은 민주주의의 핵심 원리로, 국가 권력을 입법부(국회) , 행정부(정부) , 사법부(법원) 로 나누어 서로 견제하고 균형을 이루는 제도입니다. 이는 권력의 집중을 방지하고 시민의 자유와 권리를 보호하기 위한 근본적인 정치 원리입니다. 각 권력기관의 역할 입법부 (국회) 법률 제정: 새로운 법률을 만들고 기존 법률을 수정 예산 심의: 국가 예산을 심의하고 승인 정부 감시: 행정부의 활동을 감시하고 통제 인사 동의권: 국무총리 및 국무위원 임명에 대한 동의 탄핵소추권: 대통령의 탄핵소추 가능 행정부 (정부) 법률 집행: 국회가 제정한 법률을 실제로 실행 국정 총괄: 외교, 국방, 경제정책 등 국가 운영 정책 수립: 각종 정책을 수립하고 실행 국회 감시: 국회의 감시와 통제를 받음 사법부 (법원) 법률 해석: 법률의 의미를 해석하고 적용 분쟁 해결: 법적 분쟁을 해결하고 정의 실현 위헌법률심판: 법률이 헌법에 위배되는지 심사 행정소송: 행정부의 행위가 법에 위배되는지 판단 검찰청 폐지와 권력분립 검찰청의 문제점 권력 집중: 수사권과 기소권을 독점적으로 행사 정치적 중립성: 행정부 소속이지만 사법적 권한 행사 권력분립 위배: 사법부의 독립성 침해 가능성 개혁 방향 수사권 이양: 경찰에 수사권 이양 공수처 신설: 독립적인 공수처에 기소권 부여 권력분립 강화: 각 권력기관의 독립성 확보 삼권분립의 중요성 삼권분립 제도는 각 권력기관이 서로를 견제하면서도 협력할 수 있도록 하여, 권력의 남용을 방지하고 민주주의를 보장하는 핵심적인 역할을 합니다. 특히 한국의 경우 과거 권위주의 정부의 경험을 바탕으로 권력분립의 중요성을 더욱 절실히 인식하고 있으며, 지속적인 제도 개선을 통해 민주주의를 발전시키고 있습니다. 결론 삼권분립은 단순히 권력을 나누는 것이 아니라, 각 권력기관이 서로 견제하고 균형을 이루면서도 협력할 수 있는 체계를 만드는 것입니다. 이를 통해 시민의 자유와 권리를 보호하고, 민주주의의 가치를 실현할 수 있습니다. 검찰이 속한 곳은? 검찰의 현재 소속 검찰은 행정부 소속 기관으로, 법무부 산하에 위치하여 수사와 기소를 담당해 왔습니다. 이는 검찰이 사법부가 아닌 행정부에 속한다는 점에서 권력분립의 관점에서 논란이 되어 왔습니다. 검찰청 폐지와 개편안 검찰개혁 4법 (2025년 6월 발의) 더불어민주당이 발의한 '검찰개혁 4법'에 따르면: 검찰청 폐지: 기존 검찰청을 완전히 폐지 수사권 분리: 중대범죄수사청(중수청)으로 이관 소속: 행정안전부 산하 역할: 수사 전담 기소권 분리: 공소청으로 이관 소속: 법무부 산하 역할: 기소 전담 국가수사위원회 신설: 국무총리 직속 역할: 수사기관 간 관할 조정 및 관리·감독 정부조직 개편안 (2025년 9월 확정) 정부와 여당의 고위당정협의회에서 확정된 내용: 시행 예정: 2026년 9월부터 검찰청 폐지: 완전 폐지 중수청 신설: 행정안전부 산하 공소청 신설: 법무부 산하 개편의 목적과 효과 긍정적 측면 권력 분산: 검찰의 독점적 권한을 분산시켜 권력 남용 방지 수사·기소 분리: 사법 체계의 공정성과 투명성 제고 권력분립 강화: 각 권한을 적절한 부처에 배치 우려사항 형사사법체계의 근본적 혼란 체계적 혼란: 기존 형사사법체계의 근본적 변화로 인한 혼란 가능성 역할 분담 미흡: 새로운 기관 간의 역할 분담과 협력 체계 구축에 시간 소요 전환기 혼선: 기존 시스템에서 신규 시스템으로의 전환 과정에서 발생할 수 있는 혼선 수사·기소 분리로 인한 효율성 저하 정보 공유 어려움: 수사기관과 기소기관 간 정보 공유의 비효율성 협력 체계 부재: 수사와 기소 과정에서의 긴밀한 협력 어려움 범죄 대응 능력 약화: 신속하고 효과적인 범죄 대응 능력 저하 민생범죄 대응력 저하: 일상적인 범죄에 대한 대응 능력 약화 우려 신설 기관의 독립성 및 중립성 문제 정치적 중립성: 중수청과 공수처의 정치적 중립성 확보 방안 부족 행정부 영향: 중수청이 행정안전부 산하에 위치할 경우 행정부 영향력 우려 권한 집중: 수사기관 권한 집중으로 인한 부작용 가능성 감독 체계 미흡: 신설 기관들에 대한 효과적인 감독 체계 부재 현실성 부족 및 구조적 문제 현실성 부족: 검찰 내부에서 제기하는 현실성 부족 지적 구조적 모순: 공수처가 수사권 없이 기소 적정성 판단하는 구조적 모순 검찰 내부 반발: 조직 내부의 동요와 사기 저하 우려 전문성 부족: 신설 기관들의 전문성 확보 방안 부족 위헌성 및 법적 문제 위헌 소지: 헌법에 명시된 검찰의 독립성과 권한 보장 조항과의 충돌 헌법적 정당성: 검찰청 폐지의 헌법적 정당성 확보 여부 의문 법적 근거 부족: 급진적 개편에 대한 충분한 법적 근거 부족 인권 및 국민 권익 보호 문제 인권 침해 우려: 경찰 수사권 확대로 인한 인권 침해 가능성 증가 국민 권익 보호 약화: 범죄 피해자에 대한 신속한 대응 지연 가능성 권한 오남용: 과도한 권한 집중으로 인한 오남용 가능성 비용 및 인력 문제 막대한 비용: 신설 기관 구축 및 운영에 필요한 막대한 비용 인력 재배치: 기존 검찰 인력의 재배치 및 신규 인력 충원 문제 시설 및 인프라: 새로운 기관 운영을 위한 시설 및 인프라 구축 필요 정치적 갈등 및 통과 가능성 야당 반대: 야당의 강력한 반대와 국회 통과 가능성 의문 사회적 합의 부족: 충분한 사회적 합의 없이 추진되는 개편 정치적 목적성: 정치적 목적에 의한 개편이라는 비판 권력분립 관점에서의 의미 검찰청 폐지와 개편은 삼권분립의 원칙을 더욱 명확하게 하는 중요한 변화입니다. 기존에는 행정부 소속이면서도 사법적 권한을 행사하던 검찰의 모호한 지위를 명확히 분리하여, 각 권력기관의 역할과 책임을 더욱 명확히 하려는 시도로 볼 수 있습니다. 이러한 개편을 통해 수사권은 행정부(행정안전부)에, 기소권은 행정부(법무부)에 배치하여 권력의 균형을 맞추고, 국가수사위원회를 통해 독립적인 감독 체계를 구축하려는 의도로 보입니다. 한국의 현재 정치상황과 삼권분립 침해 우려 현재 한국 정치의 구조적 문제 여당의 압도적 다수와 견제 기능 상실 행정부-입법부 일체화: 더불어민주당이 행정부(대통령)와 입법부(국회)를 동시에 장악 견제 기능 약화: 동일 정당 소속으로 인한 상호 견제 기능 상실 권력 집중: 한 정당이 모든 권력을 독점하는 상황 사법부의 독립성 의문 정치적 임명: 행정부와 입법부에서 임명한 판사들이 사법부 다수 차지 독립성 훼손: 정치적 영향력이 사법부에 미칠 가능성 삼권분립 원칙 위배: 사법부의 정치적 중립성 확보 어려움 검찰청 폐지와 권력분립 침해 우려 견제 기관의 소멸 검찰의 역할: 그동안 행정부 소속이지만 독립적인 수사와 기소를 통해 권력 남용 견제 견제 기능 상실: 검찰청 폐지로 인한 권력 견제 기관 소멸 권력 남용 가능성: 견제할 대상이 없어져 권력 남용 위험 증가 이재명 대통령의 사법 리스크와의 연관성 검찰조사 중단 상황 당선 전: 검찰조사를 받던 중 대통령 당선 당선 후: 검찰조사가 중단된 상태 사법적 특혜 의혹: 정치적 지위로 인한 사법적 특혜 가능성 관련 사법 리스크 공직선거법 위반: 과거 공직선거법 위반 혐의로 재판 진행 위증교사 혐의: 위증교사 혐의로 법정 대응 허위사실공표: 2022년 9월 검찰 소환 통보 경험 1심 선고: 2024년 11월 1심 선고를 앞두고 정치권 긴장 정치적 목적성 의혹 사법 리스크 해소: 대통령의 법적 문제를 무마하려는 의도 의혹 권력 보호: 현 정권의 권력을 보호하기 위한 조치라는 비판 민주주의 훼손: 삼권분립 원칙을 의도적으로 훼손하려는 시도라는 지적 삼권분립 침해의 심각성 권력의 집중과 남용 견제 메커니즘 붕괴: 각 권력기관 간 견제 기능 상실 독재적 권력: 한 정당이 모든 권력을 독점하는 상황 민주주의 후퇴: 민주주의의 핵심 원리인 권력분립 원칙 훼손 법치주의 신뢰 저하 사법적 특혜: 정치적 지위에 따른 사법적 특혜 가능성 법 앞의 평등: 모든 시민이 법 앞에 평등하다는 원칙 훼손 정치적 사법: 사법부의 정치적 중립성 상실 장기적 민주주의 위험 제도적 훼손: 민주주의 제도의 근본적 훼손 권력 순환 불가: 정당 간 권력 순환의 어려움 시민 권리 침해: 시민의 자유와 권리 보호 기능 약화 결론: 신중한 접근의 필요성 현재 한국의 정치상황은 삼권분립의 원칙이 심각하게 훼손되고 있는 상황입니다. 여당의 압도적 다수로 인한 견제 기능 상실, 사법부의 독립성 의문, 그리고 검찰청 폐지 논의까지 더해져 권력분립의 원칙이 더욱 약화될 위험이 있습니다. 특히 이재명 대통령의 사법 리스크와 검찰청 폐지 논의의 시기적 일치로 인해, 이 개편이 정치적 목적에 의한 것이라는 의혹이 제기되고 있습니다. 이러한 상황에서 검찰청 폐지와 같은 중대한 개혁은 충분한 사회적 합의와 신중한 검토를 거쳐 추진되어야 하며, 권력기관 간의 견제와 균형을 회복하는 방향으로 나아가야 할 것입니다.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/dimension_reduction/what-is-umap",
    "title": "UMAP 에 대해 정리\"",
    "date": "2025-09-11",
    "excerpt": "UMAP에 대해 이해해보기",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1: 블로그 2: 유튜브 --- Unifor Manifold Approximation and Projection",
    "public": true
  },
  {
    "id": "Data Science/Statistics/dimension_reduction/what-is-t-sne",
    "title": "t-SNE 에 대해 정리\"",
    "date": "2025-09-11",
    "excerpt": "T-SNE에 대해 이해해보기",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "T-distributed Stochastic Neighbor Embedding 고차원의 데이터를 2차원이나 3차원으로 축소해 시각화하기 위해 사용하는 방법론. 비슷한 데이터 포인트는 가깝게, 다른 데이터 포인트는 멀게 배치하는 것 고차원 공간에서 각 데이터 포인트 쌍의 조건부 확률을 계산해. 이 확률은 한 점이 다른 점을 이웃으로 선택할 확률이라고 생각하면 돼. 저차원 공간에서도 비슷한 확률 분포를 만들고, 두 분포 간의 차이(KL 발산)를 최소화하는 방향으로 저차원 데이터 포인트들을 배치해. 비선형적이다. PCA vs t-SNE <figure> <img src=\"/post/DataScience/pca-vs-t-sne.png\" alt=\"PCA vs t-SNE\" width=\"100%\"/> <figcaption>그림1. PCA vs t-SNE</figcaption> </figure> 차이를 이해하고 있어야 언제, 어떤 걸 사용할지 결정할 수 있다. PCA는 데이터의 분산을 최대한 유지하는 축을 찾고, 이를 이용해 새로운 축을 만드는 개념. 선형적이고, 해석이 쉬움. t-SNE는 고차원에서의 분포 간의 차이를 최대한 유지하면서 차원을 줄이는 개념. 비선형적이고 해석이 주관적(?)임. t-SNE 이론적 내용 핵심 아이디어 t-SNE는 고차원 공간에서의 데이터 포인트 간의 유사성을 저차원 공간에서도 유지하려고 합니다. 이를 위해 확률적 접근을 사용하여 각 점이 다른 점을 이웃으로 선택할 확률을 계산합니다. 수학적 정의 고차원 공간에서의 유사성 고차원 공간에서 두 점 $x_i$와 $x_j$ 사이의 조건부 확률을 다음과 같이 정의합니다: $$ p_{j i} = \\frac{\\exp(- x_i - x_j ^2 / 2\\sigma_i^2)}{\\sum_{k \\neq i} \\exp(- x_i - x_k ^2 / 2\\sigma_i^2)} $$ 그리고 대칭성을 위해: $$ p_{ij} = \\frac{p_{j i} + p_{i j}}{2n} $$ 여기서: $\\sigma_i$: 각 점 $x_i$에 대한 perplexity에 의해 결정되는 가우시안 분포의 표준편차 $n$: 데이터 포인트의 개수 저차원 공간에서의 유사성 저차원 공간에서는 t-분포를 사용합니다: $$ q_{ij} = \\frac{(1 + y_i - y_j ^2)^{-1}}{\\sum_{k \\neq l} (1 + y_k - y_l ^2)^{-1}} $$ 여기서 $y_i$는 저차원 공간에서의 점입니다. 목적 함수 t-SNE는 두 분포 $P$와 $Q$ 사이의 Kullback-Leibler divergence를 최소화합니다: $$ C = \\sum_i KL(P_i Q_i) = \\sum_i \\sum_j p_{ij} \\log \\frac{p_{ij}}{q_{ij}} $$ 알고리즘 과정 고차원 공간에서 유사성 계산: 각 점에 대해 perplexity를 설정하고 $p_{ij}$ 계산 저차원 공간 초기화: 랜덤하게 저차원 점들을 초기화 경사 하강법으로 최적화: KL divergence를 최소화하도록 저차원 점들을 업데이트 t-분포를 사용하는 이유 고차원에서는 가우시안 분포를, 저차원에서는 t-분포를 사용하는 이유: Crowding Problem 해결: t-분포는 꼬리가 더 두꺼워서 중간 거리의 점들 사이에 더 많은 공간을 제공 지역적 구조 보존: 가까운 점들은 더 가깝게, 먼 점들은 더 멀게 배치 주요 특징 장점: 복잡한 비선형 구조를 잘 보존 시각화에 매우 효과적 클러스터링 패턴을 명확하게 보여줌 단점: 계산 비용이 높음 (O(n²)) 결과가 재현 가능하지 않음 (랜덤 초기화) 전역적 구조보다 지역적 구조에 특화 perplexity 하이퍼파라미터에 민감 언제 사용할까? t-SNE 사용: 데이터 시각화, 클러스터링 패턴 탐색, 복잡한 비선형 구조 분석 PCA 사용: 차원 축소, 노이즈 제거, 빠른 계산이 필요한 경우",
    "public": true
  },
  {
    "id": "Data Science/Statistics/dimension_reduction/what-is-lda",
    "title": "LDA에 대해 정리",
    "date": "2025-09-11",
    "excerpt": "Linear Discriminant Analysis에 대해 정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1: ratsgo --- 차원 축소 방법 중 하나. 차원 축소의 목적은 정보의 양을 최대한 보존하면서 축을 줄이는 것. LDA에서는 정보의 양을 클래스끼리 구분되는 정도를 정보의 양이라고 정의하고 있다. PCA에서는 \"분산\"을 정보의 양이라고 정의. <figure> <img src=\"/post/DataScience/LDA.png\" alt=\"LDA\" width=\"100%\"/> <figcaption>그림1. LDA vs PCA</figcaption> </figure> LDA에서는 클래스 간의 구분이 최대가 되는 축을 찾는 것을 목표로 함. <figure> <img src=\"/post/DataScience/LDA2.png\" alt=\"LDA\" width=\"100%\"/> <figcaption>그림2. LDA 예시 그림</figcaption> </figure> 클래스 간의 구분이 최대가 된다, 는 말을 어떻게 정의할 수 있을까? <figure> <img src=\"/post/DataScience/LDA3.pngg\" alt=\"LDA\" width=\"100%\"/> <figcaption>그림3. LDA 예시 그림</figcaption> </figure> > 참고 1. > 그렇다면 두 범주를 잘 구분할 수 있는 직선은 어떤 성질을 지녀야 할까요? > 사영 후 두 범주의 중심(평균)이 서로 멀도록, 그 분산이 작도록 해야할 겁니다. > 왼쪽 그림을 오른쪽과 비교해서 보면 왼쪽 그림은 사영 후 두 범주 중심이 가깝고, 분산은 커서 데이터가 서로 잘 분류가 안되고 있는 걸 볼 수가 있습니다. > 반대로 오른쪽 그림은 사영 후 두 범주 중심이 멀고, 분산은 작아서 분류가 비교적 잘 되고 있죠. LDA는 바로 이런 직선을 찾도록 해줍니다. 두 범주의 중심이 서로 멀고, 분산이 작도로. 이론적 내용 LDA의 목표 LDA(Linear Discriminant Analysis)는 클래스 간의 분산을 최대화하고 클래스 내 분산을 최소화하는 선형 변환을 찾는 것이 목표입니다. 이를 통해 클래스들을 가장 잘 구분할 수 있는 새로운 축을 찾습니다. 수학적 정의 데이터가 $n$개의 샘플과 $p$개의 특성을 가진다고 할 때, LDA는 다음과 같이 정의됩니다: 클래스 간 분산 행렬 (Between-class scatter matrix): $$ S_B = \\sum_{i=1}^{c} n_i (\\mu_i - \\mu)(\\mu_i - \\mu)^T $$ 클래스 내 분산 행렬 (Within-class scatter matrix): $$ S_W = \\sum_{i=1}^{c} \\sum_{x \\in C_i} (x - \\mu_i)(x - \\mu_i)^T $$ 여기서: $c$: 클래스의 개수 $n_i$: $i$번째 클래스의 샘플 수 $\\mu_i$: $i$번째 클래스의 평균 벡터 $\\mu$: 전체 데이터의 평균 벡터 $C_i$: $i$번째 클래스의 샘플 집합 최적화 문제 LDA는 다음 비율을 최대화하는 벡터 $w$를 찾습니다: $$ J(w) = \\frac{w^T S_B w}{w^T S_W w} $$ > 클래스끼리의 구분은 커지게 하고, 클래스 내부의 분산은 작게 하도록 목적함수 정의. 이를 해결하기 위해 일반화된 고유값 문제를 풀면 됩니다: $$ S_B w = \\lambda S_W w $$``` 해법 최적의 투영 벡터들은 $S_W^{-1} S_B$의 고유벡터들입니다. 고유값이 큰 순서대로 정렬하여 상위 $k$개의 고유벡터를 선택하면 $k$차원으로 축소된 데이터를 얻을 수 있습니다. PCA와의 차이점 PCA: 데이터의 분산을 최대화 (비지도 학습) LDA: 클래스 간 분리도를 최대화 (지도 학습) LDA는 클래스 레이블 정보를 활용하여 클래스들을 가장 잘 구분할 수 있는 방향으로 데이터를 투영합니다.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/dimension_reduction/what-is-pca",
    "title": "PCA에 대해 정리",
    "date": "2025-09-10",
    "excerpt": "PCA란 무엇인가",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1 : Lee, MinYe 블로그 --- PCA는 dimension reduction을 하기 위한 방법. dimension reduction의 목적은 정보를 최대한 보존하면서 축을 줄이는 것이다. \"정보\"를 어떻게 정의하는지에 따라 다양한 방법들이 존재한다. PCA에서는 \"분산\"을 \"정보의 양\"이라고 정의하고, 분산이 최대한 보존되는 축을 찾는 것을 목표로 한다. 목표가 다르다면 다른 방법을 써야 함. <figure> <img src=\"/post/DataScience/LDA.png\" alt=\"LDA\" width=\"100%\" /> <figcaption>그림1. LDA 예시 그림</figcaption> </figure> --- > 이번 장에서 소개해드리는 변수 추출 방법인 PCA 는 데이터의 분산을 데이터의 특성으로 정의하고 이를 최대한 보존하기 위한 방법론 입니다. > PCA 는 특정 기저에 모든 데이터를 사영되었을 시, 사영된 데이터의 퍼진 정도가 최대인 기저를 찾습니다. (참고 1) <figure> <img src=\"/post/DataScience/PCA.png\" alt=\"PCA\" width=\"100%\" /> <figcaption>그림2. PCA 예시 그림</figcaption> </figure> $$ \\max_w w^T S w, \\quad \\text{s.t.} \\quad w^T w = 1 \\tag{1} $$ $S$ : covariance matrix $w^T w = 1 $ : 단위 벡터로 제한하여 방향에만 집중 이 최적화 문제의 해는 eigenvalue decomposition을 통해 구할 수 있다. --- Eigenvalue와 Eigenvector 공분산 행렬 $S$에 대해 다음을 만족하는 벡터 $v$와 스칼라 $\\lambda$가 존재한다: $$ S v = \\lambda v \\tag{2} $$ 여기서: $v$: eigenvector (고유벡터) $\\lambda$: eigenvalue (고유값) --- PCA와 Eigenvalue의 연결 수식 (1)에서 $w$는 우리가 찾고자 하는 방향 벡터(변수)이고, 이 최적화 문제의 *해 $w^$가 바로 공분산 행렬 $S$의 eigenvector가 된다. 구체적으로: $w$: 최적화 문제의 변수 (방향 벡터) $w^$: 최적해 = $S$의 가장 큰 eigenvalue에 해당하는 eigenvector 즉, 분산을 최대화하는 방향은 데이터의 주된 변동 방향과 일치하며, 이는 공분산 행렬의 첫 번째 eigenvector로 구할 수 있다. --- Eigenvector의 의미 Eigenvector는 기존 축의 선형조합이 아니라, 새로운 좌표계의 축이다. 예를 들어, 2차원 데이터 $(x_1, x_2)$가 있을 때: 기존 축: $x_1$축, $x_2$축 (원래 좌표계) Eigenvector: $v_1 = [a, b]^T$, $v_2 = [c, d]^T$ (새로운 좌표계의 축) PCA는 기존 데이터를 이 새로운 eigenvector 축들로 투영(projection)시키는 것이다. $$ \\begin{bmatrix} x_1' \\\\ x_2' \\end{bmatrix} = \\begin{bmatrix} a & c \\\\ b & d \\end{bmatrix}^T \\begin{bmatrix} x_1 \\\\ x_2 \\end{bmatrix} $$ 여기서: $(x_1, x_2)$: 기존 좌표계에서의 원본 데이터 $(x_1', x_2')$: 새로운 eigenvector 좌표계로 투영된 데이터 투영 과정: 원본 데이터를 eigenvector 방향으로 사영하여 새로운 좌표를 얻음 --- 최적화 과정: Lagrangian Multiplier 수식 (1)의 제약조건이 있는 최적화 문제를 풀기 위해 Lagrangian Multiplier를 사용한다. Lagrangian 함수 $$ L = w^T S w - \\lambda(w^T w - 1) \\tag{3} $$ 여기서: $w$: 최적화 변수 (방향 벡터) $\\lambda$: Lagrangian multiplier $w^T w - 1 = 0$: 제약조건 (벡터의 크기가 1) 최적화 조건 $w$에 대해 편미분하여 0으로 놓으면: $$ \\frac{\\partial L}{\\partial w} = 0 \\Rightarrow 2Sw - 2\\lambda w = 0 $$ $$ (S - \\lambda I)w = 0 \\tag{4} $$ Eigenvalue 문제로 변환 수식 (4)는 정확히 eigenvalue 문제 $S w = \\lambda w$와 동일하다! 따라서: 최적해 $w^$: 공분산 행렬 $S$의 eigenvector 최적값*: 해당 eigenvector의 eigenvalue 결론 분산을 최대화하는 방향 $w^$는 공분산 행렬 $S$의 가장 큰 eigenvalue에 해당하는 eigenvector이다.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-09-08-about-target",
    "title": "스스로 생각하는 나의 한계와 목표에 대해서",
    "date": "2025-09-08",
    "excerpt": "매너리즘에서 빠져나오게 해준 글과 이에 대한 생각 정리",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "참조한 글 1: Aim High , 링크드인에서 발견한 하이퍼커넥트 직원 분이 올려주신 글 --- 1~2달정도 굉장히 나태하게 지낸 것 같다. 그 전에는 회사에서 프로젝트 성공적으로 끝내려고 노력했고, 좋은 결과를 얻었다. 퇴근 후에는 사이드 프로젝트나 개인 공부를 꾸준히 해왔었다. 하지만 최근에는 번아웃인지는 모르겠지만, 푹 퍼져있는 느낌으로 지넀다. 퍼져있는게 행복했다면 다행이지만, 막연한 불안함에 행복하지도 않았다... ㅎ.ㅎ 그 이유를 생각해보면, 회사일에 흥미가 떨어졌고 이직을 준비하고 있지만 쉽지 않은 과정에서 도피를 한 것 같다. 회사에서 성과를 이룰 수 없다고 생각하는데, 그렇다고 이직도 어려우니 목표가 사라져버리는 느낌을 받은 것 같다. 그러면서 매너리즘에 빠진 것 같은데, 이런 생각이 오늘 아침에 깨졌다. 1번 글에서 나한테 도움되는 내용을 찾을 수 있었다. --- 회사에서 더 이상 성취할 수 없다. 이 생각의 기저에는 회사의 목표 수준이 낮다는데에 있다. 목표가 애매한 프로젝트가 대부분이고 거기서 원하는 목표도 낮다. 하지만 사실 이 생각은 틀렸다. 사실상 내가 프로젝트를 이끌어가고 있고, 이때 목표는 내가 정할 수 있다. 더 정확한 문제는 스스로 생각하는 목표수준이 낮다는데에 있다. 이만하면 됐지라며 끝냈던 경험을 떠올려보자. > 우리가 운동 할 때에 코치를 두는 이유는 이러한 틀을 체계적으로 부숴줄 수 있기 때문입니다. > 조직은 그렇기 때문에 이러한 틀을 부숴주는 높은 기준을 요구하는 환경을 제공할 수 있도록 노력해야 합니다. > 저희가 늘 리터러쳐 서베이를 100편씩 일주일이면 할 수 있어야 한다고 이야기하는 이유는 이를 해본 적이 없는 사람은 상상조차 하지 못하지만 막상해보고 숙달되면 실제로 할 수 있는 그런 종류의 작업이기 때문입니다. 높은 기준을 요구하는 조직에 있지 않더라도, 스스로 요구할 수 있다. 회사가 이렇기 때문에, 같이 일하는 동료가 이렇기 때문에라고 말하는 건 사실 전부 핑계다. 내가 좋아하는 \"박재훈\"이라는 보디빌더는 코치없이 세계적인 무대에서 성적을 내고 있다. 일할 때 스스로 더 높은 목표를 세워보고 성취해보자. 그런 기준은 만들고 높여나가 보자. --- 안될 것 같은 불안함 나 스스로의 행동패턴을 최근에 발견했다. 불안하고 고통스러울 때 회피하고 도파민만을 쫒고 있다. 어떤 목표가 있을 때는 그걸 달성하기 위한 작은 목표들을 세우고, 그 목표들을 하나씩 달성해나가는데에 집중해야 한다. 그렇지 않는다면 평생 제자리에 있을 것이다. 이런 행동패턴을 바꾸는 건 일종의 훈련이 필요한 영역이라고 생각이 든다. 스스로 높은 목표를 설정하고, 이를 달성하기 위한 노력을 하는 것과 같은 맥락이다. 뛰어난 사람들을 보면 이런 행동패턴들이 보인다. 자연스럽게 습득이 된 것이던, 의식적으로 만든 것이던 공통적으로 보인다. 스스로 훈련을 통해 이런 패턴을 만들어보자.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Resumes/25-09-08-carrot-ml-engineer",
    "title": "당근 - ML Engineer",
    "date": "2025-09-08",
    "excerpt": "이력서 , 경력기술서 적어보기",
    "category": "Career",
    "tags": [
      "이력서"
    ],
    "content": "채용공고 지원한 이력서 --- 하 내 이력서에 대한 자신이 너무 없네... ㅠㅠ 그래도 최대한 비벼보자. 하이퍼 커넥트에 지원한 내용에서 바꿀게 있을까? 3개의 ML 서비스 운영 경험. 그 과정에서 했던 고민들 정리해서 적기 + 포트폴리오. 기존에 썻던 내용을 깔끔하게만 정리해서 제출 ---",
    "public": false
  },
  {
    "id": "Self_Development/Career/Job Transition/25-09-08-당근-ML-Engineer",
    "title": "당근마켓 - Software Engineer, Machine Learning",
    "date": "2025-09-08",
    "excerpt": "채용공고 정리 -> 지원전략 정리",
    "category": "Career",
    "tags": [
      "당근마켓",
      "ML Engineer",
      "채용공고"
    ],
    "content": "링크드인 링크 지원 페이지 링크 --- Software Engineer, Machine Learning 회사: 당근마켓 고용형태: 정규직 경력: 경력/신입 직무: Software Engineer, Machine Learning 📋 직무 소개 당근의 Software Engineer, Machine Learning은 머신러닝을 활용해서 당근의 다양한 서비스들을 고도화하고 당근 사용자들의 만족을 끌어올려요. 더 나아가 동네 주민들의 연결을 강화하여 따뜻한 지역 사회를 만드는 것에 기여해요. 홈 피드, 상세 페이지 등에 개인화된 흥미로운 콘텐츠를 추천 누구나 쉽고 간편하게 광고를 통해서 효율적으로 동네 주민들에게 홍보할 수 있도록 지원 🚀 주요 업무 피드 품질팀 피드에서의 사용자 Engagement, 다양하고 우연한 발견을 촉진하기 위한 ML 모델들을 개발하고 개선 데이터를 분석하고 분석된 데이터에 기반해 가설을 세워 이론적 근거에 기반한 개선점을 찾기 온라인 실험으로 검증하고 실질적인 서비스 임팩트를 만들기 유저와 컨텐츠를 깊게 이해하기 위한 모델들을 개발하고 개선 LLM을 다양한 방식으로 활용하여 개인화 추천을 개선 광고추천팀 광고 추천과 랭킹을 위한 CTR, CVR 예측 모델과 자동 입찰, 타겟팅, 페이싱 모델을 개발하고 개선 사용자 행동 데이터와 광고 성과를 분석하고, 데이터 기반의 가설을 세워 A/B 테스트로 검증 광고 효율과 사용자 경험을 동시에 향상 개인화를 고려한 후보 광고 추출과 광고 랭킹 모델을 통해 사용자에게 의미 있는 광고를 추천 LLM과 최신 AI 기술을 활용하여 광고 타겟팅의 정확도를 높이고, 사용자와 광고 콘텐츠를 더 깊이 이해 > 🥕 당근의 성공을 견인하는 첫 화면의 비밀 👥 자격 요건 필수 요건 머신러닝 이론과 기본기가 탄탄하신 분 딥러닝에 대한 깊은 이해가 있으신 분 (Recommendation, NLP, Graph Neural Net, Reinforcement Learning, Vision 중 하나 이상) 효율적인 코드 아키텍처를 구상하고 가독성 뛰어난 코드 작성에 경험이 있으신 분 우대 사항 데이터 드리븐 환경에서 머신러닝을 통해 사용자 서비스에 실질적인 임팩트를 만들어 보신 분 최신 머신러닝, 딥러닝 트렌드에 관심이 있고 관련 논문 읽는 것을 즐기시는 분 ML 기반 추천 시스템, 광고 시스템에 경험이 있으신 분 머신러닝 모델 및 시스템을 large-scale 프로덕션 환경에 구축하고 사용해 보신 분 BigQuery, Cloud Dataflow, Kubeflow, TFX, TF Serving에 경험이 있으신 분 ⚠️ 참고사항 화상 면접에서 라이브 코딩 테스트와 간단한 ML 기초 지식 테스트가 진행될 예정이에요 면접 전 원활한 테스트를 위한 PC 환경 조성을 부탁드리며, 지원자분께 편하고 익숙한 장비를 사용하시길 권장드려요 직무 면접에서는 ML 모델 설계를 주제로 면접이 진행될 예정이에요 정규직 채용의 경우 3개월의 수습기간이 있어요 25년 10월부터 주 5일 오피스 출근으로 근무 제도가 변경될 예정이에요 📝 채용 프로세스 서류 전형 → 화상 인터뷰 → 직무 인터뷰 → 컬처핏 인터뷰 및 레퍼런스 체크 → 처우협의 → 최종 합격 및 입사",
    "public": false
  },
  {
    "id": "Self_Development/Career/25-09-08-mindset",
    "title": "지원자가 가져야하는 마음가짐",
    "date": "2025-09-08",
    "excerpt": "지원자가 면접을 대할 때 가져야하는 마음가짐, 태도",
    "category": "Career",
    "tags": [
      "면접준비"
    ],
    "content": "참고자료 1: Linkedin - Eunjoo Kim --- 정말 자주 읽어보고 곱씹어봐야할 글이다. (1) 지원자가 면접에 들어갈 때 어떤 마음가짐으로 들어가야하는지, 어떤 모습을 보여주는게 좋은지 잘 정리되어있다. 내가 고려하지 못했던 부분들이 많이 알 수 있게 되었다. > 3) 면접 주도 : 면접관처럼 생각하기 > 대부분의 지원자는 자신을 질문을 받는 사람으로 여기고 수동적으로 면접에 임한다. 그래서 면접관이 대화를 이끌어 가길 기다린다. 그런데, 생각을 바꾸어 스스로를 인터뷰 진행자라고 생각하는 게 좋다. > 면접관은 나에 대해 전혀 아는 것이 없고, 나의 이력서를 보지도 않았고, 나와의 대화가 준비 되어 있지 않은 사람이라고 전제를 하는 게 좋다. > 나를 뽑으려는 기업의 입장에서, 나를 평가해야 하는 면접관의 입장에서, 나라는 사람을 어떻게 이해해야 하는지 대화를 끌고 가도록 한다. 그 중에서 이 부분이 가장 기억에 남는다. 면접을 준비할 때 질문에 대해 잘 대답할 수 있게만 준비하고 생각했었는데, 면접하는 과정을 어떻게 잘 이끌어갈 수 있을지도 고민해보면 좋을 것 같다. 왜냐하면 질문에만 대답하는 수동적인 자세로는 나를 온전히 표현할 수 없고, 내가 대단히 매력적인 이력서를 가지고 있지 않기 때문에 나의 경쟁력을 적극적으로 표현할 필요가 있기 때문이다. 긴장한 티를 내며 자신감없게 쭈뼛대기 보다는, 밝고 자신감있는 모습으로 면접에 임할 필요가 있겠다. 이것도 하나의 능력이고 내 연봉에 큰 영향을 줄 요인이다. 훈련이 필요한 영역으로 생각이 되며, 꾸준히 연습해보자.",
    "public": true
  },
  {
    "id": "Data Science/Tech Interview/about_system_design",
    "title": "System Design 관련 인터뷰 준비",
    "date": "2025-09-08",
    "excerpt": "",
    "category": "Data Science",
    "tags": [
      "Interview"
    ],
    "content": "참고자료 1: 링크드인 - Shantanu Ladhwe --- 87% of AI engineers fail system design interviews. The gap isn't code, it's thinking at scale. This will help you 👇 Most crack under these bottlenecks: 🔸 Feature thinking vs. failure planning 🔸 Zero production battle scars 🔸 Can't architect for 10M users These 4 resources fix your foundation: System Design Primer → https://lnkd.in/gmAg4nBb → Master the patterns that matter Complete System Design → https://lnkd.in/gTJmDMYD → Real architectures, not toy examples Stanford MLSys Seminars → https://lnkd.in/gP3UDqwn → How Netflix/Uber actually scale AI Engineering (Chip Huyen) → https://lnkd.in/gp-WeZte → Modern systems for AI workloads The difference? Most engineers optimize for features. System architects optimize for failures. And very much applies to AI Agents as well! Start with #1. Build battle-tested thinking. What's your biggest system design blind spot? ♻️ Repost to help someone ace their next architecture interview. ➕ Follow me, Shantanu for production AI/ML/MLOps & careers ➕ Join 20.500+ AI/ML builders here: https://lnkd.in/ds_SzEUH",
    "public": true
  },
  {
    "id": "Data Science/Tech Interview/about_statistics",
    "title": "통계 관련 인터뷰 준비",
    "date": "2025-09-08",
    "excerpt": "",
    "category": "Data Science",
    "tags": [
      "Interview"
    ],
    "content": "참고자료 1: ai 테크 인터뷰 --- 고유값(eigen value)와 고유벡터(eigen vector)이 무엇이고 왜 중요한지 설명해주세요. 샘플링(Sampling)과 리샘플링(Resampling)이 무엇이고 리샘플링의 장점을 말씀해주세요. 확률 모형과 확률 변수는 무엇인가요? 누적 분포 함수와 확률 밀도 함수는 무엇인가요? 수식과 함께 표현해주세요. 조건부 확률은 무엇인가요? 공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요. 신뢰 구간의 정의는 무엇인가요? p-value를 모르는 사람에게 설명한다면 어떻게 설명하실 건가요? R square의 의미는 무엇인가요? 평균(mean)과 중앙값(median)중에 어떤 케이스에서 뭐를 써야할까요? 중심극한정리는 왜 유용한걸까요? 엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요. 어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요? “likelihood”와 “probability”의 차이는 무엇일까요? 통계에서 사용되는 bootstrap의 의미는 무엇인가요. 모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요? 베이지안과 프리퀀티스트 간의 입장차이를 설명해주실 수 있나요? 검정력(statistical power)은 무엇일까요? missing value가 있을 경우 채워야 할까요? 그 이유는 무엇인가요? 아웃라이어의 판단하는 기준은 무엇인가요? 필요한 표본의 크기를 어떻게 계산합니까? Bias를 통제하는 방법은 무엇입니까? 로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요. 베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포에 대해 설명해주세요. 그리고 분포 간의 연관성도 설명해주세요. 출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?",
    "public": true
  },
  {
    "id": "Data Science/Tech Interview/about_ML",
    "title": "ML 관련 인터뷰 준비",
    "date": "2025-09-08",
    "excerpt": "",
    "category": "Data Science",
    "tags": [
      "Interview"
    ],
    "content": "참고자료 1: ai 테크 인터뷰 --- 알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...) regression task -> RMSE, MAE 등이 있겠고 classification task -> recall, precision, ROC-AUC 등이 있겠다. 먼저 regression task의 경우, RMSE와 MAE가 가장 많이 사용됩니다. RMSE는 Root Mean Square Error로, 예측값과 실제값의 차이를 제곱해서 평균을 구한 후 제곱근을 취한 값입니다. 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. MAE는 Mean Absolute Error로, 절댓값 차이의 평균을 구하는데, 이상치에 덜 민감합니다. 그래서 이상치가 많은 데이터에서는 MAE가 더 적절할 수 있어요. classification task에서는 상황에 따라 다양한 metric을 사용합니다. 가장 기본적인 accuracy는 전체 예측 중 맞춘 비율인데, 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. precision은 양성으로 예측한 것 중 실제로 양성인 비율이고, recall은 실제 양성 중 올바르게 찾아낸 비율입니다. precision이 중요한 경우는 스팸 메일 분류처럼 false positive 비용이 클 때이고, recall이 중요한 경우는 의료 진단처럼 false negative 비용이 클 때입니다. F1-score는 precision과 recall의 조화평균으로, 두 지표를 균형있게 고려하고 싶을 때 사용해요. ROC-AUC는 ROC 곡선 아래 면적을 의미하는데, 다양한 threshold에서의 모델 성능을 종합적으로 평가할 수 있어서 이진 분류에서 많이 사용됩니다. 실제 프로젝트에서는 비즈니스 요구사항과 데이터 특성을 고려해서 적절한 metric을 선택하는 것이 중요합니다. 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요? 과적합을 피하기 위해서. lasso, ridge처럼 모델의 가중치를 loss에 포함시켜서 과적합 되지 않도록 유도한다. tree계열에서도 마찬가지로 정규화를 사용함. 정규화는 주로 과적합을 방지하기 위해 사용합니다. 모델이 훈련 데이터에만 너무 잘 맞춰져서 새로운 데이터에 대한 일반화 성능이 떨어지는 것을 막기 위해서죠. 정규화 방법으로는 크게 두 가지가 있습니다. 첫 번째는 L1 정규화, 즉 Lasso입니다. 이는 가중치의 절댓값의 합을 loss function에 추가하는 방식으로, 불필요한 feature들을 0으로 만들어서 자동으로 feature selection 효과를 가져옵니다. 두 번째는 L2 정규화, 즉 Ridge입니다. 이는 가중치의 제곱합을 loss function에 추가하는 방식으로, 가중치들을 0에 가깝게 만들어서 모델을 단순화시킵니다. Lasso와 달리 feature selection은 하지 않고 모든 feature를 유지하면서 가중치만 줄여줍니다. Elastic Net은 L1과 L2를 결합한 방식이고, Dropout은 딥러닝에서 랜덤하게 일부 뉴런을 비활성화시키는 방법입니다. Tree 계열 모델에서도 정규화를 사용하는데, 예를 들어 XGBoost에서는 max_depth, min_child_weight, gamma 등의 파라미터로 모델 복잡도를 제어하고, Random Forest에서는 max_depth나 min_samples_split 같은 파라미터로 과적합을 방지합니다. 실제로는 데이터의 크기, feature의 개수, 모델의 복잡도 등을 고려해서 적절한 정규화 방법을 선택하는 것이 중요합니다. Local Minima와 Global Minimum에 대해 설명해주세요. 지역적 최솟값과 전역적 최솟값을 의미하며, 최솟값이 부분적으로 최소인지 전체에서 최소인지를 구분. global minima는 local minima 중 가장 작은 값. 네, Local Minima와 Global Minimum은 최적화 문제에서 중요한 개념입니다. Local Minima는 지역적 최솟값으로, 특정 구간에서 가장 작은 값을 의미합니다. 즉, 그 점 주변의 작은 영역에서는 더 작은 값이 없지만, 전체 함수를 봤을 때는 더 작은 값이 존재할 수 있어요. Global Minimum은 전역적 최솟값으로, 전체 함수 영역에서 가장 작은 값을 의미합니다. 모든 가능한 점들 중에서 가장 작은 값을 가지는 지점이죠. 간단한 예로 설명하면, 산맥을 생각해보시면 됩니다. 여러 개의 골짜기가 있는데, 각 골짜기 바닥이 Local Minima이고, 그 중에서 가장 낮은 골짜기가 Global Minimum입니다. 머신러닝에서 이 문제가 중요한 이유는, Gradient Descent 같은 최적화 알고리즘이 Local Minima에 빠져서 Global Minimum을 찾지 못할 수 있기 때문입니다. 특히 딥러닝에서는 수많은 Local Minima가 존재하는데, 다행히도 대부분의 Local Minima들이 비슷한 성능을 보인다는 연구 결과도 있습니다. 이를 해결하기 위해 다양한 기법들을 사용하는데, momentum, learning rate scheduling, 여러 초기값으로 시작하기, 또는 simulated annealing 같은 방법들이 있습니다. 차원의 저주에 대해 설명해주세요. 차원이 많이 질수록 sparse해진다는 것. 차원의 저주는 차원이 증가할수록 발생하는 여러 문제들을 통칭하는 개념입니다. 가장 핵심적인 문제는 데이터의 희소성(sparsity)입니다. 차원이 늘어날수록 같은 데이터 개수로는 공간을 채우기 어려워져요. 예를 들어, 1차원에서는 10개의 점으로 선을 채울 수 있지만, 10차원에서는 같은 10개의 점으로는 공간의 극히 일부분만 커버하게 됩니다. 이로 인해 몇 가지 구체적인 문제들이 발생합니다. 첫째, 근접성의 의미가 사라집니다. 고차원에서는 모든 점들이 거의 비슷한 거리에 있게 되어서, \"가까운\" 점과 \"먼\" 점을 구분하기 어려워져요. 둘째, 과적합이 쉽게 발생합니다. 차원이 많을수록 모델이 복잡해지고, 같은 데이터 개수로는 충분한 학습이 어려워집니다. 셋째, 계산 복잡도가 기하급수적으로 증가합니다. 차원이 하나씩 늘어날 때마다 필요한 계산량이 폭발적으로 증가해요. 실제로는 차원 축소 기법들로 이 문제를 해결합니다. PCA, t-SNE, UMAP 같은 방법들로 중요한 정보는 유지하면서 차원을 줄이거나, feature selection을 통해 불필요한 차원을 제거하는 방식으로 접근합니다. dimension reduction기법으로 보통 어떤 것들이 있나요? 차원 축소 기법은 크게 선형과 비선형 방법으로 나뉩니다. 가장 기본적이고 널리 사용되는 방법은 PCA(Principal Component Analysis)입니다. PCA는 데이터의 분산이 가장 큰 방향으로 새로운 축을 찾아 차원을 축소하는 선형 방법으로, 주성분을 통해 원본 데이터의 정보를 최대한 보존하면서도 해석이 용이하고 계산이 빠릅니다. 주로 이미지 압축, 노이즈 제거, 시각화 등에 사용됩니다. 선형 방법 중에는 LDA(Linear Discriminant Analysis)도 중요한데, 이는 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 지도학습 방법으로 분류 성능 향상에 특화되어 있습니다. 또한 Factor Analysis는 관찰된 변수들을 잠재 변수로 설명하는 방법으로 심리학이나 사회과학 분야에서 주로 사용됩니다. 비선형 방법으로는 t-SNE가 가장 유명합니다. t-SNE는 고차원 데이터를 2D나 3D로 시각화하는 데 특화되어 있으며, 지역적 구조를 잘 보존하기 때문에 클러스터링 결과 시각화에 매우 효과적입니다. 다만 전역적 구조는 보존하지 못하고 하이퍼파라미터에 민감하다는 단점이 있습니다. UMAP은 t-SNE의 단점을 보완한 방법으로, t-SNE보다 빠르면서도 전역적 구조를 어느 정도 보존합니다. 지역적과 전역적 구조의 균형을 잘 맞춰서 대용량 데이터에도 적용 가능합니다. Autoencoder는 신경망 기반의 차원 축소 방법으로, 인코더-디코더 구조를 통해 데이터를 압축하고 복원합니다. 비선형 관계를 잘 학습할 수 있어서 딥러닝과 결합하여 사용되는 경우가 많습니다. 기타 방법들로는 Isomap이 있는데, 이는 지오데식 거리를 사용하는 매니폴드 학습의 대표적인 방법입니다. LLE(Locally Linear Embedding)는 지역적으로 선형인 관계를 가정하는 방법으로 작은 데이터셋에서 효과적입니다. 실제로 어떤 방법을 선택할지는 목적에 따라 다릅니다. 시각화가 목적이라면 t-SNE나 UMAP을, 전처리가 목적이라면 PCA나 Autoencoder를, 분류 성능 향상이 목적이라면 LDA를 사용하는 것이 좋습니다. 대용량 데이터를 다룬다면 UMAP이나 PCA를, 해석 가능성이 중요하다면 PCA나 Factor Analysis를 선택하는 것이 적절합니다. PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요? PCA가 이 세 가지 역할을 모두 할 수 있는 이유는 PCA의 핵심 동작 원리에 있습니다. PCA는 데이터의 분산이 가장 큰 방향, 즉 주성분(principal component)을 찾아서 새로운 좌표계를 만드는 기법인데, 이 과정에서 자연스럽게 세 가지 효과가 나타납니다. 먼저 차원 축소 측면에서는, PCA가 원본 데이터의 분산을 가장 잘 설명하는 몇 개의 주성분만 선택하기 때문입니다. 예를 들어 100차원 데이터에서 분산의 95%를 설명하는 10개의 주성분만 선택하면, 100차원을 10차원으로 줄이면서도 중요한 정보는 대부분 보존할 수 있어요. 데이터 압축 측면에서는, 주성분을 통해 원본 데이터를 재구성할 수 있기 때문입니다. 원본 데이터를 주성분들의 선형결합으로 표현하고, 다시 원래 공간으로 복원할 수 있어요. 물론 일부 정보 손실은 있지만, 압축률에 비해 품질 손실이 적어서 이미지 압축 등에 활용됩니다. 노이즈 제거 측면에서는, 노이즈가 보통 작은 분산을 가지기 때문입니다. PCA는 분산이 큰 방향, 즉 신호가 강한 방향을 우선적으로 선택하고, 분산이 작은 방향, 즉 노이즈가 많은 방향은 제거하게 됩니다. 따라서 주성분만 사용해서 데이터를 재구성하면 자연스럽게 노이즈가 제거된 깨끗한 데이터를 얻을 수 있어요. 이 세 가지 효과는 모두 PCA가 \"분산이 큰 방향을 우선적으로 선택한다\"는 하나의 원리에서 나오는 결과입니다. 그래서 PCA는 단순한 차원 축소 도구가 아니라, 데이터의 본질적인 구조를 찾아내는 강력한 기법이라고 할 수 있습니다. LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요? 이 세 기법들은 모두 행렬 분해를 기반으로 하는 차원 축소 기법들이지만, 각각 다른 목적과 특성을 가지고 있습니다. 먼저 SVD(Singular Value Decomposition)는 가장 기본이 되는 행렬 분해 기법입니다. 임의의 행렬을 세 개의 행렬의 곱으로 분해하는데, A = UΣV^T 형태로 나타납니다. 여기서 U와 V는 직교행렬이고, Σ는 대각행렬입니다. SVD는 선형대수학의 핵심 도구로, 데이터의 내재적 구조를 찾아내는 데 사용됩니다. LSA(Latent Semantic Analysis)는 SVD를 텍스트 마이닝에 특화해서 적용한 기법입니다. 문서-단어 행렬을 SVD로 분해해서 잠재 의미 공간을 찾아내는 방법이에요. 예를 들어 \"자동차\"와 \"차량\"이라는 서로 다른 단어가 같은 문서에서 자주 나타나면, LSA는 이들이 비슷한 의미를 가진다는 것을 학습할 수 있습니다. 이렇게 해서 단어의 동의어나 유의어 관계를 자동으로 찾아내고, 문서의 주제를 파악하는 데 활용됩니다. LDA(Linear Discriminant Analysis)는 앞의 두 기법과는 다른 목적을 가진 지도학습 기법입니다. 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 방향으로 차원을 축소하는 방법이에요. 분류 성능 향상에 특화되어 있어서, 같은 차원 축소를 하더라도 분류에 유리한 방향으로 축을 선택합니다. 이 세 기법의 관계를 정리하면, SVD는 수학적 기반이 되는 행렬 분해 기법이고, LSA는 SVD를 텍스트 분석에 적용한 비지도학습 방법이며, LDA는 분류 성능 향상을 위한 지도학습 방법입니다. 모두 차원 축소라는 공통점이 있지만, LSA는 의미적 유사성을 찾는 데, LDA는 분류 성능을 높이는 데 초점을 맞춘다는 차이점이 있습니다. 실제로는 텍스트 분석에서는 LSA를, 분류 문제에서는 LDA를, 그리고 일반적인 행렬 분해나 노이즈 제거에는 SVD를 직접 사용하는 경우가 많습니다. Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요? 고등학생에게 Markov Chain을 설명할 때는 일상생활의 구체적인 예시를 사용하는 것이 가장 효과적입니다. 가장 좋은 예시는 날씨 예측입니다. \"오늘 비가 오면 내일도 비가 올 확률이 60%, 맑을 확률이 40%이고, 오늘 맑으면 내일 맑을 확률이 70%, 비가 올 확률이 30%다\"라고 설명해보세요. 이때 중요한 점은 과거의 모든 날씨 기록이 아니라 오늘의 날씨만이 내일 날씨를 결정한다는 것입니다. 이것이 바로 Markov Chain의 핵심인 \"현재 상태가 다음 상태를 결정한다\"는 Markov 성질입니다. 과거의 모든 정보를 기억할 필요 없이, 현재 상태만 알면 미래를 예측할 수 있다는 것이죠. 다른 좋은 예시로는 게임의 이동이 있습니다. 보드게임에서 현재 위치에서만 다음 위치가 결정되고, 어떻게 그 위치에 도달했는지는 중요하지 않다는 점을 설명하면 됩니다. 이런 식으로 일상생활의 친숙한 예시를 통해 Markov Chain의 개념을 자연스럽게 이해시킬 수 있습니다. 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요? 텍스트에서 주제를 추출하는 작업은 토픽 모델링(Topic Modeling)이라고 하며, 여러 단계의 체계적인 접근이 필요합니다. 먼저 데이터 전처리 단계에서 텍스트를 정제합니다. 불용어(stopwords) 제거, 어간 추출(stemming), 소문자 변환 등을 통해 노이즈를 줄이고, 토큰화를 통해 단어 단위로 분리합니다. 이때 특수문자나 숫자 처리도 고려해야 합니다. 다음으로 문서-단어 행렬(Document-Term Matrix)을 생성합니다. TF-IDF나 단순 빈도 기반으로 각 문서에서 단어의 중요도를 계산하여 수치화합니다. 주제 추출의 핵심 방법으로는 LDA(Latent Dirichlet Allocation)가 가장 널리 사용됩니다. LDA는 각 문서가 여러 주제의 혼합으로 구성되어 있고, 각 주제는 단어들의 확률 분포로 표현된다고 가정합니다. 이를 통해 문서의 주제 분포와 주제별 단어 분포를 동시에 학습합니다. 대안으로는 LSA(Latent Semantic Analysis)나 NMF(Non-negative Matrix Factorization)도 사용할 수 있습니다. LSA는 SVD를 활용한 선형 방법이고, NMF는 음수가 아닌 행렬 분해를 통해 주제를 찾는 방법입니다. 최근에는 BERT 기반의 토픽 모델링도 주목받고 있습니다. 사전 훈련된 언어 모델을 활용하여 더 정교한 의미적 유사성을 반영한 주제 추출이 가능합니다. 실제 적용 시에는 주제 개수 설정, 하이퍼파라미터 튜닝, 결과 해석의 어려움 등을 고려해야 하며, 도메인 지식과 함께 결과를 검증하는 것이 중요합니다. SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요? SVM이 차원을 확장시키는 이유는 선형 분리가 불가능한 데이터를 고차원 공간에서 선형 분리가 가능하도록 만들기 위해서입니다. 이를 커널 트릭(Kernel Trick)이라고 합니다. 예를 들어, 2차원에서 원형으로 분포된 데이터는 직선으로 분리할 수 없습니다. 하지만 3차원으로 올려서 z = x² + y² 같은 변환을 하면, 3차원에서 평면으로 분리할 수 있게 됩니다. SVM은 이런 아이디어를 극한까지 활용해서, 데이터를 무한 차원의 공간으로 보내서도 선형 분리가 가능하도록 만듭니다. 핵심은 실제로 고차원 공간으로 변환하지 않고도, 커널 함수를 통해 고차원에서의 내적을 계산할 수 있다는 점입니다. RBF 커널, 다항식 커널 등이 대표적인 예시입니다. SVM이 좋은 이유는 여러 가지가 있습니다. 첫째, 고차원 데이터에서도 효과적입니다. 차원이 높아져도 성능이 크게 떨어지지 않아요. 둘째, 메모리 효율적입니다. 서포트 벡터만 저장하면 되므로 전체 데이터를 기억할 필요가 없습니다. 셋째, 과적합에 강합니다. 마진을 최대화하는 원리로 인해 일반화 성능이 좋습니다. 넷째, 수학적으로 잘 정립되어 있어서 이론적 근거가 탄탄합니다. 또한 이상치에 강하고, 비선형 관계도 잘 학습할 수 있으며, 작은 데이터셋에서도 좋은 성능을 보입니다. 다만 대용량 데이터에서는 계산 비용이 높고, 하이퍼파라미터 튜닝이 중요하다는 단점도 있습니다. 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요. 나이브 베이즈가 오래된 기법이라고 해서 무시할 수 없는 여러 가지 강력한 장점들이 있습니다. 가장 큰 장점은 훈련 속도가 매우 빠르다는 점입니다. 복잡한 최적화 과정 없이 단순히 확률만 계산하면 되므로, 대용량 데이터에서도 실시간으로 학습이 가능합니다. 이는 스트리밍 데이터나 실시간 분류가 필요한 상황에서 큰 장점이 됩니다. 메모리 효율성도 뛰어납니다. 모델 파라미터가 매우 적어서 저장 공간을 거의 차지하지 않으며, 모바일 환경이나 임베디드 시스템에서도 쉽게 배포할 수 있습니다. 작은 데이터셋에서도 잘 동작합니다. 다른 복잡한 모델들이 과적합에 빠지는 상황에서도 나이브 베이즈는 안정적인 성능을 보여줍니다. 특히 텍스트 분류에서는 여전히 매우 경쟁력 있는 성능을 보입니다. 해석 가능성이 뛰어납니다. 각 feature가 클래스에 미치는 영향을 확률로 직접 해석할 수 있어서, 비즈니스 의사결정에 도움이 됩니다. 스팸 필터에서 어떤 단어가 스팸 확률을 높이는지 바로 알 수 있는 것처럼 말이죠. 이상치에 강하고, feature 간 상관관계가 있어도 어느 정도 견딜 수 있으며, 다중 클래스 분류에서도 자연스럽게 확장됩니다. 또한 온라인 학습이 가능해서 새로운 데이터가 들어올 때마다 점진적으로 모델을 업데이트할 수 있습니다. 특히 텍스트 분류, 스팸 필터링, 감정 분석 같은 도메인에서는 여전히 최고 수준의 성능을 보이며, 베이스라인 모델로서의 역할도 훌륭합니다. 복잡한 모델을 구현하기 전에 나이브 베이즈로 시작해서 성능을 확인해보는 것이 좋은 접근 방식입니다. 회귀 / 분류시 알맞은 metric은 무엇일까? 회귀와 분류 문제에서는 각각의 특성에 맞는 서로 다른 metric을 사용해야 합니다. 회귀 문제에서는 주로 RMSE(Root Mean Square Error)와 MAE(Mean Absolute Error)를 사용합니다. RMSE는 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. 반면 MAE는 이상치에 덜 민감해서 이상치가 많은 데이터에서는 더 적절할 수 있습니다. R²(결정계수)는 모델이 데이터의 분산을 얼마나 잘 설명하는지를 보여주는 지표로, 0과 1 사이의 값을 가지며 1에 가까울수록 좋습니다. 분류 문제에서는 상황에 따라 다양한 metric을 선택해야 합니다. Accuracy는 가장 직관적이지만 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. Precision은 양성으로 예측한 것 중 실제로 양성인 비율이고, Recall은 실제 양성 중 올바르게 찾아낸 비율입니다. F1-score는 이 둘의 조화평균으로 균형을 맞춘 지표입니다. ROC-AUC는 다양한 threshold에서의 성능을 종합적으로 평가할 수 있어서 이진 분류에서 널리 사용됩니다. PR-AUC는 클래스 불균형이 심할 때 ROC-AUC보다 더 적절한 지표가 될 수 있습니다. 다중 클래스 분류에서는 Macro/Micro F1-score를 사용하며, Confusion Matrix를 통해 각 클래스별 성능을 자세히 분석할 수 있습니다. 실제로는 비즈니스 요구사항을 고려해서 metric을 선택해야 합니다. 의료 진단에서는 Recall이 중요하고, 스팸 필터에서는 Precision이 중요하죠. 또한 데이터의 특성과 클래스 분포도 고려해야 합니다. Association Rule의 Support, Confidence, Lift에 대해 설명해주세요. Association Rule은 장바구니 분석에서 \"A를 사는 사람이 B도 산다\"는 규칙을 찾는 기법으로, 세 가지 핵심 지표가 있습니다. Support(지지도)는 전체 거래 중에서 해당 아이템들이 함께 나타나는 비율입니다. Support(A→B) = P(A∩B)로 계산되며, 이 규칙이 얼마나 자주 발생하는지를 보여줍니다. 예를 들어 전체 거래 1000건 중 빵과 우유가 함께 구매된 거래가 100건이라면, Support(빵→우유) = 0.1입니다. 너무 낮으면 의미 없는 규칙이고, 너무 높으면 당연한 규칙이 될 수 있어요. Confidence(신뢰도)는 A를 구매한 사람 중에서 B도 구매한 비율입니다. Confidence(A→B) = P(B A) = P(A∩B)/P(A)로 계산됩니다. 빵을 산 사람 200명 중 100명이 우유도 샀다면, Confidence(빵→우유) = 0.5입니다. 이는 A를 구매했을 때 B를 구매할 확률을 의미하므로, 추천 시스템에서 중요한 지표가 됩니다. Lift(향상도)는 A와 B가 독립적일 때 대비 얼마나 더 자주 함께 나타나는지를 보여줍니다. Lift(A→B) = P(B A)/P(B) = Confidence(A→B)/P(B)로 계산됩니다. Lift가 1보다 크면 양의 상관관계, 1이면 독립, 1보다 작으면 음의 상관관계를 의미합니다. 예를 들어 Lift가 2라면, A를 구매했을 때 B를 구매할 확률이 일반적인 경우보다 2배 높다는 뜻입니다. 실제로는 이 세 지표를 모두 고려해서 규칙을 평가합니다. Support는 빈도를, Confidence는 예측 정확도를, Lift는 독립성 대비 향상 정도를 측정하므로, 각각 다른 관점에서 규칙의 유용성을 평가할 수 있습니다. 최적화 기법중 Newton's Method와 Gradient Descent 방법에 대해 알고 있나요? Gradient Descent와 Newton's Method는 모두 함수의 최솟값을 찾는 최적화 기법이지만, 접근 방식과 특성이 다릅니다. Gradient Descent는 가장 기본적인 최적화 방법으로, 함수의 기울기(gradient) 방향으로 조금씩 이동하면서 최솟값을 찾습니다. 수식으로는 θ_{t+1} = θ_t - α∇f(θ_t)로 표현되며, 여기서 α는 학습률(learning rate)입니다. 기울기가 가파른 곳에서는 크게 이동하고, 기울기가 완만한 곳에서는 작게 이동하는 직관적인 방법이에요. 장점은 구현이 간단하고 메모리 효율적이며, 대용량 데이터에도 적용 가능하다는 점입니다. 하지만 학습률 설정이 중요하고, 수렴 속도가 느리며, 골짜기나 평지에서 비효율적으로 움직일 수 있습니다. Newton's Method는 2차 도함수(헤시안 행렬)까지 사용해서 더 정확한 최적화를 수행합니다. 수식으로는 θ_{t+1} = θ_t - H^{-1}∇f(θ_t)로 표현되며, 여기서 H는 헤시안 행렬입니다. 1차 도함수뿐만 아니라 2차 도함수 정보까지 활용해서 곡률을 고려한 더 정확한 방향으로 이동합니다. 장점은 수렴 속도가 매우 빠르고(2차 수렴), 골짜기나 평지에서도 효율적으로 움직인다는 점입니다. 하지만 헤시안 행렬을 계산하고 역행렬을 구해야 하므로 계산 비용이 매우 높고, 메모리 사용량도 많습니다. 또한 헤시안이 양정치가 아닌 경우 문제가 될 수 있어요. 실제로는 Quasi-Newton 방법들(BFGS, L-BFGS)이 두 방법의 장점을 결합한 하이브리드 접근법으로 널리 사용됩니다. Newton's Method의 빠른 수렴 속도는 유지하면서도 헤시안을 직접 계산하지 않고 근사하는 방식으로 계산 비용을 줄입니다. 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요? 머신러닝과 통계학은 모두 데이터에서 패턴을 찾는 학문이지만, 접근 방식과 목적에서 차이가 있습니다. 통계학적 접근은 해석 가능성과 추론에 중점을 둡니다. 가설을 세우고, 모델의 가정을 검증하며, p-value나 신뢰구간을 통해 통계적 유의성을 평가합니다. 모델의 파라미터가 실제 현상을 어떻게 설명하는지에 관심이 많고, 인과관계 추론에 강점이 있습니다. 또한 표본에서 모집단으로의 일반화 가능성을 엄밀하게 다룹니다. 머신러닝적 접근은 예측 성능에 중점을 둡니다. 복잡한 패턴을 찾아내고, 새로운 데이터에 대한 예측 정확도를 최대화하는 것이 목표입니다. 모델이 블랙박스여도 예측이 정확하면 괜찮다고 보는 경향이 있어요. 또한 대용량 데이터와 고차원 문제에 특화되어 있습니다. 데이터 크기에 대한 관점도 다릅니다. 통계학은 전통적으로 작은 표본에서도 유의미한 결론을 도출하는 데 집중했지만, 머신러닝은 빅데이터 환경에서 복잡한 패턴을 학습하는 데 특화되어 있습니다. 모델 복잡도에 대한 접근도 다릅니다. 통계학은 간단하고 해석 가능한 모델을 선호하는 반면, 머신러닝은 성능 향상을 위해 복잡한 모델도 주저하지 않습니다. 하지만 최근에는 이 경계가 모호해지고 있습니다. 통계학에서도 예측 성능을 중시하는 경향이 생겼고, 머신러닝에서도 해석 가능성에 대한 관심이 높아지고 있어요. 실제로는 두 접근법을 적절히 조합해서 사용하는 것이 가장 효과적입니다. 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요? 전통적인 인공신경망은 여러 가지 근본적인 문제점들로 인해 한계에 부딪혔습니다. 가장 큰 문제는 차원의 저주(Curse of Dimensionality)였습니다. 입력 차원이 늘어날수록 필요한 파라미터 수가 기하급수적으로 증가하고, 학습에 필요한 데이터도 폭발적으로 늘어났습니다. 이로 인해 고차원 데이터에서는 실용적이지 못했습니다. 기울기 소실 문제(Vanishing Gradient Problem)도 심각했습니다. 역전파 과정에서 기울기가 계층을 거치면서 점점 작아져서, 깊은 네트워크에서는 앞쪽 층의 가중치가 거의 업데이트되지 않았습니다. 이로 인해 깊은 네트워크를 학습시키는 것이 거의 불가능했어요. 과적합 문제도 만성적이었습니다. 복잡한 네트워크는 훈련 데이터에 너무 잘 맞춰져서 일반화 성능이 떨어졌고, 당시에는 효과적인 정규화 기법이 부족했습니다. 지역 최솟값(Local Minima)에 빠지는 문제도 있었습니다. 복잡한 손실 함수에서 전역 최솟값을 찾지 못하고 지역 최솟값에 갇혀서 성능이 제한되었습니다. 계산 자원의 한계도 큰 문제였습니다. 당시의 하드웨어로는 복잡한 신경망을 학습시키기에 부족했고, 병렬 처리 기술도 제한적이었습니다. 초기화 문제도 있었습니다. 가중치 초기화가 잘못되면 학습이 전혀 진행되지 않거나 매우 느려졌습니다. 이러한 문제들로 인해 전통적인 신경망은 1990년대 후반부터 2000년대 초반까지 \"AI 겨울\"을 겪게 되었고, 대신 SVM이나 Random Forest 같은 다른 기법들이 주목받게 되었습니다. 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요? 현재 딥러닝의 혁신은 여러 요소들이 복합적으로 작용한 결과라고 생각합니다. 가장 근본적인 변화는 데이터와 컴퓨팅 파워의 폭발적 증가입니다. 인터넷과 모바일 기기의 보급으로 엄청난 양의 데이터가 생성되고, GPU와 클라우드 컴퓨팅의 발전으로 이를 처리할 수 있는 환경이 마련되었습니다. 이는 딥러닝이 필요로 하는 \"빅데이터 + 고성능 컴퓨팅\" 환경을 제공했습니다. 알고리즘적 혁신도 핵심입니다. ReLU 활성화 함수가 기울기 소실 문제를 크게 완화했고, Dropout, Batch Normalization 같은 정규화 기법들이 과적합을 효과적으로 방지했습니다. 또한 Xavier/He 초기화, Adam 옵티마이저 등이 학습 안정성을 크게 향상시켰습니다. 아키텍처 혁신도 중요한 역할을 했습니다. CNN은 이미지 처리에서, RNN/LSTM은 시퀀스 데이터에서, Transformer는 자연어 처리에서 각각 혁신을 일으켰습니다. 특히 Attention 메커니즘은 모델이 중요한 부분에 집중할 수 있게 해주었습니다. 사전 훈련(Pre-training)과 전이 학습(Transfer Learning)의 개념도 혁신적이었습니다. 대용량 데이터로 사전 훈련한 모델을 특정 태스크에 맞게 미세 조정하는 방식으로, 적은 데이터로도 좋은 성능을 얻을 수 있게 되었습니다. 오픈소스 생태계의 발전도 빼놓을 수 없습니다. TensorFlow, PyTorch 같은 프레임워크가 연구자들의 접근성을 높였고, GitHub를 통한 지식 공유가 혁신 속도를 가속화했습니다. 하지만 가장 근본적인 혁신은 \"end-to-end 학습\"이라는 패러다임 변화라고 생각합니다. 기존에는 feature engineering이 핵심이었지만, 이제는 원시 데이터부터 최종 결과까지 모든 과정을 신경망이 자동으로 학습하는 방식으로 바뀌었습니다. 이는 인간의 직관과 경험에 의존하던 부분을 데이터와 알고리즘이 대체한 혁신입니다. ROC 커브에 대해 설명해주실 수 있으신가요? ROC 커브(Receiver Operating Characteristic Curve)는 이진 분류 모델의 성능을 평가하는 중요한 시각화 도구입니다. ROC 커브는 True Positive Rate(TPR)와 False Positive Rate(FPR)의 관계를 그래프로 나타낸 것입니다. TPR은 실제 양성 중 올바르게 양성으로 예측한 비율(민감도, Sensitivity)이고, FPR은 실제 음성 중 잘못 양성으로 예측한 비율(1-특이도)입니다. X축은 FPR, Y축은 TPR로 설정하며, 그래프는 (0,0)에서 (1,1)까지의 곡선으로 나타납니다. 분류 threshold를 변화시키면서 각각의 TPR과 FPR을 계산해서 점들을 연결하면 ROC 커브가 완성됩니다. 완벽한 분류기는 (0,1) 지점에 위치하며, 이는 FPR=0, TPR=1, 즉 모든 양성을 올바르게 찾아내면서 음성은 하나도 잘못 분류하지 않는다는 의미입니다. 무작위 분류기는 대각선(0,0)에서 (1,1)을 잇는 직선으로 나타나며, 이는 동전 던지기와 같은 수준의 성능을 의미합니다. ROC-AUC(Area Under Curve)는 ROC 커브 아래 면적을 의미하며, 0과 1 사이의 값을 가집니다. 1에 가까울수록 좋은 성능이고, 0.5는 무작위 분류기와 같은 성능을 의미합니다. ROC 커브의 장점은 클래스 불균형에 상대적으로 덜 민감하다는 점입니다. 또한 threshold에 관계없이 모델의 전반적인 성능을 평가할 수 있어서, 다양한 threshold에서의 성능을 종합적으로 비교할 수 있습니다. 하지만 클래스 불균형이 매우 심한 경우에는 PR 커브(Precision-Recall Curve)가 더 적절할 수 있습니다. ROC 커브는 FPR을 사용하기 때문에 음성 클래스가 매우 많을 때는 실제 성능을 과대평가할 수 있기 때문입니다. 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요? 100대의 서버 환경에서는 Random Forest가 인공신경망보다 훨씬 유리한 여러 가지 이유가 있습니다. 가장 큰 이유는 병렬 처리의 효율성입니다. Random Forest의 각 트리는 독립적으로 학습할 수 있어서, 100대 서버에 트리를 분산시켜서 동시에 학습할 수 있습니다. 이는 학습 시간을 크게 단축시킬 수 있어요. 반면 신경망은 순차적인 역전파 과정 때문에 완전한 병렬화가 어렵습니다. 메모리 효율성도 중요한 장점입니다. Random Forest는 각 트리가 상대적으로 작고, 전체 모델을 메모리에 로드할 필요가 없어서 서버당 메모리 사용량이 적습니다. 신경망은 전체 모델을 메모리에 올려야 하므로 메모리 요구사항이 높습니다. 안정성과 견고성도 Random Forest의 강점입니다. 일부 서버가 다운되거나 네트워크 문제가 발생해도, 남은 서버들로 학습을 계속할 수 있습니다. 각 트리가 독립적이기 때문에 부분적인 실패가 전체 모델에 치명적이지 않아요. 하이퍼파라미터 튜닝의 용이성도 있습니다. Random Forest는 상대적으로 튜닝할 파라미터가 적고, 각 파라미터의 효과가 직관적이어서 분산 환경에서도 쉽게 최적화할 수 있습니다. 해석 가능성도 실무에서 중요한 장점입니다. Random Forest는 feature importance를 제공하고, 각 트리의 의사결정 과정을 추적할 수 있어서 비즈니스 의사결정에 도움이 됩니다. 학습 속도도 빠릅니다. Random Forest는 각 트리를 독립적으로 학습하므로 전체 학습 시간이 단축되고, 조기 종료나 증분 학습도 쉽게 구현할 수 있습니다. 물론 신경망이 더 좋은 성능을 보일 수 있는 복잡한 패턴이 있다면 신경망을 선택해야 하지만, 대부분의 경우 Random Forest가 분산 환경에서 더 실용적이고 효율적인 선택이 됩니다. K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고) K-means의 가장 큰 의미론적 단점은 클러스터 개수를 미리 정해야 한다는 점입니다. 실제 데이터에서는 몇 개의 클러스터가 있는지 알 수 없는 경우가 대부분인데, K-means는 이를 사전에 지정해야 합니다. 잘못된 K값을 선택하면 의미 없는 클러스터링 결과를 얻을 수 있어요. 구형(원형) 클러스터만 잘 찾는다는 한계도 있습니다. K-means는 유클리드 거리를 기반으로 하기 때문에, 타원형이나 비선형적으로 분포된 클러스터는 제대로 찾지 못합니다. 실제 데이터는 다양한 모양의 클러스터를 가질 수 있는데, 이에 대한 가정이 너무 제한적입니다. 초기값에 민감하다는 문제도 있습니다. 초기 중심점이 어떻게 설정되느냐에 따라 최종 결과가 크게 달라질 수 있어요. 특히 클러스터가 겹치거나 경계가 모호한 경우에는 더욱 그렇습니다. 이상치에 취약하다는 점도 의미론적 단점입니다. 이상치가 있으면 중심점이 그쪽으로 끌려가서 전체 클러스터링 결과가 왜곡될 수 있습니다. 실제 데이터에는 이상치가 흔한데, 이를 처리하는 메커니즘이 부족합니다. 클러스터 크기가 균등하다고 가정한다는 한계도 있습니다. 실제로는 클러스터마다 크기가 다를 수 있는데, K-means는 모든 클러스터가 비슷한 크기를 가진다고 가정합니다. 연속적인 특성만 다룰 수 있다는 제약도 있습니다. 범주형 데이터나 혼합형 데이터에는 직접 적용하기 어렵고, 전처리가 필요합니다. 이러한 의미론적 한계들 때문에 실제로는 K-means보다는 DBSCAN, Gaussian Mixture Model, 또는 계층적 클러스터링 같은 다른 방법들이 더 적합한 경우가 많습니다. L1, L2 정규화에 대해 설명해주세요. L1과 L2 정규화는 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법으로, 손실 함수에 정규화 항을 추가하는 방식입니다. L1 정규화(Lasso)는 가중치의 절댓값의 합을 손실 함수에 추가합니다. 수식으로는 Loss + λ∑ w_i 로 표현되며, 여기서 λ는 정규화 강도를 조절하는 하이퍼파라미터입니다. L1 정규화의 가장 큰 특징은 feature selection 효과가 있다는 점입니다. 불필요한 feature의 가중치를 0으로 만들어서 자동으로 feature selection을 수행합니다. 이는 모델을 단순화시키고 해석 가능성을 높입니다. L2 정규화(Ridge)는 가중치의 제곱합을 손실 함수에 추가합니다. 수식으로는 Loss + λ∑w_i²로 표현됩니다. L2 정규화는 가중치들을 0에 가깝게 만들지만 완전히 0으로 만들지는 않습니다. 모든 feature를 유지하면서 가중치만 줄여서 모델의 복잡도를 제어합니다. 기하학적 차이도 있습니다. L1 정규화는 다이아몬드 모양의 제약 영역을 만들어서 축과 만나는 지점(가중치가 0이 되는 지점)에서 최적해를 찾는 경향이 있습니다. 반면 L2 정규화는 원형 제약 영역을 만들어서 모든 가중치를 비슷하게 줄이는 경향이 있습니다. 계산적 차이도 있습니다. L1 정규화는 절댓값 함수 때문에 미분이 불연속적이어서 최적화가 복잡하지만, L2 정규화는 미분 가능해서 최적화가 상대적으로 쉽습니다. Elastic Net은 L1과 L2를 결합한 방법으로, 두 정규화의 장점을 모두 활용할 수 있습니다. 특히 feature가 많은 경우에 효과적입니다. 실제로는 데이터의 특성과 목적에 따라 선택해야 합니다. feature selection이 중요하다면 L1을, 모든 feature를 유지하면서 정규화하고 싶다면 L2를, 둘 다 고려하고 싶다면 Elastic Net을 사용하는 것이 좋습니다. Cross Validation은 무엇이고 어떻게 해야하나요? Cross Validation(교차 검증)은 모델의 일반화 성능을 평가하고 과적합을 방지하기 위한 기법으로, 데이터를 여러 번 나누어서 반복적으로 학습과 검증을 수행하는 방법입니다. 가장 기본적인 K-Fold Cross Validation은 데이터를 K개의 폴드로 나누고, 각 폴드를 한 번씩 검증 데이터로 사용하면서 나머지 K-1개 폴드로 학습을 수행합니다. 예를 들어 5-Fold CV라면 데이터를 5개로 나누고, 5번의 학습-검증 과정을 거쳐서 5개의 성능 점수를 얻습니다. 최종 성능은 이 5개 점수의 평균과 표준편차로 평가합니다. Stratified K-Fold는 클래스 비율을 유지하면서 폴드를 나누는 방법으로, 분류 문제에서 클래스 불균형이 있을 때 유용합니다. 각 폴드가 원본 데이터와 비슷한 클래스 분포를 가지도록 보장합니다. Leave-One-Out Cross Validation(LOOCV)은 K를 데이터 개수와 같게 설정하는 극단적인 경우로, 각 샘플을 하나씩 검증 데이터로 사용합니다. 계산 비용이 높지만 작은 데이터셋에서는 유용합니다. Time Series Cross Validation은 시계열 데이터에 특화된 방법으로, 시간 순서를 고려해서 과거 데이터로 학습하고 미래 데이터로 검증합니다. 일반적인 K-Fold는 시계열 데이터에 부적절할 수 있어요. Cross Validation의 장점은 더 안정적인 성능 평가가 가능하다는 점입니다. 단일 train-test split보다 훨씬 신뢰할 수 있는 성능 추정을 제공합니다. 또한 하이퍼파라미터 튜닝에도 활용할 수 있어서, 다양한 파라미터 조합을 체계적으로 평가할 수 있습니다. 하지만 계산 비용이 높다는 단점이 있고, 데이터가 적을 때는 각 폴드의 크기가 작아져서 성능 평가가 불안정할 수 있습니다. 또한 시계열 데이터나 공간적 상관관계가 있는 데이터에서는 특별한 주의가 필요합니다. 실제로는 데이터의 크기와 특성, 그리고 계산 자원을 고려해서 적절한 Cross Validation 방법을 선택해야 합니다. XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요? XGBoost(eXtreme Gradient Boosting)는 그래디언트 부스팅의 확장된 버전으로, 캐글에서 압도적인 인기를 얻고 있는 모델입니다. XGBoost가 캐글에서 유명한 이유는 여러 가지가 있습니다. 가장 큰 이유는 뛰어난 성능입니다. 대부분의 테이블 데이터에서 최고 수준의 성능을 보여주며, 특히 구조화된 데이터에서 딥러닝보다도 더 좋은 결과를 내는 경우가 많습니다. 속도와 효율성도 큰 장점입니다. 병렬 처리와 분산 컴퓨팅을 지원해서 대용량 데이터도 빠르게 처리할 수 있고, 메모리 사용량도 최적화되어 있습니다. 캐글의 대용량 데이터셋에서도 실용적으로 사용할 수 있어요. 과적합 방지 기능이 뛰어납니다. 정규화 항, 조기 종료, 드롭아웃 등 다양한 과적합 방지 기법이 내장되어 있어서, 복잡한 모델임에도 불구하고 일반화 성능이 좋습니다. 유연성도 큰 장점입니다. 회귀, 분류, 랭킹 등 다양한 태스크에 적용할 수 있고, 커스텀 손실 함수와 평가 지표도 쉽게 정의할 수 있습니다. 또한 결측값 처리, 범주형 변수 처리 등도 자동으로 해줍니다. 하이퍼파라미터 튜닝이 상대적으로 쉽습니다. 많은 파라미터가 있지만 각각의 효과가 직관적이고, 그리드 서치나 베이지안 최적화로 쉽게 튜닝할 수 있어요. 해석 가능성도 제공합니다. Feature importance를 통해 어떤 변수가 중요한지 알 수 있고, SHAP 같은 도구와 결합하면 더 자세한 해석도 가능합니다. 안정성도 뛰어납니다. 다양한 데이터 타입과 분포에서 일관되게 좋은 성능을 보여주며, 이상치에도 상대적으로 강합니다. 캐글에서는 \"XGBoost로 시작해서 성능이 좋으면 그대로 사용하고, 더 개선이 필요하면 딥러닝이나 다른 기법을 시도한다\"는 전략이 일반적입니다. 실제로 많은 캐글 우승 솔루션에서 XGBoost가 핵심 모델로 사용되었습니다. 앙상블 방법엔 어떤 것들이 있나요? 앙상블 방법은 여러 모델의 예측을 결합해서 더 좋은 성능을 얻는 기법으로, 크게 세 가지 접근 방식이 있습니다. Bagging(Bootstrap Aggregating)은 같은 알고리즘을 여러 번 학습시키되, 각각 다른 데이터 샘플을 사용하는 방법입니다. 대표적인 예가 Random Forest로, 여러 의사결정나무를 학습시키되 각각 다른 feature와 데이터 샘플을 사용합니다. Bagging은 분산을 줄여서 과적합을 방지하는 효과가 있어요. Boosting은 순차적으로 모델을 학습시키되, 이전 모델이 잘못 예측한 샘플에 더 집중해서 다음 모델을 학습시키는 방법입니다. AdaBoost, Gradient Boosting, XGBoost 등이 대표적인 예시입니다. Boosting은 편향을 줄여서 전체적인 성능을 향상시키는 효과가 있습니다. Stacking은 여러 다른 알고리즘의 예측을 메타 모델(meta-model)이 학습해서 최종 예측을 만드는 방법입니다. 예를 들어 Random Forest, SVM, Neural Network의 예측을 Logistic Regression이 학습해서 최종 결과를 만드는 방식이에요. Stacking은 모델의 다양성을 활용해서 더 정확한 예측을 할 수 있습니다. Voting은 여러 모델의 예측을 단순히 투표나 평균으로 결합하는 방법입니다. Hard Voting은 다수결로 결정하고, Soft Voting은 확률의 평균을 사용합니다. Blending은 Stacking과 비슷하지만, 메타 모델을 학습할 때 별도의 검증 데이터를 사용하는 방법입니다. Stacking보다 과적합 위험이 적지만 데이터 사용 효율성이 떨어집니다. 각 방법의 특징을 정리하면, Bagging은 과적합이 심한 모델에 효과적이고, Boosting은 약한 학습기에 효과적이며, Stacking은 서로 다른 특성을 가진 모델들을 결합할 때 효과적입니다. 실제로는 여러 방법을 조합해서 사용하는 경우가 많으며, 캐글 같은 대회에서는 앙상블이 우승의 핵심 전략이 되는 경우가 많습니다. feature vector란 무엇일까요? Feature vector는 머신러닝에서 데이터의 특성을 수치로 표현한 벡터입니다. 쉽게 말해서 하나의 데이터 포인트를 숫자들의 리스트로 나타낸 것이에요. 예를 들어 사람의 정보를 feature vector로 표현한다면, [나이, 키, 몸무게, 소득] 같은 형태가 될 수 있습니다. 이때 [25, 170, 65, 3000]은 25세, 170cm, 65kg, 월소득 3000만원인 사람을 나타내는 feature vector입니다. Feature vector의 각 원소는 feature 또는 attribute라고 하며, 이는 데이터의 한 가지 특성을 나타냅니다. 벡터의 차원은 feature의 개수와 같아서, 위 예시에서는 4차원 벡터가 됩니다. Feature Engineering은 원시 데이터를 의미 있는 feature vector로 변환하는 과정입니다. 텍스트 데이터를 TF-IDF 벡터로 변환하거나, 이미지를 픽셀 값들의 벡터로 변환하는 것처럼 말이죠. Feature Selection은 중요한 feature만 선택해서 차원을 줄이는 과정이고, Feature Scaling은 feature들의 스케일을 맞춰주는 전처리 과정입니다. Feature vector는 머신러닝 알고리즘의 입력으로 사용되며, 모델은 이 벡터들을 학습해서 패턴을 찾아냅니다. 따라서 feature vector의 품질이 모델의 성능에 직접적인 영향을 미칩니다. 최근에는 딥러닝의 발전으로 Representation Learning이 주목받고 있는데, 이는 모델이 자동으로 좋은 feature vector를 학습하는 방법입니다. 하지만 여전히 도메인 지식을 활용한 feature engineering이 중요한 역할을 합니다. 좋은 모델의 정의는 무엇일까요? 좋은 모델의 정의는 상황과 목적에 따라 달라지지만, 일반적으로 고려해야 할 몇 가지 핵심 요소들이 있습니다. 가장 기본적인 요소는 예측 정확도입니다. 모델이 새로운 데이터에 대해 얼마나 정확하게 예측하는지가 가장 중요한 평가 기준이에요. 하지만 정확도만으로는 부족하고, 일반화 성능이 중요합니다. 훈련 데이터에만 잘 맞는 것이 아니라, 보지 못한 새로운 데이터에서도 좋은 성능을 보여야 합니다. 해석 가능성도 현대 머신러닝에서 점점 더 중요해지고 있습니다. 특히 의료, 금융, 법률 같은 도메인에서는 모델이 왜 그런 예측을 했는지 설명할 수 있어야 합니다. 비즈니스 의사결정에 활용하려면 해석 가능성이 필수적이에요. 안정성과 견고성도 중요한 요소입니다. 이상치나 노이즈가 있어도 일관된 성능을 보여야 하고, 데이터 분포가 조금 바뀌어도 크게 성능이 떨어지지 않아야 합니다. 효율성도 고려해야 합니다. 학습 시간, 예측 시간, 메모리 사용량 등이 실용적인 수준이어야 합니다. 정확도가 아무리 높아도 실시간으로 예측할 수 없다면 실무에서 사용하기 어려워요. 유지보수성도 중요합니다. 모델을 업데이트하거나 수정하기 쉬워야 하고, 새로운 데이터가 들어와도 쉽게 재학습할 수 있어야 합니다. 비용 효율성도 고려해야 합니다. 모델 개발과 운영에 드는 비용 대비 얻는 이익이 합리적이어야 합니다. 하지만 가장 중요한 것은 비즈니스 목표 달성입니다. 기술적으로 완벽한 모델이라도 비즈니스 문제를 해결하지 못한다면 좋은 모델이 아닙니다. 따라서 도메인 전문가와의 협업을 통해 실제 문제를 정확히 정의하고, 그에 맞는 적절한 모델을 선택하는 것이 핵심입니다. 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요? 일반적으로 50개의 작은 의사결정나무가 하나의 큰 의사결정나무보다 더 좋은 성능을 보입니다. 이는 앙상블 효과 때문입니다. 가장 큰 이유는 분산 감소(Variance Reduction)입니다. 하나의 큰 나무는 훈련 데이터에 과적합되기 쉽고, 작은 변화에도 민감하게 반응합니다. 하지만 50개의 작은 나무들의 평균을 내면 이런 변동성이 상쇄되어 더 안정적인 예측을 할 수 있어요. 편향과 분산의 균형도 더 좋습니다. 하나의 큰 나무는 편향은 낮지만 분산이 높은 반면, 50개의 작은 나무는 각각은 편향이 높을 수 있지만 분산이 낮아서 전체적으로는 더 좋은 성능을 보입니다. 다양성(Diversity)도 중요한 요소입니다. 50개의 나무가 서로 다른 패턴을 학습할 수 있어서, 하나의 나무가 놓친 패턴을 다른 나무가 보완할 수 있습니다. 이는 Random Forest에서 각 나무가 다른 feature와 데이터 샘플을 사용하는 것과 같은 원리입니다. 견고성(Robustness)도 뛰어납니다. 일부 나무가 잘못된 예측을 해도 다른 나무들이 이를 보완할 수 있어서, 이상치나 노이즈에 더 강합니다. 병렬 처리도 가능합니다. 50개의 나무를 동시에 학습할 수 있어서, 하나의 큰 나무를 학습하는 것보다 시간이 더 오래 걸리지 않을 수 있습니다. 하지만 해석 가능성은 떨어집니다. 하나의 큰 나무는 의사결정 과정을 쉽게 추적할 수 있지만, 50개 나무의 결합은 복잡해서 해석하기 어려워요. 메모리 사용량도 더 많을 수 있습니다. 50개의 나무를 모두 저장해야 하므로 하나의 큰 나무보다 메모리를 더 사용합니다. 전반적으로는 성능 면에서 50개의 작은 나무가 더 우수하지만, 해석 가능성이 중요한 상황이라면 하나의 큰 나무를 선택할 수도 있습니다. 실제로는 Random Forest나 Gradient Boosting 같은 앙상블 방법이 이런 아이디어를 체계적으로 구현한 것입니다. 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요? 스팸 필터에서 로지스틱 회귀가 널리 사용되는 이유는 여러 가지 실용적인 장점들 때문입니다. 가장 큰 이유는 해석 가능성입니다. 로지스틱 회귀는 각 feature(단어)가 스팸 확률에 미치는 영향을 계수로 직접 해석할 수 있어요. 예를 들어 \"무료\"라는 단어의 계수가 2.5라면, 이 단어가 있을 때 스팸 확률이 2.5배 증가한다는 의미입니다. 이는 사용자에게 왜 이메일이 스팸으로 분류되었는지 설명할 수 있게 해줍니다. 확률 출력도 중요한 장점입니다. 로지스틱 회귀는 0과 1 사이의 확률값을 출력하므로, 스팸 확률이 0.8인 이메일과 0.95인 이메일을 구분할 수 있습니다. 이는 사용자가 임계값을 조정해서 민감도를 조절할 수 있게 해줍니다. 학습 속도가 빠릅니다. 텍스트 데이터는 보통 수십만 개의 단어로 구성된 고차원 벡터인데, 로지스틱 회귀는 이런 고차원 데이터도 빠르게 학습할 수 있습니다. 실시간으로 새로운 스팸 패턴을 학습해야 하는 환경에서 중요한 장점이에요. 메모리 효율성도 뛰어납니다. 모델 파라미터가 단순한 계수들이므로 저장 공간이 적고, 예측 시에도 빠르게 계산할 수 있습니다. 안정성도 좋습니다. 이상치나 노이즈에 상대적으로 강하고, 하이퍼파라미터 튜닝이 간단해서 안정적인 성능을 보장합니다. 확장성도 뛰어납니다. 새로운 feature를 쉽게 추가할 수 있고, 온라인 학습도 가능해서 새로운 스팸 패턴이 나타나면 점진적으로 모델을 업데이트할 수 있습니다. L1 정규화와 잘 맞습니다. L1 정규화를 사용하면 불필요한 단어들의 계수를 0으로 만들어서 자동으로 feature selection이 되고, 모델이 더 간단해집니다. 물론 딥러닝 모델이 더 복잡한 패턴을 학습할 수 있지만, 스팸 필터링에서는 해석 가능성과 실시간 처리가 더 중요하기 때문에 로지스틱 회귀가 여전히 널리 사용됩니다. 또한 베이스라인 모델로서의 역할도 훌륭해서, 더 복잡한 모델을 구현하기 전에 로지스틱 회귀로 시작하는 것이 일반적입니다. OLS(ordinary least squre) regression의 공식은 무엇인가요? OLS(Ordinary Least Squares) 회귀는 선형 회귀의 가장 기본적인 형태로, 잔차의 제곱합을 최소화하는 방법입니다. 기본 모델은 y = Xβ + ε 형태로 표현되며, 여기서 y는 종속변수 벡터, X는 독립변수 행렬, β는 회귀계수 벡터, ε는 오차 벡터입니다. 목적 함수는 잔차의 제곱합을 최소화하는 것입니다: $$ \\min_{\\beta} \\sum_{i=1}^{n} (y_i - x_i^T\\beta)^2 = \\min_{\\beta} y - X\\beta ^2 $$ 정규 방정식(Normal Equation)을 통해 해를 구할 수 있습니다: $$ \\hat{\\beta} = (X^T X)^{-1} X^T y $$ 이 공식은 X^T X가 역행렬을 가질 때 유효하며, 이는 X가 full rank일 때입니다. 기하학적 해석으로는, X의 열공간에서 y에 가장 가까운 점을 찾는 것으로 볼 수 있습니다. 이는 y를 X의 열공간에 정사영(projection)하는 것과 같습니다. 가정사항으로는 선형성, 독립성, 등분산성, 정규성이 있으며, 이 가정들이 만족될 때 OLS 추정량은 BLUE(Best Linear Unbiased Estimator)가 됩니다. 잔차는 e = y - Xβ̂로 계산되며, 잔차의 제곱합(RSS)은: $$ RSS = e^T e = (y - X\\hat{\\beta})^T (y - X\\hat{\\beta}) $$ 결정계수(R²)는: $$ R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2} $$ 여기서 TSS는 총 제곱합(Total Sum of Squares)입니다. OLS는 수학적으로 우아하고 해석이 용이하지만, 다중공선성이나 이상치에 민감하다는 한계가 있습니다. 현대에는 Ridge, Lasso 같은 정규화된 회귀 방법들이 더 널리 사용되지만, OLS는 여전히 회귀 분석의 기초가 되는 중요한 방법입니다.",
    "public": true
  },
  {
    "id": "Data Science/Tech Interview/about_DL",
    "title": "딥러닝 관련 인터뷰 준비",
    "date": "2025-09-08",
    "excerpt": "",
    "category": "Data Science",
    "tags": [
      "Interview"
    ],
    "content": "참고자료 1: ai 테크 인터뷰 --- 딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는? Cost Function과 Activation Function은 무엇인가요? Tensorflow, PyTorch 특징과 차이가 뭘까요? Data Normalization은 무엇이고 왜 필요한가요? 알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등) 오버피팅일 경우 어떻게 대처해야 할까요? 하이퍼 파라미터는 무엇인가요? Weight Initialization 방법에 대해 말해주세요. 그리고 무엇을 많이 사용하나요? 볼츠만 머신은 무엇인가요? TF, PyTorch 등을 사용할 때 디버깅 노하우는? 뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가? 요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는? Non-Linearity라는 말의 의미와 그 필요성은? ReLU로 어떻게 곡선 함수를 근사하나? ReLU의 문제점은? Bias는 왜 있는걸까? Gradient Descent에 대해서 쉽게 설명한다면? 왜 꼭 Gradient를 써야 할까? 그 그래프에서 가로축과 세로축 각각은 무엇인가? 실제 상황에서는 그 그래프가 어떻게 그려질까? GD 중에 때때로 Loss가 증가하는 이유는? Back Propagation에 대해서 쉽게 설명 한다면? Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는? GD가 Local Minima 문제를 피하는 방법은? 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은? Training 세트와 Test 세트를 분리하는 이유는? Validation 세트가 따로 있는 이유는? Test 세트가 오염되었다는 말의 뜻은? Regularization이란 무엇인가? Batch Normalization의 효과는? Dropout의 효과는? BN 적용해서 학습 이후 실제 사용시에 주의할 점은? 코드로는? GAN에서 Generator 쪽에도 BN을 적용해도 될까? SGD, RMSprop, Adam에 대해서 아는대로 설명한다면? SGD에서 Stochastic의 의미는? 미니배치를 작게 할때의 장단점은? 모멘텀의 수식을 적어 본다면? 간단한 MNIST 분류기를 MLP+CPU 버전으로 numpy로 만든다면 몇줄일까? 어느 정도 돌아가는 녀석을 작성하기까지 몇시간 정도 걸릴까? Back Propagation은 몇줄인가? CNN으로 바꾼다면 얼마나 추가될까? 간단한 MNIST 분류기를 TF, PyTorch 등으로 작성하는데 몇시간이 필요한가? CNN이 아닌 MLP로 해도 잘 될까? 마지막 레이어 부분에 대해서 설명 한다면? 학습은 BCE loss로 하되 상황을 MSE loss로 보고 싶다면? 딥러닝할 때 GPU를 쓰면 좋은 이유는? GPU를 두개 다 쓰고 싶다. 방법은? 학습시 필요한 GPU 메모리는 어떻게 계산하는가?",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/t1/성장에 필요한 마음가짐/The Greatest of All Time",
    "title": "The Greatest of All Time - 대상혁~~",
    "date": "2025-09-05",
    "excerpt": "You don't grow from winning all the time",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "T1"
    ],
    "content": "참 좋은 말이네 ㅎㅎ 대상혁~~~",
    "public": false
  },
  {
    "id": "Data Science/Causal Inference/residualization-fwl-theorem-test",
    "title": "FWL에 기반한 잔차화 방법론 구현 노트북",
    "date": "2025-07-10T00:00:00.000Z",
    "excerpt": "잔차화를 하기 위해 nusiance function을 만들고 결과를 분석하는 노트북",
    "category": "Causal Inference",
    "tags": [
      "residualization",
      "FWL"
    ],
    "content": "--- title: \"FWL에 기반한 잔차화 방법론 구현 노트북\" date: 2025-07-10 category: \"Causal Inference\" tags: [\"residualization\", \"FWL\"] excerpt: \"잔차화를 하기 위해 nusiance function을 만들고 결과를 분석하는 노트북\" --- # Load Data # from src.dataloader.vault.main import VaultDatasetLoader from src.dataloader.vault.utils import split_df_od_os from src.utils.db import SOFTCRM_DBINFO, get_conn import warnings import pandas as pd import numpy as np warnings.filterwarnings(\"ignore\") # pandas display 옵션 설정 - 모든 열 보이기 pd.set_option('display.max_columns', None) pd.set_option('display.width', None) pd.set_option('display.max_colwidth', None) loader = VaultDatasetLoader() feature_df, y_df = loader.run(start_date=\"2022-01-01\", end_date=\"2024-12-31\", piol_data_path=\"./data/PIOL렌즈주문서.xlsx\", refresh=False, db=\"crm_emr\") # y_df의 oper_date와 feature_df의 date를 기준으로 병합 merged_df = pd.merge(y_df, feature_df, left_on=['cust_num', 'oper_date'], right_on=['cust_num', 'date'], how='left') merged_df = split_df_od_os(merged_df) query = \"\"\"SELECT CUST_NUM, BIRTH_DAY FROM VIEW_CUSTOM_JUMIN\"\"\" with get_conn(SOFTCRM_DBINFO) as conn, conn.cursor() as cursor: cursor.execute(query) result = cursor.fetchall() age_df = pd.DataFrame([list(res) for res in result], columns=[\"cust_num\", \"birthday\"]) age_df = age_df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip() == \"\" else x) age_df[\"birthday\"] = pd.to_datetime(age_df[\"birthday\"], errors=\"coerce\", format=\"%Y-%m-%d\") age_df = age_df.dropna().reset_index(drop=True) age_df[\"cust_num\"] = age_df[\"cust_num\"].apply(lambda x: x.strip()) cust_age_dict = dict(zip(age_df[\"cust_num\"], age_df[\"birthday\"])) merged_df['birthday'] = merged_df['cust_num'].map(cust_age_dict) merged_df['age'] = merged_df.apply(lambda row: pd.to_datetime(row['oper_date']).year - row['birthday'].year if pd.notnull(row['birthday']) else np.nan, axis=1) # merged_df.to_csv(\"./merged_df_2022_01_01_2024_12_31_with_age.csv\", index=False) # Modules from src.preprocessing.constants import ALL_RANGES_NO_SIDE import pandas as pd import numpy as np def load_data() -> tuple[pd.DataFrame, list, list, list]: \"\"\" 데이터를 로드하고 전처리하여 분석에 필요한 형태로 반환합니다. Returns: tuple[pd.DataFrame, list, list, list]: df_clean (pd.DataFrame): 전처리된 데이터프레임 features (list): 특성 변수명 리스트 [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] treatment (list): 처치 변수명 리스트 [\"size\"] y (list): 결과 변수명 리스트 [\"meas_vault\"] Note: NaN 값이 있는 행은 제거됩니다 size가 [12.1, 12.6, 13.2]가 아닌 경우는 제거됩니다 \"\"\" df = pd.read_csv(\"./data/merged_df_2022_01_01_2024_12_31_with_age.csv\") df.head() features = [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] treatment = [\"size\"] y = [\"meas_vault\"] # df 중 features, treatment, y 컬럼만 남기고 칼럼 제거, 이 칼럼들에 대한 값이 NaN인 것도 제거 selected_columns = features + treatment + y df_clean = df[selected_columns].copy() # df['size']가 [12.1, 12.6, 13.2] 이 아닌 경우 제거 df_clean = df_clean[df_clean[\"size\"].isin([12.1, 12.6, 13.2])].reset_index(drop=True) # 이상치 제거 로직 # 모든 컬럼에 대해 범위 적용 for column, range_obj in ALL_RANGES_NO_SIDE.items(): if column in df_clean.columns: # 컬럼 값이 범위를 벗어나면 np.nan으로 변경 df_clean[column] = df_clean[column].where((df_clean[column] >= range_obj.min) & (df_clean[column] <= range_obj.max), np.nan) # Cas_cct로 vault 범위 만들고, 넘어가면 없애주기 max_vault = ALL_RANGES_NO_SIDE[\"Cas_cct\"].max 1.5 df_clean = df_clean[df_clean[\"meas_vault\"] <= max_vault].reset_index(drop=True) # NaN 값이 있는 행 제거 df_clean = df_clean.dropna() # 나이가 60이상 제거 df_clean = df_clean[df_clean[\"age\"] < 60].reset_index(drop=True) df_clean[y] = df_clean[y].abs() return df_clean, features, treatment, y # 잔차화 진행하기 > R-learner는 FWL theorem에 이론적 배경을 두고 있습니다. FWL 정리는 다중 회귀분석에서 특정 변수의 계수를 구하는 두 가지 방법이 동일한 결과를 준다는 것을 보여줍니다: 직접 회귀: 모든 변수를 포함한 전체 회귀모형 잔차화 후 회귀: 관심 변수와 종속변수를 다른 변수들에 대해 각각 회귀하여 잔차를 구한 후, 이 잔차들 간의 회귀 수학적 표현: 모형: Y = Xβ + Zγ + ε에서 β를 구하고 싶다면: 방법 1: 전체 회귀 `` β̂ = (X'M_Z X)^(-1) X'M_Z Y ` 방법 2: 잔차화 ` X̃ = M_Z X (X를 Z에 회귀한 잔차) Ỹ = M_Z Y (Y를 Z에 회귀한 잔차) β̂ = (X̃'X̃)^(-1) X̃'Ỹ ` 여기서 M_Z = I - Z(Z'Z)^(-1)Z'는 Z에 수직인 투영행렬입니다. 잔차화가 가능해지는 이유: 직교성: 잔차화는 다른 변수들의 영향을 제거하여 순수한 효과만 분리 차원 축소: 복잡한 다변량 문제를 단순한 이변량 문제로 변환 해석 용이성: 다른 변수들을 통제한 후의 순수한 관계 파악 > R-learner는 다음과 같은 두 단계로 구현됩니다: Propensity Score: Treatment를 Features로 예측 Outcome Function: Outcome을 Features로 예측 Step 1: Propensity Score (Treatment 잔차화): `python X → T 회귀 (Treatment를 Features로 예측) model = LogisticRegression() model.fit(X_train, y_train) propensity_scores = model.predict_proba(X_val) ` Step 2: Outcome Function (Y 잔차화): `python X → Y 회귀 (Outcome을 Features로 예측) Y_residual = Y - E[Y X] T_residual = T - E[T X] (propensity score 활용) ` 왜 잔차화가 \"가능\"해지는가? : A. 조건부 독립성 확보 ` E[T_residual X] = 0 E[Y_residual X] = 0 ` X의 영향을 제거함으로써 순수한 T→Y 효과만 분리 B. 편향 제거 (Bias Elimination) Selection bias: 관측되지 않은 교란변수의 영향 Confounding: X가 T와 Y 모두에 영향을 주는 문제 C. 과적합 방지 `python Cross-fitting 사용 kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) ` Out-of-sample 예측으로 과적합 방지 실제 인과효과 추정 최종적으로는 다음과 같은 회귀를 수행: ` Y_residual = τ × T_residual + noise ` 여기서 τ`가 순수한 treatment effect입니다. 결론: FWL 정리에서 잔차화가 가능해지는 이유는: 수학적: 직교 투영의 성질로 인한 계수 불변성 통계적: 교란변수 통제를 통한 편향 제거 실용적: 복잡한 다변량 문제를 단순한 이변량 문제로 변환 > R-learner 구현은 이 이론적 기반을 인과추론에 실제 적용한 사례로, 특히 이질적 처치효과(Heterogeneous Treatment Effects) 추정에 활용되고 있습니다. # 1. propensity score function > P(T X) X일 때 T일 확률을 구해서, 이를 통해 잔차화 진행 propensity score를 통제하는 것과 T를 통제하는 것은 같은 의미이기 때문에 df, features, treatment, y = load_data() df[treatment] = df[treatment].astype(\"category\") (treatment,) = treatment print(treatment) df.head() 중요한점! > 과적합을 피하기 위해 cross-fitting (또는 sample splitting)을 사용해서 out-of-sample 예측값을 활용 import numpy as np import pandas as pd from sklearn.model_selection import StratifiedKFold from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score from sklearn.preprocessing import StandardScaler RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) 카테고리형 treatment를 숫자로 변환 X_features = df[features] y_treatment_cat = df[treatment].cat.codes # 카테고리를 숫자로 변환 print(f\"Treatment 카테고리: {df[treatment].cat.categories}\") print(f\"Treatment 분포:\\n{df[treatment].value_counts()}\") print(f\"숫자 변환된 분포:\\n{pd.Series(y_treatment_cat).value_counts()}\") 각 클래스별 propensity score 저장용 배열 class_categories = df[treatment].cat.categories n_classes = len(class_categories) propensity_scores_all = np.zeros((len(df), n_classes)) StratifiedKFold로 카테고리 비율 맞춤 kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) fold_performances = [] for fold, (train_idx, val_idx) in enumerate(kf.split(X_features, y_treatment_cat)): print(f\"\\n=== Fold {fold + 1} ===\") X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx] y_train, y_val = y_treatment_cat[train_idx], y_treatment_cat[val_idx] # 각 fold의 treatment 분포 확인 print(f\"Train 분포: {pd.Series(y_train).value_counts().to_dict()}\") print(f\"Val 분포: {pd.Series(y_val).value_counts().to_dict()}\") # 데이터 스케일링 (수렴 성능 향상) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_val_scaled = scaler.transform(X_val) # 모델 훈련 (수렴 문제 해결을 위한 설정) model = LogisticRegression( random_state=42, max_iter=1000, # 반복 횟수 증가 (기본값: 100) solver=\"liblinear\", # 다중 클래스 분류에 적합한 솔버 ) model.fit(X_train_scaled, y_train) # 훈련 성능 확인 train_pred = model.predict(X_train_scaled) train_accuracy = accuracy_score(y_train, train_pred) # 검증 데이터에 대한 모든 클래스별 propensity score 계산 val_propensity = model.predict_proba(X_val_scaled) propensity_scores_all[val_idx] = val_propensity # 모든 클래스 확률 저장 # 검증 성능 확인 val_pred = model.predict(X_val_scaled) val_accuracy = accuracy_score(y_val, val_pred) fold_performances.append( {\"fold\": fold + 1, \"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy, \"train_size\": len(X_train), \"val_size\": len(X_val)} ) print(f\"훈련 정확도: {train_accuracy:.3f}\") print(f\"검증 정확도: {val_accuracy:.3f}\") 원본 df에 각 클래스별 propensity_score 컬럼 추가 for i, category in enumerate(class_categories): df[f\"propensity_score_{category}\"] = propensity_scores_all[:, i] print(f\"\\n=== 전체 결과 ===\") performance_df = pd.DataFrame(fold_performances) print(f\"평균 훈련 정확도: {performance_df['train_accuracy'].mean():.3f} ± {performance_df['train_accuracy'].std():.3f}\") print(f\"평균 검증 정확도: {performance_df['val_accuracy'].mean():.3f} ± {performance_df['val_accuracy'].std():.3f}\") print(f\"\\n처음 10개 샘플:\") propensity_cols = [f\"propensity_score_{cat}\" for cat in class_categories] print(df[[\"meas_vault\", treatment] + propensity_cols].head(10)) print(f\"\\n생성된 propensity score 컬럼들:\") for col in propensity_cols: print(f\"{col}: 평균 {df[col].mean():.3f}\") df # 2. Outcome function >m(Y X) 이를 통해 Y에서 X의 영향을 제거한 잔차를 구할 수 있음. df, features, treatment, y = load_data() df[treatment] = df[treatment].astype(\"category\") (treatment,) = treatment print(treatment) df.head() 중요한점! > 과적합을 피하기 위해 cross-fitting (또는 sample splitting)을 사용해서 out-of-sample 예측값을 활용 import numpy as np import pandas as pd import lightgbm as lgb from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error import matplotlib.pyplot as plt 한글 폰트 설정 (맥 환경) import matplotlib.font_manager as fm plt.rcParams[\"font.family\"] = \"AppleGothic\" # 맥용 한글 폰트 plt.rcParams[\"axes.unicode_minus\"] = False # 마이너스 기호 깨짐 방지 RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) Outcome function (features로 y 예측) print(\"=== Outcome Function 학습 (features → y) ===\") X_features = df[features] y_outcome = df[y[0]] # y는 리스트이므로 첫 번째 요소 추출 print(f\"Features: {features}\") print(f\"Target: {y[0]}\") print(f\"데이터 크기: X={X_features.shape}, y={y_outcome.shape}\") print(f\"Target 통계:\") print(f\" 평균: {y_outcome.mean():.2f}\") print(f\" 표준편차: {y_outcome.std():.2f}\") print(f\" 범위: [{y_outcome.min():.2f}, {y_outcome.max():.2f}]\") Out-of-sample 예측값 저장용 배열 outcome_predictions = np.zeros(len(df)) KFold로 분할 (회귀 문제이므로 StratifiedKFold 대신 KFold 사용) kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) fold_performances = [] LightGBM 파라미터 설정 (과적합 방지에 중점) lgb_params = { \"objective\": \"regression\", \"metric\": \"rmse\", \"boosting_type\": \"gbdt\", \"num_leaves\": 15, # 작게 설정하여 과적합 방지 \"learning_rate\": 0.1, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 5, \"lambda_l1\": 0.1, # L1 정규화 \"lambda_l2\": 0.1, # L2 정규화 \"min_data_in_leaf\": 20, \"min_split_gain\": 0.1, \"max_depth\": 6, \"verbose\": -1, \"random_state\": RANDOM_SEED, } for fold, (train_idx, val_idx) in enumerate(kf.split(X_features, y_outcome)): print(f\"\\n=== Fold {fold + 1} ===\") X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx] y_train, y_val = y_outcome.iloc[train_idx], y_outcome.iloc[val_idx] print(f\"Train 크기: {len(X_train)}, Val 크기: {len(X_val)}\") print(f\"Train target 평균: {y_train.mean():.2f}, Val target 평균: {y_val.mean():.2f}\") # LightGBM 데이터셋 생성 train_data = lgb.Dataset(X_train, label=y_train) val_data = lgb.Dataset(X_val, label=y_val, reference=train_data) # 모델 훈련 model = lgb.train( lgb_params, train_data, valid_sets=[train_data, val_data], valid_names=[\"train\", \"eval\"], num_boost_round=500, callbacks=[ lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0), # 로그 출력 비활성화 ], ) # 예측 train_pred = model.predict(X_train, num_iteration=model.best_iteration) val_pred = model.predict(X_val, num_iteration=model.best_iteration) # Out-of-sample 예측값 저장 outcome_predictions[val_idx] = val_pred # 성능 계산 train_rmse = np.sqrt(mean_squared_error(y_train, train_pred)) val_rmse = np.sqrt(mean_squared_error(y_val, val_pred)) train_r2 = r2_score(y_train, train_pred) val_r2 = r2_score(y_val, val_pred) train_mae = mean_absolute_error(y_train, train_pred) val_mae = mean_absolute_error(y_val, val_pred) fold_performances.append( { \"fold\": fold + 1, \"train_rmse\": train_rmse, \"val_rmse\": val_rmse, \"train_r2\": train_r2, \"val_r2\": val_r2, \"train_mae\": train_mae, \"val_mae\": val_mae, \"best_iteration\": model.best_iteration, \"train_size\": len(X_train), \"val_size\": len(X_val), } ) print(f\"훈련 RMSE: {train_rmse:.3f}, 검증 RMSE: {val_rmse:.3f}\") print(f\"훈련 R²: {train_r2:.3f}, 검증 R²: {val_r2:.3f}\") print(f\"훈련 MAE: {train_mae:.3f}, 검증 MAE: {val_mae:.3f}\") print(f\"최적 반복횟수: {model.best_iteration}\") 원본 df에 outcome 예측값 추가 df[\"outcome_prediction\"] = outcome_predictions print(f\"\\n=== 전체 Outcome Function 결과 ===\") performance_df = pd.DataFrame(fold_performances) print(f\"평균 훈련 RMSE: {performance_df['train_rmse'].mean():.3f} ± {performance_df['train_rmse'].std():.3f}\") print(f\"평균 검증 RMSE: {performance_df['val_rmse'].mean():.3f} ± {performance_df['val_rmse'].std():.3f}\") print(f\"평균 훈련 R²: {performance_df['train_r2'].mean():.3f} ± {performance_df['train_r2'].std():.3f}\") print(f\"평균 검증 R²: {performance_df['val_r2'].mean():.3f} ± {performance_df['val_r2'].std():.3f}\") print(f\"평균 훈련 MAE: {performance_df['train_mae'].mean():.3f} ± {performance_df['train_mae'].std():.3f}\") print(f\"평균 검증 MAE: {performance_df['val_mae'].mean():.3f} ± {performance_df['val_mae'].std():.3f}\") 과적합 체크 overfitting_score = performance_df[\"train_r2\"].mean() - performance_df[\"val_r2\"].mean() print(f\"\\n과적합 지표 (Train R² - Val R²): {overfitting_score:.3f}\") if overfitting_score > 0.1: print(\"⚠️ 심각한 과적합이 감지되었습니다. 정규화를 강화하세요.\") elif overfitting_score > 0.05: print(\"⚠️ 약간의 과적합이 있습니다.\") else: print(\"✅ 과적합이 잘 제어되고 있습니다.\") print(f\"\\n처음 10개 샘플의 실제값 vs 예측값:\") comparison_df = df[[y[0], \"outcome_prediction\"]].head(10) comparison_df[\"residual\"] = comparison_df[y[0]] - comparison_df[\"outcome_prediction\"] print(comparison_df) print(f\"\\nOutcome 예측값 통계:\") print(f\" 평균: {df['outcome_prediction'].mean():.2f}\") print(f\" 표준편차: {df['outcome_prediction'].std():.2f}\") print(f\" 범위: [{df['outcome_prediction'].min():.2f}, {df['outcome_prediction'].max():.2f}]\") 잔차 계산 (R-learner의 다음 단계를 위해) df[\"outcome_residual\"] = df[y[0]] - df[\"outcome_prediction\"] print(f\"\\nOutcome 잔차 통계:\") print(f\" 평균: {df['outcome_residual'].mean():.2f} (0에 가까워야 함)\") print(f\" 표준편차: {df['outcome_residual'].std():.2f}\") 시각화 plt.figure(figsize=(12, 4)) 실제값 vs 예측값 plt.subplot(1, 3, 1) plt.scatter(df[y[0]], df[\"outcome_prediction\"], alpha=0.6, s=10) plt.plot([df[y[0]].min(), df[y[0]].max()], [df[y[0]].min(), df[y[0]].max()], \"r--\", lw=2) plt.xlabel(\"실제값\") plt.ylabel(\"예측값\") plt.title(\"실제값 vs 예측값\") 잔차 분포 plt.subplot(1, 3, 2) plt.hist(df[\"outcome_residual\"], bins=30, alpha=0.7, edgecolor=\"black\") plt.xlabel(\"잔차\") plt.ylabel(\"빈도\") plt.title(\"잔차 분포\") plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) Fold별 성능 plt.subplot(1, 3, 3) plt.plot(performance_df[\"fold\"], performance_df[\"train_r2\"], \"o-\", label=\"Train R²\", alpha=0.7) plt.plot(performance_df[\"fold\"], performance_df[\"val_r2\"], \"s-\", label=\"Val R²\", alpha=0.7) plt.xlabel(\"Fold\") plt.ylabel(\"R² Score\") plt.title(\"Fold별 성능\") plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() 성능 요약 테이블 print(f\"\\n=== Fold별 성능 요약 ===\") print(performance_df[[\"fold\", \"train_rmse\", \"val_rmse\", \"train_r2\", \"val_r2\", \"best_iteration\"]].round(3)) df.iloc[[4799, 4800]] import matplotlib.pyplot as plt import numpy as np 한글 폰트 설정 plt.rcParams[\"font.family\"] = \"AppleGothic\" plt.rcParams[\"axes.unicode_minus\"] = False Size별 잔차 분포 시각화 fig, axes = plt.subplots(2, 3, figsize=(18, 10)) Size별 히스토그램 (겹치게) plt.subplot(2, 3, 1) for size in sorted(df[\"size\"].unique()): residuals = df[df[\"size\"] == size][\"outcome_residual\"] plt.hist(residuals, bins=30, alpha=0.7, label=f\"Size {size}\", density=True) plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.xlabel(\"Outcome 잔차\") plt.ylabel(\"밀도\") plt.title(\"Size별 잔차 분포 (히스토그램)\") plt.legend() plt.grid(True, alpha=0.3) Size별 박스플롯 plt.subplot(2, 3, 2) size_groups = [df[df[\"size\"] == size][\"outcome_residual\"] for size in sorted(df[\"size\"].unique())] size_labels = [f\"Size {size}\" for size in sorted(df[\"size\"].unique())] plt.boxplot(size_groups, labels=size_labels) plt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.ylabel(\"Outcome 잔차\") plt.title(\"Size별 잔차 분포 (박스플롯)\") plt.grid(True, alpha=0.3) 바이올린 플롯 plt.subplot(2, 3, 3) parts = plt.violinplot(size_groups, positions=range(1, len(size_groups) + 1)) plt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.xticks(range(1, len(size_labels) + 1), size_labels) plt.ylabel(\"Outcome 잔차\") plt.title(\"Size별 잔차 분포 (바이올린 플롯)\") plt.grid(True, alpha=0.3) 4-6. Size별 개별 히스토그램 for i, size in enumerate(sorted(df[\"size\"].unique())): plt.subplot(2, 3, 4 + i) residuals = df[df[\"size\"] == size][\"outcome_residual\"] plt.hist(residuals, bins=30, alpha=0.7, edgecolor=\"black\") plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.axvline(residuals.mean(), color=\"blue\", linestyle=\"-\", alpha=0.7, label=f\"평균: {residuals.mean():.1f}\") plt.xlabel(\"Outcome 잔차\") plt.ylabel(\"빈도\") plt.title(f\"Size {size} 잔차 분포 (n={len(residuals)})\") plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() Size별 통계 요약 print(\"=== Size별 잔차 통계 ===\") for size in sorted(df[\"size\"].unique()): residuals = df[df[\"size\"] == size][\"outcome_residual\"] print(f\"\\nSize {size} (n={len(residuals)}):\") print(f\" 평균: {residuals.mean():8.2f}\") print(f\" 표준편차: {residuals.std():8.2f}\") print(f\" 중앙값: {residuals.median():8.2f}\") print(f\" 범위: [{residuals.min():6.1f}, {residuals.max():6.1f}]\") 전체 잔차 통계 print(f\"\\n전체 (n={len(df)}):\") print(f\" 평균: {df['outcome_residual'].mean():8.2f}\") print(f\" 표준편차: {df['outcome_residual'].std():8.2f}\") ANOVA 검정 (size별 잔차 평균 차이 검정) from scipy.stats import f_oneway size_12_1 = df[df[\"size\"] == 12.1][\"outcome_residual\"] size_12_6 = df[df[\"size\"] == 12.6][\"outcome_residual\"] size_13_2 = df[df[\"size\"] == 13.2][\"outcome_residual\"] f_stat, p_value = f_oneway(size_12_1, size_12_6, size_13_2) print(f\"\\n=== ANOVA 검정 (size별 잔차 평균 차이) ===\") print(f\"F-통계량: {f_stat:.4f}\") print(f\"p-value: {p_value:.6f}\") if p_value < 0.05: print(\"❌ Size별로 잔차 평균에 유의한 차이가 있음 (문제)\") else: print(\"✅ Size별 잔차 평균에 유의한 차이 없음 (좋음)\") > 잔차화가 잘 됐는지는 어떻게 판단할 수 있는거지? 주요 평가 기준: 조건부 독립성 (가장 중요) E[잔차 X] ≈ 0 R² < 0.01 이면 우수 R² > 0.05 이면 문제 편향 제거 잔차 평균 ≈ 0 체계적인 과대/과소예측 없음 정규성 잔차가 정규분포 형태 왜도, 첨도가 0에 가까움 잔차 간 독립성 Treatment 잔차와 Outcome 잔차 간 낮은 상관관계 상관계수 < 0.1 이면 이상적 import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.font_manager as fm from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from scipy import stats 한글 폰트 설정 plt.rcParams[\"font.family\"] = \"AppleGothic\" plt.rcParams[\"axes.unicode_minus\"] = False def check_conditional_independence_comprehensive(df): \"\"\" Outcome 잔차의 조건부 독립성을 종합적으로 확인 \"\"\" features = [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] residual_col = \"outcome_residual\" print(\"=\" 60) print(\"조건부 독립성 검사: E[outcome_residual X] = 0\") print(\"=\" 60) X = df[features] residuals = df[residual_col] # ===== 1. 전체 R² 확인 ===== print(\"\\n1. 전체 조건부 독립성 검사\") print(\"-\" 40) model = LinearRegression() model.fit(X, residuals) predictions = model.predict(X) r2_total = r2_score(residuals, predictions) print(f\"Features로 잔차 예측 R²: {r2_total:.6f}\") # R² 평가 if r2_total < 0.001: print(\"🎉 우수: 매우 강한 조건부 독립성\") grade = \"A+\" elif r2_total < 0.01: print(\"✅ 우수: 강한 조건부 독립성\") grade = \"A\" elif r2_total < 0.05: print(\"⚠️ 양호: 약간의 의존성 존재\") grade = \"B\" elif r2_total < 0.1: print(\"⚠️ 보통: 중간 정도의 의존성\") grade = \"C\" else: print(\"❌ 문제: 강한 의존성 - 모델 개선 필요\") grade = \"D\" # ===== 2. 각 feature별 상관관계 ===== print(f\"\\n2. 각 Feature별 상관관계\") print(\"-\" 40) correlations = {} for feature in features: corr = np.corrcoef(df[feature], residuals)[0, 1] correlations[feature] = corr if abs(corr) < 0.05: status = \"✅ 매우 낮음\" elif abs(corr) < 0.1: status = \"⚠️ 낮음\" elif abs(corr) < 0.2: status = \"⚠️ 중간\" else: status = \"❌ 높음\" print(f\"{feature:15}: {corr:8.4f} {status}\") # ===== 3. 잔차의 기본 통계 ===== print(f\"\\n3. 잔차 기본 통계\") print(\"-\" 40) mean_residual = residuals.mean() std_residual = residuals.std() skew_residual = stats.skew(residuals) kurt_residual = stats.kurtosis(residuals) print(f\"평균: {mean_residual:8.4f} (0에 가까워야 함)\") print(f\"표준편차: {std_residual:8.4f}\") print(f\"왜도: {skew_residual:8.4f} (0에 가까워야 함)\") print(f\"첨도: {kurt_residual:8.4f} (0에 가까워야 함)\") # ===== 4. 통계적 검정 ===== print(f\"\\n4. 통계적 검정\") print(\"-\" 40) # F-test for overall significance n = len(residuals) k = len(features) f_stat = (r2_total / k) / ((1 - r2_total) / (n - k - 1)) f_p_value = 1 - stats.f.cdf(f_stat, k, n - k - 1) print(f\"F-검정 (전체 모델 유의성):\") print(f\" F-통계량: {f_stat:.4f}\") print(f\" p-value: {f_p_value:.6f}\") if f_p_value > 0.05: print(\" ✅ 잔차와 features 간 유의한 관계 없음 (좋음)\") else: print(\" ❌ 잔차와 features 간 유의한 관계 존재 (문제)\") # Shapiro-Wilk test for normality (샘플이 너무 크면 생략) if len(residuals) <= 5000: shapiro_stat, shapiro_p = stats.shapiro(residuals) print(f\"\\nShapiro-Wilk 정규성 검정:\") print(f\" 통계량: {shapiro_stat:.4f}\") print(f\" p-value: {shapiro_p:.6f}\") if shapiro_p > 0.05: print(\" ✅ 잔차가 정규분포를 따름\") else: print(\" ⚠️ 잔차가 정규분포를 벗어남\") # ===== 5. 시각화 ===== print(f\"\\n5. 시각적 진단\") print(\"-\" 40) fig, axes = plt.subplots(2, 3, figsize=(18, 12)) fig.suptitle(\"조건부 독립성 진단\", fontsize=16) # 5-1. 각 feature vs 잔차 for i, feature in enumerate(features): row = i // 2 col = i % 2 axes[row, col].scatter(df[feature], residuals, alpha=0.6, s=10) axes[row, col].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) # 추세선 z = np.polyfit(df[feature], residuals, 1) p = np.poly1d(z) x_line = np.linspace(df[feature].min(), df[feature].max(), 100) axes[row, col].plot(x_line, p(x_line), \"r-\", alpha=0.8, linewidth=2) axes[row, col].set_xlabel(feature) axes[row, col].set_ylabel(\"Outcome 잔차\") axes[row, col].set_title(f\"{feature} vs 잔차 (상관계수: {correlations[feature]:.3f})\") axes[row, col].grid(True, alpha=0.3) # 5-2. 예측값 vs 잔차 axes[1, 0].scatter(df[\"outcome_prediction\"], residuals, alpha=0.6, s=10) axes[1, 0].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) z = np.polyfit(df[\"outcome_prediction\"], residuals, 1) p = np.poly1d(z) x_line = np.linspace(df[\"outcome_prediction\"].min(), df[\"outcome_prediction\"].max(), 100) axes[1, 0].plot(x_line, p(x_line), \"r-\", alpha=0.8, linewidth=2) axes[1, 0].set_xlabel(\"예측값\") axes[1, 0].set_ylabel(\"잔차\") axes[1, 0].set_title(\"예측값 vs 잔차 (등분산성 검사)\") axes[1, 0].grid(True, alpha=0.3) # 5-3. 잔차 분포 axes[1, 1].hist(residuals, bins=50, density=True, alpha=0.7, edgecolor=\"black\") # 정규분포 곡선 오버레이 mu, sigma = residuals.mean(), residuals.std() x_norm = np.linspace(residuals.min(), residuals.max(), 100) axes[1, 1].plot(x_norm, stats.norm.pdf(x_norm, mu, sigma), \"r-\", lw=2, label=\"정규분포\") axes[1, 1].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) axes[1, 1].set_xlabel(\"잔차\") axes[1, 1].set_ylabel(\"밀도\") axes[1, 1].set_title(\"잔차 분포\") axes[1, 1].legend() axes[1, 1].grid(True, alpha=0.3) plt.tight_layout() plt.show() # ===== 6. Q-Q Plot ===== plt.figure(figsize=(8, 6)) stats.probplot(residuals, dist=\"norm\", plot=plt) plt.title(\"Q-Q Plot: 잔차 정규성 검정\") plt.grid(True, alpha=0.3) plt.show() # ===== 7. 종합 평가 ===== print(f\"\\n\" + \"=\" 60) print(\"종합 평가\") print(\"=\" 60) issues = [] if r2_total > 0.05: issues.append(\"조건부 독립성 위반\") if abs(mean_residual) > 0.1: issues.append(\"편향 존재\") if any(abs(corr) > 0.2 for corr in correlations.values()): issues.append(\"강한 feature 의존성\") if f_p_value < 0.05: issues.append(\"통계적으로 유의한 의존성\") print(f\"조건부 독립성 등급: {grade}\") print(f\"전체 R²: {r2_total:.6f}\") print(f\"발견된 문제: {len(issues)}개\") if issues: print(\"\\n문제점:\") for issue in issues: print(f\" ❌ {issue}\") print(f\"\\n개선 방안:\") print(f\" 🔧 모델 복잡도 증가 (더 많은 features, interaction terms)\") print(f\" 🔧 비선형 변환 적용\") print(f\" 🔧 정규화 파라미터 조정\") print(f\" 🔧 다른 알고리즘 시도 (XGBoost, Neural Network)\") else: print(\"\\n🎉 조건부 독립성이 잘 만족됩니다!\") print(\"✅ R-learner의 다음 단계로 진행 가능\") return { \"r2_total\": r2_total, \"correlations\": correlations, \"mean_residual\": mean_residual, \"f_p_value\": f_p_value, \"grade\": grade, \"issues\": issues, } 실행 results = check_conditional_independence_comprehensive(df) # 3. CATE 추정 모델 > CATE(X, T) class MultiTreatmentRLearner: def __init__(self, model_y, model_t, model_final): self.model_y = model_y # Y 모델 self.model_t = model_t # Treatment propensity 모델 self.model_final = model_final # Final stage 모델 def fit(self, Y, T, X, W=None): # Step 1: Estimate E[Y X,W] if W is not None: XW = np.concatenate([X, W], axis=1) else: XW = X self.model_y.fit(XW, Y) Y_pred = self.model_y.predict(XW) Y_res = Y - Y_pred # Y residuals # Step 2: Multi-treatment propensity estimation # One-hot encode treatments T_onehot = pd.get_dummies(T, drop_first=True) T_pred = np.zeros_like(T_onehot) for k, col in enumerate(T_onehot.columns): # Each treatment vs rest self.model_t.fit(XW, T_onehot[col]) T_pred[:, k] = self.model_t.predict_proba(XW)[:, 1] T_res = T_onehot - T_pred # Treatment residuals # Step 3: Final stage - regress Y_res on T_res X # Create interaction terms features = [] for k in range(T_res.shape[1]): # θ_k(X) = X α_k (linear case) interaction = T_res[:, k : k + 1] X features.append(interaction) final_features = np.concatenate(features, axis=1) self.model_final.fit(final_features, Y_res) def predict(self, X, T0, T1): # Predict treatment effects between T0 and T1 # θ(X) (T1 - T0) pass from econml.dml import DML from econml.metalearners import XLearner, TLearner EconML의 meta-learners는 multi-treatment 지원 xl = XLearner(models=RandomForestRegressor()) xl.fit(Y, T, X=X) # T can be multi-valued from causalml.inference.meta import XLearner Multi-treatment support",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/what-can-I-do-in-extrapolation-problem",
    "title": "외삽 문제에서 DS가 할 수 있는 방법들",
    "date": "2025-09-05",
    "excerpt": "외삽 문제 상황에 어떻게 대처할 수 있을까?",
    "category": "Causal Inference",
    "tags": [
      "extrapolation"
    ],
    "content": "ML(AI) 모델들은 학습된 분포에 과적합되기 쉽고, robustness가 보장되기 어렵습니다. 즉 입력되는 데이터 분포가 바뀌면 (domain shift) 성능을 보장할 수 없습니다. 이를 해결하기 위해 domain adaptation과 같은 연구들이 진행되고 있습니다. 학습된 데이터 분포 밖의 영역을 예측하는 문제인, 외삽 문제 (extrapolation)에도 마찬가지로 취약합니다. 하지만 서비스를 구성하다보면 외삽을 해야하는 경우가 있습니다. 외삽을 해야만할때 모델을 만드는 사람은 어떻게 대응을 할 수 있는지, 현재 서비스에서는 어떻게 적용하려고 하는지 글로 정리했습니다. 외삽을 해야하는 문제 상황 시력교정을 위한 렌즈 삽입 수술에 사용되는, 렌즈 추천 서비스를 구성하고 있습니다. 고객에게 적절한 \"크기\"의 사이즈를 선택하도록 도와주는 문제입니다. 이를 위해 렌즈 크기 별로 결과를 예측해 제공하고, 이를 통해 의사 (surgeon)가 적절한 크기를 선택하도록 돕습니다. 여기서 문제 상황은 현재 사용하고 있는 렌즈 크기가 10, 30, 50 이 있을 때, 회사에서 그 중간 크기인 20, 40인 렌즈 크기를 개발해 판매를 하려고 하는 상황입니다. 이때 데이터에 없는 20, 40에 예측값을 제공해줘야 합니다. ( 크기에 대한 관계가 비선형적이고, 새로운 제품이기 때문에 내삽으로 보기엔 어려움 ) 해결방안에 대한 생각 간단히 정리 <figure> <img src=\"/post/Causal_Inference/extrapolation_idea_sketch.png\" alt=\"extrapolation_idea_sketch\" width=\"100%\"/> <figcaption>그림1. 아이디어 스케치</figcaption> </figure> 외삽에 대응하는 방법은 크게 4가지가 있습니다. Uncertainty Quantification (불확실성 정량화) 모델이 예측할 때 얼마나 확신하는지, 얼마나 불확실한지를 수치로 나타내는 방법 예: \"20 크기 렌즈의 성공률은 70% ± 15%\"처럼 신뢰구간을 제공 의사가 예측의 신뢰도를 판단할 수 있게 도움 Inductive Bias (귀납적 편향) 데이터의 구조나 특성을 반영하도록 모델을 설계하는 방법 예: 렌즈 크기와 성공률이 비선형 관계라면, 비선형 모델을 사용 도메인 지식을 모델 구조에 반영 Domain Adaptation, Transfer Learning 다른 도메인의 데이터나 지식을 활용하는 방법 예: 다른 의료기기 데이터나 다른 병원의 렌즈 데이터를 활용 적은 데이터로도 더 나은 성능을 얻을 수 있음 Counterfactual Reasoning, Causal Inference \"만약 이 환자에게 20 크기 렌즈를 사용했다면?\" 같은 가정적 추론 단순한 상관관계가 아닌 인과관계를 파악 새로운 크기의 효과를 더 정확히 예측할 수 있음 2가지 순서로 연구를 진행하고, 논문을 작성 중입니다. 첫번째로 작성 중인 내용은 신뢰할 수 있는 범위 (외삽의 범위, 양수성이 보장되는 범위)를 판단하고 이에 따라 불확실성을 정량화해서 표현하는 방법에 대한 주제입니다. 기존의 연구들은 모두 예측 지표만을 고도화하는 방향으로만 진행되어 왔는데, 이에 대한 한계점을 지적하며 해결방안을 제시하는 방향입니다. 두번쨰로 작성할 부분은 인과추론 모델입니다. 외삽 문제에서 가장 좋은 방법은 모델 자체가 robustness를 가지는 것으로, 데이터 분포에서 인과적인 패턴을 학습하는 것입니다. 이를 위해 R-learner와 같은 모델을 사용합니다. 다만 여기서 가장 어려운 점은 \"평가\"를 어떻게 할 것이냐에 대한 부분인데, 이를 위해서 2가지 방법을 생각 중입니다. 시뮬레이션 데이터 생성 casual effect + noise (ex: confounding bias) 를 가지는 데이터를 만들고, 이때 인과추론 모델은 실제로 인과패턴을 찾아서 학습하는지를 확인하는 것입니다. 데이터 안에서 CATE 평가 가지고 있는 데이터에서 간접적으로 모델의 성능을 평가하는 방법입니다. 정리 외삽 문제를 다뤄야하는 상황에서 합리적인 모델을 만들기 위해 노력하고 있습니다. ( 그냥 학습한 예측 모델로 결과를 뽑지 않고 ) 그래서 이런 연구가 회사의 상품 혹은 서비스를 더 잘 팔리게 하는지? 에 대한 생각을 하게 됩니다. 이 부분은 추측이긴 하지만, 논문을 통해 모델의 신뢰성을 보여준다는 측면에서 매력을 가지지 않을까? 에 대한 생각과 외삽에서 발생하는 문제들을 보완함으로써 사용하면서 신뢰가져갈 수 있지 않을까? 라는 생각을 해봅니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/수입과지출/25-09-04-할여유가된다",
    "title": "I can afford it - 수입과 지출 표현 익히기",
    "date": "2025-09-04",
    "excerpt": "일상에서 자주 사용하는 '할 여유가 있다/없다' 표현을 다양한 상황별 대화를 통해 자연스럽게 익혀보세요. 쇼핑, 여행, 식사 등 실제 상황에서 바로 쓸 수 있는 실용적인 영어 표현들을 학습합니다.",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "수입과 지출에 대한 표현들"
    ],
    "content": "I can afford it. I've been saving, so I can afford it. bed and breakfast : 민박집 I'm saving for retirement, so I can't affort it. 대화 시뮬레이션 상황 1: 친구와 쇼핑할 때 A: Hey, do you want to go to that new restaurant downtown? I heard the food is amazing! B: I'd love to, but I'm saving for retirement, so I can't afford it right now. A: Oh, I understand. How about we try that bed and breakfast place instead? It's much cheaper. B: Actually, I've been saving, so I can afford it. Let's go to the original restaurant! 상황 2: 가족과 여행 계획할 때 Mom: Kids, we're thinking about a family trip to Europe this summer. What do you think? Son: Mom, I can afford it! I've been working part-time and saving money. Daughter: I'm saving for college, so I can't afford it. But maybe we could find a cheaper alternative? Dad: That's very responsible of you both. Let's look for some budget-friendly options. 상황 3: 동료와 점심 식사할 때 Colleague: Want to grab lunch at that fancy sushi place? You: I can afford it, but I'm trying to be more careful with my spending lately. Colleague: Fair enough. How about the food court instead? You: Perfect! I've been saving, so I can afford that easily. 상황 4: 룸메이트와 생활비 논의할 때 Roommate: The rent is going up next month. Can you handle the increase? You: I'm saving for a down payment on a house, so I can't afford it if it's too much. Roommate: I understand. I've been saving too, so I can afford a bit more. Let's split it fairly. You: Thanks! That really helps me out.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-causal-representation-learning",
    "title": "Causal Representaion Learning에 대한 정리",
    "date": "2025-09-04",
    "excerpt": "Causal Representation에 대한 개념 정리",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "참고 자료 1: LG 리서치 블로그 ---",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/수입과지출/25-09-03-나는돈의대부분을월세에써",
    "title": "나는 돈의 대부분을 월세에 써",
    "date": "2025-09-03",
    "excerpt": "I spend most of my money on rent",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "수입과 지출에 대한 표현들"
    ],
    "content": "self-care vs impulsive buy I spend some of money on groceries. some - a lot of - most I sometimes treat myself to a fancy dinner. I don't spend that much on gas. I spend too much money on gas. He treated me to brunch. 위에 대한 대화 시뮬레이션 대화 1: 월세와 생활비에 대한 대화 A: How's your budget looking this month? B: Honestly, I spend most of my money on rent. It's really eating into my savings. A: I know what you mean. I spend some of my money on groceries, but rent takes up the biggest chunk. B: Yeah, and I sometimes treat myself to a fancy dinner, but I have to be careful not to overspend. A: At least you don't spend that much on gas. I spend too much money on gas because I commute so far. B: That's true. I'm lucky to live close to work. 대화 2: 친구와의 식사 A: Thanks for treating me to brunch today! B: No problem! I don't mind spending money on good food with friends. A: You're so generous. I usually try to balance self-care purchases with my budget. B: I understand. It's important to treat yourself sometimes, but not make impulsive buys. A: Exactly. I spend some of my money on things I enjoy, but I try to be mindful about it. 대화 3: 생활비 관리 A: How do you manage your monthly expenses? B: Well, I spend most of my money on rent, so I have to be careful with everything else. A: What about other necessities? B: I spend some of my money on groceries, and I don't spend that much on gas since I walk to work. A: That's smart. I spend too much money on gas because I drive everywhere. B: Maybe you could try walking or taking public transportation more often? A: That's a good idea. I could use that money for other things instead. 대화 4: 쇼핑과 소비 A: I'm trying to cut back on unnecessary spending. B: That's a good goal. What do you usually spend money on? A: I sometimes treat myself to a fancy dinner, but I'm trying to cook more at home. B: Cooking at home is definitely more cost-effective. I spend some of my money on groceries, but it's still cheaper than eating out. A: You're right. I need to avoid those impulsive buys and focus on self-care that doesn't cost much. B: Exactly. There are plenty of ways to treat yourself without spending a lot of money.",
    "public": true
  },
  {
    "id": "Data Science/AI Engineering/paper-review-causal-transformer",
    "title": "[Paper Review] Causal Transformer for Estimating Counterfactual Outcomes ",
    "date": "2025-09-03",
    "excerpt": "Causal Transformer for Estimating Counterfactual Outcomes 논문 리뷰",
    "category": "AI Engineering",
    "tags": [
      "Transformer",
      "Causal Inference"
    ],
    "content": "논문 링크 <figure> <img src=\"/post/ML/하이퍼커넥트-모델설명2.png\" alt=\"하이퍼커넥트-모델설명\" width=\"100%\" /> </figure> <figure> <img src=\"/post/ML/하이퍼커넥트-모델설명.png\" alt=\"하이퍼커넥트-모델설명\" width=\"100%\" /> <figcaption>그림1. 하이퍼커넥트 모델설명</figcaption> </figure> <small>설명 블로그 링크</small> 아자르에서 two-tower 모델 기반으로 추천 모델을 구성하고 있고, 이때 유저의 임베딩을 구할 때 causal transformer를 사용한다고 합니다. 인과추론 모델들을 적용해보려고 시도하던 중, 이 causal transformer를 알면 도움이 될 수도 있을 것 같아 찾아보게 됐습니다.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-09-02-self-introduction",
    "title": "자기소개의 어려움",
    "date": "2025-09-02",
    "excerpt": "3줄 자기소개... 나를 소개하는 건 늘 어렵다",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "자기소개는 늘 어려운 것 같다... 나를 소개한다는게, 내가 아직 많은 모습 중에 어떤 부분을 말해야할지 모르겠다는 어려움이 있다. 10차원의 공간을 2차원으로 줄이는 느낌? 어쨌든 \"선택\"이 필요한 부분인데, 선택을 잘하기 위해서는 명확한 기준이 필요하다. \"기준\" 이번에 가짜연구소 스터디에 지원하면서 3줄 자기소개를 적게 됐다. 스터디에 맞는 나의 모습은 어떤걸까? 어떤 부분을 이야기하면 좋을까? 성장을 목적으로 하는 곳이니, 성장에 대한 나의 모습을 들어내면 좋을 것 같다. 키워드만 적어보자. 데싸, 인과추론, 업무 적용, 문제를 고민하고 푸는 걸 좋아함,",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/수입과지출/25-09-02-나는주식으로돈을별로못벌었어",
    "title": "나는 주식으로 돈을 별로 못 벌었어",
    "date": "2025-09-02",
    "excerpt": "I didn't make much money from stocks",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "수입과 지출에 대한 표현들"
    ],
    "content": "It's payday. I model on the side. I tutor on the side. I made quite a bit of money from real estate. I didn't make any money from side hustle. 관련된 대화 시뮬레이션 상황 1: 친구와 투자 경험 나누기 Friend: \"How's your investment going? Did you make any money from stocks?\" You: \"Actually, I didn't make much money from stocks. I lost some money in the beginning, so I'm being more careful now.\" Friend: \"That's too bad. What about other investments?\" You: \"I made quite a bit of money from real estate, but stocks haven't been profitable for me.\" --- 상황 2: 가족과 수입에 대해 이야기하기 Family Member: \"How are you doing financially these days?\" You: \"It's payday today, so I'm doing okay. I have some side hustles too.\" Family Member: \"What kind of side jobs do you do?\" You: \"I model on the side and tutor on the side. But I didn't make any money from my other side hustle.\" --- 상황 3: 동료와 부업에 대해 이야기하기 Colleague: \"Do you have any side jobs to make extra money?\" You: \"Yes, I do some modeling and tutoring on the side.\" Colleague: \"How's that working out for you?\" You: \"It's okay, but I didn't make much money from stocks when I tried investing. Real estate has been better for me.\" --- 핵심 표현 정리 I didn't make much money from stocks - 주식으로 돈을 별로 못 벌었어 I made quite a bit of money from real estate - 부동산으로 꽤 돈을 벌었어 I model on the side - 부업으로 모델 일을 해 I tutor on the side - 부업으로 과외를 해 It's payday - 월급날이야 I didn't make any money from side hustle - 부업으로는 돈을 전혀 못 벌었어",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/과거경험/25-08-31-얼마나했는지",
    "title": "얼마나 했는지에 대한 질문, 대답",
    "date": "2025-08-31",
    "excerpt": "About duration",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "과거 경험을 표현할 때 쓰는 표현들"
    ],
    "content": "How long have you been studying English? I've been studying English for ten years. I've been studying English since ~ > 얼마나 오랫동안 해왔는지 물어보거나 답할 때 사용할 수 있는 표현들 Simple writing Today I learned expressions that can used when I ask or answer the how long I did. These are very useful. Maybe I have little concert 연말에. So some can ask me, \"How long have you been playing the guitar\" I can answer that \"I've been playing the guitar for 1 year\". Corrected version: Today I learned expressions that can be used when I ask or answer how long I have been doing something. These are very useful. Maybe I have a little concert at the end of the year. So someone can ask me, \"How long have you been playing the guitar?\" I can answer that \"I've been playing the guitar for 1 year\". Feedback & rewrting version Grammar corrections made: \"can used\" → \"can be used\" (missing verb \"be\") \"the how long I did\" → \"how long I have been doing something\" (correct tense and structure) \"little concert\" → \"a little concert\" (missing article) \"연말에\" → \"at the end of the year\" (translated to English) \"some can ask me\" → \"someone can ask me\" (correct pronoun) Added question mark after the question Key learning points: Use Present Perfect Continuous (have/has been + -ing) for ongoing actions \"How long have you been + -ing?\" is the correct question format \"for + time period\" shows duration (e.g., \"for 1 year\") \"since + specific time\" shows starting point (e.g., \"since 2020\")",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-positivity",
    "title": "Positivity(양수성)이란? Overlap assumption이란?",
    "date": "2025-08-29",
    "excerpt": "양수성이란? 분포의 겹침이 의미하는건?",
    "category": "Causal Inference",
    "tags": [
      "Positivity",
      "Overlap assumption"
    ],
    "content": "positivity 혹은 overlap assumption은 인과추론의 기본적인 가정들 중 하나입니다. 이러한 가정들은 왜 존재하는지 어떤 의미를 가지는지에 대해 정리했습니다. 인과추론의 기본적인 가정들 --- 처치효과를 구하는 기본적인 아이디어 처치효과란 말 그대로 \"처치(Treatment)\"가 결과에 미치는 영향을 의미합니다. 처치효과를 구하는 가장 단순한 아이디어는 처치를 받은 상태와 아닌 상태를 비교하는 것입니다. <figure> <img src=\"/post/Causal_Inference/처치효과_예시1.png\" alt=\"RCT\" width=\"60%\" /> <figcaption>그림1. 처치효과를 구하는 단순한 아이디어</figcaption> </figure> 물이 피부에 미치는 효과를 알고 싶을 때 다음처럼 생각해볼 수 있습니다. 그림1의 1번처럼 내가 물을 많이 마셨을 때의 피부와 아닐 때의 피부를 비교해본다. 또는 나는 물을 많이 마시고 친구는 많이 안마시니까 둘의 피부를 비교해본다. 하지만 이러한 방법을 통해 처치효과를 추정하긴 어렵습니다. 왜냐하면 처치효과(마시는 물의 양)외에도 다른 요인들이 피부에 영향을 주기 때문입니다. --- RCT (Randomized Controlled Trial) - 양수성 (Positivity) 참고하기 좋은 자료 이러한 문제들을 RCT (무작위 배정 실험)을 통해 극복할 수 있습니다. <figure> <img src=\"/post/Causal_Inference/Positivity/RCT.png\" alt=\"RCT\" width=\"80%\" /> <figcaption>그림2. RCT, 무작위 배정 실험</figcaption> </figure> RCT는 실험 대상자를 통제 집단과 종속 집단에 무작위로 배정하고, 집단 간의 결과 차이를 통해 처치의 효과를 추정하는 방식입니다. 통제집단 (Control Group): 처치를 받지 않는 집단으로, 비교의 기준이 되는 집단 종속집단 (Treatment Group): 처치를 받는 집단으로, 처치의 효과를 측정하고자 하는 집단 그림1에서의 실험과 차이점은 \"통제\"집단이 존재한다는 것입니다. 이는 처치를 제외하고는 다른 조건들이 비슷한, 통제된 집단을 의미합니다. 이를 통해 그림1에서의 문제를 해결할 수 있습니다. > 정확히는 \"무작위 배정\"을 통해 해결되었다고 \"가정\"할 수 있습니다. 통제집단을 설정한다는 의미는 실제로 종속집단과 동일하게 조건들을 \"통제\"한다는 의미가 아닙니다. 모든 조건을 통제할 수 없습니다. 대신 무작위 배정(Randomization) 을 통해 통계적으로 균형을 맞춥니다. 무작위 배정이란 실험 대상자들을 통제집단과 종속집단에 완전히 무작위로 배정하는 것을 의미합니다. 이렇게 하면: 처치를 제외한 다른 모든 요인들(나이, 성별, 건강상태 등)이 두 집단에 비슷한 비율로 분포하게 됩니다 결과적으로 두 집단 간의 차이는 오직 처치의 효과만으로 해석할 수 있게 됩니다 이것이 RCT의 핵심 아이디어입니다. 이는 분포의 겹칩 (Overlap assumption) 가정의 개념과 일치합니다. > 즉, 두 집단 간의 분포가 겹쳐져 있다면 (처치 효과를 제외한 다른 모든 요인들이 비슷한 비율로 분포한다면), 처치효과를 추정할 수 있습니다. 이것이 바로 양수성(Positivity) 또는 Overlap assumption의 핵심 개념입니다. 양수성은 다음과 같이 정의됩니다: $$0 < P(T=1 X=x) < 1$$ 이는 특정 공변량 값 $X=x$가 주어졌을 때, 처치를 받을 확률이 0과 1 사이에 있어야 한다는 의미입니다. $P(T=1 X=x) = 0$: 특정 조건에서는 절대 처치를 받지 않음 $P(T=1 X=x) = 1$: 특정 조건에서는 항상 처치를 받음 $0 < P(T=1 X=x) < 1$: 특정 조건에서도 처치를 받을 수도, 받지 않을 수도 있음 RCT에서는 무작위 배정을 통해 자연스럽게 양수성이 충족됩니다. 모든 사람이 동일한 확률로 처치·비처치 집단에 들어갈 수 있기 때문입니다. --- 예측 문제에서 양수성 개념 양수성이 보장된다는 건, 모든 사람이 비슷한 확률로 처치 집단에 들어갈 수 있다는 것을 의미합니다. 이는 A라는 사람이 처치집단에 존재한다면, A와 비슷한 사람이 비처치 집단에 존재한다는 말과 같습니다. > 즉 양수성이 보장된다면 실제 결과가 없더라도 이를 간접적으로 평가할 수 있습니다. <figure> <img src=\"/post/Causal_Inference/선택문제_평가_고민.png\" alt=\"평가고민\" width=\"80%\" /> <figcaption>그림3. 간접적인 평가방식</figcaption> </figure> 회사에서 풀고 있는 문제는, 렌즈 사이즈 추천 문제입니다. 고객이 시력교정을 위해 렌즈 수술을 할 때에 적절한 렌즈 크기를 결정하는게 굉장히 중요한데, 이때 선택에 도움을 주는 서비스를 만들고 있습니다. > 이 상황에서 고민하는 문제는 \"여러 선택지 중 하나에 대한 결과만 얻을 수 있다\"는 것입니다. 선택할 수 있는 렌즈 사이즈가 3개라면, 고객은 1개의 렌즈 크기에 대해서만 수술을 받기 때문에, 3개의 예측 결과가 정확한지 평가할 수 없다는 것입니다. 이때 양수성의 개념을 사용해서 \"간접적으로\" 평가할 수 있다는 사실을 확인할 수 있습니다. > 처치의 분포가 겹친다면, 간접적으로 평가할 수 있다. 왜냐하면 나는 A라는 수술을 받았지만, 나와 비슷한 사람은 B라는 수술을 받았을 것이기 때문입니다. 따라서 내가 B라는 수술을 받았을 때의 결과는 알 수 없지만, 비슷한 사람을 통해 B라는 수술을 받았을 때의 예측 결과를 간접적으로 알 수 있고, 이를 통해 모델을 간접적으로 평가할 수 있습니다. <figure> <img src=\"/post/Causal_Inference/Positivity/overlap.png\" alt=\"분포가 겹칠 때\" width=\"80%\" /> <figcaption>그림4. 분포가 겹칠 때의 상황</figcaption> </figure> 그림4처럼 처치 간의 분포가 겹칠 때는 모델의 성능을 간접적으로 평가할 수 있고 추론 결과를 신뢰할 수 있습니다. <figure> <img src=\"/post/Causal_Inference/Positivity/no-overlap.png\" alt=\"평가고민\" width=\"80%\" /> <figcaption>그림5. 분포가 겹치지 않을 때</figcaption> </figure> 하지만 그림5처럼 처치 간의 분포가 겹치지 않을 때는 간접적으로 평가할 수 없습니다. 즉 T=0인 사람에 대해 T=1에 대한 결과를 추론 결과를 신뢰할 수 없습니다. 처치 간에 분포가 완전히 다른 상황에서는 성능이 보장되지 않은 완전한 \"외삽\" 문제가 됩니다. --- 관찰 연구에서의 양수성 문제 관찰 연구 (RCT처럼 통제된 연구가 아닌 자연스럽게 수집된 데이터로 진행하는 연구) 에서는 양수성이 깨지기 쉽습니다. 왜냐하면 \"무작위\"로 처치가 배정되지 않기 때문입니다. <figure> <img src=\"/post/what-is-statistical-bias/처치별분포_피처이름제거.png\" alt=\"평가고민\" width=\"100%\" /> <figcaption>그림6. 처치 간의 분포 차이</figcaption> </figure> 의사는 환자의 조건을 보고 처치를 배정하고, 따라서 처치별로 분포가 다르게 형성된 것을 확인할 수 있습니다. 그림 6에서 feature1 (눈의 가로 크기)에 대한 처치 분포를 살펴보면, 분포 간에 차이가 보이는 것을 확인할 수 있습니다. 이러한 집단간의 차이를 편향이라고 하고, 실제 관측 데이터에 편향이 존재하는 것을 확인할 수 있습니다. 데이터에 존재하는 통계적 편향 즉, 양수성이 보장되지 않는 상황에서는 단순히 예측 평가지표만을 보는 것은 한계가 있습니다. 왜냐하면 \"간접적인\" 평가가 안된 추론 결과이기 때문입니다. 직관적으로 생각해보면, 그림 6에서 보이는 데이터로 학습한 모델은 그림 5의 상황과 같습니다. feature1 (눈의 가로 크기)가 12.5 이하인 환자의 경우 13.2로 처치받은 데이터가 없습니다. 이때 이런 환자가 13.2를 처치 받았을 때의 추정값은 신뢰하기 어렵습니다. 완전한 \"외삽\" 문제이기 때문입니다. --- 정리 선택의 문제에서 선택되지 않은 나머지에 대한 예측값을 간접적으로 평가할 수 있는 방법을 양수성으로 이해할 수 있습니다. 양수성이 보장된다 -> 비슷한 사람이 다른 집단에 존재한다. -> 간접적인 성능평가 가능 무작위 배정은 양수성을 자연스럽게 충족시키는 방법이고, 관찰 연구에서는 양수성이 위배될 수 있음을 확인했습니다. 양수성이 위배될 때는 선택의 문제에 예측 평가지표만을 신뢰하면 문제가 생길 수 있습니다. 완전한 \"외삽\"문제이기 때문에 평가지표로 확인한 성능이 전혀 보장이 안되기 때문입니다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/about-silent-failures",
    "title": "ML 서비스에서 모니터링이 중요한 이유 - silent failures",
    "date": "2025-08-27",
    "excerpt": "",
    "category": "ML Engineering",
    "tags": [
      "Monitoring",
      "Silent Failures",
      "Data Drift",
      "Hallucination"
    ],
    "content": "3개의 프로젝트를 리드해서 서비스를 운영해보면서 모니터링의 중요성을 깨닫고 있습니다. 특히 분명한 에러를 내지 않아서 잘 운영되는 것처럼 보이지만, 실제로는 문제가 있는 silent failures를 파악하는 것이 굉장히 중요하다고 느끼고 있습니다. silent failure란 무엇인지와 어떻게 모니터링 할 수 있는지에 대해 정리했습니다. --- silent failure란? silent failure는 ML 시스템에서 발생하는 문제로, 즉시 감지되지 않거나 명확한 에러 메시지를 보여주지 않는 문제들을 의미합니다. 기존 시스템은 프로그램이나 서버가 죽거나 명확한 에러 메시지를 표시하지만, ML 시스템에서는 서비스가 정상적으로 작동하는 것처럼 보이지만 정확하지 않은 결과를 내뱉고 있는 경우가 많습니다. 구체적인 예시는 다음과 같습니다 주요 silent failure 유형들 Hallucination Hallucination은 특히 Large Language Model(LLM)에서 자주 발생하는 현상으로, 모델이 잘못된 정보를 확신을 가지고 생성하는 것을 의미합니다. `` 예시: 사용자: \"2024년 한국의 GDP는 얼마인가요?\" 모델: \"2024년 한국의 GDP는 약 2.1조 달러입니다.\" (실제로는 1.7조 달러) ` 이 경우 모델은 에러 메시지를 보여주지 않고, 마치 정확한 정보인 것처럼 답변을 제공합니다. 사용자는 이를 신뢰할 수 있어 더욱 위험합니다. Data Drift Data Drift는 입력 데이터의 분포가 모델 훈련 시와 다르게 변하는 현상을 의미합니다. ` 예시: 훈련 시: 주로 20-30대 사용자 데이터 운영 시: 40-50대 사용자 데이터가 급증 결과: 모델 성능이 점진적으로 저하되지만 즉시 감지되지 않음 ` 가장 많이 발생하는 문제로, 학습했던 데이터 분포와 실제 데이터 분포 간에 차이가 발생하는 문제입니다. 주로 모델의 성능과 함께 모니터링하는 대상입니다. data drift란? 수치적으로 어떻게 정의할 수 있는지에 대한 글 Concept Drift Concept Drift는 데이터와 타겟 간의 관계가 시간이 지나며 변화하는 현상입니다. 즉 정답에 대한 정의나 개념 혹은 패턴이 바뀌는 경우입니다. ` 예시: 훈련 시: \"스팸 메일 = 특정 키워드 포함\" 운영 시: \"스팸 메일 = 이미지 기반, 키워드 없음\" 결과: 모델이 새로운 패턴을 인식하지 못함 ` 감지하기 굉장히 어려운 부분으로 통합적인 모니터링 시스템이 필요하다고 합니다. 모델의 성능을 확인하고, 성능에 확인이 안되는 부분은 비즈니스 지표 등을 함께 확인해야합니다. Model Degradation Model Degradation은 모델의 예측 품질이 서서히 떨어지는 현상입니다. ` 예시: 초기 정확도: 95% 3개월 후: 92% 6개월 후: 88% 변화가 점진적이어서 즉시 감지하기 어려움 ` 이런 현상이 발견되면 원인을 진단하기 위해 data drfit나 concept drift를 확인해볼 수 있습니다. --- 왜 silent failure가 위험할까? 서버가 다운되지 않아서 서비스는 계속 운영되지만, ML 시스템이 달성하고자 하는 목표를 달성하지 못하기 때문입니다. 사용자 경험 저하 서비스가 정상 작동하는 것처럼 보이지만 실제로는 부정확한 결과를 제공합니다. 사용자들은 서비스에 문제가 있다는 것을 인지하지 못하고 잘못된 정보를 신뢰하게 됩니다. \"고객상담용 챗봇\" 서비스를 운영하면서 Hallucination이 사용자 경험을 저하시켰던 경험이 있습니다. 예를 들어, 사용자에게 잘못된 운영시간을 알려줘 헛걸음을 하게 하거나, 가격을 잘못 알려줘 예상보다 비싼 가격에 실망하거나 화가 나게 하는 경우들이 있습니다. 신뢰도 하락 시간이 지나면서 사용자들이 서비스의 정확성을 의심하게 되고, 결국 서비스 사용을 중단할 수 있습니다. \"렌즈 사이즈 추천\" 서비스를 운영할 때 이러한 경험을 했습니다. 의료 분야에서는 모델의 신뢰도가 의사의 일반적인 직관에 맞는 추론 결과를 내는 것이 매우 중요한데, 학습 데이터에서는 이에 맞았지만 실제 추론 상황에서는 맞지 않아서 서비스를 사용하지 않게 되었고, 이후에 이런 부정적인 피드백을 받은 경험이 있습니다. 이를 모니터링을 통해 사전에 문제를 확인했다면, 빠르게 대응해서 신뢰도 하락을 막을 수 있었을 것입니다. 디버깅의 어려움 전통적인 에러와 달리 명확한 에러 로그가 없어 문제를 파악하고 해결하기가 어렵습니다. 조용히 서비스를 망쳐가고 있는 부분입니다. 그렇기 때문에 서비스를 운영하는 측면에서 굉장히 위험한 부분입니다. --- 모니터링 시스템 구축 전략 다층 모니터링 아키텍처 ` ┌─────────────────┐ │ Business │ ← 비즈니스 지표 모니터링 │ Metrics │ └─────────────────┘ │ ┌─────────────────┐ │ Model │ ← 모델 성능 모니터링 │ Performance │ └─────────────────┘ │ ┌─────────────────┐ │ Data │ ← 데이터 품질 모니터링 │ Quality │ └─────────────────┘ │ ┌─────────────────┐ │ Infrastructure│ ← 인프라 모니터링 │ Health │ └─────────────────┘ `` AI가 제안한 다층 모니터링 아키텍처인데요, 확인해야 하는 부분들에 대해서 로그를 정리하고, 이를 활용해 현재 상황을 확인할 수 있는 형태로 구성하는게 핵심인 것 같습니다. --- 실제 사례 연구 사례 1: 추천 시스템의 Concept Drift 상황: 전자상거래 추천 시스템이 코로나19 이후 사용자 행동 변화를 따라가지 못함 문제점: 훈련 데이터: 2019년 이전 사용자 행동 실제 데이터: 2020년 이후 온라인 쇼핑 증가, 카테고리 선호도 변화 해결책: 주기적인 모델 재훈련 (월 1회) 실시간 사용자 행동 패턴 모니터링 A/B 테스트를 통한 모델 성능 검증 사례 2: 금융 사기 탐지의 Data Drift 상황: 신용카드 사기 탐지 모델이 새로운 사기 패턴을 감지하지 못함 문제점: 훈련 데이터: 기존 사기 패턴 (온라인 쇼핑몰, 해외 결제) 실제 데이터: 새로운 사기 패턴 (배달 앱, 가상화폐 거래) 해결책: 실시간 트랜잭션 패턴 분석 이상 거래 패턴 자동 탐지 전문가 검토를 위한 플래그 시스템 --- 모니터링 도구 및 플랫폼 오픈소스 도구: Evidently AI: 데이터 품질 및 모델 성능 모니터링 MLflow: 실험 추적 및 모델 버전 관리 Prometheus + Grafana: 메트릭 수집 및 시각화 Great Expectations: 데이터 품질 검증 --- 결론 ML 서비스에서 조용한 에러는 전통적인 소프트웨어 에러보다 더 위험할 수 있습니다. 서비스가 정상 작동하는 것처럼 보이면서도 부정확한 결과를 제공하기 때문입니다. 효과적인 모니터링을 위해서는: 다층적 모니터링: 데이터, 모델, 비즈니스 지표를 모두 모니터링 실시간 감지: 이상 징후를 즉시 감지할 수 있는 시스템 구축 자동화된 알림: 문제 발생 시 즉시 대응할 수 있는 알림 시스템 지속적인 개선: 모니터링 결과를 바탕으로 시스템을 지속적으로 개선 이러한 모니터링 시스템을 구축하고 유지함으로써 ML 서비스의 안정성과 신뢰성을 확보할 수 있습니다. ---",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/과거경험/25-08-26-뭐하며지냈어",
    "title": "뭐 하며 지냈어?",
    "date": "2025-08-26",
    "excerpt": "안부를 물어볼 때 쓰는 표현들",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "과거 경험을 표현할 때 쓰는 표현들"
    ],
    "content": "What have you been up to? 배운 것들 What have you been up to? I've been traveling. I've been studying lately. I've been studying for test. What else is new with you? 헷갈리는 거 is been vs have been -> is been이 뭔가 헷갈렸는데 그런 표현은 없고 has been이구나 ㅋㅋ He's been studying for a test. 간단한 작문 Ask my self and then answer. What have you been up to? I've been preparing for a job transition. Make a resume better, prepare the job interview. (좀 있다가 마저 적자)",
    "public": true
  },
  {
    "id": "Self_Development/Career/Resumes/25-08-25-hyperconnect",
    "title": "HyperConnect ML Engineer",
    "date": "2025-08-25",
    "excerpt": "하이퍼 커넥트 이력서 컨셉과 방향 적어보기",
    "category": "Career",
    "tags": [
      "이력서"
    ],
    "content": "하이퍼 커넥트 채용공고 지원한 이력서 --- 탈락 ㅠ -> Fit하지 않다는 피드백. 빈말일 수도 있지만, 실제로 내 이력서에 관련 업무 경험이 없긴 하다. 이 부분이 가장 어려운 부분... 사이드프로젝트를 꾸준히 잘 정리했다면 결과가 달라질 수도 있지 않을까? `` 안녕하세요 김민규님, 하이퍼커넥트 Talent Acquisition Team입니다. 하이퍼커넥트에 관심 가지고 지원해 주셔서 진심으로 감사합니다. 서류 지원 과정 중 저희가 의도치 않게 불편을 드린 점은 없었는지 마음이 쓰입니다. 김민규님의 뛰어난 역량에도 불구하고, 안타깝게도 이번 서류 심사 과정에서 Machine Learning Engineer (HYPERCONNECT AI - Contents Understanding) 포지션으로 귀하를 모실 수 없게 되었습니다. 이는 역량의 부족이 아닌 포지션과의 Fit이 맞지 않아서임을 양해해 주시길 부탁드립니다. 늘 건승하시기 바라며 더 좋은 기회로 인사 드릴 수 있길 희망합니다. 하이퍼커넥트도 김민규님이 보여주신 관심에 보답할 수 있도록 더욱 더 성장할 수 있도록 하겠습니다. 감사합니다. 하이퍼커넥트 Talent Acquisition Team Confidential / 대외비 CONFIDENTIALITY NOTICE: This e-mail communication and any attachments contains confidential and privileged information for the sole use of the designated recipients named above. It is prohibited to disclose, disseminate, distribute or copy this e-mail and any attachments beyond the scope which is mutually agreed with the sender of this e-mail. If you are not the intended recipient, please notify the sender by return e-mail and delete and/or destroy all copies of this communication and any attachments immediately. 본 이메일 및 첨부파일은 지정된 수신인만을 위한 것이며, 회사의 비밀정보를 포함합니다. 본 정보를 발신인과 합의된 범위 외에 사용, 공개, 배포하는 것은 엄격히 금지됩니다. 귀하가 만일 지정된 수신인이 아닌 경우, 발신인에게 그 사실을 통지해 주시고 원본 이메일과 첨부파일 및 모든 사본을 즉시 삭제하여 주시기 바랍니다. `` --- 채용공고 스케치 거의 2주동안 포트폴리오를 완성하지 못하고 있는데, 다른 이유가 아니라 그냥 원하는 기업에 비해 내가 가진게 너무 볼품없어 보여서 그런 것 같다. 회피는 그만! 뭐가 부족한지 똑바로 확인하고 이 부분들을 채워나가보자. JD에서 확인한 필수요구사항들. AI/ML에 대한 기본 지식과 적어도 한 개 이상의 특정 도메인에 대한 깊이 있는 지식을 갖추고, 관련 프로젝트 경험이 있으신 분 Exploratory Data Analysis(EDA)를 통해, 데이터의 통계적 특성과 패턴을 발견하고 이를 ML 모델에 반영하실 수 있는 분 공개된 벤치마크 데이터 셋을 이용해 모델의 테스트 성능을 올리기 위해 여러 가지 모델링을 해본 경험이 있으신 분 구현체가 공개되지 않은 논문을 읽고, 빠르고 정확하게 구현할 수 있는 구현 역량을 갖추신 분 Tensorflow, PyTorch, JAX 등 오픈소스 프레임워크 및 전반적인 파이썬 개발에 능숙하신 분 ML 모델 학습 및 서비스 배포에 필요한 엔지니어링 역량을 갖추신 분 AI 기술의 서비스화에 관심이 많으신 분 학위/국적 무관, 영어로 기초적인 의사소통이 가능하고, 한국어로 원활한 의사소통이 가능한 분 앞으로 내 행동 방향. 필수요구사항과 선호경험에 맞는 경험들 최대한 살려서 이력서 제출하기 부족한 부분들은 일할 때 어떻게 채워나갈 수 있을지 고민해보기 3개의 프로젝트를 리드한 경험 적어보자. + CV 다뤘던 경험 이력서 스케치 (영문 PDF 제출) Led 3 projects with development involvement at a startup, demonstrating rapid growth and leadership capabilities. Projects: Lenze size recommendation Achievement (How + Result one line) Quantified prediction uncertainty using causal inference to evaluate prediction reliability for unobserved lens sizes and ensured model stability through integrated data pipeline and monitoring system Role/Solution Process (3-4 lines) Prediction Evaluation Problem Solving: Resolved limitations in evaluating predictions for unobserved treatments by leveraging causal inference's positivity assumption and propensity scores, expressing prediction uncertainty through intervals using CQR and partial identification Data Consistency Assurance: Addressed data inconsistency between training and inference environments by building integrated data pipeline and feature store, standardizing data preprocessing processes and ensuring consistency across environments Monitoring System Development: Developed automated monitoring pipeline using Airflow for real-time data drift detection and performance tracking, enabling early detection of silent failures and ensuring service stability Domain-Specific Modeling: Enhanced model interpretability through clinically meaningful feature engineering and monotonicity constraints aligned with physician intuition, and provided model explanations using Shapley values to increase medical staff trust 2025.03 - Present (6 months) Project Overview: Developed a service that predicts surgical outcomes for vision correction, enabling surgeons to select optimal lens sizes based on data-driven predictions rather than solely on experience and intuition. Problem-Solving Process: Model Validation Challenge Problem Faced challenges in evaluating prediction accuracy when the model generated four size options but only one could be validated with real customer data. Solution Leveraged causal inference (positivity assumption) to exploit distributional overlap across treatments (lens sizes), facilitating indirect performance evaluation. Accounted for non-random treatment assignment (lens sizes) by quantifying overlap in the data and separating overlapping from non-overlapping regions for analysis. Validated this approach not only through domain expertise but also by statistically examining the distribution across treatments. Used prediction intervals where treatment distributions overlapped and applied partial identification to quantify prediction ranges in non-overlapping regions. Motivated by the goal of increasing user trust, expressed predictive uncertainty explicitly through intervals. Result Indirectly evaluated unobserved predictions and visualized both guaranteed and non-guaranteed ranges to effectively communicate predictive uncertainty. Data quality and consistency challenges between training and inference environments Problem In the absence of a unified data pipeline, training and inference each pulled data directly from the data lake, leading to inconsistencies in data processing between environments. Solution Developed a robust data pipeline to cleanse both OCR-collected and external data sources, incorporating data validation and schema enforcement to ensure data quality. In addition, built a feature store that unified offline (training) and online (serving) features, securing data consistency across environments and improving the reliability of model deployment. Result Secured data quality through the new pipeline and established consistency across training and inference via the feature store, increasing confidence in model outputs and stability in deployment. Modeling Approach Problem The goal of model development was to deliver results that achieve user satisfaction, thereby fostering trust. This required not only improving predictive accuracy within the dataset, but also ensuring robust performance in inference settings. In addition, the model needed to produce outputs aligned with user intuition—particularly important as the primary users were physicians, necessitating medically interpretable predictions. Solution To improve model performance, I placed strong emphasis on thoroughly examining the data. I first analyzed feature distributions and filtered out implausible values that deviated from clinical standards, ensuring a reliable dataset. Drawing on physician intuition, I engineered clinically meaningful features—for example, capturing the relationship between age and lens thickness—which not only improved predictive accuracy but also aligned the model’s behavior with medical reasoning. To further strengthen reliability, I incorporated medical domain knowledge into the model by applying monotonicity constraints where clinically appropriate. Finally, I used interpretability tools such as Shapley values to explain prediction outcomes, enabling physicians to better understand and trust the model’s decisions. Result Through detailed data analysis, I improved data quality and developed new features that enhanced model performance while producing results consistent with medical intuition. In addition, the model was not treated as a black box; I provided explanations for its outputs, ensuring that the reasoning behind predictions was transparent and understandable. Lack of monitoring for silent failures and model performance Problem Monitoring was required to ensure model performance in inference settings. It was important to track not only how well the model performed during real-world usage, but also to detect potential issues such as data drift. Without such monitoring, the service could appear to function normally while causing user discomfort through silent failures, ultimately reducing user trust and engagement. Solution Developed an automated monitoring pipeline with Airflow to detect data drift using statistical measures (e.g., Jensen–Shannon distance) and to continuously compare training vs. inference data distributions. Implemented performance tracking by monitoring prediction–outcome gaps in real time, enabling early detection of silent failures and ensuring reliable model performance in production. Result Enhanced service stability and maintained user trust by proactively identifying data drift and performance degradation before they impacted end users. OCR Pipeline Achievement (How + Result): Achieved error rate below 1% and 99%+ OCR accuracy by implementing object-oriented design and unit testing framework Role (3-4 lines): Refactored procedural code into object-oriented architecture to clearly separate functional responsibilities and improve debugging efficiency Built unit testing and type validation system using pytest and mypy to prevent runtime errors proactively Improved OCR accuracy from 70% to 99%+ by applying domain-specific models tailored to fixed regions per device type and optimizing image preprocessing/postprocessing logic Established clinical validity verification and anomaly detection monitoring system to prevent silent failures and ensure stable service operations 2024.07 - 2024.10 (4 months) Project Overview: Developed a robust OCR pipeline to extract test results from medical images and store them in a structured database. Since the pipeline served as a critical component for collecting data used in production services, it was designed with a strong focus on real-time processing, stability, and high accuracy. Problem-Solving Process: Ensuring Stable Service Operations Problem Inherited a legacy project with a large backlog of error logs. The codebase was written in a purely procedural manner, making it extremely difficult to trace the root causes of errors. The lack of clear structure and responsibility boundaries hindered both debugging efficiency and long-term maintainability. Solution Refactored the system into an object-oriented architecture, applying core principles such as single-responsibility per function and well-defined ownership for each object. This restructuring enabled clearer separation of concerns and improved readability. Additionally, integrated unit testing with pytest and static type checking with mypy to proactively detect potential runtime errors. Together, these practices established a robust foundation for reliable and maintainable service operations. Result Achieved an error rate below 1%, while significantly improving debuggability. Even when issues occurred, the modular design and testing framework enabled rapid root-cause identification and resolution, ensuring stable service delivery and reducing operational overhead. Improving OCR Accuracy Problem The existing deployed OCR model had been developed without a proper test set and was only validated manually by its original developer. To rigorously evaluate performance, I constructed a dedicated test dataset covering ~100 samples for each of the six different diagnostic devices. Benchmarking revealed that the deployed model achieved less than 70% accuracy, which was far below the reliability required for production use. In addition, silent failures in production (e.g., plausible but clinically invalid outputs) were not being detected, creating risks for downstream systems and users. Solution Leveraged domain-specific characteristics to boost performance. Although the image types varied across devices, the regions of interest to extract were fixed per device type. This allowed the use of rule-based localization to reliably identify the relevant regions, effectively simplifying the text detection step. For text recognition, integrated TrOCR, an open-source model specialized in OCR tasks. Model evaluation with the curated dataset showed accuracy around 95%, which was further improved to over 99% by applying tailored image preprocessing and position-specific postprocessing strategies. To address silent failures, designed a monitoring layer that applied clinical plausibility checks and probabilistic anomaly detection. The system raised alerts whenever extracted values were outside valid ranges or statistically improbable given the device type, enabling proactive detection of hidden errors. Result Achieved over 99% OCR accuracy in a mission-critical pipeline while also ensuring reliability through monitoring. The monitoring layer consistently surfaced out-of-range or anomalous values, reducing undetected OCR errors and enabling faster triage when issues occurred. These improvements ensured stable service operations, minimized error propagation to downstream systems, and reinforced trust in the automated data pipeline. Chatbot Achievement (How + Result) Automated customer service operations through RAG pipeline implementation (classification model integration, search accuracy improvement) and monitoring system development Role (3-4 lines) Led end-to-end process from problem definition to RAG pipeline design and implementation for automating repetitive customer service tasks Improved user satisfaction by implementing classification model at pipeline frontend and developing context-aware question rephrasing and keyword extraction features, reducing unsatisfied feedback by over 50% Built Qdrant-based VectorDB system enabling users to select document versions and perform real-time updates Established monitoring system using RAGAS framework integrated with Airflow for silent failure detection and performance metric tracking to minimize maintenance costs 2024.11 - 2025.03 (4 months) Project Overview: Developed an AI chatbot service to automate repetitive customer inquiries, reducing the burden on human agents and improving overall customer service efficiency. The system processed an average of 100-200 daily inquiries, significantly enhancing agent productivity. Problem-Solving Process: Improving User Satisfaction through Classification Problem User feedback revealed that the most common source of dissatisfaction was the LLM generating ambiguous responses to questions it couldn't clearly answer. This was particularly problematic when customers asked about services that required manual intervention (e.g., appointment scheduling with personal information), yet the system continued to engage in unnecessary back-and-forth conversations. Solution Implemented a classification model at the front-end of the RAG pipeline to categorize incoming questions and route them appropriately. Used LLM-based classification prompts to identify question types and ensure proper handling of different inquiry categories. Result Reduced \"unsatisfied\" feedback by over 50% by preventing inappropriate responses and unnecessary conversations. Context-Aware Document Retrieval Problem The system needed to maintain conversation context to provide relevant responses. For example, when a user asked \"What's the price of LASIK?\" followed by \"What about LASEK?\", the system should understand the second question refers to LASEK pricing. Solution Implemented question rephrasing using Redis to store short-term conversation history. The system retrieves conversation context and rephrases questions to match the conversation flow, enabling more accurate document retrieval. Result Improved response relevance by maintaining conversation context and reducing the need for users to repeat information. Enhanced Search Performance through Query Decomposition and Keyword Extraction Problem Single questions often contained multiple semantic components (e.g., \"Tell me the prices of both LASIK and LASEK\"), requiring retrieval of documents for multiple sub-questions. Additionally, semantic search limitations and embedding model constraints made it difficult to find documents containing proper nouns like doctor names. Solution Implemented query decomposition to break complex questions into multiple sub-queries, each targeting specific information needs. Added keyword extraction to complement semantic search, enabling filtering based on extracted entities like proper nouns. Result Improved search accuracy for complex queries and enhanced retrieval of documents containing specific entities that embedding models might miss. Document Retrieval Optimization Problem The core challenge was creating high-quality embedding vectors for semantic search to retrieve the most relevant FAQ documents. Solution Built a test dataset from FAQ data using LLM-generated questions to evaluate different embedding models. Selected the best-performing model through systematic comparison. Implemented Qdrant as the vector database with real-time updates when documents are modified, minimizing maintenance costs. Used multi-tenancy features to clearly separate document versions. Result Achieved optimal document retrieval performance with a maintainable and scalable vector database solution. Response Generation Problem Generated responses needed to be both accurate (based only on retrieved documents) and user-friendly, maintaining a conversational tone that customers would find approachable and easy to understand. Solution Applied prompt engineering to constrain the LLM to generate responses based solely on retrieved document content while maintaining a friendly and accessible tone. This dual focus ensured both accuracy and positive user experience. Result Delivered responses that were both factually accurate and user-friendly, improving overall customer satisfaction. Monitoring and Maintenance Problem Silent failures in the chatbot system could go undetected, potentially causing user discomfort and reducing trust in the service. Continuous monitoring was needed to ensure reliable performance and minimize maintenance costs. Solution Implemented an automated monitoring system using the RAGAS framework to evaluate response quality. The system scores responses based on their grounding in retrieved documents and tracks performance metrics. Used Airflow to process chat logs from spreadsheets, generating daily and weekly performance reports and alerting high-scoring conversations via Slack. Result Proactively detected silent failures and performance issues, ensuring reliable service operation and maintaining user trust through continuous quality monitoring. Cover letter I am writing to express my strong interest in the Machine Learning Engineer role at HYPERCONNECT. Although I am still a junior developer, my career goal has always been clear: to grow into a developer who not only builds technology but also creates meaningful impact for users and the business. During my time at a startup, I had the opportunity to work on real services, which taught me how technology can directly influence user experience and business outcomes. I consistently strived to go beyond solving technical problems in isolation, and instead focused on how each solution could improve the service and deliver value to both users and the company. I am particularly inspired by HYPERCONNECT’s work, which I have followed through technical blogs and research publications. I was impressed by how your team demonstrates not only strong technical expertise but also a commitment to driving tangible business results. This aligns closely with my own aspiration to become a developer who bridges technical innovation with real-world impact. At HYPERCONNECT, I would be eager to contribute my hands-on startup experience, adaptability, and strong motivation to grow. I believe my background equips me to quickly integrate into your team, learn from the challenges ahead, and make meaningful contributions to your mission. Thank you for considering my application. I look forward to the opportunity to discuss how my skills and passion can contribute to HYPERCONNECT’s continued success.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-08-25-causal-inference-in-rs",
    "title": "추천시스템에 사용되는 인과추론 개념정리",
    "date": "2025-08-25",
    "excerpt": "인과추론의 기본적인 개념과 인과추론이 추천시스템에서 어떻게 쓰이는지에 대한 내용들에 대해 정리했습니다.",
    "category": "Recommendation",
    "tags": [
      "추천시스템",
      "인과추론"
    ],
    "content": "참고자료 1: A Survey on Causal Inference for Recommendation --- 상관관계에 기반해 패턴을 찾는 ML 모델의 한계 인과추론(causal inference)와 예측(prediction)은 목적이 다릅니다. <figure> <img src=\"/post/Recommendation/CS-vs-Prediction.png\" alt=\"CS-vs-Prediction\" width=\"100%\" /> <figcaption>그림1. 인과추론과 예측의 목적 차이 (출처 : Korea Summer Workshop on Causal Inference 2022)</figcaption> </figure> 원하는 결과를 얻기 위해 Input을 조절하기 위해서는 \"인과추론\"이 적절할 것이고, Output을 정확하게 아는 것이 목표라면 \"예측\"이 더 적절할 것입니다. 기존의 예측 모델로 적절한 Input을 찾는게 왜 어려운지 이해해보겠습니다. 상관관계에 기반한 예측 모델의 한계점 예측 모델은 \"상관관계\"에 기반해 예측을 수행합니다. 즉 모델이 찾는 패턴은 \"상관관계 (correlation)\" 입니다. 이는 원인과 결과의 관계를 나타내는 \"인과관계 (causality)\"와 구분됩니다. $$ \\text{인과관계} \\subset \\text{상관관계} $$ 인과관계는 상관관계에 포함됩니다. 즉 모델이 찾은 상관관계 중 일부분은 인과관계가 될 수 있습니다. 다만 데이터에 \"편향 (bias)\"가 존재할 때 모델은 인과관계를 찾기 어렵습니다. 모델이 찾은 상관관계에 기반해 Input을 바꾸는 \"선택\"한다면 잘못된 결과를 가져올 수 있습니다. 예시를 통해 이를 알아보겠습니다. 상어 빈도 횟수와 매출 간의 관계 <figure> <img src=\"/post/Recommendation/example1.png\" alt=\"example\" width=\"80%\" /> <figcaption>그림2. 상어 출몰 횟수와 아이스크림 매출</figcaption> </figure> 상어 출몰 횟수와 아이스크림 매출 데이터가 있다면, 상어 출몰 횟수와 아이스크림의 상관관계를 바탕으로 모델은 패턴을 찾아낼 것입니다. 직관적으로 알 수 있듯, 이는 인과관계가 아닙니다. 상어의 출몰 횟수가 아이스크림의 판매량을 결정하지 않습니다. <figure> <img src=\"/post/Recommendation/example2.png\" alt=\"example\" width=\"80%\" /> <figcaption>그림3. 상어 출몰 횟수와 아이스크림 매출, 온도 변수</figcaption> </figure> 상어의 출몰 횟수와 아이스크림의 매출에 모두 영향을 주는 \"온도\"라는 변수가 빠져있기 때문에 발생한 문제였습니다. 온도를 통해 상어와 아이스크림 매출 간의 상관관계가 생성된 것입니다. 이 예시를 통해 말하고자 하는 건, 인과관계가 없어도 상관관계가 발생할 수 있고, 이를 통해 예측하는 모델의 결과를 인과관계로 오해하면 안된다는 것입니다. 가격과 매출 간의 관계 <figure> <img src=\"/post/Recommendation/example3.png\" alt=\"example\" width=\"80%\" /> <figcaption>그림4. 가격과 매출</figcaption> </figure> 그림4와 같은 데이터가 있다고 합시다. 가격과 매출 간의 상관관계가 보입니다. 모델을 학습시키면 이 상관관계를 바탕으로 학습을 할 것입니다. 그렇다면 매출을 올리기 위해서 가격을 올리면 되는 것일까요? <figure> <img src=\"/post/Recommendation/example4.png\" alt=\"example\" width=\"80%\" /> <figcaption>그림5. 가격과 매출</figcaption> </figure> 가격이 매출에 미치는 영향을 파악하기 위해서는 그림3과 같이 변수 간에 상관관계를 형성하는 변수를 찾아야 합니다. 가격이 매출에 미치는 영향을 파악하기 위해서는 성수기/비수기 여부가 같은 상황에서 가격과 매출을 비교해보면 됩니다. 단순하게 생각해서, 성수기일 때 가격이 100일 때보다 500일 떄 매출이 더 높으니, 가격을 높이는게 괜찮다고 생각할 수 있습니다. 하지만 비수기일 때는 비교대상이 없다보니 가격의 영향력을 알 수 없습니다. 2개의 예제를 통해 말하고자 하는 부분은 2가지입니다. 상관관계가 인과관계를 의미하지 않는다. 상관관계에 기반한 선택이 문제가 될 수 있다. --- 인과추론의 목적 인과추론의 목적은 원하는 결과를 얻기 위해 입력변수를 어떻게 바꿔야할지 알아내는 것입니다. 여기서 바꾸는 입력값, 조작 변수를 \"처치\"라고 칭합니다. 그림 4에서는 가격이 처치가 될 것입니다. 즉 인과추론은 최적의 처치를 찾기 위한 방법론입니다. 이를 위해 처치가 결과에 미치는 \"처치효과\"를 추정하게 됩니다. 처치효과를 추정할 때 데이터에 편향이 존재하면 이를 잘못 구하게 됩니다. 처치효과는 \"처치를 한 후의 값\" 과 \"처치를 하기 전의 값\"을 빼서 구할 수 있습니다. $$ \\text{처치효과} = E[Y T=1] - E[Y T=0] $$ 여기서: $T=1$: 처치를 받은 그룹 (treatment group) $T=0$: 처치를 받지 않은 그룹 (control group) $Y$: 결과 변수 (outcome variable) 그림5의 예시를 보면, 가격을 높인 그룹과 가격을 낮춘 그룹의 차이를 통해, 가격을 높였을 때의 효과를 추정할 수 있습니다. 하지만 \"성수기/비수기\" 여부를 보면 결과에 영향을 미친게 가격 때문만은 아님을 알 수 있습니다. 즉 진짜 가격의 처치효과를 추정하기 위해서는, \"성수기\"일때 가격을 높인 것과 낮은 것을 빼야할 것입니다. 이는 \"통제집단\"과 \"종속집단\"을 통해 가설을 검증하는 과정과 같은 개념입니다. 통제집단 (Control Group): 실험에서 독립변수의 영향을 받지 않는 비교 기준이 되는 집단 종속집단 (Treatment Group): 실험에서 독립변수의 영향을 받는 집단 실험을 진행할 때 가장 중요한 부분은, 독립변수를 제외한 다른 변수는 완전히 같아야한다는 점입니다. 그렇지 않으면 독립변수 (처치)의 영향을 정확히 파악할 수 없을 것입니다. 음식 배달 앱 실험 예시 : 잘못된 실험 설계: 실험집단: 비 오는 날 새로운 할인 정책 적용 통제집단: 맑은 날 기존 정책 적용 문제점: 날씨 조건이 다름 비 오는 날: 집에서 주문하는 경향 증가 맑은 날: 외식 선호 결과: 할인 정책 효과인지, 날씨 효과인지 구분 불가 RCT(Randomized Control Trial) 다른 변수들을 통제하기 가장 좋은 방법은 참가자들을 실험집단과 통제집단에 완전히 무작위로 배정하는 것입니다. 이를 통해 두 집단 간의 차이를 최소화할 수 있습니다. 하지만 무작위 배정 실험이 불가능한 경우도 있고, 무작위 배정을 하더라도 편향이 발생할 수 있습니다. RCT를 해도 편향이 생기는 경우 RCT를 진행하더라도 통제집단과 종속집단 간의 차이가 발생할 수 있습니다. 제가 생각하는 가장 대표적인 예시는 \"여론 조사\"입니다. 무작위로 여론조사 대상을 선정하고 이를 통해 전체 여론에 대해 추정할 수 있습니다. 하지만 여론조사는 RCT의 한계를 보여주는 가장 대표적인 예시입니다. 이론적으로는 전화번호부에서 완전 무작위로 대상을 선정하여 전체 여론을 추정할 수 있다고 하지만, 실제로는 여러 가지 문제가 발생합니다. 가장 큰 문제는 \"응답률 편향\"입니다. 무작위로 선정했다고 해도 모든 사람이 동일하게 응답하지 않습니다. 바쁜 직장인이나 외출 중인 사람들은 전화를 받지 못하고, 전화를 받더라도 낯선 번호라면 무시하는 경우가 많습니다. 또한 응답한다고 해도 솔직하게 답하지 않는 경우도 있습니다. 특히 정치적 관심도에 따른 차이가 두드러집니다. 정치에 관심이 많거나 특정 정당을 지지하는 사람들이 여론조사에 더 적극적으로 응답하는 반면, 정치에 무관심하거나 중도 성향을 가진 사람들은 상대적으로 응답하지 않는 경향이 있습니다. 연령대별로도 차이가 있어서, 50대 이상의 사람들은 전화 사용에 익숙하고 시간적 여유가 있어 응답률이 높은 반면, 20-30대는 스마트폰을 주로 사용하고 전화를 거부하는 경향이 있어 응답률이 낮습니다. 실제 사례로는 2016년 미국 대선에서 여론조사가 힐러리 클린턴의 승리를 예측했지만 실제로는 도널드 트럼프가 승리한 경우가 있습니다. 이는 교육 수준이 낮은 백인 노동자층의 응답률이 낮았기 때문입니다. 한국에서도 대선 여론조사의 전화 응답률이 10-20% 수준에 머물러 있고, 응답자들이 정치에 관심이 많은 계층을 과대 대표하게 되어 실제 여론과 차이를 보이는 경우가 많습니다. 이러한 문제를 해결하기 위해 다중 모드 조사를 병행하거나, 연령과 성별, 지역별로 균등하게 표본을 추출하는 층화 표본 추출 방법을 사용하기도 합니다. 또한 응답자와 비응답자 간의 특성 차이를 분석하여 편향 정도를 정량적으로 측정하는 노력도 이루어지고 있습니다. 여론조사는 이론적으로는 무작위 표본 추출을 지향하지만, 실제로는 다양한 편향이 발생하여 RCT의 이상적인 조건을 만족하지 못하는 대표적인 사례입니다. RCT에서는 실험자가 직접 개입하여 무작위 배정을 하고 처치 효과를 명확하게 측정할 수 있지만, 여론조사에서는 완전한 무작위 배정이 불가능하고 응답 여부가 사용자의 선택에 의존하며, 시간이나 상황과 같은 외부 요인의 영향을 받게 됩니다. RCT를 할 수 없는 경우 RCT 자체를 진행할 수 없는 경우도 있습니다. 가장 대표적인 것은 윤리적 문제로 인한 제한입니다. 예를 들어, 새로운 암 치료 약물의 효과를 테스트하려고 할 때 통제집단에 위약을 주는 것은 생명에 위험할 수 있어 윤리적으로 문제가 됩니다. 이런 경우에는 RCT를 진행할 수 없고 관찰 연구나 다른 방법을 사용해야 합니다. 교육 분야에서도 비슷한 문제가 발생합니다. 새로운 교육 방법의 효과를 테스트하려고 할 때, 통제집단 학생들에게 열등한 교육을 제공하는 것은 교육 기회의 불평등을 야기할 수 있어 부당합니다. 특정 학생들만 새로운 수업 방식을 배제하는 것은 윤리적으로 문제가 되므로 RCT 대신 자연 실험이나 준실험을 사용하게 됩니다. 실용적 제약으로 인한 경우도 있습니다. 회사 전체에 새로운 인사 정책을 적용하려고 할 때, 일부 직원에게만 적용하고 나머지는 기존 정책을 유지하는 것이 불가능한 경우가 많습니다. 급여 체계나 근무 시간 정책 같은 것들은 전체 조직에 동시에 적용해야 하므로 RCT를 통한 비교가 어렵습니다. 이런 경우에는 Before-After 비교나 다른 회사와의 비교 연구를 통해 효과를 측정하게 됩니다. 자연재해의 영향을 연구하려는 경우에도 RCT는 불가능합니다. 지진이나 홍수 같은 자연재해를 인위적으로 발생시켜서 특정 지역에만 적용하고 통제집단과 비교하는 것은 불가능하기 때문입니다. 이런 경우에는 자연 실험을 활용하여 비슷한 조건의 지역들을 비교하는 방법을 사용합니다. 인과추론에서 하려는 것 인과추론의 목적은 \"처치효과\"를 추정하는 것입니다. 이를 위해서는 통제집단과 종속집단이 잘 설정되어 있어야 합니다. 가장 이상적인 조건은 RCT가 잘 진행된 상태입니다. 하지만 RCT의 한계도 있고, RCT를 할 수 없는 상황들도 있습니다. 이런 상황에서는 통제집단과 종속집단의 차이, 즉 편향이 발생하게 됩니다. 그리고 이는 잘못된 처치효과 추정으로 이어집니다. 인과추론에서는 이러한 편향을 파악하고 제거해 처치효과를 추정해나가는 것을 목표로 하고 있습니다. 이상적인 RCT 형태로 만들어 온전한 처치효과를 추정하려고 합니다. --- 추천시스템과 인과추론 추천시스템의 목표를 생각해보면 좋을 것 같습니다. 사용자가 가장 만족하고 선택할 수 있는 리스트를 제공하는 것 사용자가 만족하고 선택할 결과를 정확하게 예측하는 것 앞서 언급한 것처럼 목표가 첫 번째라면 인과관계를 추론하고 이를 통해 최적의 Input을 찾아나가는 인과추론 방법이 적절할 것이고, 두 번째라면 예측 결과를 정확하게 예측할 수 있는 예측 모델이 적절할 것입니다. 첫 번째 목표를 달성하려고 한다면 기존의 추천시스템에는 한계가 있습니다. 앞서 언급한 것처럼 상관관계에 기반하여 패턴을 찾는 예측 모델로는 어떤 리스트를 보여주는 것이 최선의 결과를 얻을 수 있을지 판단하기 어렵습니다. 또한 이는 데이터에 존재하는 편향에 기반하고 있습니다. 기존 추천시스템의 한계 어떤 사용자가 특정 아이템을 클릭하거나 사용했다면, 해당 사용자는 그 아이템을 선호한다고 판단하는 것이 일반적입니다. 하지만 실제로는 사용자가 개인적인 취향 때문에 선택한 것이 아니라, 단순히 인기도가 높아서 선택했을 가능성이 큽니다. 예를 들어, 음악 추천에서 특정 곡이 차트 상위에 올라있다면, 사용자들은 실제로 그 곡을 좋아해서가 아니라 \"많은 사람들이 듣고 있으니 한번 들어보자\"는 심리로 선택할 수 있습니다. 마찬가지로 쇼핑몰에서 베스트셀러 상품을 선택하는 경우도, 개인적인 선호도보다는 \"인기 있는 제품이니 좋을 것이다\"라는 믿음에 기반한 선택일 수 있습니다. 이러한 현상은 추천시스템이 사용자의 진정한 선호도를 파악하는 데 어려움을 겪게 만듭니다. 단순히 클릭이나 구매 기록만으로는 사용자가 실제로 얼마나 만족했는지, 다른 아이템을 보여줬다면 더 만족했을지에 대한 답을 얻기 어렵습니다. --- Causal Recommendation (참고 1 정리) <figure> <img src=\"/post/Recommendation/survey-fig1.png\" alt=\"figure1\" width=\"80%\" /> <figcaption>그림6. causal inference를 적용했을 때의 장점</figcaption> </figure> 인과적 추천시스템이 전통적인 접근 방식을 능가할 수 있는 이유는 크게 두 가지 핵심 강점 때문입니다. 첫 번째는 인과관계를 모델링할 수 있다는 점입니다. : 현재 대부분의 ML은 변수 간의 상관관계에 기반해 패턴을 찾습니다. 하지만 실제 상황에서는 상관관계보다 인과관계가 더 중요하며, 상관관계로는 잘못된 선택을 할 수 있습니다. <figure> <img src=\"/post/Recommendation/example5.png\" alt=\"example\" width=\"80%\" /> <figcaption>그림7. 과거 기록으로 취향을 추정할 때 생길 수 있는 문제</figcaption> </figure> 예를 들어, 영화 추천 플랫폼에서 여성 사용자가 액션 영화를 시청했다고 해서 그녀가 액션 영화를 좋아한다고 결론짓고 관련 액션 영화를 많이 추천할 수 있습니다. 하지만 실제로는 사용자가 개인적인 선호도 때문이 아니라 단순히 인기도 때문에 그 영화를 봤을 가능성이 큽니다. 전통적인 추천시스템이 학습한 사용자 관심사와 영화 장르 간의 허위 상관관계(spurious correlation)는 사용자 경험을 저하시킬 수 있습니다. <figure> <img src=\"/post/Recommendation/example6.png\" alt=\"example\" width=\"80%\" /> <figcaption>그림8. 인과효과를 추정하는 경우</figcaption> </figure> 반면 인과적 추천시스템은 사용자의 개별적 관심사와 순응성(conformity)의 인과적 효과를 각각 학습할 수 있어서, 나중에 액션 영화를 잘못 추천하는 것을 방지할 수 있습니다. 두 번째는 반사실 질문에 답할 수 있다는 점입니다. : <small>* 반사실 질문(counterfactual question)은 \"만약 ~했다면 어떻게 되었을까?\"라는 가정적 상황을 다루는 질문입니다.</small> 많은 추천시스템 문제들, 즉 데이터 증강(data augmentation), 분포 외 일반화(out-of-distribution generalization), 정책 평가(policy evaluation) 등은 본질적으로 반사실적 문제입니다. 이는 일부 인과 변수들의 값이 현실과 다른 상황을 의미합니다. 데이터 증강 문제의 경우, 관찰된 데이터의 중요한 보완 자원으로서 반사실적 데이터는 \"추천된 아이템이 달랐다면 사용자의 상호작용은 어떠했을까?\" 또는 \"이전에 추천받지 못한 사용자에게 아이템을 추천했다면 클릭 확률은 어떠했을까?\"와 같은 질문에 답해야 합니다. 분포 외 일반화 문제는 훈련 기간과 테스트 기간 간의 상호작용이 독립적이고 동일하게 분포한다는 IID(Independent and Identically Distributed) 가정을 위반하는 추천을 의미합니다. 전통적인 추천은 사용자와 아이템 간의 허위 연관관계를 학습할 수 있지만, 인과적 추천시스템은 반사실적 수단을 통해 추천 작업에서 불변하거나 변경되지 않는 변수나 인과관계를 찾아내고, 분포가 변경될 때 이를 재사용하여 일반화할 수 있습니다. 또한 정책 평가나 업리프트 모델링(uplift modeling)에서도 반사실적 접근이 중요합니다. 업리프트 모델링은 추천으로 인한 사용자 상호작용의 증가를 추정하는데, 이는 한 아이템에 대해 두 가지 상호 배타적인 결과(특정 사용자에게 아이템 i를 추천하거나 추천하지 않는 것) 간의 차이를 추정해야 하기 때문에 반사실적 문제입니다. --- <figure> <img src=\"/post/Recommendation/survey-fig2.png\" alt=\"figure2\" width=\"100%\" /> <figcaption>그림8. 추천에 사용되는 causal inference 방법론들</figcaption> </figure> PO-based Methods PO-based Methods는 Potential Outcome Framework를 기반으로 한 방법들을 의미합니다. Potential Outcome Framework는 인과추론의 핵심 개념 중 하나로, 각 개체(사용자)가 처치를 받았을 때와 받지 않았을 때의 잠재적 결과를 비교하는 접근 방식입니다. 추천시스템에서의 예시로는: 처치를 받은 경우: 사용자 A에게 아이템 X를 추천했을 때의 결과 (클릭 여부, 구매 여부 등) 처치를 받지 않은 경우: 사용자 A에게 아이템 X를 추천하지 않았을 때의 결과 이 두 잠재적 결과의 차이를 통해 추천의 실제 인과 효과를 측정할 수 있습니다. PO-based Methods는 이러한 잠재적 결과를 모델링하고 추정하는 다양한 기법들을 포함합니다. Propensity Score Strategy Propensity Score Strategy는 처치 할당의 확률을 모델링하여 선택 편향을 조정하는 방법입니다. 사용자가 특정 아이템을 추천받을 확률을 추정하고, 이를 가중치로 사용하여 인과 효과를 추정합니다. 이 방법은 관찰 데이터에서 RCT와 유사한 조건을 만들어내어 인과관계 추론을 가능하게 합니다. Causal Effect Strategy Causal Effect Strategy는 직접적으로 인과 효과를 모델링하는 접근 방식입니다. 사용자-아이템 상호작용의 인과 구조를 파악하고, 처치(추천)가 결과(클릭, 구매 등)에 미치는 직접적인 영향을 추정합니다. 이를 통해 추천의 실제 효과를 정확하게 측정할 수 있습니다. SCM-based Methods SCM-based Methods는 Structural Causal Model을 기반으로 한 방법들을 의미합니다. Structural Causal Model은 변수들 간의 인과 관계를 방향성 그래프(directed graph)로 표현하는 모델입니다. 각 변수는 다른 변수에 영향을 미치거나 영향을 받는 구조적 관계를 가지며, 이를 통해 복잡한 인과 구조를 시각화하고 분석할 수 있습니다. 추천시스템에서 SCM은: 사용자 특성 → 추천 결과 → 사용자 행동 아이템 특성 → 추천 결과 → 사용자 행동 사용자 선호도 → 추천 결과 → 사용자 만족도 와 같은 인과 관계를 모델링합니다. SCM의 장점은 변수들 간의 관계를 명확하게 파악하고, 혼동 요인(confounder), 중재 변수(mediator), 결과 변수(collider) 등의 구조를 이해하여 정확한 인과 효과를 추정할 수 있다는 점입니다. 이를 통해 추천시스템의 편향을 제거하고 더 정확한 추천을 제공할 수 있습니다. Model with Collider Structure Collider Structure를 가진 모델은 두 변수가 공통 결과에 영향을 미치는 구조를 다룹니다. 추천시스템에서는 사용자 특성과 아이템 특성이 모두 상호작용 결과에 영향을 미치는 경우를 의미합니다. 이 구조를 올바르게 모델링하여 편향을 방지하고 정확한 인과 효과를 추정할 수 있습니다. Model with Mediator Structure Mediator Structure는 처치가 중간 변수를 통해 결과에 영향을 미치는 구조입니다. 예를 들어, 추천이 사용자의 관심도를 높이고, 이 관심도가 최종 구매 결정에 영향을 미치는 경우입니다. 이 구조를 통해 추천의 직접적 효과와 간접적 효과를 구분하여 분석할 수 있습니다 Model with Confounder Structure Confounder Structure는 처치와 결과 모두에 영향을 미치는 공통 원인 변수를 다루는 구조입니다. 추천시스템에서는 사용자의 과거 행동이나 선호도가 현재 추천과 상호작용 결과 모두에 영향을 미치는 경우입니다. 이 구조를 제대로 모델링하여 혼동 요인을 통제하고 인과 관계를 명확히 할 수 있습니다. General Counterfactuals-based Methods General Counterfactuals-based Methods는 일반적인 반사실적 방법들을 의미합니다. Counterfactual(반사실적)은 \"만약 ~했다면 어떻게 되었을까?\"라는 가정적 상황을 다루는 개념입니다. 이 방법들은 추천시스템에서 실제로 일어나지 않은 상황에 대해 \"만약 그랬다면\"이라는 가정 하에 결과를 추정하는 기법들을 포함합니다. 추천시스템에서의 구체적인 예시: 데이터 증강: \"만약 사용자에게 다른 아이템을 추천했다면 클릭했을까?\" 도메인 적응: \"만약 다른 환경에서 이 추천을 했다면 어떻게 되었을까?\" 공정성: \"만약 사용자의 민감한 특성이 달랐다면 추천이 어떻게 달라졌을까?\" 설명 가능성: \"어떤 요인이 이 추천 결과를 만들었을까?\" 이러한 반사실적 접근을 통해 추천시스템의 인과관계를 더 정확하게 파악하고, 다양한 시나리오에 대한 예측과 분석이 가능해집니다. Strategy for Domain Adaptation Domain Adaptation 전략은 한 도메인에서 학습된 모델을 다른 도메인에 적용할 때 사용됩니다. 반사실적 접근을 통해 도메인 간 불변하는 인과 관계를 찾아내고, 이를 새로운 환경에서 활용하여 추천 성능을 향상시킵니다. Strategy for Data Augmentation Data Augmentation 전략은 관찰되지 않은 사용자-아이템 상호작용을 생성하는 방법입니다. \"만약 다른 아이템을 추천했다면 어떻게 되었을까?\"라는 반사실적 질문에 답하여 가상의 상호작용 데이터를 생성하고, 이를 통해 모델의 학습을 보강합니다. Strategy for Fairness and Explanation Fairness와 Explanation 전략은 추천시스템의 공정성과 설명 가능성을 향상시키는 방법입니다. 반사실적 분석을 통해 민감한 특성의 영향을 제거하고, 실제 세계와 반사실적 세계를 비교하여 추천 결과에 영향을 미치는 사용자 상호작용을 찾아내어 설명 가능한 추천을 제공합니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/과거경험/25-08-20-Audrey를_만난적_있나요",
    "title": "Audrey를 만난 적 있나요?",
    "date": "2025-08-20",
    "excerpt": "Audrey를 만난 적 있나요?",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "과거 경험을 표현할 때 쓰는 표현들"
    ],
    "content": "배운 것들 Have we met before? Have you been to San Francisco? Have you ever been to New York? Have you try any calm chowder? 추가 대화 표현들 인사 및 소개 Nice to meet you! / Pleased to meet you! What's your name? / May I ask your name? Where are you from? / Where do you come from? How long have you been here? 과거 경험 묻기 Have you ever...? Did you use to...? When was the last time you...? What was it like when you...? 현재 상황 묻기 What do you do? / What's your job? How long have you been working here? What brings you here today? Are you enjoying your stay? 취미 및 관심사 What do you like to do in your free time? Are you into...? (영화, 음악, 스포츠 등) What kind of music do you listen to? Do you enjoy traveling? 감정 및 의견 표현 How do you feel about...? What do you think of...? I'm really into... / I'm not really into... That sounds interesting / That sounds fun 약속 및 계획 Are you free this weekend? Would you like to grab coffee sometime? What are your plans for...? Maybe we could meet up again? 감사 및 인사 Thanks for chatting with me! It was great talking to you! See you around! / Take care! Have a great day! 작문 해보기 What did I learn today? I learned expression about how to ask past experiences. I can ask to my friend, \"Hi bro, have you been to Gangnam?\". Also I can ask \"Did you try any taco?\" One of the reason I practice speaking english is that I want to have casual conversation with foreign friends. So, this expresions are helpful. 피드백 및 수정 주요 수정 사항: 문법 오류: \"expression\" → \"expressions\" (복수형) 전치사: \"ask to my friend\" → \"ask my friend\" (to 불필요) 문법: \"One of the reason\" → \"One of the reasons\" (복수형) 철자: \"english\" → \"English\" (대문자) 문법: \"this expresions\" → \"these expressions\" (복수형 + 철자) 수정된 버전: What did I learn today? I learned expressions about how to ask about past experiences. I can ask my friend, \"Hi bro, have you been to Gangnam?\" I can also ask, \"Have you tried any tacos?\" One of the reasons I practice speaking English is that I want to have casual conversations with foreign friends. So, these expressions are helpful. 추가 개선 제안: \"Hi bro\"는 매우 친근한 표현이므로 상황에 맞게 사용 \"Have you tried\"가 \"Did you try\"보다 더 자연스러움 \"tacos\"는 보통 복수형으로 사용 \"conversations\"도 복수형이 더 자연스러움",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-manski-bounds",
    "title": "Manski Bounds 개념 정리",
    "date": "2025-08-20",
    "excerpt": "Manski bounds에 대한 설명",
    "category": "Causal Inference",
    "tags": [
      "Paper Review",
      "Partial Identification",
      "Manski bounds"
    ],
    "content": "참고자료 1: 원본 논문, pdf 링크 2: 유튜브 - Brady Neal - Causal Inference --- Manski bounds는 parital identification의 가장 기본적인 버전 혹은 개념으로 이해할 수 있습니다. partial identification은 causal inference에서 가져가는 가정들을 충족시키지 못할 때의 대처방안으로 이해할 수 있고, 직접적인 추정은 못하더라도 가정을 통해 범위 등을 추정하겠다는 개념입니다. partial identification에 대한 설명 프로젝트를 진행하면서 인과추론의 개념들을 적용해보니, 양수성을 보장할 수 없는 경우가 생기고 이때 어떻게 추정을 해야할지 고민이 됐습니다. Partial Identification은 이런 경우에 어떻게 식별할 수 있는지에 대한 내용들이며, 그 중 하나가 Mansk bounds라고 이해할 수 있습니다. --- 기본적인 컨셉 기본적인 가정 (양수성 등) 이 만족되지 않은 상황에서의 추정 방법이기 때문에, 합리적인 \"가정\"을 통해 예측값의 범위를 추정해야합니다. > 이때 Manski boudns는 가정을 최소화하고 데이터만으로 논리적으로 가능한 인과효과의 상한/하한을 계산하고자 합니다. --- 문제상황 > \"No Unobserved Confounding\" is Unrealistic 실제 데이터에서 관측되지 않은 교란변수가 없다는 가정은 매우 비현실적입니다. 결과에 영향을 주는 모든 변수들을 측정할 수 없고, 또 데이터로 가지고 있을 수 없기 때문입니다. 따라서 인과추론에서 가져가는 가정들은 이상적일 수 있습니다. > \"The Law of Decreasing Credibility : The credibility of inference decreases with the strength of the assumptions maintained\" (Manski, 2003) 저자는 가정들이 많아 질수록 추론의 신뢰도는 떨어질 것이라고 말합니다. $$ E[Y(1) - Y(0)] = E_{X} [E[Y T=1, X] - E[Y T=0, X]] \\tag{1} $$ 가정들이 성립한다면 인과효과는 수식 1처럼 정의할 수 있습니다. > Make weaker assumptions: Identify an interval. \"Partial identification\" or \"set identification\" 가정들을 약하게하고, 이때 구간을 구하는는 개념이 Partial Identification 입니다. --- No-Assumptions Bound > Example: $Y(0)$ and $Y(1)$ are between 0 and 1 $$ \\text{Min}: 0 - 1 = -1 \\leq Y_i(1) - Y_i(0) \\leq 1 = 1 - 0, \\quad \\text{Max}: 1 - 0 = 1 \\tag{2} $$ Trival length limit: 2 가정이 없는 상황에서는 인과효과의 범위를 데이터의 상한/하한을 통해 추정할 수 있습니다. > More generally, potential outcomes are bounded: $\\forall t, a \\leq Y(t) \\leq b$ $$ a - b \\leq Y_i(1) - Y_i(0) \\leq b - a \\\\ a - b \\leq E[Y_i(1) - Y_i(0)] \\leq b - a \\tag{3} $$ Trival length limit: $2(b-a)$ 일반화해서 표현하면 수식3과 같습니다. <figure> <img src=\"/post/Causal_Inference/Observational-Counterfactual-Decomposition.png\" alt=\"Observational-Counterfactual Decomposition\" width=\"80%\" /> <figcaption>그림1. Observational-Counterfactual Decomposition</figcaption> </figure> $E[Y(1) T = 0]$: treatment를 받지 않은 그룹의 사람들이 만약 treatment를 받았다면 어떤 결과를 보였을지의 기댓값 그림1과 같이 수식을 전개할 수 있고 <figure> <img src=\"/post/Causal_Inference/No-Assumptions-Bound.png\" alt=\"No-Assumptions-Bound\" width=\"80%\" /> <figcaption>그림2. No-Assumptions-Bound</figcaption> </figure> 추정할 수 없는 값들에 대해서는 수식 3에서 정의한 것과 같이, Y(t)의 범위를 넣어 추정할 수 있습니다. <figure> <img src=\"/post/Causal_Inference/No-Assumptions-Interval-Length.png\" alt=\"No-Assumptions-Interval-Length\" width=\"80%\" /> <figcaption>그림3. No-Assumptions-Interval-Length</figcaption> </figure> No-assumptions interval lenght: $(1-\\pi)b + \\pi b - \\pi a - (1 - \\pi)a = b - a$ 최종적으로 manski bounds 를 통해 추정한 범위는 b-a가 되고 이는 Y의 범위만으로 추정한 범위보다 더 좁아지게 됩니다. --- 정리 > 추정할 수 없는 값에 대해서는 Y의 상한/하한을 통해 추정하겠다. 제가 이해하는 manski bounds의 핵심 컨셉입니다. paritial identification의 가장 쉽고 직관적인 개념입니다. 다만, 범위가 굉장히 넓기 때문에 실제로 사용하지는 않고 baseline 정도로 사용해볼만하다고 생각합니다. --- 실제로 적용할 떄 고민해야하는 부분들 실제로 적용할 때는 이 컨셉만을 차용해서 사용할 것 같습니다. 이때 고려해야할 점을 간단히 적어봤습니다. 수식에 X가 들어가야 함 (고객별로 bound를 다르게 그려주기 위해서) $a(x) \\leq Y(x) \\leq b(x)$ : 풀고 있는 문제에서는 Y는 vault, b와 a는 ACD와 같은 값으로 정의할 수 있겠다. 추론시점에서는 observational한 값이 없음 -> 다 추정값",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/비교및비유/08-19-리뷰데이",
    "title": "리뷰 데이",
    "date": "2025-08-19",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "비교 및 비유할 때 쓰는 표현들"
    ],
    "content": "표현들 정리",
    "public": true
  },
  {
    "id": "Data Science/Statistics/pearson-vs-spearman-coefficient",
    "title": "Pearson vs Spearman 정리",
    "date": "2025-08-19",
    "excerpt": "상관관계 지표에 대한 비교",
    "category": "Data Science",
    "tags": [
      "statistics",
      "correlation"
    ],
    "content": "참고 글 링크드인 - Timer Bikmukhametov 링크드인 글 정리 > Pearson vs Spearman Coefficient. 어떤 것을 선택할까? <figure> <img src=\"/post/DataScience/linear_corr.gif\" alt=\"Pearson vs Spearman\" style=\"width: 50%; height: auto;\" /> <figcaption>Pearson vs Spearman</figcaption> </figure> Pearson coefficient Pearson coefficient는 두 변수의 covariance를 각 변수의 standard deviation의 곱으로 나눈 값입니다. 절댓값이 정확히 ±1이면 X와 Y 사이의 관계를 linear equation으로 완벽하게 설명할 수 있음을 의미합니다. 장점: 간단하며, normally distributed data에 잘 작동합니다 linear relationship을 잘 나타냅니다 단점: non-linear relationship을 포착하지 못합니다 outlier에 민감하여 값이 왜곡될 수 있습니다 Spearman coefficient monotonic relationship을 측정합니다 (데이터가 linear하지 않더라도 함께 증가하거나 감소하는 관계). 중복된 data value가 없다면, +1 또는 -1의 완벽한 Spearman correlation이 발생하는 경우는 각 변수가 다른 변수의 완벽한 monotone function일 때입니다. 장점: non-linear relationship과 outlier에 잘 작동합니다 단점: absolute difference를 무시하고 rank만 중요합니다 ranking에 의존하므로 outlier에 대한 sensitivity가 낮습니다 선택 가이드 기본 원칙 데이터가 linear trend를 따른다면, Pearson이 좋은 선택입니다 데이터가 non-linear pattern이나 outlier를 가진다면, Spearman이 더 신뢰할 만합니다 추가 고려사항 데이터 분포 확인: 정규분포: Pearson이 더 적합 (parametric method) 비정규분포: Spearman이 더 안전 (non-parametric method) 순서형 데이터: Spearman만 의미가 있음 샘플 크기: 작은 샘플 (n < 30): Spearman이 더 robust 큰 샘플: 둘 다 사용 가능하지만 목적에 따라 선택 해석의 목적: 선형 관계의 강도 측정이 목적: Pearson 단조 관계의 방향성 확인이 목적: Spearman 순위 기반 분석: Spearman (예: 설문조사 순위 데이터) 실무 팁: 두 방법 모두 계산해보고 결과가 크게 다르면 데이터를 재검토 Pearson ≫ Spearman: 강한 선형 관계 존재 Spearman ≫ Pearson: 비선형이지만 단조적 관계 존재 둘 다 낮음: 관계가 약하거나 복잡한 비선형 관계 주의사항: 상관관계 ≠ 인과관계 (둘 다 동일하게 적용) 구간별로 다른 패턴을 보이는 데이터는 전체 상관계수로 판단하지 말 것 이상치 제거 전후로 비교 분석 권장 수식과 계산 방식 Pearson 상관계수 (r) > Pearson 상관계수는 두 변수 간의 선형 관계의 강도를 측정합니다. 수식: $$ r = \\frac{\\sum_{i=1}^{n}(x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_i - \\bar{x})^2}\\sqrt{\\sum_{i=1}^{n}(y_i - \\bar{y})^2}} $$ 또는 공분산을 사용하여: $$ r = \\frac{Cov(X,Y)}{S_x \\cdot S_y} $$ 여기서: $x_i, y_i$: 개별 데이터 포인트 $\\bar{x}, \\bar{y}$: 각 변수의 평균 $Cov(X,Y)$: X와 Y의 공분산 $S_x, S_y$: 각 변수의 표준편차 계산 과정: 각 변수의 평균을 계산 각 데이터 포인트에서 평균을 뺀 편차를 계산 편차의 곱의 합을 구함 (공분산의 분자) 각 변수의 편차 제곱합을 구하고 제곱근을 취함 공분산을 표준편차의 곱으로 나눔 Spearman 상관계수 (ρ) > Spearman 상관계수는 두 변수의 순위(rank) 간의 상관관계를 측정합니다. 수식: $$ \\rho = 1 - \\frac{6\\sum_{i=1}^{n}d_i^2}{n(n^2-1)} $$ 여기서: $d_i$: 각 관측치의 두 변수 순위 차이 $n$: 관측치의 개수 또는 순위에 대한 Pearson 상관계수로: $$ \\rho = \\frac{\\sum_{i=1}^{n}(R(x_i) - \\overline{R(x)})(R(y_i) - \\overline{R(y)})}{\\sqrt{\\sum_{i=1}^{n}(R(x_i) - \\overline{R(x)})^2}\\sqrt{\\sum_{i=1}^{n}(R(y_i) - \\overline{R(y)})^2}} $$ 여기서: $R(x_i), R(y_i)$: 각 변수의 순위 $\\overline{R(x)}, \\overline{R(y)}$: 순위의 평균 계산 과정: 각 변수의 데이터를 순위로 변환 (가장 작은 값이 1순위) 동일한 값이 있는 경우 평균 순위를 부여 각 관측치에 대해 두 변수의 순위 차이 $d_i$를 계산 $d_i^2$의 합을 구함 공식에 대입하여 상관계수를 계산 예시: 데이터: X = [1, 2, 3, 4, 5], Y = [2, 4, 1, 5, 3] 순위 변환: X의 순위: [1, 2, 3, 4, 5] Y의 순위: [2, 4, 1, 5, 3] 순위 차이: d = [1-2, 2-4, 3-1, 4-5, 5-3] = [-1, -2, 2, -1, 2] $d^2$ = [1, 4, 4, 1, 4], 합계 = 14 $\\rho = 1 - \\frac{6 \\times 14}{5(25-1)} = 1 - \\frac{84}{120} = 1 - 0.7 = 0.3$",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/when-assumptions-failed",
    "title": "양수성 가정 등 필요한 가정이 깨졌을 때 대처방안",
    "date": "2025-08-19",
    "excerpt": "양수성 가정 등 필요한 가정이 깨졌을 때 대처방안",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "GPT Pro 답변 아주 핵심만 먼저 요약할게요. 양수성(positivity/overlap)이 안 맞으면 ‘원래 ATE를 그대로’는 비(非)식별일 수 있습니다. 따라서 (1) 문제의 성격을 진단하고, (2) 표본/대상·가중치·추정량·질문(estimand) 중 어디를 바꿀지 결정하며, (3) 바뀐 점(대상·평가량)을 투명하게 보고하는 것이 정석입니다. 이 아래에 이론적 배경→진단→대응전략(옵션별 장단점)→실무 레시피 순으로 정리했습니다. --- 1) 이론적 배경: 왜 양수성이 중요한가? (이산 처리 $A\\in\\{0,1\\}$) 양수성은 $0< P(A=a\\mid X=x)$ 가 해당 공변량 지지집합 전체에서 성립해야 한다는 가정입니다. 강한 의미로는 $\\exists\\ \\epsilon>0$ s.t. $\\epsilon\\le P(A=1\\mid X)\\le 1-\\epsilon$. 이 가정이 깨지면 어떤 $x$에 대해서는 $A=a$가 “절대” 관측되지 않아 $\\mathbb{E}[Y\\mid A=a,X=x]$ 자체가 데이터로 확인 불가(무한 외삽)라 ATE의 비모수적 식별이 무너집니다. Hernán & Robins 교과서에서도 양수성을 교환가능성(무교란)·일관성과 함께 핵심 식별가정으로 설명합니다. ([Miguel Hernán][1], [Harvard SPH Content][2]) 실무에서는 두 부류로 나눕니다. 구조적 위반(structural): 규정/의료 금기 등으로 특정 $X=x$에서 어떤 처치가 원천 불가능. 확률적(실무) 위반(practical): 가능은 하지만 표본에서 거의/전혀 나타나지 않아 가중치가 폭주. Petersen 등은 두 상황을 구분하고, 전자는 “질문을 바꿔야” 하고 후자는 “추정 절차를 안정화”하는 접근을 정리합니다. ([biostats.bepress.com][3]) 시간가변 노출에서는 각 시점마다 순차 양수성이 필요합니다. 위반되면 MSM/IPW 같은 방법의 가중치가 폭주해 분산·편향이 커집니다. (가중치 안정화는 분산을 줄여주지만, 식별 자체를 복원해 주진 않습니다.) ([Epidemiologic Research][4]) --- 2) 먼저 하는 일: 진단 체크리스트 추정된 PS 분포의 겹침(히스토그램/커널밀도·Q–Q/eCDF 겹침): 꼬리에서 한쪽 집단이 “사라지면” 경고. Imbens & Rubin 교과서의 “overlap 진단” 논의와 유사. ([Cambridge University Press & Assessment][5]) 극단 가중치/유효표본크기(ESS): $\\text{ESS}=(\\sum_i w_i)^2/\\sum_i w_i^2$ (집단별 ESS도 확인). ESS가 급감하면 실무적 양수성 문제가 의심됩니다. ([R Project Search][6]) 분리(separation)/완전예측: PS 로지스틱에서 완전분리 징후(계수 발산). 연속/다단계 노출: “연속 노출용 양수성 진단” 같은 전용 테스트/시각화를 병행. ([Wiley Online Library][7]) 부트스트랩 기반 취약성 평가: 파라메트릭 부트스트랩으로 양수성 취약 구간을 정량화(폭주 가중치가 추정량을 지배하는지). ([biostats.bepress.com][3]) --- 3) 대응 전략 지도(무엇을 바꿀 것인가?) A. 표본/대상(population)을 바꾸기 — 겹침이 있는 부분만 추정 공통지지(common support) 제한/트리밍: PS가 $[\\alpha,1-\\alpha]$에 있는 단위만 분석. Crump et al.(2009)는 분산을 최소화하는 근사 규칙으로 $0.1\\sim0.9$ 트리밍을 제안(상황 의존적이며 보고에 투명성 필요). ([EconPapers][8]) 트리/규칙 기반 지지영역 선택: Traskin & Small(2011)은 CART로 해석 가능한 겹침 영역을 구성. ([SpringerLink][9]) Coarsened Exact Matching (CEM): 사전 코어스닝한 층에서 처치/대조가 모두 존재하는 층만 유지 → 양수성을 “설계로” 강화. 단, \\\\대상(estimand)이 ‘매칭 후 표본’\\\\으로 바뀝니다. ([JSTOR][10]) > 장점: 외삽을 피하고 안정적. 단점: \\\\ATE가 아니라 ‘겹침 있는 부분의 효과’\\\\로 질문이 바뀝니다. (반드시 보고) --- B. 가중치를 바꾸기(안정화·캘리브레이션) — 실무 위반 진정시키기 Stabilized IPTW: 분산을 줄이고 소수 관측치가 지배하는 것을 막습니다(식별을 복원하는 건 아님). ([Epidemiologic Research][4]) 가중치/PS 절단(truncation): $[c,1-c]$로 자르거나 극단 가중치를 윈저라이즈. 편향–분산 절충이므로 cutoff를 데이터 구동적으로 고르려는 연구가 있습니다(예: AJE 2022). ([Oxford Academic][11]) Overlap Weights(OW): 처치군은 $1-e(X)$, 대조군은 $e(X)$로 가중 → 가중치가 유계이고 겹침이 큰 구간에 초점(ATO). 이론적으로 분산 최적·작은 표본에서 평균의 정확균형까지 보장. 의료 응용 튜토리얼도 다수. ([Oxford Academic][12]) 균형지향 PS/보정: CBPS(공변량 균형을 일으키도록 PS를 추정)·Entropy Balancing(모멘트 제약으로 직접 균형). 극단 가중치를 억제하며 균형을 보장. ([imai.fas.harvard.edu][13], [Massachusetts Institute of Technology][14]) > 장점: 원표본을 크게 버리지 않고 안정화. 단점: ATE→ATO/ATT 등 대상이 달라질 수 있음(특히 OW), 또는 절단으로 작은 편향이 들어옴(반드시 민감도 분석). ([Oxford Academic][11]) --- C. 추정량을 바꾸기(모형기반·DR) — 외삽 민감도를 줄이기 이중강건(AIPW)·TMLE: 결과모형과 PS모형을 함께 사용해 한쪽이 약간 빗나가도 일관성 보존. \\\\다만 식별 가정(양수성 자체)\\\\은 여전히 필요. TMLE/CTMLE는 작은 $g(X)=P(A=1\\mid X)$ 에 민감한 “clever covariate”를 안정화하려는 절단/협업형 방법이 제안되어 왔습니다. ([CRAN][15], [arXiv][16], [Oxford Academic][11]) --- D. 질문(estimand)을 바꾸기 — 양수성 가정을 약화/회피 Incremental Propensity Score Interventions (IPSI): “모든 사람을 1로/0으로”가 아니라 \\\\처치 확률의 log-odds를 $\\delta$만큼 키우면 결과가 얼마나 변하나?\\\\를 묻는 점진적 개입 효과. 양수성 가정이 필요 없습니다(처치 확률을 ‘증감’시키는 개입). 비모수 이론·추정·신뢰대역이 정리되어 있습니다. ([arXiv][17], [Beyond the Average Treatment Effect][18]) Stochastic/Modified Treatment Policies (MTP/LMTP): “자연적 처치값을 기준으로 $\\delta$만큼 늘리기/상한을 두기” 같은 현실적 정책 개입을 정의·추정 → 시간가변 노출에서도 순차 양수성의 부담을 덜어줍니다. ([IDEAS/RePEc][19], [Epidemiologic Research][20]) > 장점: 현실적·해석 가능, 양수성 부담 완화/회피. 단점: 질문이 ATE와 다름—정책/과학적 목적과 일치하는지 합의가 필요. --- E. 부분식별/바운딩 — 구조적 위반에서 정직한 보고 Manski bounds 등: “최악–최선” 범위를 제시(필요 시 IV/단조성으로 축소). 구조적 위반에선 이런 바운드 보고가 솔직한 선택입니다. 최근엔 위반 강도를 매개변수화해 breakdown frontier까지 제시하는 프레임워크도 나옵니다. ([JSTOR][21], [arXiv][22]) --- 4) 실무 레시피(이진 처치, 단일 시점 예) 진단: PS 겹침·ESS·극단가중치 체크(집단별 ESS, max/min PS 등). 연속/다수준이면 전용 진단도 병행. ([R Project Search][6], [Wiley Online Library][7]) 구조적 vs 실무 위반 구분 구조적: 그 $X$ 영역의 ATE는 비식별. → 대상 변경 (겹침 영역으로 제한/OW) 또는 질문 변경(IPSI/MTP), 또는 부분식별 바운드. ([biostats.bepress.com][3]) 실무 위반: 극단가중치/저 ESS. → 가중치 안정화(Stabilized IPTW, 절단)·균형지향 가중(CBPS/EB)·OW·DR/TMLE을 조합. ([Epidemiologic Research][4], [Oxford Academic][11]) 대상과 추정기의 매칭 ATE 유지 고집: 최소한 대상 제한+DR(예: $[0.05,0.95]$ 트리밍 + AIPW/TMLE) & 절단 민감도 곁들임. ([Oxford Academic][11]) 겹침집단 효과(ATO) 허용: Overlap Weights + AIPW/TMLE 권장(가중치 유계·균형 우수). ([Oxford Academic][12]) 정책/현실 질문에 관심: IPSI(단일/종단) 또는 LMTP(연속/시간가변)로 질문을 재정의. ([arXiv][17], [Epidemiologic Research][20]) 보고: (i) 선택한 대상/estimand와 이유, (ii) 트리밍/절단 규칙, (iii) 가중치 분포·ESS, (iv) 균형 지표, (v) 민감도(컷오프·방법 간 비교). --- 5) 시간가변·연속 노출의 주의점 순차 양수성이 특히 취약. Stabilized IPW·Super Learner 등으로 품질을 높이되, 폭주하면 LMTP/확률적 개입/IPSI로 문답을 바꾸는 것이 합리적일 때가 많습니다. ([Epidemiologic Research][4], [tlverse][23], [arXiv][17]) 연속 노출: 양수성 진단 도구를 활용하고(지역적 밀도 결핍), shift/truncation 정책 효과 같은 MTP류를 고려. ([Wiley Online Library][7], [IDEAS/RePEc][19]) --- 6) 어떤 선택이 언제 좋은가? (요약 표) 상황 권장 전략 핵심 장점 주의점 ---------------------- ------------------------- --------------------------- ---------------------------------------------------- 꼬리에서 겹침 거의 없음(실무 위반) Overlap Weights + DR 가중치 유계, 겹침집단(ATO)에 최적 효율·균형 ATO(질문)로 변경됨 ([Oxford Academic][12]) ATE 고수하되 극단가중치 문제 트리밍/절단 + TMLE/AIPW 편향–분산 절충으로 안정화 절단이 체계적 편향 도입 가능 → 민감도 보고 필수 ([Oxford Academic][11]) 구조적 금기(어떤 $X$에서 처치 불가) 겹침영역 한정 또는 바운드 보고 외삽 회피/정직한 불확실성 보고 원래 ATE는 비식별 ([biostats.bepress.com][3], [JSTOR][21]) 정책적 “얼마나 더 권장/감소”가 궁금 IPSI 양수성 불요, 추세곡선으로 해석 용이 ATE와 다른 질문 ([arXiv][17]) 종단·연속 노출에서 폭주 LMTP/확률적 개입 현실적 정책 정의, 순차 양수성 부담 완화 개입 정의·해석을 명확히 문서화 필요 ([Epidemiologic Research][20]) --- 7) 참고·근거(일부) 진단/대응 종합: Petersen et al., Diagnosing and Responding to Violations in the Positivity Assumption (Berkeley Working Paper; 구조적/확률적 위반, 표본 제한·질문 변경의 원칙). ([biostats.bepress.com][3]) 트리밍 규칙: Crump–Hotz–Imbens–Mitnik (2009, Biometrika): $[0.1,0.9]$ 규칙의 근사 최적성. ([EconPapers][8]) Overlap Weights/ATO: Li–Morgan–Zaslavsky (2018, JASA) 이론·유계성·정확균형; 의료 응용 요약(JAMA 2020). ([Oxford Academic][12]) Stabilized IPW: Cole & Hernán (2008, AJE). ([Epidemiologic Research][4]) 절단의 데이터구동 선택/CTMLE: Gruber et al. (2022, AJE); Positivity‑C‑TMLE (2017). ([Oxford Academic][11], [arXiv][16]) CBPS/Entropy Balancing: Imai & Ratkovic (2014, JRSS‑B); Hainmueller (2012, Political Analysis). ([imai.fas.harvard.edu][13], [Massachusetts Institute of Technology][14]) IPSI: Kennedy (2019, JASA): 양수성 없이 점진적 개입 효과. ([arXiv][17]) Stochastic/Modified Treatment Policies: Díaz & van der Laan (2012, Biometrics); Young–Hernán–Robins (2014, Epidemiologic Methods) 등. ([IDEAS/RePEc][19], [De Gruyter Brill][24]) 연속노출 양수성 진단: “A Simple Diagnostic for the Positivity Assumption for Continuous Exposures.” ([Wiley Online Library][7]) ESS 지표: WeightIt 문서(ESS 공식). ([R Project Search][6]) --- 8) 실전 적용시 체크리스트(짧게) [ ] (필수) 겹침/ESS/가중치 분포 보고 [ ] (필수) \\\\선택한 estimand(ATE/ATT/ATO/IPSI/LMTP)\\\\와 대상(population) 명시 [ ] (권장) 트리밍/절단 규칙과 민감도 분석(컷오프·방법 간 비교) [ ] (권장) 균형 지표(SMD 등)와 추정기 DR 사용 여부 [ ] (구조적 위반 시) 부분식별 바운드 또는 질문 변경 이유 설명 --- 원하시면, 가지고 계신 데이터/주제에 맞춰 (i) 겹침 진단 플롯과 ESS 계산, (ii) ATE vs ATO(OW), (iii) 절단 민감도, (iv) IPSI/LMTP로 질문 재정의까지 한 번에 돌아가는 분석 워크플로를 만들어 드릴게요. [1]: https://miguelhernan.org/whatifbook?utm_source=chatgpt.com \"Causal Inference: What If (the book) — Miguel Hernán\" [2]: https://content.sph.harvard.edu/wwwhsph/sites/1268/2018/12/hernanrobins_v1.10.37.pdf?utm_source=chatgpt.com \"Causal Inference\" [3]: https://biostats.bepress.com/ucbbiostat/paper269/ \" \\\"Diagnosing and Responding to Violations in the Positivity Assumption\\\" by Maya L. Petersen, Kristin Porter et al. \" [4]: https://epiresearch.org/wp-content/uploads/2014/07/Cole_AJE_2008_168_656.pdf?utm_source=chatgpt.com \"kwn164 656..664 - epiresearch.org\" [5]: https://www.cambridge.org/core/books/causal-inference-for-statistics-social-and-biomedical-sciences/assessing-overlap-in-covariate-distributions/0186880D45FC793BE1D23E4016A3419F?utm_source=chatgpt.com \"Assessing Overlap in Covariate Distributions (Chapter 14) - Causal ...\" [6]: https://search.r-project.org/CRAN/refmans/WeightIt/html/ESS.html?utm_source=chatgpt.com \"R: Compute effective sample size of weighted sample\" [7]: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sim.70194?utm_source=chatgpt.com \"A Simple Diagnostic for the Positivity Assumption for Continuous Exposures\" [8]: https://econpapers.repec.org/RePEc%3Aoup%3Abiomet%3Av%3A96%3Ay%3A2009%3Ai%3A1%3Ap%3A187-199 \"EconPapers: Dealing with limited overlap in estimation of average treatment effects\" [9]: https://link.springer.com/content/pdf/10.1007/s12561-011-9036-3.pdf?utm_source=chatgpt.com \"Defining the Study Population for an Observational Study to Ensure ...\" [10]: https://www.jstor.org/stable/pdf/41403736.pdf?utm_source=chatgpt.com \"Causal Inference without Balance Checking: Coarsened Exact Matching on ...\" [11]: https://academic.oup.com/aje/article/191/9/1640/6580570?utm_source=chatgpt.com \"Data-Adaptive Selection of the Propensity Score Truncation Level for ...\" [12]: https://academic.oup.com/aje/article/188/1/250/5090958?utm_source=chatgpt.com \"Addressing Extreme Propensity Scores via the Overlap Weights\" [13]: https://imai.fas.harvard.edu/research/files/CBPS.pdf?utm_source=chatgpt.com \"Covariate balancing propensity score - Harvard University\" [14]: https://www.mit.edu/~jhainm/Paper/eb.pdf?utm_source=chatgpt.com \"Entropy Balancing for Causal Effects: A Multivariate Reweighting ... - MIT\" [15]: https://cran.r-project.org/web/packages/tmle/tmle.pdf?utm_source=chatgpt.com \"tmle: Targeted Maximum Likelihood Estimation\" [16]: https://arxiv.org/pdf/1707.05861?utm_source=chatgpt.com \"On Adaptive Propensity Score Truncation in Causal Inference\" [17]: https://arxiv.org/abs/1704.00211?utm_source=chatgpt.com \"Nonparametric causal effects based on incremental propensity score interventions\" [18]: https://beyondtheate.com/08_R_ipsi.html?utm_source=chatgpt.com \"Incremental Propensity Score Interventions – Beyond the Average ...\" [19]: https://ideas.repec.org/a/bla/biomet/v68y2012i2p541-549.html?utm_source=chatgpt.com \"Population Intervention Causal Effects Based on Stochastic I\" [20]: https://epiresearch.org/wp-content/uploads/2024/04/Nonparametric-Causal-Effects-Based-on-Longitudinal-Modified-Treatment-Policies.pdf?utm_source=chatgpt.com \"Nonparametric Causal Effects Based on Longitudinal Modified Treatment ...\" [21]: https://www.jstor.org/stable/pdf/2006592?utm_source=chatgpt.com \"Nonparametric Bounds on Treatment Effects - JSTOR\" [22]: https://arxiv.org/pdf/2505.24296?utm_source=chatgpt.com \"Data Fusion for Partial Identification of Causal Effects\" [23]: https://tlverse.org/tlverse-handbook/shift.html?utm_source=chatgpt.com \"Chapter 9 Stochastic Treatment Regimes Targeted Learning in R - tlverse\" [24]: https://www.degruyterbrill.com/document/doi/10.1515/em-2012-0001/pdf?utm_source=chatgpt.com \"Identification, Estimation and Approximation of Risk under ...\"",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-partial-identification",
    "title": "Partial Identification에 대한 설명",
    "date": "2025-08-19",
    "excerpt": "Partial Identification이 무엇이고, 양수성이 위반되었을 때 어떻게 쓸 수 있는지",
    "category": "Causal Inference",
    "tags": [
      "Partial Identification"
    ],
    "content": "Partial Idenfication은 인과추론의 가정들이 깨졌을 때 사용하는 방법입니다. --- Treatment positivity assumption (positivity / overlap assumption) 인과추론(causal inference)에서 필수 가정 중 하나입니다. $$ 0 < P(T=1 \\mid X=x) < 1 \\quad \\text{for all } x \\tag{1} $$ 수식 1과 같이 정의됩니다. 이는 모든 공변량 $x$에 대해 처리를 받을 확률도 0보다 크고, 받지 않을 확률도 0보다 커야 한다는 조건을 뜻합니다. 이는 어떤 $x$ 값을 가진 집단도 완전히 한쪽 처우(treated/untreated)에만 속하면 안 된다는 것을 의미하고, 이게 만족해야 비교(반사실적 추정)가 가능해지기 떄문입니다. 하지만 현실에서는 positivity violation이 자주 발생합니다. 예를 들면 특정 연령대(아주 어린 아이)는 어떤 약을 절대 처방하지 않는 경우가 있습니다. 그러면 $P(T=1 \\mid X=x) = 0$이 되어버립니다. --- Partial identification 완전식별과 부분식별로 구분지어 이해할 수 있습니다. 완전한 식별(full identification): 모든 가정(ignorability, positivity 등)이 만족되면 인과효과(ATE 등)를 점추정(point estimate)할 수 있습니다. 부분 식별(partial identification): 가정이 일부 깨져서 정확한 점추정이 불가능할 때, 가능한 인과효과의 범위(bound, interval)를 추정하는 방법입니다. → “효과가 정확히 β다” 대신 “효과는 \\[a, b] 사이에 있다”라고 말하는 방식. --- Positivity 위반과 partial identification의 관계 Partial identification 접근은 positivity assumption이 위반된 상황을 다루는 대표적인 방법 중 하나입니다. positivity가 깨지면 특정 $X$ 영역에서 반사실적 결과를 추정할 수 없게 되는데, 이때 그 영역에서 인과효과가 어떤 범위에 있을지 논리적·수학적 제약을 통해 interval bound를 구합니다. Manski(1990s 이후)의 work나 최근 causal inference 문헌에서는 이런 상황을 \"bounded causal effects\" 문제로 다룹니다. --- 예시 어떤 약이 어린이에게는 절대 투여되지 않았다고 해봅시다. → positivity violation. 그럼 어린이 집단에서는 “투여 시 결과”를 관측할 수 없습니다. Partial identification 접근 (a) 이 집단에서의 결과가 최소한 관측된 범위 안에 있을 것이라는 가정, (b) monotonicity 같은 추가 가정 등을 사용해서, \"효과는 최소 0 이상, 최대 5 이하\" 처럼 구간 추정을 제시하는 개념입니다. --- 부분 식별이 필요한 대표적 상황 (1) Positivity violation (overlap 위반)* 어떤 $X$ 값에서 처리(Treatment)가 전혀 주어지지 않는 경우. 예: 어떤 나이대 아이들에게는 절대 약물이 투여되지 않았다면 $P(T=1 X)=0$. 이때 그 구간에서 반사실적(counterfactual) 결과는 관측 불가능. 해결: 그 구간에서 가능한 결과 범위를 가정하여 bound 제시. → Manski bounds (1989, 1990): 관찰 가능한 부분과 불가능한 부분을 분리해, 논리적으로 가능한 최대·최소 효과를 제시. --- (2) Unobserved confounding (비관측 교란) 교란변수 $U$가 관측되지 않으면 ignorability가 깨짐. 이 경우 “효과의 점추정”은 불가능하지만, 추가 가정(예: 교란의 최대 효과 크기 제한)을 넣으면 bounded effect를 제시할 수 있음. 예: Rosenbaum의 sensitivity analysis → 교란이 얼마나 강해야 추정된 효과가 무너질지를 bound로 제시. --- (3) Instrumental Variables (IV) 도구변수 접근에서도 부분 식별이 자주 등장. 강한 가정(완전한 독립성, exclusion restriction 등)이 만족되면 LATE(Local Average Treatment Effect)를 점추정할 수 있음. 하지만 가정을 완화하면 → LATE 자체는 interval identification으로만 가능. 대표적: Imbens & Manski (2004) bounds. --- (4) Selection bias / Missing data 데이터가 무작위로 빠지지 않고 선택적 결측(non-ignorable missingness)이 있을 때. 이 경우도 관찰 불가능한 부분에 대해 최소·최대 범위를 계산하는 Manski’s missing data bounds가 있음. --- 대표적인 Bound 기법들 Manski bounds (worst-case bounds) 가장 기본적인 형태. 가정을 최소화하고, 관측된 데이터만으로 논리적으로 가능한 인과효과의 상한/하한을 계산. 단점: 보통 interval이 너무 넓어서 실용성이 떨어짐. Lee bounds (2009) \\\\선택적 이탈(selection)\\\\이 있는 프로그램 평가에서 사용. monotonicity (단조성) 가정을 추가해서 bound를 좁힘. Imbens-Manski bounds (2004) 도구변수 기반 추론에서 treatment effect 범위 제시. Sensitivity analysis 기반 bounds 비관측 교란이 있을 때, 특정 “감도(sensitivity)” 가정을 통해 효과 범위를 제시. --- 이론적 철학 점추정(point identification) 은 강한 가정(positivity, ignorability 등)을 요구. 현실에서는 이 가정들이 자주 깨짐 → 강한 가정 대신 약한 가정만 두고, 추정 가능한 부분만으로 inference → 부분 식별. 따라서 partial identification은 \"robust inference under weaker assumptions\" 라고 할 수 있습니다. Manski는 이를 \"honest inference\" 라고 부릅니다. (불필요하게 강한 가정으로 잘못된 점추정을 하느니, 범위를 제시하는 것이 더 정직하다). ---",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/비교및비유/08-18-근력운동은할수록힘이세져",
    "title": "근력 운동은 할수록 힘이 세져",
    "date": "2025-08-18",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "비교 및 비유할 때 쓰는 표현들"
    ],
    "content": "하면 할수록 좋아 As you speak more, you get more confident. The more you speak, the more confident you get. The more you lift, the stronger you get. 배운 것들 정리 The more you run, the fitter you get. Lift is like a box of chocalates. 간단한 작문 Today I learned expressions using \"the more ~, the more\" and \"~ is like ~\". These expression is used to metaphor ~~. I had band 합주 class today. It was so much fun. 합이 맞는다는 느낌은 굉장히 기분이 좋더라. I learend guiatr a year from last year. I'm sure that the more I practice, the better I play. (조금씩 기타를 잘 치게 되는 걸 느낄 때 굉장히 기쁘다.) When I feel I'm playing guitar little better than before, I feel so happy. Practing guitar is like a life. No matter what I do, I can be better if I practice consistently. Don't be hurry up, just take the rythm. 더 자연스러운 버전 Today I learned expressions using \"the more ~, the more\" and \"~ is like ~\". These expressions are used for metaphors. I had a band practice class today. It was so much fun. The feeling of being in sync was amazing. I have been learning guitar for a year since last year. I'm sure that the more I practice, the better I will play. When I feel myself getting a little better at guitar than before, I feel so happy. Playing guitar is like life itself. No matter what I do, I can get better if I practice consistently. Don't rush, just keep the rhythm. 피드백 👍 잘한 점들: \"the more ~, the more\" 구조를 정확하게 사용했습니다 전체적인 메시지가 명확하게 전달됩니다 은유(metaphor) 표현을 잘 활용했습니다 🔧 문법 수정사항: \"These expression\" → \"These expressions\" (복수형) \"metaphor\" → \"metaphors\" (동사 뒤에는 명사 복수형) \"I learend\" → \"I have been learning\" (현재완료진행형이 더 자연스럽) \"a year from last year\" → \"for a year since last year\" (기간 표현) \"Practing\" → \"Playing\" (철자 + 더 자연스러운 표현) \"be hurry up\" → \"rush\" (더 간단하고 자연스러운 표현) \"rythm\" → \"rhythm\" (철자) 💡 표현 개선: \"합이 맞는다\" → \"being in sync\"가 더 자연스럽습니다 \"life\" → \"life itself\"로 강조 괄호 안의 한국어를 영어로 자연스럽게 번역했습니다 전반적으로 좋은 작문이었습니다! 계속 연습하시면 더욱 자연스러워질 거예요! 🎸",
    "public": true
  },
  {
    "id": "Data Science/Statistics/confidence-interval-vs-prediction-interval",
    "title": "Confidence Interval vs Prediction Interval",
    "date": "2025-08-18",
    "excerpt": "CI와 PI 각각에 대한 정리와, 이 개념 간의 차이에 대한 정리",
    "category": "Data Science",
    "tags": [
      "Confidence-Interval",
      "Prediction-Interval",
      "statistics"
    ],
    "content": "Linear Regression에서의 CI 와 PI 개념 Confidence Interval for the Mean Response 참고자료 1: psu edu > In this section, we are concerned with the confidence interval, called a \"t-interval,\" for the mean response when the predictor value is $x_h$. formula in words: $$ \\text{Sample estimate} \\pm (t\\text{-multiplier} \\times \\text{standard error}) $$ formula in notation: $$ \\hat{y}_h \\pm t_{(1-\\alpha/2, n-2)} \\times \\sqrt{MSE \\times \\left( \\frac{1}{n} + \\dfrac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right)} $$ where: $\\hat{y}_h$ is the \"fitted value\" or \"predicted value\" of the response when the predictor is $x_h$ $t_{(1-\\alpha/2, n-2)}$ is the \"t-multiplier.\" Note that the t-multiplier has $n-2$ (not $n-1$) degrees of freedom because the confidence interval uses the mean square error (MSE) whose denominator is $n-2$. The expression under the square root is the \"standard error of the fit,\" which depends on the mean square error (MSE), the sample size ($n$), how far in squared units the predictor value $x_h$ is from the average of the predictor values $\\bar{x}$, and the sum of the squared distances of the predictor values $x_i$ from the average of the predictor values $\\bar{x}$. 수식을 알아야하는 이유는 width of CI를 결정하는게 뭔지 알기 위해서. Prediction Interval for a New Response Mean Response vs New Response Mean Response (평균 응답) 정의: 특정 $x_h$ 값에서 무한히 많은 관측값들의 평균 예시: 키가 170cm인 모든 사람들의 평균 몸무게 특징: 고정된 값 (변하지 않음) 모집단의 특성 Confidence Interval의 대상 New Response (새로운 응답) 정의: 특정 $x_h$ 값에서 새로운 개별 관측값 예시: 키가 170cm인 특정 한 사람의 실제 몸무게 특징: 랜덤한 값 (매번 다를 수 있음) 개별 관측값의 특성 Prediction Interval의 대상 > CI는 mean estimator의 불확실성(주로 epistemic)을, PI는 mean 주변의 noise(aleatoric)까지 포함해 “개별 예측값”의 불확실성을 다룸. 따라서 동일한 설정에서 PI의 폭이 CI보다 넓음. 수식에서의 표현 formula in words: $$ \\text{Sample estimate} \\pm (t\\text{-multiplier} \\times \\text{standard error}) $$ formula in notation: $$ \\hat{y}_h \\pm t_{(1-\\alpha/2, n-2)} \\times \\sqrt{MSE \\times \\left( 1 + \\frac{1}{n} + \\dfrac{(x_h-\\bar{x})^2}{\\sum(x_i-\\bar{x})^2}\\right)} $$ PI: 평균 추정 불확실성 + 개별 관측값의 변동성 개별 관측값은 평균 주변에서 RMSE만큼 변동 RMSE의 정의 $$\\text{RMSE} = \\sqrt{\\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}i)^2}{n-2}}$$ RMSE의 의미 Residual의 표준편차 실제 관측값($y_i$)과 예측값($\\hat{y}i$) 사이의 차이 모델이 예측하지 못하는 부분 = 개별 관측값의 고유한 변동성 선형회귀에서는 오차항 $\\epsilon_i$가 평균 0, 분산 $\\sigma^2$인 정규분포를 따른다고 가정하기 때문에 위와 같은 내용이 성립함.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-calibrated-probability",
    "title": "보정된 확률(Calibrated Probability)이란?",
    "date": "2025-08-18",
    "excerpt": "머신러닝 모델의 확률 예측을 실제 관찰 빈도와 일치시키는 방법에 대한 정리",
    "category": "Data Science",
    "tags": [
      "calibration",
      "probability",
      "machine-learning",
      "logistic-regression",
      "platt-scaling"
    ],
    "content": "참고자료 1: Scikit-learn - Probability Calibration 2: nannyML 보정된 확률(Calibrated Probability)이란? --- 머신러닝 모델이 예측한 확률이 실제 관찰된 빈도와 일치하는 정도를 의미합니다. 많은 경우 모델의 출력은 확률처럼 보이지만, 실제로는 보정되지 않은(uncalibrated) 상태입니다. > 보정되지 않은 확률: 모델이 예측한 확률이 실제와 다를 수 있음 > 보정된 확률: 모델의 예측 확률이 실제 관찰 빈도와 일치 왜 보정이 필요한가? Logistic Regression의 경우 Logistic Regression은 0~1 사이의 값을 출력하지만, 이는 실제 확률과 다를 수 있습니다: `` 예측 확률 예측 횟수 실제 발생 횟수 실제 비율 80% 100 65 65% (overconfident) 30% 100 45 45% (underconfident) 90% 100 95 95% (well-calibrated) ` 보정되지 않은 확률의 원인 정규화(Regularization): L1/L2 정규화가 확률 추정을 왜곡 클래스 불균형: 소수 클래스에 대해 underconfident 모델 복잡성: 복잡한 모델이 훈련 데이터에 과적합 특성 스케일링: 특성의 스케일이 확률 추정에 영향 보정 방법 Platt Scaling 로지스틱 회귀를 사용한 후처리 방법입니다: $$P(y=1 x) = \\frac{1}{1 + \\exp(-a \\cdot f(x) + b)}$$ 여기서: $f(x)$: 원본 모델의 예측값 $a, b$: 보정 파라미터 (검증 데이터로 학습) Isotonic Regression 단조성 제약을 가진 비모수적 방법입니다: $$P(y=1 x) = m(f(x))$$ 여기서 $m$은 monotonic increasing function입니다. Temperature Scaling Softmax 출력을 조정하는 방법입니다: $$P(y=k x) = \\frac{\\exp(z_k/T)}{\\sum_{i=1}^{K} \\exp(z_i/T)}$$ 여기서 $T$는 temperature parameter입니다. 실제 모델별 보정 필요성 Tree-based Models (LightGBM, XGBoost) Tree-based 모델들은 특히 보정이 필요합니다: ` 예측 확률 예측 횟수 실제 발생 횟수 실제 비율 90% 100 70 70% (overconfident) 10% 100 30 30% (underconfident) ` 왜 이런 현상이 발생할까? Boosting 특성: 여러 weak learner를 순차적으로 결합하면서 확률 추정이 왜곡됨 과적합: 강력한 모델이 훈련 데이터에 과적합되어 과신(overconfident)하게 됨 Loss function: 분류 정확도에 최적화되어 있지 확률 추정에는 최적화되지 않음 해결 방법: `python from sklearn.calibration import CalibratedClassifierCV import lightgbm as lgb LightGBM 모델 lgb_model = lgb.LGBMClassifier() 보정된 모델 (isotonic이 tree-based 모델에 더 적합) calibrated_model = CalibratedClassifierCV( lgb_model, method='isotonic', # LightGBM에는 isotonic이 더 적합 cv=5 ) 보정된 확률 (이제 사용 가능) calibrated_probs = calibrated_model.predict_proba(X_test)[:, 1] ` 보정 평가 방법 Reliability Plot (Reliability Diagram) 예측 확률과 실제 관찰 빈도(observed frequency)를 비교하는 시각화: ` 예측 확률 구간 샘플 수 실제 양성 비율 [0.0, 0.1) 100 0.05 [0.1, 0.2) 150 0.12 ... [0.9, 1.0) 80 0.92 ` Calibration Error 보정 품질을 정량적으로 측정: $$\\text{Calibration Error} = \\sum_{i=1}^{B} \\frac{n_i}{N} p_i - \\hat{p}_i $$ 여기서: $B$: number of bins $n_i$: number of samples in i-th bin $N$: total number of samples $p_i$: predicted probability in i-th bin $\\hat{p}_i$: observed frequency in i-th bin 실제 구현 예시 Python 코드 `python from sklearn.calibration import CalibratedClassifierCV from sklearn.linear_model import LogisticRegression 원본 모델 base_model = LogisticRegression() 보정된 모델 (Platt Scaling 사용) calibrated_model = CalibratedClassifierCV( base_model, method='sigmoid', # Platt Scaling cv=5 ) 학습 calibrated_model.fit(X_train, y_train) 보정된 확률 예측 calibrated_probs = calibrated_model.predict_proba(X_test)[:, 1] ` sklearn.calibration 모듈 sklearn.calibration은 실제로 존재하는 공식 모듈입니다: `python 주요 import from sklearn.calibration import CalibratedClassifierCV from sklearn.calibration import calibration_curve calibration_curve 사용 예시 fraction_of_positives, mean_predicted_value = calibration_curve( y_true, y_pred, n_bins=10 ) ` CalibratedClassifierCV의 주요 파라미터: method='sigmoid': Platt Scaling (로지스틱 회귀 기반) method='isotonic': Isotonic Regression (비모수적 방법) cv: Cross-validation fold 수 보정의 중요성 의사결정 지원 확률 기반 의사결정에서 신뢰성(reliability) 확보 리스크 평가의 정확성 향상 비즈니스 응용 의료진단: 질병 발생 확률의 정확한 추정 금융리스크: 대출 상환 확률의 신뢰성 추천시스템: 사용자 선호도 확률의 정확성 모델 평가 단순 정확도 외에 확률적 성능(probabilistic performance) 측정 모델의 불확실성(uncertainty)에 대한 이해 주의사항 데이터 누수(Data Leakage): 보정 시 검증 데이터를 별도로 사용 도메인 변화(Domain Shift): 새로운 도메인에서는 재보정 필요 계산 비용(Computational Cost): 보정 과정에서 추가적인 계산 오버헤드 결론 보정된 확률은 머신러닝 모델의 출력을 실제 확률과 일치시키는 중요한 과정입니다. 특히 의사결정이 중요한 응용 분야에서는 모델의 확률 예측이 신뢰할 수 있어야 하므로, 적절한 보정 방법을 적용하는 것이 필수적입니다. 중요한 점: Logistic Regression: 기본적으로 보정이 필요 Tree-based Models (LightGBM, XGBoost): 특히 보정이 필수적 sklearn.calibration: 공식적으로 제공되는 보정 도구 method 선택: sigmoid: 선형 모델에 적합 isotonic: tree-based 모델에 더 적합 따라서 predict_proba를 직접 확률로 사용하면 안 되고, 반드시 보정 과정을 거쳐야 합니다. --- Bad Machine Learning models can still be well-calibrated (2번 글 정리) > Machine learning models are often evaluated based on their performance, measured by how close some metric is to zero or one (depending on the metric), > but this is not the only factor that determines their usefulness. explore the difference between good calibration and good performance and when one might be preferred over the other. Probability calibration? > Probability calibration, in its strong definition, is the degree to which the probabilities predicted by a classification model match the true frequencies of the target classes in a dataset > A model with strong calibration guarantees that its predictions satisfy the frequentist definition of probability (as opposed to the Bayesian one), which states that an event’s probability is the limit of its relative frequency in many trials. 대부분의 ML 알고리즘은 ill-calibrated하다. 확률로 쓰기는 어려움. > Most machine learning models are ill-calibrated, and the reasons depend on the learning algorithm Tree-based Ensembles의 문제점 Random Forest: 여러 개별 트리의 예측을 평균내어 최종 예측 생성 확률 추정의 한계: 0과 1에 가까운 확률을 얻기 어려움 이유: 개별 트리들 간에 항상 분산(variance)이 존재 확률 추정의 왜곡 현상 0 근처: 실제보다 높게 추정 (overestimate) 1 근처: 실제보다 낮게 추정 (underestimate) 결과: 극단적인 확률(0% 또는 100%)을 신뢰할 수 없음 모델 최적화의 문제 Binary metrics: 정확도(accuracy) 등은 단순히 맞다/틀리다만 판단 확실성 무시: 모델이 얼마나 확신하는지는 고려하지 않음 Gini-impurity: 의사결정트리가 분할을 결정할 때 사용하는 지표 목표: 가능한 한 빠르고 정확하게 분류하는 것 왜 이런 현상이 발생할까? Ensemble 특성: 여러 트리의 예측을 평균내면서 극단값이 완화됨 분산 존재: 각 트리가 서로 다른 패턴을 학습 최적화 목표: 분류 정확도에 집중, 확률 추정은 부차적 > The consequence of this is that while the scores produced by most machine learning models preserve order (the higher the number, the more likely the positive class), they cannot be interpreted as frequentist probabilities. Do you even need calibration? > When training a classification model, you need to ask yourself a crucial question: do you actually need the model to be well-calibrated? > The answer will depend on how the model will be used. Let’s take a look at some examples. 문제에 따라 calibration이 필요한지 정해진다. ranking 같은 문제에서는 필요 없을 것이고, 은행에서 대출을 갚을 확률 같은 경우에는 엄밀한 calibration이 필요할 것이다. A story of a poor, well-calibrated model > But guess what, getting accurate predictions is not what we need! Just like in the credit line assignment and performance estimation examples, here too, the game is all about getting the probabilities right. 모델의 성능이 안좋을때도 유용할 수 있다. > The model’s test accuracy was 63%. This is certainly better than a dummy model, which always predicts the home team to win; such a model would score 46% as the hosts tend to win almost half of the games. That said, 63% does not seem to be a great result. When it’s impossible to get a good performance > Now consider an attempt to predict dice rolls. Our model should produce a probability of the die facing up to six after it has been rolled. 주사위를 굴려서 6이 나올 확률을 예측하는 모델의 성능은 높일 수가 없다. 어떤 값에 의해 결정되는게 아닌 랜덤이니까. > Consider these two competing approaches. Model A is a dummy binary classifier that always predicts with full confidence that the rolled number is not a six; that is, it predicts a six 0% of the time and a not-six 100% of the time. Model B also never predicts a six, but the probabilities it outputs are different: it always predicts a six with the probability of ⅙ and a not-six with the probability of ⅚. > In the long run, both models feature the same accuracy: they are correct 5 out of 6 times. And this is as good as any model can get. However, an important fact differentiates between the two models: model B is perfectly calibrated while Model A is not calibrated at all. 되게 좋은 예시네. 예측 성능만으로는 평가할 수 없는 이유. > As far as calibration is concerned, the two models couldn’t be more different. How about the usefulness of the two models? > Model A doesn’t really deliver any value. > Model B, on the other hand, allows us to accurately predict the target frequency in the long run. > It also allows us to run simulations to answer more complex questions such as: what is the probability of rolling four not-six and seven sixes in 11 rolls? Once again, despite the poor predictive performance, good calibration yields the model useful! Calibration probability 성능 확인하는 법 🔎 Calibration 성능 평가 관점 2가지 Calibration quality (확률 보정 품질) 여기서 calibration curve가 가장 직관적인 도구예요. 하지만 curve는 시각적 확인이므로, 정량적인 지표도 함께 씁니다: ECE (Expected Calibration Error) 예측확률 bin별 편차를 가중평균한 값. 작을수록 확률이 잘 보정됨. MCE (Maximum Calibration Error) bin별 최대 편차. Brier Score 확률 예측과 실제 라벨(0/1)의 평균제곱오차. 분류 성능 + 보정 품질 모두 반영. 따라서 “curve + ECE/Brier 같은 수치”를 함께 봐야 함. --- Discrimination ability (분류력, 순위 유지 성능) 모델이 보정되었더라도, 분류력(예: AUC, Accuracy, F1)이 나빠지면 의미가 없죠. calibration은 보통 \\\\단조 변환(sigmoid, isotonic)\\\\이라서 순위 기반 metric(AUC)은 거의 유지되지만, 임계값 기반 정확도·정밀도 등은 달라질 수 있어요. 따라서 calibration 후에도 AUC나 Accuracy를 확인해야 합니다. --- Calibration curve만으로는 부족합니다. 추천 평가 방법: Calibration curve → 직관적 모양 확인 ECE / Brier score → 정량적 calibration 품질 AUC / Accuracy / F1 → 분류력 유지 여부 --- > 확률 보정의 성능은 “확률의 신뢰도”와 “분류력” 두 축을 모두 확인해야 하고, 따라서 curve 하나로만 판단하면 위험 Method 자세하게 정리 Platt Scaling (method='sigmoid') Platt Scaling은 분류기의 raw score를 보정된 확률로 변환하는 가장 널리 사용되는 calibration 방법 중 하나입니다. 📌 핵심 개념 Platt Scaling은 분류기의 \"점수(score)\" $s$를 확률로 변환하는 sigmoid 형태의 보정 함수입니다: $$ \\hat{p}(y=1\\mid s) = \\sigma(As+B) = \\frac{1}{1+\\exp(-(As+B))} $$ 여기서 $A$, $B$는 검증 데이터에서 로그 손실(log loss)을 최소화하도록 학습됩니다. 🤔 왜 필요한가? 많은 머신러닝 모델들이 겪는 공통적인 문제: SVM, Random Forest 등은 확률이 아닌 \"결정 점수\"를 출력 이 점수들은 순위는 정확하지만(높은 ROC-AUC) 확률로는 부정확 과신(overconfident) 또는 과소신(underconfident) 문제 발생 Platt Scaling은 이를 S자 곡선 하나로 해결하여 순위는 보존하면서 확률 품질을 개선합니다. 🔍 수학적 배경 본질적으로 특징이 1개(점수 $s$)인 로지스틱 회귀입니다: 데이터: 검증 셋 $\\{(s_i, y_i)\\}_{i=1}^n$ where $y_i \\in \\{0,1\\}$ 모델: $\\text{logit}\\, \\hat{p}_i = As_i + B$ 목적함수: 로그 손실 최소화 $$ \\mathcal{L}(A,B) = -\\sum_{i=1}^n [y_i\\log \\hat{p}_i + (1-y_i)\\log(1-\\hat{p}_i)] $$ 파라미터 해석 $B$: 전체 기저율(base rate) 보정 → 좌우 이동 $A$: 기울기(온도) 조절 $ A $ 작음 → 완만한 S곡선 → 과신 교정 $ A $ 큼 → 가파른 S곡선 → 과소신 교정 💡 실전 사용법 1) 데이터 분할 (⚠️ 누수 방지 필수!) 권장 방법: K-fold Cross Validation `python from sklearn.calibration import CalibratedClassifierCV 자동으로 데이터 누수 방지 calibrated = CalibratedClassifierCV(base_model, method='sigmoid', cv=5) calibrated.fit(X, y) ` 대안: 독립 검증셋 `python X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2) base_model.fit(X_train, y_train) calibrated = CalibratedClassifierCV(base_model, method='sigmoid', cv='prefit') calibrated.fit(X_valid, y_valid) ` 2) 추론 과정 새 샘플 $x$ → 베이스 모델 점수 $s = f(x)$ → 보정된 확률 $\\hat{p} = \\sigma(As+B)$ 📊 개선 효과 Platt Scaling 적용 후 개선되는 지표들: 로그 손실(Log Loss) ✅ 거의 항상 개선 브라이어 점수(Brier Score) ✅ 확률 예측의 MSE 감소 신뢰도 곡선 & ECE ✅ 빈도-확률 일치 개선 ⚖️ 장단점 비교 ✅ 장점 파라미터 2개만 → 적은 데이터로도 안정적 단조성 보장 → 순위/ROC 유지 구현 간단 → 빠른 학습/추론 ❌ 한계 S자 형태 제약 → 복잡한 왜곡 패턴 한계 극단적 불균형 → 충분한 검증 샘플 필요 🔄 다른 방법과 비교 방법 특징 적합한 상황 ------ ------ ------------- Platt Scaling 모수적, S자 곡선 일반적인 경우, 적은 데이터 Isotonic Regression 비모수적, 단조 많은 데이터, 복잡한 패턴 Temperature Scaling 스케일링만 멀티클래스, 과신만 교정 💻 실제 코드 예시 `python from sklearn.svm import LinearSVC from sklearn.calibration import CalibratedClassifierCV from sklearn.metrics import log_loss, brier_score_loss 기본 모델 (확률 출력 없음) base = LinearSVC() base.fit(X_train, y_train) Platt Scaling 적용 calibrated = CalibratedClassifierCV(base, method='sigmoid', cv='prefit') calibrated.fit(X_valid, y_valid) 성능 비교 raw_scores = base.decision_function(X_test) p_uncalibrated = 1 / (1 + np.exp(-raw_scores)) # 단순 sigmoid p_calibrated = calibrated.predict_proba(X_test)[:, 1] # Platt scaling print(f\"Log Loss (보정 전): {log_loss(y_test, p_uncalibrated):.4f}\") print(f\"Log Loss (보정 후): {log_loss(y_test, p_calibrated):.4f}\") ` ✅ 실무 체크리스트 [ ] 검증 데이터로 $A$, $B$ 학습 (데이터 누수 방지) [ ] 보정 전후 성능 비교 (Log Loss, Brier Score, 신뢰도 곡선) [ ] 클래스 불균형 심한 경우 → 타깃 스무딩 고려 [ ] 멀티클래스 → one-vs-rest 또는 다른 방법 검토 [ ] 데이터 분포 변화 → 주기적 재보정 > 핵심: Platt Scaling은 \"점수의 로그-오즈가 선형\"이라는 가정 하에 검증 데이터로 최적의 변환을 학습하는 가성비 높은 확률 보정 방법입니다. Isotonic Regression (method='isotonic') Isotonic Regression은 \"단조성\"만을 가정하고 데이터가 스스로 최적의 변환 형태를 찾도록 하는 비모수적 확률 보정 방법입니다. 📌 핵심 개념 Isotonic Regression은 점수 $s$와 확률 사이의 관계를 단조 증가 제약만으로 학습하는 유연한 보정법입니다: $$ \\min_{g:\\, \\text{nondecreasing}} \\sum_{i=1}^n (g(s_i) - y_i)^2 $$ 여기서 $g$는 계단형 단조 함수이며, 각 계단의 높이는 해당 구간 샘플들의 실제 양성률 평균입니다. 🔄 Platt vs Isotonic 비교 특성 Platt Scaling Isotonic Regression ------ --------------- ------------------- 가정 S자 곡선 형태 고정 단조성만 가정 유연성 제한적 매우 높음 데이터 요구량 적음 (안정적) 많음 (과적합 위험) 국소 보정 전역적 국소적 우수 결과 형태 부드러운 곡선 계단형 함수 🛠️ 작동 원리 (PAVA 알고리즘) Pool Adjacent Violators Algorithm을 통해 효율적으로 해결: 정렬: 점수를 오름차순으로 정렬 위반 감지: 왼쪽 블록 평균 > 오른쪽 블록 평균인 경우 찾기 병합: 위반하는 인접 블록들을 합쳐서 새로운 평균 계산 반복: 모든 위반이 해결될 때까지 반복 🔍 병합 과정 예시 Before 병합 (단조성 위반 상황): ` 점수 구간: [0.1-0.2] [0.2-0.3] [0.3-0.4] 각 구간 평균: 0.2 0.4 0.3 ← 위반! 0.4 > 0.3 ` After 병합 (단조성 복구): ` 점수 구간: [0.1-0.2] [0.2-0.4] ← 병합됨 각 구간 평균: 0.2 0.35 ← 새로운 가중평균 ` 결과: 이제 0.2~0.4 범위의 모든 점수들은 동일하게 0.35의 확률을 받게 됩니다. `python 예시: 병합된 구간 내의 점수들 scores = [0.25, 0.28, 0.32, 0.38] # 모두 병합된 구간 안 predicted_probs = [0.35, 0.35, 0.35, 0.35] # 모두 같은 값! ` > 💡 직관: \"높은 점수일수록 높은 확률\"이라는 순서는 지키되, 구체적인 관계는 데이터가 결정하게 함. 병합된 구간에서는 실제 관측된 양성률 평균으로 통일됩니다. 🎯 왜 \"더 확률적\"인가? Isotonic Regression이 Platt보다 \"더 확률적\"인 핵심 이유는 실제 관측 빈도를 그대로 확률로 사용한다는 점입니다: Platt의 접근 (형태 우선): `python 미리 정해진 S자 형태에 데이터를 맞춤 p = 1 / (1 + exp(-(A*score + B))) \"모든 점수-확률 관계는 S자여야 해!\" ` Isotonic의 접근 (관측 우선): `python 실제 데이터에서 관찰된 패턴을 그대로 사용 if score in range_1: p = observed_positive_rate_in_range_1 # 진짜 관측 확률! elif score in range_2: p = observed_positive_rate_in_range_2 # 진짜 관측 확률! ` 실제 예시: `python 어떤 점수 구간에서 scores_in_range = [0.3, 0.32, 0.35, 0.38] actual_labels = [0, 1, 1, 1] 실제 양성률 = 3/4 = 0.75 predicted_probability = 0.75 # ← 이것이 바로 경험적 확률! ` 의료 진단 예시로 보면: Platt: \"모든 점수-확률 관계는 S자여야 해!\" → 실제 급격한 변화도 S자로 스무딩 Isotonic: \"점수 0.7~0.8대에서 실제로 90%가 양성? 그럼 확률 0.9!\" → 실제 관측을 있는 그대로 반영 > 핵심: Isotonic은 \"실제 세상의 확률 분포\"를 더 정확하게 반영하며, 충분한 데이터가 있다면 경험적으로 가장 정확한 확률 추정을 제공합니다. 🎯 언제 사용하나? ✅ Isotonic이 유리한 경우 충분한 교정 데이터 (수백~수천 샘플) Tree-based 모델 (Random Forest, XGBoost, LightGBM) 복잡한 비선형 왜곡 패턴 국소적 미스캘리브레이션 문제 ❌ Platt이 더 나은 경우 적은 교정 데이터 (안정성 중요) 전역적 과신/과소신 패턴 부드러운 확률 곡선 필요 💻 실제 구현 기본 사용법 `python from sklearn.calibration import CalibratedClassifierCV from sklearn.ensemble import RandomForestClassifier 자동 교차검증으로 데이터 누수 방지 base_model = RandomForestClassifier(n_estimators=300) calibrated = CalibratedClassifierCV(base_model, method='isotonic', cv=5) calibrated.fit(X, y) 보정된 확률 예측 probabilities = calibrated.predict_proba(X_test)[:, 1] ` 직접 제어 방법 `python from sklearn.isotonic import IsotonicRegression 검증 데이터로 보정기 학습 iso_regressor = IsotonicRegression( y_min=0.0, y_max=1.0, increasing=True, out_of_bounds='clip' ) iso_regressor.fit(scores_valid, y_valid) 테스트 데이터에 적용 scores_test = base_model.decision_function(X_test) calibrated_probs = iso_regressor.predict(scores_test) ` 📊 기대 효과 Isotonic Regression 적용 후 개선되는 지표들: 브라이어 점수 ✅ 직접 최적화 대상 → 거의 항상 개선 로그 손실 ✅ 보정이 잘 되면 개선 ECE & 신뢰도 곡선 ✅ 빈도-확률 일치도 향상 ROC-AUC ⚡ 단조성으로 순위 보존 → 유지 ⚖️ 장단점 요약 ✅ 장점 최소 가정: 단조성만 요구하는 유연한 접근 국소 보정: 특정 점수 구간의 세밀한 교정 가능 표현력: 복잡한 비선형 패턴도 학습 가능 ❌ 단점 데이터 의존성: 충분한 샘플 없으면 과적합 위험 계단형 결과: 연속성이 부족한 구간별 상수 함수 외삽 한계: 학습 범위 밖에서는 경계값으로 고정 🔍 실무 예시 Tree 모델의 전형적인 문제: 과신 문제 `python Before: Random Forest 원시 확률 rf_probs = rf.predict_proba(X_test)[:, 1] print(f\"Log Loss (보정 전): {log_loss(y_test, rf_probs):.4f}\") After: Isotonic 보정 cal_rf = CalibratedClassifierCV(rf, method='isotonic', cv=5) cal_rf.fit(X_train, y_train) calibrated_probs = cal_rf.predict_proba(X_test)[:, 1] print(f\"Log Loss (보정 후): {log_loss(y_test, calibrated_probs):.4f}\") `` ✅ 실무 체크리스트 [ ] 데이터 분할: 교차검증 또는 독립 검증셋으로 누수 방지 [ ] 샘플 수 확인: 클래스별로 충분한 교정 샘플 (수백개 이상) [ ] 성능 비교: 보정 전후 Brier Score, Log Loss, ECE 측정 [ ] 신뢰도 곡선: 시각적으로 교정 품질 확인 [ ] 클리핑 설정: 0/1 극값 방지로 수치 안정성 확보 [ ] 정기 재보정: 데이터 분포 변화 시 재학습 고려 📈 히스토그램 빈닝과의 차이 방법 구간 결정 단조성 적응성 ------ ----------- -------- -------- 히스토그램 빈닝 미리 고정 보장 안됨 낮음 Isotonic Regression 데이터 기반 자동 보장됨 높음 > 핵심: Isotonic Regression은 \"단조성\"이라는 최소한의 제약만으로 데이터가 스스로 최적의 확률 변환을 찾게 하는 강력하면서도 유연한 보정 방법입니다. 충분한 데이터만 있다면 Platt Scaling보다 훨씬 정교한 교정이 가능합니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/비교및비유/08-17-기운차리는데는꿀잠이최고",
    "title": "기운 차리는 데는 꿀잠이 최고",
    "date": "2025-08-17",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "비교 및 비유할 때 쓰는 표현들"
    ],
    "content": "배운 표현들 There is nothing as motivating as a deadline. There is nothing as motivating as a good salary. There is nothing as demotivating as a rejection letter. There is noting as encouring as a supportive family. There is noting as refresing as a goodnight's sleep. He is not only my husband but also my bestfriend. He is not only intelligence but also atheletic. 간단한 작문 Today I learned expressions using \"nothing as ~ as \" and \"not only ~ but only\". These are used to explain something. When I express what it is amazing, I can say like that there is nothing as motivating as mindfulness. It is so helpful to me, it makes me more calm and focused. It is not only helpful for healthy mind but also helpful for succesful life. I can make more success when I can focused. Mindfulness make me more focused to kill monkey-mind. And if my mind be more peaceful, I can be more focused. In my opinion, focusness (몰입) is component of happy. We can be happy when we feel focusness. So mindfulness make us more happy. --- 피드백 및 수정된 버전 피드백 전반적으로 mindfulness와 focus에 대한 좋은 생각들을 표현했습니다! 문법과 어휘 선택 부분에서 몇 가지 개선점이 있습니다: 주요 개선 포인트: 동사 시제 일치: \"makes\", \"is\", \"helps\" 등 현재형 동사의 일관성 관사 사용: \"a\" 또는 \"the\" 적절한 사용 어휘 선택: 더 자연스러운 영어 표현들 문장 구조: 좀 더 매끄러운 연결 수정된 버전 Today I learned expressions using \"nothing as ~ as\" and \"not only ~ but also\". These expressions are used to emphasize and compare things. When I want to express something amazing, I can say that there is nothing as motivating as mindfulness. It is so helpful to me - it makes me calmer and more focused. It is not only helpful for a healthy mind but also helpful for a successful life. I can achieve more success when I am focused. Mindfulness helps me focus better and quiet my monkey-mind. When my mind becomes more peaceful, I can concentrate better. In my opinion, focus (몰입) is a component of happiness. We can be happy when we experience deep focus. So mindfulness makes us happier.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-17-toss-platform",
    "title": "Toss Platform 팀 관련 정리",
    "date": "2025-08-17",
    "excerpt": "Toss Platform에 대한 인터뷰, 채용공고 등을 정리",
    "category": "Career",
    "tags": [
      "Toss"
    ],
    "content": "인터뷰 정리 인터뷰 링크 > ddd",
    "public": true
  },
  {
    "id": "Self_Development/Career/25-08-17-how-to-write-resume",
    "title": "이력서를 업데이트하면서 알게 된 이력서 적는 법들",
    "date": "2025-08-17",
    "excerpt": "이력서는 어떻게 적는게 좋을지에 대한 생각들",
    "category": "Career",
    "tags": [
      "이력서"
    ],
    "content": "이력서에는 문제를 어떻게 정의했고, 이를 어떻게 해결해서, 어떤 결과를 얻었는지에 대한 논리적인 사고 흐름이 보여야 한다. 토스 채용 설명회에서도 비슷한 이야기를 했다. 어떤 경험을 했는지보다 해당 경험들에서 논리적인 사고가 보이고 깊이있는 고민을 했었는지가 더 중요하다고. 최근에 받은 이력서 피드백에서도 그런 이야기가 나왔다. 단순한 성과를 나열하는게 아닌, 해당 성과가 어떤 맥락에서 진행되었고 어떤 가치를 지니는지를 풍부하게 설명하는게 중요하다.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/deadlock",
    "title": "Deadlock에 대한 정리",
    "date": "2025-08-17",
    "excerpt": "Deadlock은 뭐고 왜 발생하는가? OCR 프로젝트에서 발생한 문제도 같이 정리",
    "category": "Data Engineering",
    "tags": [
      "Deadlock"
    ],
    "content": "Deadlock이란? > Deadlock(교착상태)은 두 개 이상의 프로세스나 트랜잭션이 서로가 가진 리소스를 기다리면서 무한정 대기하는 상태를 말합니다. <figure> <img src=\"/post/DataEngineering/Deadlock.png\" alt=\"Deadlock\" width=\"80%\" /> <figcaption>Deadlock 예시</figcaption> </figure> P1과 P2가 리소스 A, B 둘 다를 얻어야 한다고 가정할 때, t1에 P1이 리소스를 A를 얻고 P2가 리소스 B를 얻었다면 t2때 P1은 리소스 B를, P2는 리소스 A를 기다리게 됩니다. 하지만 서로 원하는 리소스가 상대방에게 할당되어 있기 때문에 이 두 프로세스는 무한정 기다리게 되는데 이러한 상태을 DeadLock상태라고 합니다. 출처: https://jwprogramming.tistory.com/12 [개발자를 꿈꾸는 프로그래머:티스토리] --- Microsoft Datalocks guide 링크 내용이 자세하게 설명되어있는게 많네. 시간날 때 읽어보자. A deadlock occurs when two or more tasks permanently block each other by each task having a lock on a resource that the other tasks are trying to lock. For example: Transaction A acquires a shared lock on row 1. Transaction B acquires a shared lock on row 2. Transaction A now requests an exclusive lock on row 2, and is blocked until transaction B finishes and releases the shared lock it has on row 2. Transaction B now requests an exclusive lock on row 1, and is blocked until transaction A finishes and releases the shared lock it has on row 1. The SQL Server Database Engine deadlock monitor periodically checks for tasks that are in a deadlock. If the monitor detects a cyclic dependency, it chooses one of the tasks as a victim and terminates its transaction with an error. OCR 프로젝트에서 발생한 에러 메세지가 이 부분에 해당하는구나. This allows the other task to complete its transaction. The application with the transaction that terminated with an error can retry the transaction, which usually completes after the other deadlocked transaction finishes.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/비교및비유/08-15-걔완전허세야",
    "title": "걔 완전 허세야",
    "date": "2025-08-15",
    "excerpt": "걔 완전 허세야",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "비교 및 비유할 때 쓰는 표현들"
    ],
    "content": "He is such a poser 공부한 것들 정리 so that such It's so sunny that you don't need to wear a jacket outside. He is so stubborn that he won't listen to me. ㅋㅋ I'm so hungry that I could eat a horse. She is such a drama quen. drama quen : 늘 오하면서 관심받고 싷ㅍ어하는 사람 night owl : 저녁형 이간 morning person : 아침형 인간 cat person : 공야이 좋아하는 사람 oportunitist : 기회주의 a know-it-all : 아는척 하는 사람 hopeless romantic : 답없는 낭만주의자 be obssessed with 간단한 작문하기 Today I learned representations(표현) using so ~ that and such. It is used to represent the situation or person. If someone ask me about me, I wll answer that I'm such a dog person. Or do you want to eat lunch, I will answer that I'm so hungry that I could eat a horse. So let's summary, when metapor somthing, I can use \"so ~ that\" and \"such\". It is very useful. --- 피드백 (Feedback) Your writing shows a good understanding of the \"so ~ that\" and \"such\" structures! Here are some suggestions for improvement: Grammar corrections: \"representations(표현)\" → \"expressions\" (더 자연스러운 표현) \"If someone ask me\" → \"If someone asks me\" (3인칭 단수) \"I wll answer\" → \"I will answer\" (오타) \"Or do you want to eat lunch\" → \"Or if you ask do you want to eat lunch\" (문맥상 더 자연스러움) \"let's summary\" → \"let's summarize\" (동사형) \"metapor somthing\" → \"metaphor something\" (철자) Structure improvements: 문장들을 더 자연스럽게 연결할 수 있어요 예시를 더 다양하게 들 수 있어요 더 자세한 버전 (Detailed Version) Understanding \"So ~ That\" and \"Such\" Expressions Today I learned about powerful expressions using \"so ~ that\" and \"such\" that help us emphasize situations or describe people more vividly. How these expressions work: \"So ~ that\" structure: Used with adjectives/adverbs Pattern: so + adjective/adverb + that + result/consequence Example: \"I'm so hungry that I could eat a horse.\" \"Such\" structure: Used with nouns (often with adjectives) Pattern: such + (adjective) + noun Example: \"He's such a poser\" or \"She's such a drama queen.\" Personal examples: When someone asks about my personality, I can confidently say \"I'm such a dog person\" because I absolutely love dogs and prefer their company to cats. This expression immediately conveys my strong preference. If someone invites me to lunch when I'm starving, I might respond, \"Yes! I'm so hungry that I could eat a horse!\" This dramatically emphasizes just how hungry I am. Why these expressions are useful: These metaphorical expressions add color and emphasis to our speech. Instead of simply saying \"I'm hungry,\" saying \"I'm so hungry that I could eat a horse\" creates a vivid image and shows the intensity of the feeling. Similarly, calling someone \"such a drama queen\" is much more expressive than just saying \"she's dramatic.\" Additional examples for practice: \"The movie was so boring that I fell asleep.\" \"He's such a night owl that he never goes to bed before 2 AM.\" \"The weather is so nice that we should go for a walk.\" \"She's such a know-it-all that nobody wants to work with her.\" These expressions help make our English more natural and emotionally expressive, allowing us to communicate not just facts but also our feelings and attitudes.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/what-is-index",
    "title": "Index에 대한 정리",
    "date": "2025-08-14",
    "excerpt": "Index가 뭐고, 어떤 원리로 동작하는지 자세히 정리",
    "category": "Data Engineering",
    "tags": [
      "DB",
      "Index"
    ],
    "content": "참고자료 1: 블로그 - Database 인덱스(index)란? 위에서는 인덱스에 대해 간단히 핵심만 다루고 있지만 PK(기본키, Primary Key)와 인덱스(Index)에 대해서 자세히 알고 있는 것은 굉장히 중요합니다. PK(기본키, Primary Key)와 인덱스(Index)에 대한 자세한 내용은 아래 포스팅을 참고해주세요! 굉장히 중요한 부분인만큼 한번 쯤은 읽어두시는 것을 추천드립니다! What is Index? > 인덱스란 추가적인 쓰기 작업과 저장 공간을 활용하여 데이터베이스 테이블의 검색 속도를 향상시키기 위한 자료구조이다. (참고 1) <figure> <img src=\"/post/DataEngineering/index.png\" alt=\"What is Index\" /> <figcaption>그림1. Index 설명</figcaption> </figure> 인덱스가 필요한 이유 테이블 스캔 vs 인덱스 스캔 테이블 스캔 (Table Scan) `` 테이블 스캔의 문제점: 모든 행을 순차적으로 검사 데이터가 많을수록 검색 시간이 선형적으로 증가 예: 100만 건 테이블에서 특정 값을 찾을 때 → 최악의 경우 100만 건을 모두 검사해야 함 → 평균적으로 50만 건 검사 필요 ` 인덱스 스캔 (Index Scan) ` 인덱스 스캔의 장점: 정렬된 인덱스 구조를 통해 빠른 검색 B-tree 구조로 O(log n) 시간 복잡도 예: 100만 건 테이블에서 특정 값을 찾을 때 → B-tree 높이만큼만 검사 (보통 3-4회) → 100만 건 → 20회 이하의 검사로 단축 ` 인덱스의 장단점 장점 검색 성능 향상: 특정 조건의 데이터를 빠르게 찾을 수 있음 정렬 성능 향상: ORDER BY 절의 성능 개선 조인 성능 향상: 테이블 간 연결 시 인덱스 활용 고유성 보장: UNIQUE 인덱스로 데이터 무결성 보장 단점 저장 공간 증가: 인덱스 데이터를 위한 추가 저장 공간 필요 쓰기 성능 저하: INSERT, UPDATE, DELETE 시 인덱스도 함께 수정 유지보수 오버헤드: 데이터 변경 시 인덱스 재구성 필요 복잡성 증가: 인덱스 설계와 관리의 복잡성 인덱스 사용 시나리오 인덱스가 효과적인 경우 ` WHERE 절에서 자주 사용되는 컬럼 예: user_id, email, created_date JOIN 조건으로 사용되는 컬럼 예: 외래키, 자주 조인되는 컬럼 ORDER BY, GROUP BY에서 사용되는 컬럼 예: 정렬이나 그룹화 기준이 되는 컬럼 고유한 값을 가진 컬럼 예: 주민등록번호, 이메일 주소 범위 검색이 자주 발생하는 컬럼 예: 날짜, 나이, 가격 범위 ` 인덱스가 비효과적인 경우 ` 데이터가 적은 테이블 (수백 건 이하) 테이블 스캔이 더 빠를 수 있음 NULL 값이 많은 컬럼 인덱스 효율성이 떨어짐 자주 변경되는 컬럼 인덱스 유지 비용이 높음 데이터 분포가 균등하지 않은 컬럼 예: 성별 (남/여만 있는 경우) 텍스트가 긴 컬럼 인덱스 크기가 커지고 효율성 저하 ` 인덱스의 기본 원리 책의 목차와 같은 개념 ` 책에서 특정 내용을 찾는 방법: 목차(인덱스)를 보고 해당 페이지 번호 확인 해당 페이지로 바로 이동 페이지 내에서 세부 내용 검색 테이블에서 특정 데이터를 찾는 방법: 인덱스를 보고 해당 데이터의 위치 확인 해당 위치로 바로 이동 정확한 데이터 추출 ` 인덱스의 핵심 아이디어 정렬된 구조: 데이터를 미리 정렬하여 빠른 검색 가능 포인터 활용: 실제 데이터 위치를 가리키는 포인터 저장 계층적 구조: B-tree 등을 통한 효율적인 검색 경로 제공 선택적 접근: 필요한 데이터만 정확하게 접근 Index data structure Hash Table > 해시 테이블은 (Key, Value)로 데이터를 저장하는 자료구조 중 하나로 빠른 데이터 검색 (시간복잡도 O(1)) 이 필요할 때 유용하다 <figure> <img src=\"/post/DataEngineering/hash_table.png\" alt=\"What is Hash Table\" /> <figcaption>그림2. Hash Table 설명</figcaption> </figure> > 하지만 DB 인덱스에서 해시 테이블이 사용되는 경우는 제한적인데, 그러한 이유는 해시가 등호(=) 연산에만 특화되었기 때문이다. > 해시 함수는 값이 1이라도 달라지면 완전히 다른 해시 값을 생성하는데, 이러한 특성에 의해 부등호 연산(>, <)이 자주 사용되는 데이터베이스 검색을 위해서는 해시 테이블이 적합하지 않다. Hash Table의 제한적 사용 이유 해시 함수의 특성 ` 해시 함수의 결정적 특성: 입력값이 1이라도 달라지면 완전히 다른 해시값 생성 예시: \"user123\" → 해시 함수 → 8472 \"user124\" → 해시 함수 → 1956 \"user125\" → 해시 함수 → 3921 이 특성의 문제점: 연속된 값들이 전혀 다른 해시값을 가짐 순서나 크기 관계가 해시값에 반영되지 않음 범위 검색이나 정렬이 불가능 ` 데이터베이스 검색 패턴과의 불일치 ` 일반적인 데이터베이스 검색: 정확한 값 검색: WHERE user_id = 'user123' ✓ (Hash Table 적합) 범위 검색: WHERE age BETWEEN 20 AND 30 ✗ (Hash Table 부적합) 정렬: ORDER BY created_date ✗ (Hash Table 부적합) 부분 검색: WHERE name LIKE 'John%' ✗ (Hash Table 부적합) 최대/최소값: WHERE price > 1000 ✗ (Hash Table 부적합) Hash Table이 지원하는 검색: 등호(=) 연산만 지원 정확한 키 값으로의 직접 접근 순서나 관계 정보 없음 ` 실제 사용 사례의 제한 ` Hash Table이 효과적인 경우: 사용자 로그인: WHERE email = 'user@example.com' 세션 조회: WHERE session_id = 'abc123' 캐시 조회: WHERE cache_key = 'user_profile_123' Hash Table이 비효과적인 경우: 나이대별 사용자: WHERE age BETWEEN 20 AND 30 가격대별 상품: WHERE price > 1000 AND price < 5000 날짜별 주문: WHERE order_date >= '2024-01-01' 이름순 정렬: ORDER BY name ASC ` B-tree vs Hash Table의 검색 능력 비교 등호 검색 (=) ` Hash Table: 성능: O(1) - 해시값 계산 후 직접 접근 예시: WHERE user_id = 'user123' 결과: 매우 빠름 B-tree: 성능: O(log n) - 트리 탐색 필요 예시: WHERE user_id = 'user123' 결과: 빠름 (하지만 Hash Table보다는 느림) ` 범위 검색 (>, <, BETWEEN) ` Hash Table: 성능: 지원하지 않음 예시: WHERE age BETWEEN 20 AND 30 결과: 불가능 B-tree: 성능: O(log n + k) - k는 결과 개수 예시: WHERE age BETWEEN 20 AND 30 결과: 효율적 (순서대로 접근 가능) ` 정렬 (ORDER BY) ` Hash Table: 성능: 지원하지 않음 예시: ORDER BY created_date DESC 결과: 불가능 B-tree: 성능: O(n log n) - 정렬된 순서로 접근 예시: ORDER BY created_date DESC 결과: 효율적 (이미 정렬된 구조) ` Hash Table의 적절한 활용 방안 하이브리드 접근 ` 최적의 인덱스 전략: 자주 사용되는 정확한 값 검색: Hash Index 예: 사용자 ID, 이메일, 세션 ID 범위 검색이나 정렬이 필요한 경우: B-tree Index 예: 날짜, 나이, 가격, 이름 복합 인덱스: B-tree 활용 예: (category, price, created_date) ` 메모리 기반 시스템에서의 활용 ` Redis, Memcached 등에서 Hash Table 활용: 빠른 키-값 조회 세션 정보 저장 캐시 데이터 관리 실시간 데이터 처리 장점: 메모리 접근으로 극도로 빠른 속도 단순한 구조로 높은 처리량 확장 가능한 분산 시스템 ` 결론 Hash Table은 데이터베이스 인덱스에서 제한적으로 사용되는 이유가 명확합니다: 등호 연산에만 특화: 정확한 값 검색만 가능 순서 정보 부재: 범위 검색이나 정렬 불가 해시 함수의 특성: 연속된 값들이 전혀 다른 해시값 생성 실용성 부족: 대부분의 데이터베이스 검색 패턴과 맞지 않음 따라서 Hash Table은 특정 용도(빠른 키-값 조회, 메모리 기반 시스템)에만 사용하고, 일반적인 데이터베이스 인덱스로는 B-tree 구조가 훨씬 더 적합합니다. Hash Table의 작동 원리 해시 함수 (Hash Function) ` 해시 함수의 역할: 입력된 키를 고정된 크기의 해시값으로 변환 예: \"user123\" → 해시 함수 → 8472 동일한 키는 항상 동일한 해시값 생성 서로 다른 키가 같은 해시값을 가질 수 있음 (해시 충돌) 해시 함수의 특징: 결정적 (Deterministic): 같은 입력에 대해 항상 같은 출력 균등 분포 (Uniform Distribution): 해시값이 균등하게 분포 빠른 계산: O(1) 시간에 해시값 계산 ` 해시 충돌 (Hash Collision) ` 해시 충돌이란: 서로 다른 키가 같은 해시값을 가지는 경우 예: \"user123\" → 8472, \"admin456\" → 8472 해결 방법: 체이닝 (Chaining): 같은 해시값에 연결 리스트 사용 개방 주소법 (Open Addressing): 다른 빈 슬롯 찾기 이중 해싱 (Double Hashing): 두 번째 해시 함수 사용 ` Hash Table의 성능 특성 시간 복잡도 ` 검색 (Search): 평균: O(1) - 해시값 계산 후 직접 접근 최악: O(n) - 해시 충돌이 많을 경우 삽입 (Insert): 평균: O(1) - 해시값 계산 후 직접 저장 최악: O(n) - 해시 충돌 해결 과정 삭제 (Delete): 평균: O(1) - 해시값 계산 후 직접 제거 최악: O(n) - 해시 충돌 해결 과정 ` 공간 복잡도 ` 저장 공간: 기본: O(n) - 데이터 개수만큼 공간 필요 해시 충돌 해결: 추가 공간 필요 체이닝: 연결 리스트 노드 공간 개방 주소법: 빈 슬롯을 위한 여유 공간 ` Hash Table의 장단점 장점 빠른 검색: 평균 O(1) 시간에 데이터 접근 빠른 삽입/삭제: 해시값 계산 후 직접 조작 메모리 효율성: 데이터 크기에 비례하는 공간 사용 단순한 구조: 구현이 비교적 간단 단점 해시 충돌: 성능 저하의 주요 원인 순차 접근 비효율: 범위 검색이나 정렬에 부적합 메모리 사용량: 해시 충돌 해결을 위한 추가 공간 해시 함수 의존성: 좋은 해시 함수 선택이 중요 Hash Table의 실제 구현 체이닝 방식 ` 구조: Bucket[0]: [Key1, Value1] → [Key2, Value2] → null Bucket[1]: [Key3, Value3] → null Bucket[2]: null ... 장점: 구현이 간단 해시 충돌에 강함 동적 확장 가능 단점: 추가 메모리 사용 캐시 지역성 떨어짐 ` 개방 주소법 ` 구조: Bucket[0]: [Key1, Value1] Bucket[1]: [Key2, Value2] (충돌로 인한 이동) Bucket[2]: [Key3, Value3] (충돌로 인한 이동) 장점: 메모리 효율성 캐시 지역성 좋음 추가 메모리 사용 최소 단점: 구현이 복잡 삭제 시 처리 복잡 클러스터링 문제 ` Hash Table의 인덱스 활용 적합한 사용 사례 ` 정확한 값 검색 (Point Query): 예: SELECT FROM users WHERE user_id = 'user123' 해시값으로 직접 접근하여 O(1) 성능 고유 키 검색: 예: 사용자 ID, 이메일 주소 중복이 없는 고유한 값 메모리 기반 인덱스: Redis, Memcached 등 빠른 응답 시간이 중요한 경우 ` 부적합한 사용 사례 ` 범위 검색 (Range Query): 예: SELECT FROM users WHERE age BETWEEN 20 AND 30 해시값은 순서가 없어 범위 검색 불가 정렬이 필요한 경우: 예: ORDER BY created_date 해시값은 정렬 순서와 무관 부분 문자열 검색: 예: LIKE '%keyword%' 해시값은 패턴 매칭에 부적합 ` Hash Table vs B-tree 비교 성능 비교 ` 검색 성능: Hash Table: 평균 O(1), 최악 O(n) B-tree: 항상 O(log n) 범위 검색: Hash Table: 지원하지 않음 B-tree: O(log n + k) (k는 결과 개수) 정렬: Hash Table: 지원하지 않음 B-tree: 순서대로 접근 가능 ` 메모리 사용량 ` 공간 효율성: Hash Table: O(n) + 충돌 해결 공간 B-tree: O(n) + 노드 오버헤드 확장성: Hash Table: 재해싱 필요 B-tree: 점진적 확장 가능 ` B+Tree <figure> <img src=\"/post/DataEngineering/InnoDB-B-Tree-Structure.png\" alt=\"InnoDB의 B+Tree 구조\" /> <figcaption>그림3. InnoDB의 B+Tree 구조</figcaption> </figure> B+Tree의 개념과 특징 B+Tree란? B+Tree는 B-Tree를 개선하여 데이터베이스 인덱스에 최적화된 자료구조입니다. 1970년대에 B-Tree의 단점을 보완하여 개발되었습니다. B+Tree의 핵심 구조 데이터 저장 방식 ` B+Tree의 노드 구조: Internal 노드: 키(Key)와 포인터만 저장 [Key1, Pointer1, Key2, Pointer2, ...] 실제 데이터는 저장하지 않음 (검색 경로 제공) Leaf 노드: 키(Key)와 데이터(Value) 저장 [Key1, Value1, Key2, Value2, ...] 실제 데이터가 저장되는 곳 모든 Leaf 노드는 논리적 순서대로 연결되어 범위/순차 접근에 유리 ` 메모리/페이지 관점의 구조적 이점 ` Internal 노드가 작아 한 페이지에 더 많은 키를 적재 → 트리 높이 감소 Leaf 노드는 데이터 밀도가 높아 I/O 효율 증가 페이지 단위(prefetch)로 읽을 때 순차 접근 이점 큼 ` B+Tree의 검색 과정 정확한 값 검색 (Point Query) ` 1) Root → Internal 노드들을 따라 하향 탐색 2) 목적 키 범위의 포인터를 선택하며 Leaf 도달 3) Leaf에서 키를 찾아 데이터 반환 시간 복잡도: O(log n) ` 범위 검색 (Range Query) ` 1) 시작 키를 찾기 위해 트리 탐색 (O(log n)) 2) 해당 Leaf에서 시작하여 다음 Leaf로 순차 접근 3) 끝 조건까지 결과 수집 (O(k)) 총 시간 복잡도: O(log n + k) ` B+Tree의 장단점 장점 ` 범위/순차 접근 최적화 (Leaf 간 논리적 연결) Internal 노드가 작아 트리 높이가 낮음 → I/O 감소 예측 가능한 검색 성능: O(log n) ` 단점 ` 항상 Leaf까지 내려가야 데이터 접근 가능 노드 분할/병합 시 연결 유지 관리 비용 존재 ` B+Tree의 실제 활용 ` MySQL InnoDB: 클러스터형 인덱스(Primary Key)와 보조 인덱스(Secondary) 모두 B+Tree 기반 PostgreSQL/Oracle: 기본 범용 인덱스로 B+Tree 채택, 복합 인덱스/범위 쿼리에 최적 ` B+Tree의 성능 특성 요약 ` 정확한 값 검색: O(log n) 범위 검색: O(log n + k) 순차 접근: 첫 원소 O(log n) + 후속 O(1) (페이지 순차 접근) 정렬: 인덱스 순서 접근으로 추가 정렬 최소화 `` 결론 B+Tree는 Internal/Leaf 분리와 Leaf 간 논리적 연결로 범위/순차 접근에 매우 강하며, 낮은 트리 높이를 통해 일관된 O(log n) 성능을 제공하기 때문에 현대 데이터베이스의 표준 인덱스 구조로 널리 쓰입니다.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/what-is-B-Tree",
    "title": "B-Tree에 대한 정리",
    "date": "2025-08-14",
    "excerpt": "B-Tree가 뭐고, 어떤 원리로 동작하는지 자세히 정리",
    "category": "Data Engineering",
    "tags": [
      "DB",
      "SQL",
      "B-Tree"
    ],
    "content": "참고자료 1 : MySQL - B-Tree로 인덱스(Index)에 대해 쉽고 완벽하게 이해하기 인덱스를 저장하는 방식 > 인덱스를 저장하는 방식(또는 알고리즘)에 따라 B-Tree 인덱스, Hash 인덱스, Fractal 인덱스 등으로 나눌 수 있습니다. > 일반적으로 B-Tree 구조가 사용되기 때문에 B-Tree 인덱스를 통해 인덱스의 동작 방식에 대해 살펴보도록 하겠습니다. (참고 1) B-Tree 구조 <figure> <img src=\"/post/DataEngineering/B-Tree-Structure.png\" alt=\"B-Tree Structure\" /> <figcaption>그림1. B-Tree Structure</figcaption> </figure> B-Tree구조는 이진트리를 확장하여 N개의 자식을 가질 수 있도록 고안된 것. Page or Block Page or Block : 디스크와 메모리(버퍼풀)에 데이터를 읽고 쓰는 최소 작업 단위 > 인덱스의 저장 방식을 이해하기 위해서는 페이지(또는 블럭)에 대해 알아야 한다. > 페이지란 디스크와 메모리(버퍼풀)에 데이터를 읽고 쓰는 최소 작업 단위이다. 일반적인 인덱스를 포함해 PK(클러스터 인덱스)와 테이블 등은 모두 페이지 단위로 관리된다. > 따라서 만약 쿼리를 통해 1개의 레코드를 읽고 싶더라도 결국은 하나의 블록을 읽어야 하는 것이다 (참고 1) 기본 정의 페이지/블록: 디스크와 메모리 간 데이터 전송의 최소 단위 크기: 일반적으로 4KB, 8KB, 16KB 등으로 표준화 목적: I/O 효율성과 메모리 관리를 위한 단위화 왜 페이지 단위로 관리하는가? 디스크 I/O 효율성 디스크 특성: 디스크는 순차 접근에 최적화되어 있음 Seek Time: 디스크 헤드가 특정 위치로 이동하는 시간 Transfer Time: 실제 데이터를 읽고 쓰는 시간 페이지 단위: 한 번의 디스크 접근으로 여러 레코드 처리 메모리 관리 효율성 버퍼 풀: 자주 사용되는 페이지를 메모리에 유지 LRU 알고리즘: 최근 사용된 페이지를 메모리에 유지 페이지 교체: 메모리 부족 시 사용 빈도가 낮은 페이지 제거 버퍼풀(Buffer Pool)과 메모리 캐싱 시스템 정의와 목적 버퍼풀: 데이터베이스가 자주 사용하는 페이지를 메모리에 유지하는 캐시 영역 크기: 일반적으로 전체 메모리의 70-80% 할당 목적: 디스크 I/O 최소화로 성능 향상 페이지 로딩 과정 쿼리 실행 요청 버퍼풀에서 페이지 검색 페이지가 없으면 (Cache Miss): 디스크에서 페이지 로드 버퍼풀에 저장 LRU 리스트에 추가 페이지가 있으면 (Cache Hit): 메모리에서 직접 데이터 접근 LRU 리스트에서 최신 위치로 이동 페이지 교체 정책 - LRU (Least Recently Used) 알고리즘: 가장 오래 사용되지 않은 페이지를 먼저 제거 새로 로드된 페이지는 리스트 맨 앞에 배치 자주 사용되는 페이지는 리스트 맨 뒤에 유지 ``text LRU 리스트 구조: [MRU] ←→ [Page1] ←→ [Page2] ←→ [Page3] ←→ [LRU] ↑ ↑ 최근 사용 오래된 사용 ` 메모리 접근 vs 디스크 접근 `text 성능 비교: 메모리 접근: 100-1000ns (나노초) 디스크 접근: 10,000-100,000ns (마이크로초) 성능 차이: 메모리가 디스크보다 100-1000배 빠름 `` B-Tree 인덱스의 구조 > 인덱스는 페이지 단위로 저장되며, 인덱스 키를 바탕으로 항상 정렬된 상태를 유지한다. > 정렬된 인덱스 키를 따라서 리프 노드에 도달하면 (인덱스 키, PK) 쌍으로 저장되어 있다. 예를 들어 다음과 같이 설계된 테이블이 있다고 하자.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/sql-vs-nosql",
    "title": "SQL vs NoSQL",
    "date": "2025-08-14",
    "excerpt": "SQL과 NoSQL 장단점 비교, 대화기록 저장할 때 더 적절한 DB는?",
    "category": "Data Engineering",
    "tags": [
      "DB",
      "SQL",
      "NoSQL"
    ],
    "content": "SQL보다 NoSQL이 빠르다? 단순한 답변: \"상황에 따라 다르다\" SQL과 NoSQL의 성능 비교는 단순히 \"어느 것이 빠르다\"로 답할 수 없는 복잡한 문제입니다. 각각의 장단점과 적합한 사용 사례가 다르기 때문입니다. 성능 비교의 핵심 요소 읽기 성능 (Read Performance) SQL 데이터베이스 인덱스 최적화: B-tree 인덱스로 복잡한 쿼리도 빠른 처리 쿼리 최적화기: 실행 계획을 최적화하여 효율적인 데이터 접근 조인 성능: 정규화된 데이터로 효율적인 조인 연산 NoSQL 데이터베이스 단순한 쿼리: Key-Value 조회는 매우 빠름 인메모리 처리: Redis 같은 인메모리 DB는 극도로 빠름 수평적 분산: 여러 노드에 부하를 분산하여 처리량 향상 쓰기 성능 (Write Performance) SQL 데이터베이스 ACID 트랜잭션: 데이터 무결성을 위한 오버헤드 인덱스 업데이트: 여러 인덱스 동시 업데이트 필요 로깅: 트랜잭션 로그 기록으로 인한 지연 NoSQL 데이터베이스 최소한의 검증: 스키마 검증 부담이 적음 비동기 처리: 일부 NoSQL은 비동기적으로 데이터 저장 인덱스 부담: 복잡한 인덱스 구조가 없어 쓰기 속도 향상 실제 성능 비교 예시 단순 조회 (Key-Value) `` SQL: SELECT value FROM table WHERE key = 'user123' NoSQL: db.get('user123') 결과: NoSQL이 2-5배 빠름 (인덱스 검색 오버헤드 없음) ` 복잡한 조인 쿼리 ` SQL: SELECT u.name, p.title FROM users u JOIN posts p ON u.id = p.user_id WHERE u.age > 25 AND p.created_at > '2024-01-01' NoSQL: 여러 번의 개별 쿼리 필요 결과: SQL이 10-100배 빠름 (최적화된 조인 알고리즘) ` 대량 데이터 쓰기 ` SQL: 100만 건 INSERT (트랜잭션 + 인덱스 업데이트) NoSQL: 100만 건 PUT (최소한의 검증) 결과: NoSQL이 3-10배 빠름 (오버헤드 최소화) ` 성능이 결정되는 요인들 데이터 크기 소규모 (GB 단위): SQL과 NoSQL 성능 차이 미미 중간 규모 (TB 단위): NoSQL의 수평적 확장성 우위 대규모 (PB 단위): NoSQL이 압도적으로 우수 쿼리 복잡도 단순 조회: NoSQL 우위 복잡한 분석: SQL 우위 실시간 집계: 상황에 따라 다름 데이터 구조 정규화된 데이터: SQL 우위 비정규화된 데이터: NoSQL 우위 계층적 구조: NoSQL 우위 결론 \"SQL이 빠르다\" vs \"NoSQL이 빠르다\"는 잘못된 질문입니다. 올바른 접근 방법: 사용 사례 분석: 어떤 종류의 작업을 주로 수행하는가? 데이터 특성 파악: 구조화된 데이터인가, 유연한 구조가 필요한가? 확장성 요구사항: 향후 데이터 증가가 예상되는가? 일관성 요구수준: 데이터 정확성이 얼마나 중요한가? 성능은 데이터베이스 선택의 한 요소일 뿐이며, 전체적인 시스템 요구사항을 종합적으로 고려해야 합니다. 속도 차이의 이론적 배경 데이터 구조와 접근 패턴 SQL: B-tree 기반 인덱스 구조 ` B-tree 구조: [Root] / \\ [Left] [Right] / \\ / \\ [Leaf] [Leaf] [Leaf] [Leaf] 특징: 균형 잡힌 트리 구조로 O(log n) 검색 시간 순차 접근과 랜덤 접근 모두 효율적 인덱스 유지 비용: 삽입/삭제 시 재균형화 필요 ` NoSQL: Hash Table 기반 구조 ` Hash Table 구조: Key: \"user123\" → Hash Function → Index: 8472 Bucket[8472] = {user123: \"데이터\"} 특징: O(1) 평균 검색 시간 (해시 충돌 없는 경우) 랜덤 접근에 최적화 순차 접근은 비효율적 인덱스 유지 비용: 거의 없음 ` 메모리 계층 구조와 I/O 패턴 SQL의 I/O 패턴 ` 쿼리 실행 과정: SQL 파싱 → 2. 실행 계획 수립 → 3. 인덱스 탐색 → 4. 데이터 페이지 로드 메모리 계층: CPU Cache (L1/L2/L3) → Buffer Pool → Disk Storage I/O 최적화: Buffer Pool: 자주 사용되는 페이지를 메모리에 유지 Prefetching: 연속된 페이지를 미리 로드 Write-Behind: 쓰기 작업을 배치로 처리 ` NoSQL의 I/O 패턴 ` 데이터 접근 과정: Key 해시 계산 → 2. 메모리/디스크 위치 확인 → 3. 직접 데이터 로드 메모리 계층: CPU Cache → In-Memory Storage → Disk Storage (선택적) I/O 최적화: In-Memory First: 가능한 한 메모리에서 처리 Minimal Validation: 최소한의 데이터 검증 Async Writes: 비동기 쓰기로 응답 시간 단축 ` 트랜잭션과 ACID 속성의 오버헤드 SQL: ACID 트랜잭션 보장 ` 트랜잭션 처리 과정: BEGIN TRANSACTION → Lock 획득 (Row/Table Level) → 데이터 변경 → Undo Log 기록 → Redo Log 기록 → Lock 해제 COMMIT 오버헤드 요소: Lock Management: 동시성 제어를 위한 락 오버헤드 Logging: ACID 보장을 위한 로그 기록 Index Updates: 모든 관련 인덱스 동시 업데이트 Validation: 제약 조건 검증 ` NoSQL: Eventual Consistency ` 데이터 처리 과정: PUT operation → 데이터 검증 (최소한) → 메모리에 저장 → 백그라운드에서 디스크에 저장 → 복제본에 전파 (선택적) 오버헤드 최소화: No Locking: 락 기반 동시성 제어 없음 Minimal Logging: 트랜잭션 로그 없음 Lazy Indexing: 인덱스 업데이트 지연 Async Replication: 비동기 복제 ` 쿼리 최적화와 실행 계획 SQL: Cost-Based Query Optimizer ` 쿼리 최적화 과정: SQL 파싱 → 2. 논리적 계획 생성 → 3. 물리적 계획 생성 → 4. 비용 계산 → 5. 최적 계획 선택 최적화 요소: Join Order: 테이블 조인 순서 최적화 Index Selection: 적절한 인덱스 선택 Access Path: 테이블 스캔 vs 인덱스 스캔 Sort Elimination: 불필요한 정렬 제거 비용 계산: CPU Cost: 연산 비용 I/O Cost: 디스크 접근 비용 Memory Cost: 메모리 사용 비용 ` NoSQL: Direct Access Pattern ` 데이터 접근 방식: Key 기반 직접 접근 Secondary Index 사용 (제한적) Map-Reduce 또는 Aggregation Pipeline 최적화 부족: No Query Planning: 실행 계획 수립 없음 Limited Indexing: 제한적인 인덱스 지원 Manual Optimization: 개발자가 직접 최적화 필요 ` 수평적 확장성과 분산 처리 SQL: 수직적 확장 (Vertical Scaling) ` 확장 방식: 더 강력한 CPU 추가 더 큰 메모리 추가 더 빠른 디스크 추가 한계점: 단일 서버의 물리적 한계 비용의 기하급수적 증가 장애 시 전체 시스템 중단 ` NoSQL: 수평적 확장 (Horizontal Scaling) ` 확장 방식: 더 많은 서버 노드 추가 데이터를 여러 노드에 분산 부하를 여러 노드에 분산 장점: 선형적 성능 향상 장애 격리 비용 효율성 ` 네트워크 지연과 분산 시스템 이론 CAP 이론과 성능 트레이드오프 ` CAP 이론: Consistency (일관성) Availability (가용성) Partition Tolerance (분할 허용성) SQL 선택: CP (일관성 + 분할 허용성) 네트워크 분할 시 일관성 유지 일부 노드 사용 불가로 가용성 희생 NoSQL 선택: AP (가용성 + 분할 허용성) 네트워크 분할 시에도 서비스 지속 일관성은 Eventually Consistent로 달성 ` 네트워크 지연의 영향 ` 분산 시스템에서의 지연: Network Latency: 노드 간 통신 지연 Serialization: 데이터 직렬화/역직렬화 Consensus Protocol: 분산 합의 프로토콜 SQL의 분산 트랜잭션: 2-Phase Commit: 모든 노드의 동시 커밋 필요 Global Locking: 전체 시스템에 걸친 락 Synchronous Replication: 동기적 복제 NoSQL의 분산 처리: Local Operations: 로컬에서 먼저 처리 Async Replication: 비동기적 복제 Eventual Consistency: 최종적 일관성 ` 메모리 관리와 가비지 컬렉션 SQL: Buffer Pool 관리 ` 메모리 관리: LRU (Least Recently Used) 알고리즘 Dirty Page Management: 변경된 페이지 추적 Checkpoint: 주기적인 디스크 동기화 오버헤드: Page Replacement: 메모리 부족 시 페이지 교체 Dirty Page Flushing: 변경된 페이지 디스크 저장 Buffer Pool Tuning: 메모리 크기 최적화 ` NoSQL: In-Memory 최적화 ` 메모리 관리: Direct Memory Access: 직접 메모리 접근 Memory-Mapped Files: 파일을 메모리에 직접 매핑 Garbage Collection: 자동 메모리 정리 최적화: Zero-Copy: 불필요한 메모리 복사 제거 Memory Pooling: 메모리 풀을 통한 할당 최적화 Cache-Aware Data Structures: 캐시 친화적 데이터 구조 ` 이러한 이론적 배경을 통해 SQL과 NoSQL의 성능 차이가 발생하는 근본적인 이유를 이해할 수 있습니다. 각각의 설계 철학과 아키텍처가 성능 특성에 직접적인 영향을 미치고 있습니다. ---- SQL vs NoSQL: 데이터베이스 선택의 기준 개요 데이터베이스 선택은 시스템 설계에서 가장 중요한 결정 중 하나입니다. SQL과 NoSQL은 각각 다른 특성과 장단점을 가지고 있어, 사용 사례에 따라 적절한 선택이 필요합니다. SQL 데이터베이스 특징 ACID 트랜잭션: Atomicity, Consistency, Isolation, Durability 보장 정규화된 스키마: 데이터 무결성과 일관성 유지 관계형 모델: 테이블 간 관계를 통한 복잡한 쿼리 지원 장점 데이터 일관성과 무결성 보장 복잡한 조인과 집계 쿼리 지원 성숙한 기술과 풍부한 생태계 단점 수평적 확장성 제한 스키마 변경의 어려움 대용량 데이터 처리 시 성능 저하 NoSQL 데이터베이스 특징 유연한 스키마: 동적 데이터 구조 지원 수평적 확장성: 분산 환경에서 높은 성능 다양한 데이터 모델: Document, Key-Value, Column-family, Graph 장점 높은 확장성과 성능 스키마 변경의 유연성 대용량 데이터 처리에 적합 단점 ACID 트랜잭션 보장의 제한 복잡한 쿼리의 어려움 데이터 일관성 보장의 어려움 대규모 데이터 처리에서의 장점 수평적 확장성 (Horizontal Scalability) 샤딩(Sharding): 데이터를 여러 서버에 분산 저장 자동 파티셔닝: 데이터 증가에 따라 자동으로 노드 추가 선형적 성능 향상: 노드 추가 시 성능이 선형적으로 증가 분산 아키텍처 Master-Slave 구조: 읽기 작업을 여러 노드에 분산 Peer-to-Peer 구조: 모든 노드가 동등한 역할 수행 지역적 분산: 지리적으로 분산된 데이터센터 지원 메모리 기반 처리 인메모리 캐싱: Redis, Memcached 등의 빠른 메모리 접근 Lazy Loading: 필요한 데이터만 메모리에 로드 Write-Behind: 쓰기 작업을 백그라운드에서 처리 데이터 모델 최적화 Column-Family DB: 컬럼 단위로 데이터 저장하여 압축 효율성 증대 Document DB: JSON 형태로 중첩 구조 데이터 효율적 저장 Key-Value DB: 단순한 구조로 빠른 읽기/쓰기 성능 CAP 이론 활용 일관성(Consistency) vs 가용성(Availability): 대용량 환경에서 가용성 우선 네트워크 분할(Partition): 네트워크 문제 시에도 서비스 지속 Eventually Consistent: 최종적으로 일관성 달성 DynamoDB: AWS의 관리형 NoSQL 서비스 특징 완전 관리형 서비스: 서버 관리, 패치, 백업 등 AWS에서 자동 처리 무제한 확장성: 자동으로 수백만 요청/초 처리 가능 글로벌 테이블: 여러 리전에 자동으로 데이터 복제 데이터 모델 Key-Value + Document: 기본적으로 Key-Value이지만 JSON 형태의 복잡한 데이터도 저장 가능 Primary Key: Partition Key (필수) + Sort Key (선택) Secondary Index: GSI(Global Secondary Index)와 LSI(Local Secondary Index) 지원 성능 특성 일관된 성능: 10ms 이하의 응답 시간 보장 자동 스케일링: 트래픽에 따라 자동으로 용량 조정 On-Demand vs Provisioned: 사용량 기반 또는 예약 용량 선택 가능 대화기록 저장에서의 장점 높은 쓰기 성능: 초당 수만 건의 쓰기 처리 가능 자동 백업: Point-in-time recovery 지원 TTL(Time To Live): 오래된 대화기록 자동 삭제 스트림 기능: 실시간 데이터 변경 감지 및 처리 사용 사례 실시간 채팅: 높은 쓰기 성능과 낮은 지연시간 게임 리더보드: 빠른 순위 조회와 업데이트 IoT 데이터: 대량의 센서 데이터 수집 및 저장 사용자 세션: 빠른 세션 정보 접근 및 업데이트 대화기록 저장 시 고려사항 대화기록의 특성 구조적 특성: 사용자 ID, 시간, 메시지 내용, 컨텍스트 데이터 크기: 텍스트 기반이지만 컨텍스트 정보 포함 접근 패턴: 사용자별 조회, 시간순 정렬, 검색 기능 SQL vs NoSQL 비교 SQL 데이터베이스 선택 시 장점: 사용자별 대화 기록의 일관성 보장 복잡한 분석 쿼리 지원 (사용자 행동 분석) 트랜잭션 기반의 데이터 무결성 단점: 대화 컨텍스트와 같은 유연한 구조 저장의 어려움 대용량 데이터 처리 시 성능 저하 NoSQL 데이터베이스 선택 시 장점: 대화 컨텍스트의 유연한 저장 높은 쓰기 성능과 확장성 JSON 형태의 자연스러운 데이터 저장 단점: 복잡한 분석 쿼리의 어려움 데이터 일관성 보장의 제한 권장사항 대화기록 저장에 적합한 선택 NoSQL (Document DB) 추천: 이유: 대화 데이터의 유연한 구조와 높은 쓰기 성능 추천 DB: MongoDB, CouchDB 적용 사례: 채팅 애플리케이션, 고객 지원 시스템 하이브리드 접근: 핵심 데이터: SQL DB (사용자 정보, 메타데이터) 대화 내용: NoSQL DB (유연한 구조, 높은 성능) 결론 SQL과 NoSQL은 각각의 장단점이 있으며, 대화기록 저장과 같은 특정 사용 사례에서는 NoSQL이 더 적합할 수 있습니다. 하지만 시스템의 전체적인 요구사항과 데이터 일관성 요구 수준을 고려하여 선택해야 합니다. ---- 부록: B-tree 구조의 상세 분석 B-tree의 수학적 기초 B-tree는 1970년대에 Bayer와 McCreight에 의해 개발된 균형 잡힌 트리 구조로, SQL 데이터베이스의 핵심 인덱스 구조입니다. B-tree의 정의와 속성 ` B-tree (Order = m)의 정의: 모든 리프 노드는 같은 레벨에 위치 루트 노드를 제외한 모든 노드는 최소 ⌈m/2⌉-1개, 최대 m-1개의 키를 가짐 루트 노드는 최소 1개, 최대 m-1개의 키를 가짐 모든 내부 노드는 최소 ⌈m/2⌉개, 최대 m개의 자식을 가짐 ` B-tree의 높이 계산 ` 수학적 공식: 최소 높이: h_min = log_m(n+1) - 1 최대 높이: h_max = log_(⌈m/2⌉)((n+1)/2) 실제 계산 예시: m = 100 (한 노드당 최대 100개 키) n = 1,000,000 (100만 개 키) 최소 높이: log_100(1,000,001) - 1 ≈ 2.5 최대 높이: log_50(500,000.5) ≈ 3.8 결론: 100만 개 키를 3-4 레벨의 B-tree로 저장 가능 ` B-tree의 노드 구조와 메모리 레이아웃 노드 타입별 구조 ` Internal Node (내부 노드): [Pointer1] [Key1] [Pointer2] [Key2] [Pointer3] ... [PointerN] Leaf Node (리프 노드): [Key1] [Data1] [Key2] [Data2] ... [KeyN] [DataN] [Next_Leaf_Pointer] 메모리 레이아웃: 각 노드는 디스크 페이지 크기에 맞춰 설계 일반적으로 4KB 또는 8KB 페이지 사용 포인터는 8바이트 (64비트 시스템) 키는 가변 길이 (VARCHAR 등) ` 페이지 최적화 ` 페이지 크기별 특성: 4KB 페이지: 메모리 효율성 우선, 작은 인덱스에 적합 8KB 페이지: 일반적인 용도, 균형 잡힌 성능 16KB 페이지: 순차 읽기 성능 우선, 큰 인덱스에 적합 Fill Factor (채움률): 높은 Fill Factor (90%): 공간 효율성 우선, 읽기 성능 향상 낮은 Fill Factor (70%): 삽입 성능 우선, 노드 분할 빈도 감소 ` B-tree의 핵심 알고리즘 검색 알고리즘 (Search) ` 검색 과정 상세 분석: 루트 노드에서 시작 현재 노드의 키들과 비교하여 적절한 포인터 선택 리프 노드에 도달할 때까지 2단계 반복 리프 노드에서 키 검색 시간 복잡도: O(log_m n) m이 클수록 트리 높이가 낮아짐 일반적으로 3-4번의 디스크 접근으로 원하는 데이터 찾음 실제 예시 (키 = 35 검색): Root: [50] → 35 < 50, 왼쪽 포인터 선택 Internal: [20, 40] → 20 < 35 < 40, 중간 포인터 선택 Leaf: [30, 35, 38] → 키 35 발견! ` 삽입 알고리즘 (Insert) ` 삽입 과정 단계별 분석: 적절한 리프 노드 찾기 (검색과 동일) 노드에 공간이 있는 경우: 직접 삽입 노드가 가득 찬 경우: 노드 분할 (Split) 노드 분할 과정: 원본 노드: [20, 25, 30, 35] 중간값 선택: 30 (⌈4/2⌉ = 2번째 위치) 왼쪽 노드: [20, 25] 오른쪽 노드: [35] 부모 노드로 중간값 30 이동 분할의 영향: 트리 높이가 증가할 수 있음 모든 리프 노드는 여전히 같은 레벨 유지 분할은 상향으로 전파됨 (부모 노드도 가득 찬 경우) ` 삭제 알고리즘 (Delete) ` 삭제 과정의 복잡성: 키 검색 및 제거 노드가 최소 채움률 미만인 경우 재균형화 재균형화 전략: a) 키 빌리기 (Borrowing): 형제 노드에서 키를 빌려와 최소 채움률 유지 부모 노드의 키도 조정 필요 b) 노드 병합 (Merging): 형제 노드와 병합하여 하나의 노드로 만듦 부모 노드의 키도 제거 필요 재균형화의 복잡성: 삭제는 삽입보다 복잡함 여러 노드에 걸친 조정이 필요할 수 있음 트리 높이 감소 가능 ` B-tree의 성능 특성과 최적화 읽기 성능 분석 ` 순차 읽기 (Range Query): 리프 노드 간 연결 포인터로 효율적 순회 예: SELECT FROM users WHERE age BETWEEN 20 AND 30 리프 노드만 순회하면 되므로 매우 빠름 I/O 효율성: 연속된 페이지 접근으로 디스크 최적화 랜덤 읽기 (Point Query): 트리 높이만큼의 디스크 접근 필요 일반적으로 3-4번의 I/O로 원하는 데이터 찾음 인덱스가 잘 설계된 경우 매우 효율적 캐시 친화적: 자주 접근하는 노드는 메모리에 유지 ` 쓰기 성능 분석 ` 삽입 성능: 리프 노드까지의 경로 탐색: O(log n) 노드 분할 발생 시: 추가 I/O 필요 평균적으로 O(log n) 시간 복잡도 분할 빈도: Fill Factor에 따라 결정 삭제 성능: 검색 + 재균형화: O(log n) 재균형화로 인한 추가 I/O 발생 삽입보다 약간 느림 재균형화 전략에 따라 성능 차이 ` 최적화 기법들 B+tree (B-plus tree) ` B+tree의 핵심 특징: 모든 데이터는 리프 노드에만 저장 Internal 노드는 키와 포인터만 포함 리프 노드 간 연결 포인터로 순차 접근 최적화 B+tree vs B-tree 비교: B-tree: 모든 노드에 데이터 저장 B+tree: 리프 노드에만 데이터 저장 B+tree의 장점: 순차 읽기 성능 향상 Internal 노드 크기 감소로 더 많은 키 저장 범위 쿼리에 최적화 리프 노드만 순회하면 모든 데이터 접근 가능 ` 압축 기법 (Compression) ` Prefix Compression (접두사 압축): 연속된 키의 공통 접두사 제거 예: \"user_001\", \"user_002\", \"user_003\" 압축 후: \"user_\" + \"001\", \"002\", \"003\" 공간 절약: 공통 접두사 길이만큼 절약 Suffix Truncation (접미사 절단): 키의 끝부분을 제거하여 공간 절약 예: \"timestamp_2024_01_01_00_00_00\" 절단 후: \"ts_20240101\" 주의: 고유성 보장 필요 압축의 효과: 노드당 더 많은 키 저장 가능 트리 높이 감소로 검색 성능 향상 메모리 사용량 감소 디스크 I/O 감소 ` 고급 최적화 기법 ` Adaptive Merging: 삭제된 키가 많은 노드들을 자동으로 병합 공간 효율성 향상 성능 저하 방지 Bulk Loading: 대량 데이터 삽입 시 최적화 노드 분할을 최소화 트리 구조를 한 번에 최적화 Parallel Processing: 여러 노드의 동시 처리 멀티코어 시스템 활용 대용량 인덱스 빌드 시간 단축 ` B-tree의 실제 구현과 모니터링 PostgreSQL의 B-tree 구현 `sql -- B-tree 인덱스 생성 CREATE INDEX idx_users_age ON users(age); -- 인덱스 사용 통계 확인 SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch FROM pg_stat_user_indexes WHERE indexname = 'idx_users_age'; -- 인덱스 크기 확인 SELECT pg_size_pretty(pg_relation_size('idx_users_age')); -- 인덱스 상세 정보 SELECT FROM pg_stat_user_indexes WHERE indexname = 'idx_users_age'; -- 인덱스 사용률 분석 SELECT schemaname, tablename, indexname, idx_scan, idx_tup_read, idx_tup_fetch, CASE WHEN idx_scan > 0 THEN ROUND((idx_tup_fetch::float / idx_scan::float) 100, 2) ELSE 0 END as selectivity_percent FROM pg_stat_user_indexes WHERE schemaname = 'public'; ` MySQL의 B-tree 구현 `sql -- B-tree 인덱스 생성 CREATE INDEX idx_users_age ON users(age); -- 인덱스 사용 현황 확인 SHOW INDEX FROM users; -- 인덱스 통계 정보 SELECT FROM information_schema.statistics WHERE table_name = 'users' AND index_name = 'idx_users_age'; -- 인덱스 크기 확인 SELECT table_name, index_name, ROUND(stat_value * @@innodb_page_size / 1024 / 1024, 2) as size_mb FROM mysql.innodb_index_stats WHERE stat_name = 'size' AND table_name = 'users'; -- 인덱스 사용률 분석 SELECT object_schema as database_name, object_name as table_name, index_name, count_read, count_write, count_fetch, count_insert, count_update, count_delete FROM performance_schema.table_io_waits_summary_by_index_usage WHERE object_schema = 'your_database'; ` B-tree의 한계와 현대적 대안 B-tree의 근본적 한계 ` 높은 삽입/삭제 비용: 노드 분할/병합으로 인한 오버헤드 재균형화 과정의 복잡성 랜덤 I/O 패턴 순차 삽입 시 비효율: 랜덤한 키 삽입에 최적화 순차 키 삽입 시 노드 분할 빈발 공간 지역성 부족 메모리 사용량: Internal 노드의 메모리 점유 포인터 오버헤드 캐시 미스 발생 캐시 지역성: 랜덤 접근으로 인한 캐시 미스 메모리 계층 구조와의 불일치 현대 CPU 아키텍처와의 궁합 ` 현대적 대안 구조들 LSM Tree (Log-Structured Merge Tree) ` LSM Tree의 핵심 개념: 쓰기 성능에 최적화 순차 쓰기로 디스크 I/O 최소화 읽기 시 여러 레벨 검색 필요 LSM Tree 구조: Level 0 (Memory): 최신 데이터, 정렬되지 않음 Level 1-N (Disk): 정렬된 데이터, 크기가 증가 장점: 매우 높은 쓰기 성능 순차 I/O 패턴으로 SSD에 최적화 압축 효율성 단점: 읽기 성능 저하 공간 증폭 (Space Amplification) 컴팩션 오버헤드 ` Fractal Tree ` Fractal Tree의 특징: B-tree의 변형으로 삽입 성능 향상 버퍼링을 통한 배치 처리 메모리와 디스크의 균형점 찾기 동작 원리: 각 노드에 버퍼 추가 작은 변경사항을 버퍼에 누적 버퍼가 가득 차면 배치로 처리 장점: B-tree보다 높은 삽입 성능 순차 삽입에 최적화 기존 B-tree와의 호환성 단점: 복잡한 구현 메모리 오버헤드 버퍼 관리의 복잡성 ` Bw-tree (Bw-tree) ` Bw-tree의 혁신적 특징: 락 없는 동시성 제어 메모리 매핑 기반 구조 로그 구조화된 접근 핵심 메커니즘: Delta Node: 변경사항을 별도 노드에 저장 Consolidation: 주기적인 노드 통합 Split/Delete: 논리적 분할과 삭제 장점: 매우 높은 동시성 락 경합 없음 메모리 효율성 단점: 복잡한 구현 읽기 성능 저하 가능 메모리 관리의 복잡성 ` B-tree의 미래와 발전 방향 현대적 요구사항과의 조화 ` 대용량 데이터 처리: 페타바이트 규모 데이터 지원 분산 환경에서의 효율성 클라우드 네이티브 아키텍처 실시간 처리: 마이크로초 단위 응답 시간 스트리밍 데이터 처리 이벤트 기반 아키텍처 하이브리드 워크로드: 읽기/쓰기 균형 분석과 트랜잭션의 통합 다양한 데이터 타입 지원 ` B-tree의 진화 방향 ` 하이브리드 구조: B-tree + LSM Tree 조합 상황에 따른 동적 전환 최적의 성능 보장 머신러닝 기반 최적화: 접근 패턴 학습 자동 인덱스 튜닝 예측적 최적화 하드웨어 최적화: NVMe SSD 최적화 Persistent Memory 활용 GPU 가속 처리 `` B-tree는 SQL 데이터베이스의 핵심 인덱스 구조로, 50년 이상 검증된 안정성과 성능을 제공합니다. 하지만 현대의 대용량 데이터와 고성능 요구사항에 따라 LSM Tree, Fractal Tree, Bw-tree 같은 대안 구조들이 함께 사용되고 있으며, 앞으로도 하이브리드 구조와 머신러닝 기반 최적화를 통해 계속 발전할 것으로 예상됩니다.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/aws-ec2-ebs",
    "title": "AWS EC2와 EBS 완벽 가이드",
    "date": "2025-08-14",
    "excerpt": "AWS EC2와 EBS의 개념, 사용법, 그리고 EBS 백업 기능에 대한 상세한 설명",
    "category": "Data Engineering",
    "tags": [
      "AWS",
      "EC2",
      "EBS",
      "Cloud Computing",
      "Infrastructure"
    ],
    "content": "AWS EC2와 EBS 완벽 가이드 EC2 (Elastic Compute Cloud)란? EC2는 AWS에서 제공하는 가상 서버 서비스입니다. 사용자가 필요에 따라 컴퓨팅 리소스를 유연하게 확장하거나 축소할 수 있는 클라우드 컴퓨팅 서비스입니다. EC2의 주요 특징 가상 서버: 물리적 서버를 가상화하여 제공 확장성: 필요에 따라 인스턴스 수를 늘리거나 줄일 수 있음 다양한 인스턴스 타입: CPU, 메모리, 스토리지 요구사항에 맞는 다양한 옵션 온디맨드 요금: 사용한 만큼만 비용 지불 EC2 사용 사례 웹 애플리케이션 호스팅 웹 서버, 애플리케이션 서버 운영 로드 밸런서와 함께 고가용성 확보 데이터 처리 대용량 데이터 분석 작업 배치 프로세싱 및 ETL 작업 개발 및 테스트 환경 개발, 스테이징, 테스트 환경 구축 CI/CD 파이프라인 구축 게임 서버 온라인 게임 서버 운영 트래픽에 따른 자동 스케일링 EBS (Elastic Block Store)란? EBS는 EC2 인스턴스에 연결할 수 있는 블록 레벨 스토리지 서비스입니다. 하드 디스크와 유사한 기능을 제공하며, EC2 인스턴스의 부팅 디스크나 추가 데이터 스토리지로 사용됩니다. EBS의 주요 특징 블록 레벨 스토리지: 파일 시스템이 아닌 블록 단위로 데이터 저장 영구 저장: 인스턴스가 종료되어도 데이터 유지 다양한 볼륨 타입: SSD, HDD 등 성능과 비용에 따른 선택 가능 스냅샷 기능: 데이터 백업 및 복구 지원 EBS 볼륨 타입 SSD 기반 볼륨 gp3: 범용 SSD, 균형잡힌 성능과 비용 io2: 고성능 SSD, 높은 IOPS 요구사항에 적합 HDD 기반 볼륨 st1: 처리량 최적화 HDD, 빅데이터 워크로드에 적합 sc1: 콜드 스토리지, 자주 접근하지 않는 데이터에 적합 EBS 사용 사례 부팅 디스크 EC2 인스턴스의 운영체제 및 애플리케이션 설치 시스템 파일 및 설정 저장 데이터베이스 스토리지 MySQL, PostgreSQL 등의 데이터베이스 파일 저장 높은 IOPS가 필요한 워크로드에 적합 파일 서버 사용자 업로드 파일 저장 공유 파일 및 문서 저장 백업 및 복구 중요한 데이터의 스냅샷 생성 재해 복구를 위한 데이터 보호 EC2와 EBS의 연동 연결 방식 부팅 볼륨: EC2 인스턴스 생성 시 자동으로 연결되는 루트 볼륨 추가 볼륨: 인스턴스에 추가로 연결할 수 있는 데이터 볼륨 볼륨 분리: 인스턴스에서 볼륨을 분리하여 다른 인스턴스에 연결 가능 성능 최적화 EBS 최적화: EC2 인스턴스와 EBS 간의 네트워크 성능 최적화 프로비저닝된 IOPS: 예측 가능한 성능을 위한 IOPS 예약 스루풋 최적화: 대용량 데이터 전송에 최적화된 처리량 EBS 백업 기능 스냅샷 (Snapshots) EBS의 핵심 백업 기능으로, 특정 시점의 볼륨 상태를 저장합니다. 스냅샷의 특징 증분 백업: 변경된 블록만 백업하여 저장 공간 절약 지역 간 복사: 다른 AWS 리전으로 백업 복사 가능 자동화: AWS Lambda와 CloudWatch를 통한 자동 스냅샷 생성 비용 효율성: 사용하지 않는 스냅샷은 삭제하여 비용 절약 스냅샷 생성 방법 수동 생성: AWS 콘솔, CLI, SDK를 통한 수동 스냅샷 생성 자동 생성: 정책 기반 자동 스냅샷 생성 및 관리 스케줄링: 정기적인 백업 스케줄 설정 AWS Backup AWS에서 제공하는 중앙화된 백업 서비스로, EBS 볼륨을 포함한 다양한 AWS 리소스를 백업할 수 있습니다. 주요 기능 통합 관리: 여러 AWS 서비스의 백업을 한 곳에서 관리 정책 기반 백업: 백업 정책을 정의하여 자동화된 백업 실행 크로스 리전 백업: 여러 리전에 백업 복사본 저장 규정 준수: 다양한 규정 및 표준에 맞는 백업 정책 적용 백업 전략 3-2-1 백업 규칙 3개의 백업 복사본 2개의 다른 저장 매체 1개의 오프사이트 백업 백업 주기 일일 백업: 중요한 데이터의 경우 주간 백업: 일반적인 데이터의 경우 월간 백업: 장기 보관이 필요한 데이터의 경우 백업 보존 기간 단기 보존: 30일 이내의 백업 중기 보존: 90일 이내의 백업 장기 보존: 1년 이상의 백업 실제 사용 예시 웹 애플리케이션 구축 `` EC2 인스턴스 (t3.medium) ├── EBS gp3 볼륨 (20GB) - 부팅 디스크 └── EBS io2 볼륨 (100GB) - 데이터베이스 ` 데이터 분석 환경 ` EC2 인스턴스 (r5.2xlarge) ├── EBS gp3 볼륨 (50GB) - 부팅 디스크 ├── EBS st1 볼륨 (500GB) - 원시 데이터 └── EBS gp3 볼륨 (200GB) - 처리된 데이터 `` 비용 최적화 팁 적절한 볼륨 타입 선택: 워크로드에 맞는 볼륨 타입으로 비용 절약 미사용 볼륨 정리: 사용하지 않는 EBS 볼륨 삭제 스냅샷 관리: 오래된 스냅샷 정리 및 보존 기간 최적화 예약 인스턴스: 장기 사용 시 예약 인스턴스로 비용 절약 결론 AWS EC2와 EBS는 클라우드 인프라의 핵심 서비스로, 함께 사용하여 안정적이고 확장 가능한 시스템을 구축할 수 있습니다. EBS의 강력한 백업 기능을 활용하면 데이터 손실 위험을 최소화하고, 비즈니스 연속성을 보장할 수 있습니다. 올바른 EC2 인스턴스 타입과 EBS 볼륨 타입을 선택하고, 체계적인 백업 전략을 수립하여 클라우드 환경을 효율적으로 운영하시기 바랍니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/비교및비유/08-13-어제에비해공기상태가별로야",
    "title": "어제에 비해 공기 상태가 별로야",
    "date": "2025-08-13",
    "excerpt": "비교할 때 쓰는 표현들 정리",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "Compared to yesterday, the air quality is not so good. 배운 것들 a dog person a cat person Are you a dog person or a cat person? morning person night owl Are you a morning person or a night owl? needy : 궁핍한, 관심이나 애정을 필요로 하는 social Compared to last week, the air quality is not so bad. outgoing : 외향적인 loyal : 충성심 있는 Toy Story sequels are as good as the original sequel : 후속작 발음팁! 강세신경쓰기 SE-quel o-RI-ginal Sequels are not as good as original. Simple writing Today I learned",
    "public": true
  },
  {
    "id": "Data Science/Statistics/conformal-prediction-explained",
    "title": "Conformal Prediction: 분포 가정 없이 유한 샘플 coverage를 보장하는 방법",
    "date": "2025-08-13",
    "excerpt": "통계적 가정 없이도 finite-sample coverage를 보장하는 conformal prediction의 원리와 실무 적용법",
    "category": "Data Science",
    "tags": [
      "statistics",
      "prediction-interval",
      "uncertainty",
      "calibration"
    ],
    "content": "참고자료 1 : 블로그 - Conformal Prediction으로 모델의 불확실성 계산하기 2 : Paper - Conformal Prediction: A Gentle Introduction 3 : 블로그 - Conformal Prediction --- <figure> <img src=\"/post/ML/CQR_그림.png\" alt=\"CQR 그림\" width=\"70%\" /> <figcaption>그림1. CQR </figcaption> </figure> conformal prediction은 calibration set을 통해 목표 coverage를 달성하는 방법입니다. 그림1처럼 quantile regressor의 결과에 conformal prediction을 붙여, 원하는 coverage의 prediction interval을 추정할 수 있습니다. --- Conformal Prediction이란? Conformal Prediction은 분포 가정 없이도 finite-sample coverage를 보장하는 prediction interval을 만드는 방법입니다. Paper - A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification conformal precition은 다음과 같은 특징을 가집니다. 분포 가정 불필요 : 데이터가 어떤 분포를 따르든 상관없음 Finite-sample coverage: 유한한 샘플에서도 목표 coverage 보장 모델 agnostic : 어떤 머신러닝 모델과도 사용 가능 Calibration 자동화: 별도 조정 없이도 coverage 보장 --- 기본 원리 Conformal prediction의 핵심 가정은 Exchangeability입니다: > 예측하고자 하는 새로운 데이터 $(x_{n+1}, y_{n+1})$와 기존 데이터 $(x_1, y_1), ..., (x_n, y_n)$이 exchangeable하다. Exchangeability: 데이터의 순서를 바꿔도 확률 분포가 동일 이는 train, valid, test을 나눴을 때 3개 모두 같은 분포를 가질 것이라는 이야기와 같습니다. conformal prediction은 train set에서 valid set과 같은 개념인 calibration set을 따로 두고, 이를 통해 모델의 결과를 보정하겠다는 개념입니다. 학습 순서는 이렇게 구성되어 있습니다. Nonconformity score 계산: 예측값과 실제값의 차이를 측정 Calibration set 에서 분위수 계산: 목표 coverage에 맞는 임계값 도출 새로운 예측 에 적용: 계산된 임계값으로 prediction interval 구성 --- Split Conformal Prediction 가장 실용적이고 널리 사용되는 방법입니다. 단계별 과정을 살펴보면, 데이터 분할 `` 전체 데이터 → Train Set + Calibration Set ` 모델 학습 Train set으로 예측 모델 $\\hat{f}$ 학습 3단계: Nonconformity score 계산 Calibration set에서 각 데이터의 nonconformity score 계산 대칭 PI의 경우: $r_i = y_i - \\hat{f}(x_i) $ 임계값 계산 $\\hat{Q} = (1-\\alpha)(1+1/n_{cal})$-quantile of $\\{r_i\\}$ $n_{cal}$: calibration set 크기 Prediction Interval 구성 새로운 입력 $x$에 대해: $[\\hat{f}(x) - \\hat{Q}, \\hat{f}(x) + \\hat{Q}]$ Python 구현하는 예시는 다음과 같습니다. `python import numpy as np from sklearn.model_selection import train_test_split from sklearn.ensemble import RandomForestRegressor def split_conformal_prediction(X, y, alpha=0.1, test_size=0.2): # 1. 데이터 분할 X_train, X_cal, y_train, y_cal = train_test_split( X, y, test_size=test_size, random_state=42 ) # 2. 모델 학습 model = RandomForestRegressor(random_state=42) model.fit(X_train, y_train) # 3. Calibration set에서 예측 y_cal_pred = model.predict(X_cal) # 4. Nonconformity scores 계산 residuals = np.abs(y_cal - y_cal_pred) # 5. 임계값 계산 n_cal = len(y_cal) quantile_level = (1 - alpha) * (1 + 1/n_cal) threshold = np.quantile(residuals, quantile_level) return model, threshold 사용 예시 model, threshold = split_conformal_prediction(X, y, alpha=0.1) 새로운 예측 y_pred = model.predict(X_new) pi_lower = y_pred - threshold pi_upper = y_pred + threshold `` --- Conformalized Quantile Regression (CQR) Quantile regression과 conformal prediction을 결합한 방법입니다. 3번 split conformal prediction의 개념과 완전히 일치합니다. 학습과정을 살펴보면, Quantile 모델 학습 $\\tau = \\alpha/2$ (하한) $\\tau = 1 - \\alpha/2$ (상한) Nonconformity score 계산 $$ e_i = \\max\\big(\\hat{q}_{\\alpha/2}(x_i) - y_i, y_i - \\hat{q}_{1-\\alpha/2}(x_i), 0\\big) $$ 임계값 계산 $\\hat{Q} = (1-\\alpha)(1+1/n_{cal})$-quantile of $\\{e_i\\}$ 최종 PI 구성 $$ [\\hat{q}_{\\alpha/2}(x) - \\hat{Q}, \\hat{q}_{1-\\alpha/2}(x) + \\hat{Q}] $$ 이를 통해 더 정확한 PI를 구성할 수 있습니다. --- 장점과 한계 Conformal prediction의 장단점을 정리하면 아래와 같습니다. 장점 분포 가정 불필요: 어떤 데이터든 사용 가능 Finite-sample 보장: 유한한 샘플에서도 coverage 보장 모델 독립적: 어떤 ML 모델과도 결합 가능 Calibration 자동: 별도 조정 불필요 한계 Marginal coverage만 보장: Conditional coverage는 보장하지 않음 데이터 분할 필요: 학습 데이터 일부를 calibration에 사용 Computational cost: 추가 계산 비용 Exchangeability 가정: 시간 순서가 중요한 데이터에는 부적합 --- 최신 발전 동향 Adaptive Conformal Prediction 온라인 학습 환경에서 coverage 유지 Concept drift에 적응 Conformal Prediction for Time Series 시계열 데이터의 특성 고려 Autocorrelation 처리 Multi-output Conformal Prediction 다중 출력 변수 동시 처리 출력 간 상관관계 고려 Conformal Prediction with Deep Learning 신경망 모델과의 결합 Uncertainty quantification",
    "public": true
  },
  {
    "id": "Data Science/Paper-Review/A-Gentle-Introduction-to-Conformal-Prediction-and-Distribution-Free-Uncertainty-Quantification",
    "title": "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification",
    "date": "2025-08-13",
    "excerpt": "Conformal Prediction 관련 Paper Review",
    "category": "Data Science",
    "tags": [
      "statistics",
      "prediction-interval",
      "uncertainty",
      "calibration",
      "paper-review"
    ],
    "content": "Abstact > Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. 되게 공감이 되는 부분이다. 모델 불확실성을 정량적으로 표현하는게 굉장히 중요함. > Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. distribution-free 의 특징을 가짐. --- Conformal Prediction <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure1.png\" alt=\"paper_figure1\" width=\"100%\" /> <figcaption>Figure 1: Prediction set examples on Imagenet. We show three progressively more difficult examples of the class fox squirrel and the prediction sets (i.e., C(Xtest)) generated by conformal prediction.</figcaption> </figure> 이미지별로 예측값에 대한 불확실성이 다른데, 이걸 어떻게 수치적으로 정량화해서 표현할 것인가. 불확실성을 정량화해서 표현할 수 있는, prediction interval을 계산하는 방법을 제시하고 있다. (식 1) $$ 1 - \\alpha \\leq \\mathbb{P}(Y_{test} \\in C(X_{test})) \\leq 1 - \\alpha + \\frac{1}{n + 1} \\tag{1} $$ > In words, the probability that the prediction set contains the correct label is almost exactly 1 − α; we call this property marginal coverage, since the probability is marginal (averaged) over the randomness in the calibration and test points. calibration step을 통해 prediction set을 생성. calibration step : <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure2.png\" alt=\"paper_figure2\" width=\"100%\" /> <figcaption>Figure 2: Illustration of conformal prediction with matching Python code.</figcaption> </figure> set the conformal score $s_i = 1 - \\hat f(X_i)_{Y_i}$ : (예시) 1 - softmax output Score가 높을수록 모델이 해당 클래스를 잘못 예측했다는 의미 define $\\hat q$ to be the $[(n+1)(1 - \\alpha)] / n$ empirical quantile of $\\{s_1, ..., s_n\\}$ $\\lceil \\cdot \\rceil$: ceiling function (올림 함수) 본질적으로는 $(1-\\alpha)$ quantile이지만 작은 보정값 포함 creat a prediction set 새로운 테스트 데이터 $X_{test}$에 대해: $C(X_{test}) = \\{y : \\hat{f}(X_{test})_y \\geq 1 - \\hat{q}\\}$ 충분히 높은 softmax 출력을 가진 모든 클래스를 포함 Remarks : > Let us think about the interpretation of C Set-valued Function C는 set-valued function입니다 Input: image Output: set of classes (Figure 1 참조) Softmax Output 활용 모델의 softmax output을 사용해서 집합을 생성 각 입력에 대해 adaptively different output set을 구성 Uncertainty에 따른 Set Size 변화 Model이 uncertain할 때: set이 커짐 Image가 intrinsically hard할 때: set이 커짐 이는 desirable property입니다! Set Size의 의미 Set size = Model certainty indicator Set이 클수록 → Model이 uncertain Set이 작을수록 → Model이 certain Interpretation C(X_test)는 \"plausible classes that image X_test could be assigned to\" 즉, 가능성이 있는 classes를 모두 포함 Mathematical Guarantee C는 valid: 수식 (1)을 만족 수식 (1): $\\mathbb{P}(Y_{test} \\in C(X_{test})) \\geq 1-\\alpha$ Generalization 이러한 C의 properties들은 다른 ML problems에도 자연스럽게 적용 Regression problems에서도 동일한 원리 사용 가능 > However, we had no guarantee that the softmax outputs were any good; they may have been arbitrarily overfit or otherwise untrustworthy. > Therefore, instead of taking the softmax outputs at face value, we used the holdout set to adjust for their deficiencies. Softmax Output의 문제점 Softmax output은 불확실성을 표현하는데 heuristic하게 사용됨 엄밀한 확률을 나타낼 수 없음 Overfitting이나 untrustworthy할 수 있음 Holdout Set을 통한 보정 Holdout Set의 역할: n ≈ 500개의 fresh data points Training 중에 모델이 보지 못한 데이터 모델 성능의 honest appraisal 제공 Conformal Score 계산: Conformal score = $1 - \\hat{f}(X_i)_{Y_i}$ Model이 uncertain할 때 score가 커짐 Quantile 기반 임계값 설정 임계값 계산: $\\hat{q} \\approx (1-\\alpha)$ quantile of scores α = 0.1일 때: 90%의 ground truth softmax outputs가 $1-\\hat{q}$ level 이상임을 보장 Test-time 적용: 새로운 이미지 X_test의 softmax outputs 획득 $1-\\hat{q}$ 이상인 모든 classes를 prediction set C(X_test) 에 수집 새로운 true class Y_test의 softmax output이 $1-\\hat{q}$ 이상일 확률이 최소 90% 보장 --- 1.1 Instructions for Conformal Prediction --- Examples of Conformal Procedures 2.1 Classification with Adaptive Prediction Sets > The previous method produces prediction sets with the smallest average size [6], but it tends to undercover hard subgroups and overcover easy ones. > Here we develop a different method called adaptive prediction sets (APS) that avoids this problem. 구간의 두께를 X에 맞게 조절하는 방법 제시 <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure3.png\" alt=\"paper_figure3\" width=\"100%\" /> <figcaption>Figure 3: Python code for adaptive prediction sets.</figcaption> </figure> $$ C(x) = \\{\\pi_1(x), \\ldots, \\pi_k(x)\\}, \\text{ where } k = \\sup\\left\\{k_0 : \\sum_{j=1}^{k_0} \\hat{f}(x)_{\\pi_j(x)} < \\hat{q}\\right\\} + 1. \\tag{3} $$ > APS는 \"각 입력에 대해 1-alpha coverage를 정확히 만족하는 최소한의 prediction set\"을 만드는 방법 Threshold 방식 기존: 모든 클래스에 동일한 threshold 적용 APS: 각 입력 x마다 다른 threshold (1-alpha 만족할 때까지) Coverage 보장 기존: Marginal coverage만 보장 (전체 평균) APS: 각 입력 x에 대해 개별적으로 1-alpha coverage 달성 Set Size 기존: 모든 입력에 대해 비슷한 크기의 prediction set APS: 입력별로 적응적으로 크기 조절 예시 쉬운 이미지: [0.8, 0.15, 0.05] → APS: [Class 1] (1개) 어려운 이미지: [0.4, 0.3, 0.2, 0.1] → APS: [Class 1, Class 2, Class 3] (3개) 기존 방법 : > 고정된 quantile threshold 사용 ``python threshold = np.quantile(softmax_scores, 1 - alpha) prediction_set = [class for class in all_classes if softmax_score[class] >= threshold] ` 문제점: Undercover hard subgroups: 어려운 데이터는 너무 적은 클래스 포함 → coverage 부족 Overcover easy subgroups: 쉬운 데이터는 너무 많은 클래스 포함 → 불필요하게 넓음 APS (Adaptive Prediction Sets) : <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure4.png\" alt=\"paper_figure4\" width=\"100%\" /> <figcaption>Figure 4: A visualization of the adaptive prediction sets algorithm in Eq. (3). Classes are included from most to least likely until their cumulative softmax output exceeds the quantile.</figcaption> </figure> `python APS: 1-alpha를 만족할 때까지 greedy하게 추가 def build_adaptive_set(softmax_scores, alpha): sorted_classes = np.argsort(softmax_scores)[::-1] # 내림차순 정렬 cumulative_prob = 0 prediction_set = [] for class_idx in sorted_classes: cumulative_prob += softmax_scores[class_idx] prediction_set.append(class_idx) if cumulative_prob >= (1 - alpha): # 1-alpha 만족하면 중단 break return prediction_set `` --- 2.2 Conformalized Quantile Regression <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure5.png\" alt=\"paper_figure5\" width=\"100%\" /> <figcaption>Figure 5: Python code for conformalized quantile regression.</figcaption> </figure> > However, because the fitted quantiles may be inaccurate, we will conformalize them. Python pseudocode for conformalized quantile regression is in Figure 5. 분류 모델과 마찬가지로 quantile regression이 정확하지 않으니, conforming 해준다. $$ s(x, y) = \\max\\left\\{ \\hat{q}_{\\alpha/2}(x) - y, y - \\hat{q}_{1-\\alpha/2}(x) \\right\\} $$ > After computing the scores on our calibration set and setting $\\hat{q} = \\text{Quantile}(s_1, \\ldots, s_n; \\frac{\\lceil(n+1)(1-\\alpha)\\rceil}{n})$, we can form valid prediction intervals by taking $$ C(x) = \\left[\\hat{q}_{\\alpha/2}(x) - \\hat{q}, \\hat{q}_{1-\\alpha/2}(x) + \\hat{q}\\right]. \\tag{4} $$ <figure> <img src=\"/post/DataScience/conformal_prediction/paper_figure6.png\" alt=\"paper_figure6\" width=\"100%\" /> <figcaption>Figure 6: A visualization of the conformalized quantile regrssion algorithm in Eq. (4). We adjust the quantiles by the constant qˆ, picked during the calibration step.</figcaption> </figure> --- 2.3 Conformalizing Scalar Uncertainty Estimates 2.3.1 The Estimated Standard Deviation > As an alternative to quantile regression, our next example is a different way of constructing prediction sets for continuous y with a less rich but more common notion of heuristic uncertainty: an estimate of the standard deviation $\\hat σ(x)$. 기존의 uncertainty quantification 방법은 Y_test X_test = x가 가우시안 분포 N(μ(x), σ(x))를 따른다고 가정하고, 모델 f̂(x)와 σ̂(x)를 학습시켜 각각 평균과 분산을 예측하는 방식입니다. 이 방법은 PyTorch의 GaussianNLLLoss 같은 내장 손실 함수로 쉽게 구현할 수 있어 널리 사용되지만, 실제로는 Y_test X_test가 가우시안이 아닌 경우가 많아서 σ̂(x)가 신뢰할 수 없을 수 있습니다. 따라서 conformal prediction을 사용하여 이러한 heuristic한 uncertainty 개념을 f̂(x) ± q̂σ̂(x) 형태의 rigorous한 prediction interval로 변환할 수 있습니다 2.3.2 Other 1-D Uncertainty Estimates --- Evaluating Conformal Prediction 이 내용을 깔끔하게 정리하면: Evaluating adaptivity: 평균 set size가 가장 작은 conformal prediction 절차가 반드시 최선은 아니라는 점을 명심해야 합니다. 좋은 conformal prediction 절차는 쉬운 입력에서는 작은 set을, 어려운 입력에서는 큰 set을 제공하여 모델의 uncertainty를 충실히 반영합니다. 이러한 적응성은 conformal prediction의 coverage guarantee에서 자동으로 보장되지 않지만, 실제 배포에서는 필수적입니다. 우리는 적응성을 공식화하고, 그 결과를 탐구하며, 평가를 위한 실용적인 알고리즘을 제안할 것입니다. Correctness checks: 정확성 검사는 conformal prediction을 올바르게 구현했는지 테스트하는 데 도움이 됩니다. 우리는 coverage가 Theorem 1을 만족하는지 경험적으로 확인할 것입니다. 이 속성이 성립하는지 엄격하게 평가하려면 실제 데이터셋의 finite-sample 변동성을 신중하게 고려해야 합니다. 우리는 coverage에서 1-α로부터의 편차를 관찰할 때 발생하는 benign fluctuations의 크기에 대한 명시적 공식을 개발합니다. 3.1 Evaluating Adaptivity",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/quantile-regression-explained",
    "title": "Quantile Regression: 평균이 아닌 분위수를 예측하는 방법",
    "date": "2025-08-13",
    "excerpt": "전통적인 회귀분석과 달리 특정 분위수를 직접 예측하는 quantile regression의 개념과 활용법",
    "category": "Data Science",
    "tags": [
      "statistics",
      "regression",
      "quantile",
      "prediction-interval"
    ],
    "content": "Quantile regression 은 전통적인 회귀분석과 달리 특정 분위수(quantile)를 예측하는 방법입니다. 이를 통해 예측구간 등을 추정할 수 있습니다. --- 전통적 회귀분석 vs Quantile Regression 기존의 regression과는 목적이 다릅니다. 따라서 다른 loss함수를 사용하고 있습니다. 전통적 회귀분석 (OLS): 목표: 조건부 평균 $\\mathbb{E}[Y X]$ 예측 Loss: Mean Squared Error (MSE) 가정: 오차가 정규분포를 따름 Quantile Regression: 목표: 조건부 분위수 $q_\\tau(Y X)$ 예측 Loss: Pinball Loss (Quantile Loss) 가정: 분포 가정 없음, 더 robust --- Pinball Loss (Quantile Loss) Quantile regression의 핵심은 pinball loss 입니다. 이를 통해 모델이 분위수를 예측하도록 유도할 수 있습니다. $$ \\mathcal{L}_\\tau(y, \\hat{q}_\\tau(x)) = \\max\\big(\\tau \\cdot [y - \\hat{q}_\\tau(x)], (1-\\tau) \\cdot [\\hat{q}_\\tau(x) - y]\\big) $$ 위의 식을 통해 아래와 같이 이해할 수 있습니다. $\\tau = 0.5$ (중앙값): 양쪽 오차에 동일한 페널티 $\\tau = 0.9$ (90분위수): 예측값이 실제값보다 작으면 큰 페널티 예측값이 실제값보다 크면 작은 페널티 $\\tau = 0.1$ (10분위수): 반대 패턴 <figure> <img src=\"/post/DataScience/pinball_loss.png\" alt=\"pinball_loss\" /><width=\"100%\"/> <figcaption>그림1. Pinball Loss</figcaption> </figure> pinball loss는 비대칭 손실을 정의한 것입니다. (MSE는 대칭 손실) 그림1을 보면, 실제 값보다 작게 예측했을 때는 큰 패널티를, 크게 예측했을 때는 작은 패널티를 부여하고 있음을 알 수 있습니다. 이를 통해 모델이 90%의 데이터가 예측값보다 작도록, 10%의 데이터가 예측값보다 크도록 예측값을 높게 설정하게 됩니다. 구체적인 예시 실제 데이터로 검증해보겠습니다. 예시: 90분위수 예측 실제 데이터가 [1, 3, 5, 7, 9, 11, 13, 15, 17, 19]라고 가정해보겠습니다. 90분위수는 17입니다 (10개 중 9개가 17보다 작음). 모델 학습 과정 초기 예측값: 10 (중간값) 손실 계산: y > 10인 데이터: [11, 13, 15, 17, 19] (5개) y < 10인 데이터: [1, 3, 5, 7, 9] (5개) 90분위수 손실 ($\\tau = 0.9$): 과소예측 오차: $0.9 \\times (11-10 + 13-10 + 15-10 + 17-10 + 19-10) = 0.9 \\times 25 = 22.5$ 과대예측 오차: $0.1 \\times (10-1 + 10-3 + 10-5 + 10-7 + 10-9) = 0.1 \\times 20 = 2.0$ 총 손실: 24.5 모델이 예측값을 높이는 이유 과소예측에 큰 페널티(0.9배)가 있기 때문에: 모델은 $y > \\hat{q}_{0.9}$인 데이터를 줄이려고 함 즉, 예측값을 높여서 더 많은 데이터가 예측값보다 작게 만들려고 함 과대예측에 작은 페널티(0.1배)가 있기 때문에: 모델은 $y < \\hat{q}_{0.9}$인 데이터가 많아져도 큰 문제없음 최적해 도달 모델이 예측값을 17로 설정하면: y > 17: [19] (1개, 10%) y < 17: [1, 3, 5, 7, 9, 11, 13, 15] (8개, 80%) y = 17: [17] (1개, 10%) 정확히 90%의 데이터가 17보다 작거나 같게 됩니다! 이렇게 비대칭 손실 함수가 모델을 자동으로 올바른 분위수로 \"밀어올리는\" 것입니다. --- Prediction Interval 구성 Quantile regression으로 prediction interval을 만들 수 있습니다. 두 개의 quantile 모델 학습: $\\tau = \\alpha/2$ (하한) $\\tau = 1 - \\alpha/2$ (상한) Prediction Interval: $$[q_{\\alpha/2}(x), q_{1-\\alpha/2}(x)]$$ 예시: 90% Prediction Interval $\\tau = 0.05$ 모델로 하한 예측 $\\tau = 0.95$ 모델로 상한 예측 결과: $[q_{0.05}(x), q_{0.95}(x)]$ --- 장점과 특징을 정리하면 아래와 같습니다. 장점과 특징 장점 분포 가정 불필요: 정규성 가정 없이도 작동 Heteroscedasticity 대응: 분산이 일정하지 않아도 적합 Outlier robust: 극값에 덜 민감 직관적 해석: 분위수는 비선형 변환에도 불변 특징 비대칭성: 분포가 비대칭일 때 더 정확 계산 효율성: 선형 프로그래밍으로 해결 가능 해석력: 각 변수의 영향력을 분위수별로 분석 가능 ---",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/소문및가십/08-12-리뷰데이",
    "title": "리뷰 데이",
    "date": "2025-08-12",
    "excerpt": "소문 및 가십에 사용되는 표현들 리뷰",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 표현들 정리 Believe it or not Believe it or not, I bumped into Jessica Alba This maybe a surprise, but You may not know this, but Are you telling me that There's no way that No wonder I knew it I would have never ~ From what I've heard From what I know As far as I understand,",
    "public": true
  },
  {
    "id": "Self_Development/Career/toss-채용설명회",
    "title": "토스 채용설명회 정리",
    "date": "2025-08-12",
    "excerpt": "토스 채용설명회 듣고 관련 내용들 정리",
    "category": "Career",
    "tags": [
      "Toss"
    ],
    "content": "Head of Data Toss가 대규모 채용 중인 이유 성장하는 중 다음 단계로 성장하기 위해 데이터 직군이 중요하다. 비즈니스 임팩트. Data 조직의 미션과 비전 고객에게 최고의 경험. 비즈니스 경험. 고객을 더 잘 이해하고. 성과의 시작. Data / ML 직군 Data Engineering Data Analytics Data Ops Manager ML Engineer AI Engineer ML Manager Data Manager DAta Architect 토스가 집중하고 있는 것은 비즈니스 임팩트를 내는데 집중하고 있다. 데이터 거버넌스 프로젝트. 광고/커머스 성장 . 추천 검색. LLM. 자동화. 혁신. Ads Domain ML Engineer 최영민님. 토스 광고. 다양한 지면에 광주 요구 수용. 결재 금액, 유저 행동 데이터 기반 개인화된 광고 제공. 해결하고 있는 문제 CTR/CVR 예측. 광고 랭킹에는 추천과 다르게 비긴가? eCPM 계산. 자동 입찰 시스템.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Resumes/25-08-12-feedback-hyundai-resume",
    "title": "[Feedback] - [탈락!] Hyundai Autoever - ML Engineer Resume",
    "date": "2025-08-12",
    "excerpt": "현대자동차 이력서",
    "category": "Career",
    "tags": [
      "현대자동차 채용공고",
      "이력서"
    ],
    "content": "피드백 받은 이력서 피드백 받은 내용들 긁어오기 이직사유를 어떻게 말해야하나 고민이네요 cv랑 llm 경험을 한 회사에서 하신건가요 도메인이 다양해서 신기하네요 세부 포트폴리오나, 뭔가 다른 자료같은건 더 없으신가요? 아무래도 지금 바쁘신 것 같으니 제가 봤을 때의 내용을 적어보자면, 지원사유 없음 경력 3년차 공고인데 비해, 경력 1년차(다른 요소로 커버 가능) 우대사항 2개 중 3.1. \"프롬프트 엔지니어링\"는 어렵지 않아서 어필이 어려움. 3.2. \"데이터 품질\" 역량이 제가 보기에는 어필이 될 정도의 난이도로 보이지 않아요. 성과의 경우, 기존 모델 개비(대비 오타) 일반화 성능 10~20% 향상 및 Dice Score 0.8 달성이라고 하는데, 이게 어느정도의 가치가 있는지도 기존 모델 대비이기 때문에 명확하게 판단하기 어렵고, OCR 파이프라인 최적화, 정확도 99% 달성 및 에러율 1% 미만 이라고 하는 것도 얼핏 보면 좋은 성과같은데, 어떤 데이터로 어떻게 테스트 했을 때 이렇게 달성된거지?가 없어요. RAG 파이프라인 구현 및 최적화로 기존 상담 중 65% 대체 및 사용자 만족도 90% 이상 달성 해당 성과도 기존 상담의 만족도는 어땠고 변경 이후 90%를 달성한건지, 만족도 조사는 어떤식으로 진행했으며 모수가 몇개인지가 없네요. 전반적으로 내용에 대한 근거가 불확실해서, 제가 채용 담당자라면 내용을 부풀렸을거라고 생각될 것 같고, 경력도 낮기 때문에 떨어뜨릴 것 같아요. 특히나 \"연구개발 역량\"을 원하기 때문에 연구개발하듯이, 명확한 배경과 근거 설명이 더더욱 필요할 것 같구요 변수통제나 그게 없으면 좋은 성과더라도 얻어걸린거나 다름 없을꺼에요, 여기선 추가적으로 이야기하자면, 동료의 한마디도 읽어보고, 인재상이나 회사 가치같은 것도 꼭 찾아보세요. 고깃집 사장이고, 싸고 맛있는 고기를 팔아야한다는 철학은 갖는데, 직원이 갑자기 우리 고기정도면 더 받을 수 있다고 하면 어떤 느낌이겠어요 철학은 갖는데 => 철학을 가지고 있는데 만족도 조사는 어떤식으로 진행했으며 모수가 몇개인지가 없네요. 연구개발하듯이, 명확한 배경과 근거 설명이 더더욱 필요할 것 같구요 이부분을 보면 성과나 역할을 정리하면서 어느정도 부연설명과 배경을 알리는게 좋다는 말씀이실까요? 저는 원래 구구절절 스타일이었다가 지금 올려주신 분 처럼 깔끔하게 성과 위주로 정리하고 있었는데 피드백이 공감이 돼서요 단지 이런 부분들이 명확하지 않으니, 개선되어야할거다. 일단 이력서에서 알아야하는게 이력서의 목적은 \"니가 찾던 사람이 바로 나\" 거든요? 1차적으로 생각해보면 제가 가진 경험이 많을 수록 좋을까, 없을 수록 좋을까를 먼저 생각해보실래요? 어떠실 것 같아요? 내용이 구구절절 하고 말고를 떠나서 근거에 대해선 아무래도 저쪽은 연구분야다보니까 더욱더 필요하다 생각해요 ㅋ_ㅋ 실험과 결과로 증명해야하는 곳이다보니까 우리 회사랑 맞는 경험이 많은 사람, 적은 사람 근데 여기 이력서에서 어필해야하는 부분이 연구거든요? 물론 다른 곳이어도 비슷하긴 하지만, 흔히 백엔드 개발자가 하는 실수가 DB 10초 걸리던 쿼리를 0.1초로 단축 이런걸 적는데 사실 이게 전혀 경쟁력이 없어요 만약에, 이미 튜닝이 다 되어있던 SQL인데도 10초가 걸리던 SQL인데 0.1초로 바뀌었으면 나라에서 상을 줘도 부족할 역량이지만 아무 인덱스도 안걸려있던 SQL을 인덱스 추가해서 개선했다 이건 수준이거든요., 그냥 SQL 인덱스에 대한 지식을 안다 \"결국 같은 정량적 성과\" 라고 하더라도 그 뒤에 있는 배경이나, 상황 등에 따라서 가치가 완전히 달라져요. 그리고 그 가치를 이력서에서 명확하게 보여주려면 연구 결과처럼 성과를 보여줘야죠. 특히나 AI처럼 돌릴 때마다 변수가 있는 케이스의 경우 변수통제가 들어가고, 시나리오가 있어야하는데 그런게 없는 상태에서는, 저게 어쩌다 운좋게 잘 나온 성과인지, 아닌지도 확신하기 어렵구요. @모죠 요약하면, 성과를 \"입증하기 위한 근거는 해당 채용공고에서 꼭 필요한 내용임.\" 이네요 AI쪽은 좀 조심스럽긴한데 관련 전공자에게 피드백들으시는것도 도움될거같습니다 🤔 모두연 출신이셔서 그쪽에 커리어 피드백이라든지 적극적으로 해주실분이 많아보여서요 분량이 애매할 것 같으면 차라리 길게 쓰세요 없는 것보단 나아요 면접관으로 하여금 아 정말로 이 사람이 3년차 이상의 역량을 가졌구나 라는 생각이 들면 면접가실 수 있을꺼에요 그리고 위에서 말씀드린 것처럼 인재상이나, 동료의 한마디 이런거 잘 보시고 어필하세요. 정리 핵심 요약: 지원사유 부재, 경력 레벨 갭, 성과 근거 불충분 → 신뢰도 하락. 연구개발 포지션 특성상 실험 설계와 증빙이 필수. 개선 방향: 지원사유·회사 적합성: 인재상/동료의 한마디 근거로 동기와 fit 명확화 경력 갭 보완: 높은 난이도 이슈 해결, 오너십, 영향 범위(조직/사용자 임팩트) 강조 우대사항 정렬: Data Quality 심화 사례 중심, Prompt Engineering은 임팩트 중심으로 요약 성과 서술 템플릿(각 항목에 공통 적용): Context: 문제/배경, 데이터 특성 Baseline: 기존 모델/프로세스와 지표(숫자) Design: 변수 통제, 실험 시나리오/세팅 Metrics: 지표 정의, 표본수(N), 평가 프로토콜 Result: 결과 수치와 개선폭, 불확실성(가능 시) Role: 본인 기여 범위와 협업 포인트 Evidence: 코드/대시보드/리포트/샘플 링크(재현성) 체크리스트: 데이터셋/분포/기간 명시, 테스트 프로토콜과 모수 기재 기준선 대비 개선 근거와 가정 제시(과적합/우연 가능성 배제) 사용자/비즈니스 임팩트로 연결(운영 비용/시간/만족도 등) 다음 액션: 이력서의 각 성과 항목을 위 템플릿으로 재작성 별도 섹션으로 지원사유 추가(회사 가치/인재상 인용) 포트폴리오·레포·산출물 링크 삽입(재현성 확보) 회사 용어와 문체에 맞게 문구 손질, 분량 애매하면 상세히 작성 GPT 너가 주는 피드백 (면접관이라고 생각하고, 내 이력서랑 채용공고 보고) 한눈 평가: JD 핵심 요구와 매칭 높음. 실무형 성과는 좋으나 R&D 팀 특성상 실험 설계/증빙의 밀도는 부족. 면접 추천. JD 매칭도 Must-have 검색/챗봇 서비스 개발: RAG 기반 챗봇 구축·운영 경험 명확. OK 프롬프트 엔지니어링/데이터 운영·품질 관리: Prompt 최적화, 자동 평가·모니터링 구축. OK Python 실무: 다수 프로젝트로 충분. OK Nice-to-have LLM/Prompt: 보유. OK Data Quality Process: OCR 품질 검증·모니터링 로직 보유. OK 딥러닝 모델링: CV 중심이지만 실전 성과 있음. 준수 NLP 데이터 구축/설계: 코퍼스/어노테이션/증강 설계의 구체 사례 언급 약함 → 보완 필요 강점 실제 서비스 운영 경험(카카오 연동, 운영 자동화 등)으로 실전성 높음 검색·RAG 아키텍처, 벡터DB 운용 등 JD와 직결 역량 보유 데이터 품질·운영 자동화 관점에서 강한 시그널 우려/갭 성과 수치의 근거 부족: 표본수(N), 평가 프로토콜(holdout/CV/시간분할), 통계적 유의성 미기재 RAG·Prompt 평가의 객관 지표 부족: faithfulness/grounding, hallucination rate 등 NLP 데이터 구축/증강 경험의 구체성 부족(스키마 설계, 어노테이션 품질, 증강 전략 등) CV 프로젝트(세그멘테이션)는 간접 연관 → JD 맥락으로 번역 필요 이력서 보완 제안 각 성과를 아래 1줄 포맷으로 재작성: “Baseline X 대비 Metric Y +Δ%(N=샘플), 방법 Z. Offline(CV/시간분할)와 Online(A/B) 일치(p<0.05). 비용/latency T↓.” 챗봇 파트 보강 데이터셋 규모/분포, split 전략과 seed Retrieval 지표(Recall@k, MRR), 응답 지표 정의·샘플수, 만족도 조사 설계(모수/방법) Hallucination/faithfulness 측정 방식, 실패 사례 에러 분석 데이터 증강 적용 사례(역번역, 템플릿, 합성 Q/A 등) 구체화 OCR 파트에 CER/WER 등 표준 지표와 테스트 프로토콜 명시 JD 키워드와 1:1 매핑 Summary 3줄 추가(대화형 데이터 설계/운영, Prompt/증강, 품질 관리) 예상 질문(준비 권장) RAG에서 grounding 평가 어떻게 했나? negative mining/Hard negative 전략은? 만족도 90%의 설계·모수·편향 통제는? 오프라인 vs 온라인 상관은? 데이터 leakage 방지/드리프트 모니터링 체계? Prompt 최적화의 실험 설계와 ablation 요약? OCR 정확도 정의(정확도 vs CER/WER) 선택 이유와 한계? 결론 지원 적합도: 높음(서비스·운영·품질 관리·RAG/Prompt 강점) R&D 신뢰도: 보완 필요(실험 설계/증빙/데이터 구축 설계의 구체화) 추천: 서류 통과 가능성 있음. 위 보완 반영 시 면접 설득력 크게 상승",
    "public": false
  },
  {
    "id": "Data Science/Statistics/what-is-KDE",
    "title": "KDE에 대한 자세한 정리",
    "date": "2025-08-12",
    "excerpt": "KDE에서 사용되는 개념들에 대한 자세히 정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1 : 블로그, KDE-Kernel Density Estimation이란? 2 : 블로그, Kernel Density Estimation? KDE 이해 KDE의 핵심 아이디어 \"점별로 밀도를 구할 때, 모든 기존 데이터 포인트에 대해서 거리로 가중치를 주는 것\" 구체적인 과정: 새로운 점 x에서 밀도를 구하고 싶음 모든 기존 데이터 포인트 x_i에 대해: 거리 x - x_i 계산 거리에 따른 가중치 K(x - x_i) 계산 모든 가중치를 합침 → $\\sum_{i=1}^{n} K(x - x_i)$ 정규화 → $\\frac{1}{nh}$ 곱하기 예시: ``python 새로운 점 x = 4에서 밀도 구하기 기존 데이터: [1, 3, 5] 각 포인트별 가중치: K(4-1) = K(3) # 거리 3, 낮은 가중치 K(4-3) = K(1) # 거리 1, 높은 가중치 K(4-5) = K(-1) # 거리 1, 높은 가중치 최종 밀도 = (K(3) + K(1) + K(-1)) / (nh) ` 핵심: 거리 기반 가중치: 가까운 데이터 포인트일수록 더 큰 영향 전체 합산: 모든 기존 데이터의 정보를 활용 부드러운 추정: 개별 포인트가 아닌 전체적인 분포 패턴 반영 --- PDF와 PMF란? PMF (Probability Mass Function, 확률질량함수) : PMF는 이산확률변수(discrete random variable)에서 특정 값이 나올 확률을 나타내는 함수입니다. 특징: 이산확률변수에만 적용 각 값에 대해 확률이 할당됨 모든 확률의 합은 1 음수 확률은 존재하지 않음 수학적 표현: $$P(X = x_i) = p_i$$ 예시: 주사위를 던질 때 P(X=1) = 1/6 P(X=2) = 1/6 ... P(X=6) = 1/6 PDF (Probability Density Function, 확률밀도함수) : PDF는 연속확률변수(continuous random variable)에서 확률 밀도를 나타내는 함수입니다. 특징: 연속확률변수에만 적용 특정 점에서의 확률은 0 구간에 대한 확률은 PDF를 적분하여 계산 전체 구간에서의 적분값은 1 음수 값이 가능 (단, 확률은 항상 양수) 수학적 표현: $$P(a \\leq X \\leq b) = \\int_a^b f(x) dx$$ 예시: 정규분포 N(0,1)의 PDF $$f(x) = \\frac{1}{\\sqrt{2\\pi}} e^{-\\frac{x^2}{2}}$$ 주요 차이점 구분 PMF PDF ------ ----- ----- 변수 타입 이산 연속 확률 계산 직접 값 적분 필요 특정 점 확률 양수 가능 항상 0 그래프 막대 그래프 곡선 np.histogram의 density 파라미터 density=False (기본값) 반환값: 각 구간(bin)에 속하는 데이터의 개수(count) 특징: 모든 구간의 합이 전체 데이터 개수 N과 같음 $\\sum_{i} \\text{count}_i = N$ 용도: 데이터의 분포를 파악할 때 예시: `python import numpy as np data = [1, 2, 2, 3, 3, 3, 4, 4, 5] counts, bins = np.histogram(data, bins=5, density=False) counts: [1, 2, 3, 2, 1] (각 구간의 데이터 개수) sum(counts) = 9 (전체 데이터 개수) ` density=True 반환값: 각 구간의 확률밀도(probability density) 특징: 각 구간의 값 × 구간 너비의 합이 1이 됨 $\\sum_{i} \\text{density}_i \\times \\text{bin\\_width}_i = 1$ 용도: PDF와 비교하거나 확률 분포를 분석할 때 예시: `python density, bins = np.histogram(data, bins=5, density=True) density: [0.25, 0.5, 0.75, 0.5, 0.25] (확률밀도) bin_width = 1.0 sum(density bin_width) = 1.0 `` 수학적 관계 density=False → density=True 변환: $$\\text{density}_i = \\frac{\\text{count}_i}{N \\times \\text{bin\\_width}_i}$$ density=True → density=False 변환: $$\\text{count}_i = \\text{density}_i \\times N \\times \\text{bin\\_width}_i$$ 언제 사용할까? density=False: 데이터의 실제 분포, 빈도 분석 density=True: PDF 추정, 다른 분포와 비교, 확률적 분석 --- Kernel function이란? 수학적으로 원점을 중심으로 Symmetric, 적분값이 1인 Non-Negative function Normalization: $$\\int_{-\\infty}^{\\infty} K(x) dx = 1$$ Symmetry: $$K(-x) = K(x)$$ Non-negativity: $$K(x) \\geq 0 \\text{ for all } x$$ 주요 Kernel 함수들 <figure> <img src=\"/post/DataScience/kernel_functions.png\" alt=\"Kernel functions\" /> <figcaption>Kernel functions</figcaption> </figure> Kernel의 역할 Kernel function은 각 데이터 포인트 주변에 \"가중치\"를 부여하는 역할을 합니다: 데이터 포인트가 있는 곳에서는 높은 가중치 멀어질수록 가중치가 감소 전체적으로 부드러운 PDF 추정을 가능하게 함 Density Estimation > Density Estimation은 주어진 데이터로부터 확률밀도함수(PDF)를 추정하는 방법입니다. 데이터의 분포를 파악하고 새로운 데이터가 어디에 위치할 확률이 높은지 예측하는 데 사용됩니다. 주요 방법들 Histogram (히스토그램) 가장 기본적인 방법으로, 데이터를 구간(bin)으로 나누어 각 구간의 빈도를 계산합니다. 장점: 구현이 간단하고 직관적 계산 속도가 빠름 단점: 구간의 위치와 크기에 민감 불연속적이고 부드럽지 않음 구간 경계에서 불연속성 발생 Kernel Density Estimation (KDE) 각 데이터 포인트에 kernel function을 배치하여 부드러운 PDF를 추정합니다. 수식: $$\\hat{f}(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)$$ 여기서: $\\hat{f}(x)$: 추정된 PDF $n$: 데이터 개수 $h$: bandwidth (대역폭) $K(\\cdot)$: kernel function $x_i$: i번째 데이터 포인트 장점: 부드럽고 연속적인 PDF 추정 데이터의 실제 분포를 잘 반영 bandwidth 조절로 smoothness 조정 가능 단점: bandwidth 선택이 중요 계산 비용이 상대적으로 높음 Parametric Methods 특정 분포(예: 정규분포, 지수분포)를 가정하고 모수를 추정합니다. 예시: $$f(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$$ 장점: 계산이 빠름 해석이 용이 적은 데이터로도 추정 가능 단점: 분포 가정이 틀리면 부정확 복잡한 분포를 잘 표현하지 못함 언제 어떤 방법을 사용할까? Histogram: 빠른 탐색적 분석, 대략적인 분포 파악 KDE: 정확한 PDF 추정, 시각화, 비모수적 분석 Parametric: 분포가 명확할 때, 빠른 계산이 필요할 때 --- Kernel Density Estimation (KDE) > \"각 데이터 포인트 주변에 작은 언덕(kernel)을 만들고, 이들을 모두 더해서 전체적인 지형(PDF)을 만드는 것\" KDE 수식 $$\\hat{f}(x) = \\frac{1}{nh} \\sum_{i=1}^{n} K\\left(\\frac{x - x_i}{h}\\right)$$ 여기서: $\\hat{f}(x)$: 추정된 PDF $n$: 데이터 개수 $h$: bandwidth (대역폭). 크기에 따라 kernel의 형태가 조절되는 parameter. scaled kernel이라고도 함. $K(\\cdot)$: kernel function $x_i$: i번째 데이터 포인트",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-KDE-Histogram",
    "title": "KDE와 Histogram에 대한 정리",
    "date": "2025-08-12",
    "excerpt": "KDE와 Histogram이 뭔지, 차이점에 대한 정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "What is Histogram? > 히스토그램은 데이터의 분포(어디에 값이 많이 몰려 있는지)를 보기 위해 데이터 범위를 여러 구간(bin)으로 나누고, 각 구간에 속한 데이터 개수를 세어 막대그래프로 표현한 것 히스토그램의 쓰임 데이터 분포의 형태 파악 (정규분포? 치우침? 두 개의 봉우리가 있음?) 분포 비교 (예: A 그룹과 B 그룹의 측정값 분포 차이) 통계/머신러닝 전처리(확률분포 추정, KL divergence 계산 등) 히스토그램 기반 확률분포 추정의 장점 구현이 쉽고 빠름 np.histogram, np.histogramdd로 바로 가능 시각화하기 좋음 이산형 데이터에서는 자연스럽게 적용 가능 단점 bin 크기(폭), bin 개수에 따라 결과가 민감하게 변함 데이터가 적으면 bin이 비게 되고, 분포가 부정확해짐 다차원 데이터일수록 bin 개수가 기하급수적으로 늘어(희소화), KL 값이 불안정해짐 → \"차원의 저주\" 문제 연속형 데이터에서는 부드러운 분포 형태를 제대로 못 잡을 수 있음 python code ``python import numpy as np data = [1, 2, 2, 3, 3, 3, 4] counts, bin_edges = np.histogram(data, bins=3, density=False) # 차원이 여러개면 np.ravel()을 써서 모든 값을 1차원으로 펼쳐서 계산 print(counts) # [2 3 2] print(bin_edges) # [1. 2. 3. 4.] ` `python hist, edges = np.histogramdd(data.drop([\"size\"], axis=1).to_numpy(), bins=30, range=None, density=False) # 차원 별로 bin을 쪼개줌 ` What is KDE? KDE(Kernel Density Estimation, 커널 밀도 추정)는 히스토그램보다 한 단계 더 세련된 확률밀도 추정 방법 정의와 수식 데이터의 각 관측치에 작은 kernel을 얹어 평균함으로써 연속적이고 부드러운 확률밀도함수(pdf)를 추정하는 비모수적(non-parametric) 방법 $$ \\hat{f}_h(x) = \\frac{1}{n h} \\sum_{i=1}^{n} K\\!\\left(\\frac{x - x_i}{h}\\right) $$ K(kernel): Gaussian, Epanechnikov, Top-hat 등. 일반적으로 kernel 선택보다 bandwidth h 선택이 성능에 더 큰 영향 h(bandwidth): 각 데이터 포인트에 얹는 kernel의 폭 (width)을 결정. 곡선의 매끄러움 조절. 작으면 과적합(울퉁불퉁), 크면 과평활(모드가 사라짐) 장점 히스토그램 대비 매끄러운 분포 추정, bin 경계 인공효과가 적음 연속형 데이터에서 모드, 꼬리(tail) 구조 확인에 유리 단점/주의 성능이 bandwidth에 매우 민감 → Cross-Validation, Silverman's rule, Scott's rule 등으로 선택 경계(boundary) 근처 bias 발생 가능 → reflection 등 보정 기법 고려 고차원에서는 표본 희소화로 성능 급감(차원의 저주) Python 예시 `python import numpy as np import matplotlib.pyplot as plt from sklearn.neighbors import KernelDensity rng = np.random.default_rng(0) x = rng.normal(loc=0, scale=1, size=200)[:, None] KDE fit kde = KernelDensity(kernel=\"gaussian\", bandwidth=0.3).fit(x) x_plot = np.linspace(-4, 4, 400)[:, None] density = np.exp(kde.score_samples(x_plot)) visualize vs histogram plt.hist(x.ravel(), bins=30, density=True, alpha=0.3, label=\"hist\") plt.plot(x_plot, density, label=\"KDE\") plt.legend(); plt.show() ` bandwidth 선택은 GridSearchCV`로 교차검증해 결정하는 것이 실전에서 가장 안전함. KDE vs Histogram 특징 히스토그램 KDE ---- ---------------- ----------------- 형태 계단식 부드러운 곡선 파라미터 bin 개수/폭 밴드폭(bandwidth) 고차원 bin 폭 설정 어려움 bandwidth 설정 어려움 민감도 bin 개수에 민감 bandwidth에 민감 장점 구현 쉬움, 직관적 매끄럽고 부드러운 분포 추정 단점 고차원/데이터 적을 때 부정확 bandwidth 선택이 어려움",
    "public": true
  },
  {
    "id": "Data Science/Statistics/how-to-define-prediciton-interval",
    "title": "ML의 prediction interval은 어떻게 정의할 수 있을까?",
    "date": "2025-08-12",
    "excerpt": "ML의 prediction interval 정의하는 법 정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1 : Confidence vs Prediction Intervals: Understanding the Difference 2 : IBM Developer - Prediction intervals explained: A LightGBM tutorial Prediction Interval vs Confidence Interval Prediction Interval (PI): 새 관측치 $Y$가 주어진 $X$에서 포함될 구간. 조건부 coverage를 목표로 함. $$ \\mathbb{P}\\big( Y \\in [L_\\alpha(X),\\ U_\\alpha(X)]\\mid X\\big) \\ge 1-\\alpha $$ Confidence Interval (CI): 조건부 평균 $\\mu(X)=\\mathbb{E}[Y\\mid X]$ 같은 모수적 target이 구간에 포함될 확률. $$ \\mathbb{P}\\big( \\mu(X)\\in \\hat C_\\alpha(X) \\big) \\ge 1-\\alpha $$ 핵심 차이: CI는 mean estimator의 불확실성(주로 epistemic)을, PI는 mean 주변의 noise(aleatoric)까지 포함해 “개별 예측값”의 불확실성을 다룸. 따라서 동일한 설정에서 PI의 폭이 CI보다 넓음. ML에서의 불확실성 (Uncertainty) Aleatoric uncertainty (우연적 불확실성): 데이터 자체의 내재적 변동성(heteroscedastic noise 포함). 더 많은 데이터로도 완전히 제거되지 않음. 예: 측정 오차, 자연적인 변동성. Epistemic uncertainty (인식론적 불확실성): 모델/파라미터에 대한 불확실성. 데이터가 많아지면 감소. 예: 모델 파라미터 추정의 불확실성, 모델 구조의 한계. CI vs PI: CI는 주로 epistemic uncertainty를, PI는 epistemic + aleatoric uncertainty 모두를 다룸. 따라서 동일한 설정에서 PI의 폭이 CI보다 넓음. Calibration: 목표 coverage(예: 90%)에 실제 커버리지가 맞도록 조정. Coverage 유형: marginal vs conditional. 실무에서는 분포 가정이 약할 때도 사용할 수 있는 marginal coverage 도구(예: conformal)가 유용. LightGBM에서 Prediction Interval (참고 2) LGBM에서 prediction interval 기능도 제공하는구나. quantile-regression 기반. > When you are performing regression tasks, you have the option of generating prediction intervals by using quantile regression, which is a fancy way of estimating the median value for a regression value in a specific quantile. > Simply put, a prediction interval is just about generating a lower and upper bound on the final regression value. > This is incredibly important for some tasks, which I explain in this article Why use them? (them -> prediction interval) > You can never be 100 percent certain about one prediction from one model. > Instead, the idea is to give an interval back to a person who ends up controlling the final decision based on the range that is given by the model. Quantile regression > In the typical linear regression model, you track the mean difference from the ground truth to optimize the model. > However, in quantile regression, as the name suggests, you track a specific quantile (also known as a percentile) against the median of the ground truth. Quantile and assumptions > Using the median approach lets you specify the quantiles 중앙값(median) 접근법의 장점: 분위수 지정 가능: 5% 분위수(데이터의 5%를 포함)와 95% 분위수(데이터의 95%를 포함)를 지정하여 하한과 상한 경계를 설정할 수 있습니다. Outlier에 강함: 평균(mean)을 사용할 때는 outlier가 있으면 예측 성능이 떨어질 수 있습니다. 평균은 outlier 값에 크게 영향을 받기 때문입니다. 중앙값을 사용하면 outlier에 덜 민감하여 더 나은 하한과 상한 경계를 만들 수 있습니다. 분포 가정 불필요: 선형 회귀와 달리 데이터의 분포에 대한 가정을 하지 않아, 특정 상황에서 더 유용하고 정확합니다. Regression $$ L = \\begin{cases} \\alpha(y - X\\theta), & \\text{if } (y - X\\theta) \\geq 0 \\\\ (\\alpha - 1)(y - X\\theta), & \\text{if } (y - X\\theta) < 0 \\end{cases} $$ > That changes in quantile regression because you must be able to account for the different quantiles. > - When the alpha is high (for example, 0.95), the errors that are less than zero receive a lower error value than if they are greater than zero. > - When the alpha is low (for example, 0.05), the errors that are less than zero receive a higher error value than if they are greater than zero, where they receive a smaller error value python code ``python import lightgbm as lgb regressor = lgb.LGBMRegressor() regressor.fit(x_train, y_train) regressor_pred = regressor.predict(x_test) lower = lgb.LGBMRegressor(objective = 'quantile', alpha = 1 - 0.95) lower.fit(x_train, y_train) lower_pred = lower.predict(x_test) upper = lgb.LGBMRegressor(objective = 'quantile', alpha = 0.95) upper.fit(x_train, y_train) upper_pred = upper.predict(x_test) plt.figure(figsize=(10, 6)) plt.scatter(x_test.MedInc, lower_pred, color='limegreen', marker='o', label='lower', lw=0.5, alpha=0.5) plt.scatter(x_test.MedInc, regressor_pred, color='aqua', marker='x', label='pred', alpha=0.7) plt.scatter(x_test.MedInc, upper_pred, color='dodgerblue', marker='o', label='upper', lw=0.5, alpha=0.5) plt.plot(sorted(x_test.MedInc), sorted(lower_pred), color='black') plt.plot(sorted(x_test.MedInc), sorted(regressor_pred), color='red') plt.plot(sorted(x_test.MedInc), sorted(upper_pred), color='black') plt.legend() plt.show() ` 구현 방법들 1 Quantile Regression 아이디어: $\\tau$-quantile 함수 $q_\\tau(x)$를 직접 학습해 $[q_\\alpha(x), q_{1-\\alpha}(x)]$를 PI로 사용. Loss(“pinball loss”): $$ \\mathcal{L}_\\tau\\big(y, \\hat q_\\tau(x)\\big) = \\max\\big(\\, \\tau\\,[y-\\hat q_\\tau(x)],\\ (\\tau-1)[y-\\hat q_\\tau(x)]\\,\\big) $$ 구현 팁: 두 모델(or multi-head)로 $q_\\alpha, q_{1-\\alpha}$ 동시 학습 quantile crossing 방지: monotonicity penalty 또는 joint training 장점: heteroscedastic noise에 강함, 분포 가정 최소화. 단점: calibration 보장은 없음(사후 조정 가능). 2 Conformal Prediction 가정: 예측 시점과 calibration 데이터가 exchangeable. Split conformal(대중적 실무 절차): 데이터 분할: train / calibration train으로 predictor $\\hat f$ 학습 calibration에서 비적합도(nonconformity) 계산. 대칭 PI의 경우 $r_i = y_i - \\hat f(x_i) $ $\\hat Q$ = $(1-\\alpha)(1+1/n_{cal})$-quantile of $\\{r_i\\}$ 새 입력 $x$에 대해 $[\\hat f(x)-\\hat Q,\\ \\hat f(x)+\\hat Q]$ Quantile 기반(CQR): quantile regressors $\\hat q_\\alpha, \\hat q_{1-\\alpha}$의 비적합도 $$ e_i = \\max\\big(\\hat q_\\alpha(x_i)-y_i,\\ y_i-\\hat q_{1-\\alpha}(x_i),\\ 0\\big) $$ 를 통해 $\\hat Q$를 구하고, 최종 구간을 $[\\hat q_\\alpha(x)-\\hat Q,\\ \\hat q_{1-\\alpha}(x)+\\hat Q]$로 설정. 장점: 분포/모델에 거의 비의존적이며 finite-sample marginal coverage 보장. 단점: 데이터 분할 필요, conditional coverage는 일반적으로 미보장. 3 Ensemble Methods Bootstrap ensemble, Random Forest, Deep Ensemble 등으로 여러 예측 $\\{\\hat f_k(x)\\}_{k=1}^K$을 얻어 분산을 추정. 단순히 ensemble 분산으로 $\\hat f(x) \\pm z\\cdot \\hat\\sigma_{ens}(x)$를 만들면 calibration이 보장되지 않음. 실무 팁: Jackknife+ / Bootstrap+ 같은 conformalized ensemble을 사용하면 finite-sample coverage에 가까운 보정을 달성. 예: Jackknife+는 leave-one-out 예측 분포를 이용해 하한/상한의 적절한 order statistic을 취해 구간 구성. 장점: 구현 용이, epistemic 반영. 단점: 데이터/계산 비용 증가, 별도 calibration 권장. 4 Bayesian Approaches 목표: posterior predictive $p(y\\mid x, \\mathcal{D})$로부터 구간 산출. $$ p(y\\mid x, \\mathcal{D}) = \\int p(y\\mid x, \\theta)\\, p(\\theta\\mid \\mathcal{D})\\, d\\theta $$ 구현 예: Bayesian Linear Regression, Gaussian Process, MC Dropout(Bayesian approximation), Stochastic VI, Laplace Approximation 등. 절차: posterior predictive의 quantile을 취해 PI 구성. 장점: 불확실성 분해 가능(epistemic/aleatoric), 해석력. 단점: 모델/사전 분포에 민감, 대규모 딥러닝에서 근사 필요. 5 MAE 기반 Prediction Interval MAE(Mean Absolute Error)를 기반으로 prediction interval을 구성하는 방법도 있습니다. 기본 아이디어 MAE의 의미: 예측값과 실제값 간의 평균 절대 오차 분포 가정: 오차가 대칭 분포를 따른다고 가정할 때, MAE를 이용해 prediction interval을 구성할 수 있음 구현 방법 단순 MAE 기반 `python MAE 계산 mae = np.mean(np.abs(y_true - y_pred)) Prediction interval 구성 (대칭 분포 가정) 90% coverage를 위한 경우 alpha = 0.1 z_score = norm.ppf(1 - alpha/2) # 1.645 for 90% coverage MAE를 표준편차로 변환 (Laplace 분포 가정) Laplace 분포에서 MAE = σ√2, 따라서 σ = MAE/√2 sigma_est = mae / np.sqrt(2) Prediction interval lower_bound = y_pred - z_score sigma_est upper_bound = y_pred + z_score sigma_est ` Heteroscedastic MAE 기반 `python 각 예측값별로 MAE를 계산 (예: binning 또는 local averaging) def heteroscedastic_mae_based_pi(y_pred, y_true, x_features, bins=10): # 예측값을 기준으로 binning pred_bins = pd.cut(y_pred, bins=bins) # 각 bin별로 MAE 계산 bin_mae = {} for bin_name in pred_bins.cat.categories: mask = pred_bins == bin_name if mask.sum() > 0: bin_mae[bin_name] = np.mean(np.abs(y_true[mask] - y_pred[mask])) # 각 예측값에 대해 해당 bin의 MAE 사용 pi_bounds = [] for i, pred in enumerate(y_pred): bin_name = pred_bins.iloc[i] if bin_name in bin_mae: mae_local = bin_mae[bin_name] sigma_local = mae_local / np.sqrt(2) # 90% coverage z_score = 1.645 lower = pred - z_score sigma_local upper = pred + z_score sigma_local pi_bounds.append((lower, upper)) else: pi_bounds.append((pred, pred)) return np.array(pi_bounds) ` 장단점 장점: 구현 간단: 복잡한 모델링 없이 기존 MAE 지표를 활용 해석 용이: MAE의 의미를 그대로 prediction interval에 반영 계산 효율: 추가적인 모델 학습이나 복잡한 추론 과정 불필요 단점: 분포 가정: 오차가 대칭 분포를 따른다는 가정이 필요 Coverage 보장 없음: 실제 coverage가 목표 coverage와 일치하지 않을 수 있음 Homoscedastic 가정: 기본적으로는 모든 예측값에 동일한 불확실성을 적용 개선 방안 Calibration 적용 `python def calibrate_mae_based_pi(y_pred, y_true, target_coverage=0.9): # 초기 MAE 기반 PI 생성 mae = np.mean(np.abs(y_true - y_pred)) initial_pi = mae 1.645 # 90% coverage # Calibration을 위한 scaling factor 찾기 empirical_coverage = np.mean( (y_true >= y_pred - initial_pi) & (y_true <= y_pred + initial_pi) ) # Scaling factor 조정 scaling_factor = target_coverage / empirical_coverage calibrated_pi = initial_pi scaling_factor return calibrated_pi ` Ensemble MAE 활용 `python def ensemble_mae_based_pi(ensemble_predictions, y_true): # Ensemble 예측값들의 표준편차 계산 pred_std = np.std(ensemble_predictions, axis=0) # MAE 계산 ensemble_mean = np.mean(ensemble_predictions, axis=0) mae = np.mean(np.abs(y_true - ensemble_mean)) # MAE와 ensemble 분산을 결합 combined_uncertainty = np.sqrt(mae2 + pred_std2) # Prediction interval 구성 z_score = 1.645 # 90% coverage lower_bound = ensemble_mean - z_score combined_uncertainty upper_bound = ensemble_mean + z_score combined_uncertainty return lower_bound, upper_bound `` 실무 적용 시 고려사항 데이터 특성: 오차의 분포가 대칭에 가까운지 확인 Calibration 필수: 실제 coverage와 목표 coverage 간의 차이를 보정 Heteroscedasticity: 예측값에 따라 불확실성이 달라지는 경우 local MAE 사용 고려 Ensemble과 결합: MAE의 장점과 ensemble의 epistemic uncertainty 추정을 결합하여 더 robust한 PI 구성 실무 체크리스트 Coverage vs Width: 목표 coverage(예: 90%)를 달성하면서 구간 폭을 최소화. Calibration plot: nominal vs empirical coverage 곡선 확인. Shift robustness: OOD 상황에서 coverage 붕괴 가능 → 위험구간 탐지, abstention, 최근접-밀도 기반 보완 고려. 메소드 선택 가이드 데이터가 크고 분포 가정이 약함: Conformal(CQR) 우선 Heteroscedastic 뚜렷: Quantile Regression(+ conformal calibration) Ensemble을 이미 사용: Jackknife+/Bootstrap+로 conformalize Probabilistic modeling 선호/가능: Bayesian posterior predictive",
    "public": true
  },
  {
    "id": "Data Science/Statistics/distribution-distance-implementation_notebook",
    "title": "분포간 차이 구현 노트북",
    "date": "2025-08-12",
    "excerpt": "분포간 차이 구현 노트북",
    "category": "Data Science",
    "tags": [
      "KDE",
      "Distribution Distance",
      "KL",
      "JS",
      "Wasserstein"
    ],
    "content": "--- title: \"분포간 차이 구현 노트북\" date: \"2025-08-12\" category: \"Data Science\" tags: [\"KDE\", \"Distribution Distance\", \"KL\", \"JS\", \"Wasserstein\"] excerpt: \"분포간 차이 구현 노트북\" --- 기본적인 설정들 notebook kernel : docker jupyter lab kernel 사용 실험내용 > 분포 간에 차이를 측정할 떄 사용하는 지표 구현 Kullback-Leibler Divergence Jensen-Shannon Divergence Wasserstein Distance KDE를 통해 분포 추정 (사이즈별) 분포 간 차이 측정 # Estimate Density Function from sklearn.neighbors import KernelDensity from sklearn.preprocessing import StandardScaler import numpy as np 자동 대역폭 선택을 위한 함수 def auto_bandwidth(data): \"\"\" Silverman의 규칙을 사용하여 자동 대역폭 선택 \"\"\" n = len(data) std = np.std(data) # Silverman's rule of thumb bandwidth = 1.06 std (n * (-1 / 5)) return bandwidth def kde_density_estimation(kde_data, bandwidth=20, kernel=\"gaussian\"): \"\"\" KDE를 사용하여 밀도 추정 \"\"\" kde = KernelDensity(bandwidth=bandwidth, kernel=kernel) kde.fit(kde_data) density_values = np.exp(kde.score_samples(kde_data)) return kde, density_values scaler = StandardScaler() kde_data = data.drop([\"size\"], axis=1) normalized_data = scaler.fit_transform(kde_data) auto_bw = auto_bandwidth(normalized_data) print(auto_bw) kde, density_values = kde_density_estimation(normalized_data, bandwidth=auto_bw, kernel=\"gaussian\") print(density_values.shape) type(normalized_data) 추정한 밀도 분포 통계값 뽑아서 확인해보기 # 밀도 값들의 기본 통계 확인 print(\"=== 밀도 값 현황 ===\") print(f\"밀도 값 개수: {len(density_values)}\") print(f\"최소값: {np.min(density_values):.6f}\") print(f\"최대값: {np.max(density_values):.6f}\") print(f\"평균값: {np.mean(density_values):.6f}\") print(f\"중간값: {np.median(density_values):.6f}\") print(f\"표준편차: {np.std(density_values):.6f}\") print() 0.1보다 큰 값이 있는지 확인 above_threshold = density_values[density_values > 0.1] print(f\"0.1보다 큰 값 개수: {len(above_threshold)}\") print(f\"0.1보다 큰 값들: {above_threshold}\") 특정 포인트에 대한 밀도 값을 해석하는 방법 중 하나 def interpret_density_value(density_values, target_density): \"\"\" 특정 밀도 값의 의미 해석 \"\"\" print(f\"=== 밀도 값 {target_density} 해석 ===\") # 1. 상대적 위치 percentile = np.mean(density_values <= target_density) 100 print(f\"상대적 위치: 하위 {percentile:.1f}%\") # 2. 일반성 정도 if percentile < 25: generality = \"매우 특이함\" elif percentile < 50: generality = \"특이함\" elif percentile < 75: generality = \"일반적\" else: generality = \"매우 일반적\" print(f\"일반성 정도: {generality}\") # 3. 유사한 밀도를 가진 데이터 수 similar_count = np.sum(np.abs(density_values - target_density) < 0.005) print(f\"유사한 밀도 데이터: {similar_count}개\") return percentile, generality 해석 실행 percentile, generality = interpret_density_value(density_values, 0.02) # 사이즈 별 분포 차이 확인하기 # 1. 사이즈별 밀도 추정 import numpy as np from scipy.stats import entropy from sklearn.preprocessing import StandardScaler from src.utils.dist_diff import auto_bandwidth, kde_density_estimation scaler = StandardScaler() data_12_1 = data.loc[data[\"size\"] == \"12.1\"].drop([\"size\"], axis=1) data_12_1 = scaler.fit_transform(data_12_1) scaler = StandardScaler() data_12_6 = data.loc[data[\"size\"] == \"12.6\"].drop([\"size\"], axis=1) data_12_6 = scaler.fit_transform(data_12_6) scaler = StandardScaler() data_13_2 = data.loc[data[\"size\"] == \"13.2\"].drop([\"size\"], axis=1) data_13_2 = scaler.fit_transform(data_13_2) kde_12_1, density_values_12_1 = kde_density_estimation(data_12_1, bandwidth=auto_bandwidth(data_12_1), kernel=\"gaussian\") kde_12_6, density_values_12_6 = kde_density_estimation(data_12_6, bandwidth=auto_bandwidth(data_12_6), kernel=\"gaussian\") kde_13_2, density_values_13_2 = kde_density_estimation(data_13_2, bandwidth=auto_bandwidth(data_13_2), kernel=\"gaussian\") # 2. KL Divergence 계산 np.random.seed(42) epsilon = 1e-10 p_hist = np.clip(density_values_12_1, epsilon, None) q_hist = np.clip(density_values_12_6, epsilon, None) q_hist = np.random.choice(q_hist, size=p_hist.shape[0]) # 사이즈 맞춰주기 위해서 kl_value = entropy(p_hist, q_hist) # scipy의 entropy가 KL divergence 계산 print(\"KL Divergence (12.1 vs 12.6):\", kl_value) kl_value = entropy(q_hist, p_hist) # scipy의 entropy가 KL divergence 계산 print(\"KL Divergence (12.6 vs 12.1):\", kl_value) KL Divergence 특징. 순서 바꾸면 값이 달라짐 noise에 취약함 혹은 민감함 # 3. JS Divergence 계산 import numpy as np from scipy.stats import entropy def js_divergence(p: np.ndarray, q: np.ndarray) -> float: \"\"\" Jensen-Shannon Divergence 계산 Args: p, q: 두 확률 분포 (1차원 배열) Returns: float: JS Divergence 값 \"\"\" # 배열 길이 맞추기 min_length = min(len(p), len(q)) p = np.random.choice(p, min_length) q = np.random.choice(q, min_length) # 확률 분포로 정규화 (합이 1이 되도록) p_norm = p / np.sum(p) q_norm = q / np.sum(q) # 중간 분포 m = (p + q) / 2 m = (p_norm + q_norm) / 2 # JS Divergence = (KL(p m) + KL(q m)) / 2 js_div = (entropy(p_norm, m) + entropy(q_norm, m)) / 2 return js_div 사용 예시 js_div_value = js_divergence(p_hist, q_hist) print(f\"JS Divergence: {js_div_value:.6f}\") # 4. JS distance 계산 def js_distance(p: np.ndarray, q: np.ndarray) -> float: \"\"\" Jensen-Shannon Distance 계산 Args: p, q: 두 확률 분포 (1차원 배열) Returns: float: JS Distance 값 (0~1 범위) \"\"\" # JS Divergence 계산 js_div = js_divergence(p, q) # JS Distance = sqrt(JS Divergence) js_dist = np.sqrt(js_div) return js_dist 사용 예시 js_dist_value = js_distance(p_hist, q_hist) print(f\"JS Distance: {js_dist_value:.6f}\") JS Distance는 0~1 범위 print(\"=== JS Distance 해석 가이드 ===\") if js_dist_value < 0.1: print(\"0.0 ~ 0.1: 매우 유사한 분포 (거의 동일)\") elif js_dist_value < 0.2: print(\"0.1 ~ 0.2: 유사한 분포 (비슷함)\") elif js_dist_value < 0.3: print(\"0.2 ~ 0.3: 약간 다른 분포 (차이가 있음)\") elif js_dist_value < 0.5: print(\"0.3 ~ 0.5: 상당히 다른 분포 (명확한 차이)\") elif js_dist_value < 0.7: print(\"0.5 ~ 0.7: 매우 다른 분포 (큰 차이)\") else: print(\"0.7 ~ 1.0: 완전히 다른 분포 (극도로 다름)\") from scipy.stats import wasserstein_distance def wasserstein_distance_scipy(p: np.ndarray, q: np.ndarray) -> float: \"\"\" scipy를 사용한 Wasserstein Distance 계산 수식: W(p,q) = Σ x_i - y_i / n \"\"\" return wasserstein_distance(p, q) wasserstein_distance(p_hist, q_hist)",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-symbolic-regressor-model",
    "title": "Symbolic Regressor Model이란?",
    "date": "2025-08-11",
    "excerpt": "Symbolic Regressor Model의 개념과 활용되는 사례에 대한 정리",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "statistics"
    ],
    "content": "Symbolic Regressor Model이란? --- 참고자료 Wikipedia - Symbolic regression PySR: High-Performance Symbolic Regression in Python gplearn (Genetic Programming in Python) [Koza (1992) Genetic Programming] 개념 요약 Symbolic Regression은 데이터로부터 “함수 형태(functional form)” 자체를 탐색해 발견하는 방법. 즉, 고정된 모델 형태에 파라미터만 맞추는 것이 아니라, operators (e.g., +, −, ×, ÷, sin, exp, log)와 variables를 조합해 interpretable equations를 찾는다. 목표: predictive accuracy와 parsimony 사이의 균형을 맞춘 interpretable model 발견 핵심 아이디어: operators/constants/variables로 이뤄진 tree-structured expressions를 탐색(주로 Genetic Programming)하여 loss를 최소로 하는 수식 찾기 어떻게 작동하나 (전형적 파이프라인) 1) Expression space 정의: 가능한 operator set, constants, variables 목록 정의 2) Initialize expressions: 무작위로 여러 expression trees 생성 3) Evaluate objective: data error (e.g., MSE)와 complexity (예: node count)를 함께 고려 4) Search (evolution): selection, crossover, mutation으로 더 좋은 수식을 점진적으로 탐색 5) Pareto frontier: accuracy와 complexity 간 최적 절충의 후보를 여러 개 유지 loss function은 일반적으로 다음과 같이 표현할 수 있다. $$ \\mathcal{L}(f) = \\underbrace{\\frac{1}{N} \\sum_{i=1}^N (y_i - f(x_i))^2}_{\\text{prediction error}} \\; + \\; \\lambda \\cdot \\underbrace{\\text{Complexity}(f)}_{\\text{expression complexity}} $$ 장점과 단점 장점 Interpretability: 결과가 명시적 수식이라 설명이 쉽다 Expressiveness: 비선형 관계를 유연하게 포착 Knowledge discovery: 데이터에 숨겨진 규칙이나 법칙을 드러내기 좋음 단점 Computational cost: 표현식 공간이 방대, 탐색 비용이 큼 Overfitting risk: operators/maximum depth를 과도하게 허용하면 노이즈에 적합 Reproducibility: 진화적 탐색은 난수 시드에 민감할 수 있음 언제 유용한가 관계를 설명할 “closed-form equation”이 필요한 경우 (과학/공학 모델링) feature interactions가 복잡해 전통적 선형모델이 한계일 때 black-box 성능도 중요하지만, interpretability와 parsimony가 핵심일 때 간단한 예시 예를 들어, 데이터가 다음과 같은 규칙을 따른다고 하자: $$ y = 2\\,\\sin(x) + 0.1\\,x^2 + \\varepsilon $$ Symbolic Regression은 위와 유사한 형태의 수식을 데이터만 보고 발견하려 시도한다. 실제로는 coefficients나 operator 선택이 조금 다르게 나올 수 있다. Python으로 맛보기 (PySR) ``python pip install pysr import numpy as np from pysr import PySRRegressor rng = np.random.default_rng(42) X = rng.uniform(-3.0, 3.0, size=(200, 1)) y = 2 np.sin(X[:, 0]) + 0.1 (X[:, 0] * 2) + rng.normal(0, 0.1, size=200) model = PySRRegressor( niterations=200, unary_operators=(\"sin\", \"cos\", \"exp\", \"log\"), binary_operators=(\"+\", \"-\", \"\", \"/\"), loss=\"loss(x, y) = (x - y)^2\", maxsize=20, populations=20, progress=False, random_state=42, ) model.fit(X, y) print(model) # 파레토 프론티어 후보 요약 print(model.sympy()) # 최고 성능/단순성 절충 식 하나 출력 ` 모델은 accuracy-complexity 절충을 고려한 여러 수식을 제시한다. 예를 들어 다음과 같은 형태가 나올 수 있다. $$ \\hat{y} = 1.98\\,\\sin(x) + 0.12\\,x^2 $$ 실무 팁 operator set을 도메인 지식으로 제한하면 탐색 효율과 해석 가능성이 크게 개선 expression complexity 제약(maxsize, operator weights)을 강하게 주어 overfitting 방지 cross-validation으로 일반화 성능 점검, random seed 고정으로 reproducibility 관리 탐색 시간 예산을 단계적으로 늘려가며 Pareto frontier를 비교 관련 모델/개념 Genetic Programming Interpretable ML Sparse modeling(L1/L0)과의 비교: 전자는 주어진 형태 내 계수 희소화, SR은 형태 자체를 탐색 Genetic Programming(GP)란? Genetic Programming은 evolutionary algorithm 계열로, 프로그램 또는 수식(expression)을 직접 “진화”시켜서 주어진 목적함수(fitness)를 최대화/최소화하는 기법이다. Symbolic Regression에서는 operators, variables, constants로 구성된 expression tree를 개체로 취급하여 탐색한다. Expression representation: tree-structured expressions (nodes = operators, leaves = variables/constants) Search operators: Selection: tournament selection 등으로 상위 개체 선별 Crossover: subtree 단위 교차로 두 식을 결합 Mutation: point/subtree/hoist mutation으로 식 일부를 무작위 변이 Objective(Fitness): 예측 오차(MSE 등)와 complexity(노드 수, 깊이 등)를 함께 고려 $$ \\min_f \\; (\\, \\text{MSE}(f),\\; \\text{Complexity}(f) \\,) $$ Key hyperparameters: population size, generations, max depth/size, crossover rate, mutation rate, tournament size, parsimony coefficient Pros/Cons: Pros: 매우 유연한 가설 공간, 높은 해석 가능성, 변수 상호작용 자동 발견 Cons: 계산 비용 큼, expression bloat(불필요하게 커짐) 위험, 확률적 변동성 Mitigation: parsimony pressure(복잡도 패널티), depth/size cap, 도메인 priors로 초기 개체 시딩 관계 정리: GP는 SR에서 가장 널리 쓰이는 search backbone이며, 대안으로 simulated annealing, MCMC, reinforcement learning 기반 탐색 등이 활용되기도 한다. Genetic Programming 상세 설명 Representation (tree-based expressions): 각 개체는 expression tree로 표현. 내부 노드 = operators(예: +, −, ×, ÷, sin, exp, log), 리프 = variables/constants(ephemeral random constants 포함). Function/Terminal set 설계: 도메인 지식으로 operator 집합과 변수, 상수 범위를 제한하면 탐색 효율과 해석 가능성이 향상. Initialization (ramped half-and-half): full/grow 방식을 섞어 다양한 깊이/구조의 초기 개체를 생성. Selection (tournament 등): 여러 개체를 무작위로 뽑아 최적 개체를 선택. 간단하고 병렬화 용이. Variation operators: Crossover: 두 부모의 subtree를 교환해 자식 생성 Mutation: point/subtree/hoist mutation으로 일부 구조를 무작위 변경 Reproduction: 우수 개체를 그대로 다음 세대로 복사(elitism와 함께 사용) Fitness/objective: 보통 예측 오차(MSE, MAE 등)를 최소화하면서 complexity(size, depth, description length 등)도 함께 최소화하는 multi-objective 관점 $$ \\min_f \\; \\Big[\\; \\text{Error}(f),\\; \\text{Complexity}(f) \\;\\Big] $$ Scalarization (Parsimony pressure): multi-objective를 단일 목적로 합쳐 penalty를 주기도 함 $$ \\text{Fitness}_{\\text{penalized}}(f) = \\text{MSE}(f) + \\alpha\\,\\text{size}(f) $$ Non-dominated sorting (예: NSGA-II): Pareto frontier 기반으로 dominance 관계로 순위를 매겨 다양한 절충해를 유지 Bloat control: expression bloat 방지용으로 max depth/size, lexicographic parsimony pressure, size-fair crossover 등을 사용 Constants 처리: ephemeral random constants(무작위 상수)를 두고, 필요 시 local search/gradient-free tuning으로 계수 미세 조정 Typing/Grammar constraints: strongly typed GP나 grammar-guided GP로 유효한 수식만 생성하도록 제한 Validation/Elitism: validation 성능으로 조기 종료와 model pick을 안정화, 우수 개체를 다음 세대에 보존 간단 알고리즘 스케치 `text Initialize population P0 with ramped half-and-half for gen in 1..G: Evaluate Error(f) and Complexity(f) for all f in P_{gen-1} Update Pareto frontier with elitism while P_gen < population_size: Select parents via tournament selection With prob pc: offspring = crossover(parent1, parent2) With prob pm: offspring = mutate(offspring) Enforce size/depth constraints on offspring Add offspring to P_gen Return best model on validation (accuracy-parsimony trade-off) ` 실무 셋업 팁 function set: 도메인에 맞는 연산자만(예: 안전한 division, log1p) 사용해 수식 안정성 확보 constraints: max depth/size, safe operators, domain bounds로 탐색 공간 제어 objectives: validation error + complexity를 함께 모니터링(또는 parsimony penalty) search budget: generations, population size를 점진적으로 늘리며 Pareto set 비교 reproducibility: random seed 고정, 결과 수식은 심플한 것부터 우선 검토 PySR와의 연결 PySR는 multi-population evolutionary search와 parsimony pressure를 활용해 빠르게 Pareto frontier를 구축 하이퍼파라미터 예: niterations, maxsize, binary_operators, unary_operators, loss, populations gplearn로 구현하기 (SymbolicRegressor) gplearn은 scikit-learn 스타일의 Symbolic Regression/Transformation을 제공한다. 기본 연산자는 protected operators를 사용해 div(0 나눗셈 보호), log(음수/0 보호) 등 수치 안정성을 확보한다. Quick start `python pip install gplearn import numpy as np from gplearn.genetic import SymbolicRegressor rng = np.random.default_rng(42) X = rng.uniform(-3.0, 3.0, size=(200, 1)) y = 2 np.sin(X[:, 0]) + 0.1 (X[:, 0] 2) + rng.normal(0, 0.1, size=200) est = SymbolicRegressor( population_size=2000, generations=30, function_set=(\"add\", \"sub\", \"mul\", \"div\", \"sin\", \"cos\", \"log\", \"sqrt\"), metric=\"mse\", parsimony_coefficient=0.001, p_crossover=0.8, p_subtree_mutation=0.05, p_hoist_mutation=0.01, p_point_mutation=0.05, init_depth=(2, 5), init_method=\"half and half\", const_range=(-2.0, 2.0), tournament_size=20, stopping_criteria=0.0, max_samples=0.9, n_jobs=-1, random_state=42, verbose=1, ) est.fit(X, y) print(est._program) # 발견된 최종 수식 문자열 print(est.predict(X)[:5]) ` 주요 인자 설명 population_size: 세대당 개체 수. 클수록 탐색 폭↑, 시간/메모리 비용↑ generations: 세대 수. 예산(budget)과 함께 조절 function_set: 사용 연산자 세트. 도메인에 맞게 축소 권장. div, log, sqrt 등은 protected 버전 metric: \"mse\", \"mae\" 또는 사용자 정의 함수 가능 parsimony_coefficient: 모델 크기(size)에 대한 패널티 강도. 과적합/블로트 완화 p_crossover / p_subtree_mutation / p_hoist_mutation / p_point_mutation: variation 확률. 교차/서브트리/호이스트/포인트 변이 비율 init_depth, init_method: 초기 트리 깊이/방식(\"full\", \"grow\", \"half and half\") const_range: ephemeral random constants 범위. None이면 상수 비활성화 tournament_size: selection 압력 강도. 클수록 exploitation↑, 다양성↓ stopping_criteria: metric이 임계값 이하가 되면 조기 종료 max_samples: 각 세대 학습에 사용하는 샘플 비율(bootstrap/bagging 효과) n_jobs: 병렬 처리 수. -1은 CPU 전부 사용 random_state: 재현성 verbose: 진행 로그 출력 참고: SymbolicTransformer는 특징 생성기처럼 새 feature들을 만들어 downstream 모델에 사용할 수 있음 실무 권장 설정 연산자: 도메인에 맞는 최소 집합으로 시작하고 필요 시 추가 패널티: parsimony_coefficient를 충분히 크게 설정해 단순한 식 우선 제약: init_depth, const_range, tournament_size를 완만히 조정하며 overfitting/bloat 균형 검증: validation 성능으로 best program을 선택하고 seed 고정 gplearn 실험 가이드: 하이퍼파라미터 셋업 빠른 베이스라인 권장값 Small data (< 10k rows): population_size=2000, generations=30 function_set=(\"add\",\"sub\",\"mul\",\"div\",\"sin\",\"cos\",\"log\",\"sqrt\") parsimony_coefficient=0.001 p_crossover=0.8, p_subtree_mutation=0.05, p_point_mutation=0.05, p_hoist_mutation=0.01 init_depth=(2,5), init_method=\"half and half\", const_range=(-2,2) tournament_size=20, metric=\"mse\", max_samples=1.0, n_jobs=-1, random_state=42 Medium data (10k–100k rows): population_size=3000–5000, generations=50–100 parsimony_coefficient=0.003–0.01 (bloat 발생 시 상향) max_samples=0.7–0.9로 약한 bagging 적용해 generalization 향상 주요 하이퍼파라미터 상호작용 population_size × generations: 탐색 예산. 둘 다 증가하면 성능 잠재력↑, 시간/메모리 비용↑ tournament_size: selection 압력. 크게 하면 exploitation↑, 다양성↓ → 너무 크면 조기 수렴 위험 parsimony_coefficient: size penalty 강도. 클수록 단순한 식 선호, 과적합/블로트 완화 function_set: 도메인 맞춤 축소 권장. 불필요 연산자 제거로 탐색 공간 축소, 해석성↑ variation 확률: p_crossover는 높게, mutation 비율은 작게 시작. 정체 시 mutation 소폭↑ init_depth / init_method: 초기 다양성에 영향. 너무 얕으면 탐색 부족, 너무 깊으면 초기에 bloat max_samples: 1.0 미만이면 세대별 subsampling으로 bagging 유사 효과 → 과적합 완화 안정성과 전처리 gplearn의 div, log, sqrt는 protected operators라 수치 안정성 양호 feature scaling은 필수는 아니지만 탐색 안정화에 도움(표준화/정규화 권장) 타깃 스케일도 식 복잡도에 영향. 필요 시 타깃 스케일링 후 추론 시 역변환 Validation과 재현성 train/validation 분리 후 validation MSE로 설정 비교 random_state 고정, 결과 수식 길이와 성능을 함께 기록해 단순한 식 우선 `python from sklearn.model_selection import train_test_split from gplearn.genetic import SymbolicRegressor X_tr, X_val, y_tr, y_val = train_test_split(X, y, test_size=0.2, random_state=42) est = SymbolicRegressor( population_size=3000, generations=50, function_set=(\"add\",\"sub\",\"mul\",\"div\",\"sin\",\"cos\",\"log\",\"sqrt\"), metric=\"mse\", parsimony_coefficient=0.003, p_crossover=0.8, p_subtree_mutation=0.05, p_hoist_mutation=0.01, p_point_mutation=0.05, init_depth=(2,5), init_method=\"half and half\", const_range=(-2,2), tournament_size=20, max_samples=0.9, n_jobs=-1, random_state=42, ) est.fit(X_tr, y_tr) val_mse = ((est.predict(X_val) - y_val) 2).mean() ` Custom function 추가하기 make_function으로 도메인 친화적 연산자를 추가하면 탐색 효율이 크게 향상될 수 있다. `python import numpy as np from gplearn.functions import make_function def safe_exp(x): return np.exp(np.clip(x, -10, 10)) # 폭주 방지 Exp = make_function(function=safe_exp, name=\"exp\", arity=1) est = SymbolicRegressor( function_set=(\"add\",\"sub\",\"mul\",\"div\",\"sin\",\"cos\",\"log\",\"sqrt\", Exp), # ... 기타 하이퍼파라미터 ... ) ` 모니터링과 블로트 제어 verbose=1로 진행 상황 모니터링, 수식 길이가 과도하면 parsimony_coefficient 상향 필요 시 function_set 축소 또는 tournament_size 하향으로 다양성 회복 최종 식은 est._program`으로 문자열 형태 확인 가능",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-genetic-programming",
    "title": "Genetic Programming이란?",
    "date": "2025-08-11",
    "excerpt": "Genetic Programming의 개념과 활용되는 사례에 대한 정리",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "statistics"
    ],
    "content": "Genetic Programming(GP)이란? --- 참고자료 [Koza (1992) Genetic Programming] Wikipedia - Genetic programming DEAP: Distributed Evolutionary Algorithms in Python gplearn: Genetic Programming in Python PySR: High-Performance Symbolic Regression in Python 개념 요약 > Genetic Programming은 evolutionary algorithm의 한 형태로, 개체를 고정 길이 벡터가 아니라 “프로그램/수식(expression)” 그 자체로 간주하고 selection, crossover, mutation 같은 연산을 통해 점진적으로 개선하는 방법이다. > 특히 tree-structured representation을 사용한 수식 탐색에 널리 쓰이며, Symbolic Regression의 핵심 backbone으로 활용된다. 목표: 주어진 목적함수(fitness)를 최소화/최대화하는 program을 자동으로 발견 표현(Representation): 보통 tree로 표현된 expression(내부 노드=operators, 리프=variables/constants) 탐색(Search): population 기반 evolutionary search (selection → crossover/mutation → replacement) 다목적 최적화: 정확도와 복잡도(해석성) 사이의 절충을 함께 고려 목적함수와 다목적 관점 예측 문제에서 흔한 설정은 예측 오차와 표현 복잡도를 동시에 줄이는 것이다. $$ \\min_f \\; \\Big[\\; \\text{Error}(f),\\; \\text{Complexity}(f) \\;\\Big] $$ 단일 스칼라로 합치는 방식(parsimony pressure)도 많이 쓴다. $$ \\text{Fitness}_{\\text{penalized}}(f) = \\text{Error}(f) + \\lambda\\,\\text{size}(f) $$ 여기서 size(f)는 노드 수, 트리 깊이, description length 등으로 정의한다. \\lambda는 복잡도 패널티 강도를 조절한다. 또는 NSGA-II 같은 non-dominated sorting으로 Pareto frontier를 유지한다. 전형적 파이프라인 1) Function/Terminal set 정의: 사용할 operators(+, −, ×, ÷, sin, exp, log, ...), variables, constants 범위 설정 2) 초기 개체 생성: ramped half-and-half로 다양한 깊이/모양의 트리를 만들기 3) Fitness 평가: train(또는 CV)에서 Error와 Complexity 산출 4) Selection: tournament 등으로 부모 선택, elitism으로 최상위 보존 5) Variation: subtree crossover, mutation(point/subtree/hoist)로 새 개체 생성 6) 제약/안전장치: max depth/size, safe operators(0-division 보호 등) 7) 세대 반복: budget(세대 수, population size) 내에서 반복, validation 기준으로 best 선택 핵심 연산자와 설계 요소 Selection: tournament selection이 단순하면서도 효과적. tournament size가 클수록 exploitation↑, 다양성↓ Crossover (subtree): 두 부모의 서브트리를 교환해 자식 생성. 구조적 탐색의 핵심 Mutation: point/subtree/hoist mutation으로 부분 구조를 무작위 변화해 다양성 확보 Elitism/Reproduction: 우수 개체를 보존해 퇴행 방지 Bloat control: 표현이 불필요하게 커지는 bloat 방지용으로 max depth/size, lexicographic parsimony, size-fair crossover 등을 사용 Ephemeral Random Constants(ERC): 트리 말단에 난수 상수를 두고 필요 시 지역 탐색으로 미세 조정 Typing/Grammar constraints: strongly typed GP나 grammar-guided GP로 유효한 프로그램만 생성하도록 제한 대표 변형들 Tree-based GP: 가장 표준적 형태. Symbolic Regression에 광범위 사용 Strongly-Typed GP: 노드별 타입 제약으로 불법 표현 방지, 탐색 안정성↑ Grammar-Guided GP (GGGP): BNF/CFG 등 문법으로 합법 프로그램만 생성 Linear GP / Cartesian GP: 비트스트림 또는 graph-like 표현으로 코드/회로 형태 탐색 간단 알고리즘 스케치 ``text Initialize population P0 with ramped half-and-half for gen in 1..G: Evaluate Error(f) and Complexity(f) for all f in P_{gen-1} Update Pareto frontier with elitism while P_gen < population_size: Select parents via tournament selection With prob pc: offspring = crossover(parent1, parent2) With prob pm: offspring = mutate(offspring) Enforce size/depth constraints on offspring Add offspring to P_gen Return best model on validation (accuracy-parsimony trade-off) ` Python 예시 1: gplearn으로 Symbolic Regression `python pip install gplearn import numpy as np from gplearn.genetic import SymbolicRegressor rng = np.random.default_rng(42) X = rng.uniform(-3.0, 3.0, size=(200, 1)) y = 2 np.sin(X[:, 0]) + 0.1 (X[:, 0] * 2) + rng.normal(0, 0.1, size=200) est = SymbolicRegressor( population_size=2000, generations=30, function_set=(\"add\", \"sub\", \"mul\", \"div\", \"sin\", \"cos\", \"log\", \"sqrt\"), metric=\"mse\", parsimony_coefficient=0.001, p_crossover=0.8, p_subtree_mutation=0.05, p_hoist_mutation=0.01, p_point_mutation=0.05, init_depth=(2, 5), init_method=\"half and half\", const_range=(-2.0, 2.0), tournament_size=20, n_jobs=-1, random_state=42, ) est.fit(X, y) print(est._program) ` Python 예시 2: DEAP으로 최소 구현 스케치 아래는 DEAP으로 매우 축약된 Symbolic Regression 골격이다. 실제로는 protected operators, depth 제약, 평가/선택 루프를 더 보강해야 한다. `python pip install deap import operator import numpy as np from deap import base, creator, gp, tools, algorithms Function/Terminal set pset = gp.PrimitiveSet(\"MAIN\", arity=1) pset.addPrimitive(operator.add, 2) pset.addPrimitive(operator.sub, 2) pset.addPrimitive(operator.mul, 2) pset.addPrimitive(np.sin, 1) pset.addEphemeralConstant(\"rand\", lambda: np.random.uniform(-1, 1)) pset.renameArguments(ARG0=\"x\") Fitness/Individual creator.create(\"FitnessMin\", base.Fitness, weights=(-1.0,)) creator.create(\"Individual\", gp.PrimitiveTree, fitness=creator.FitnessMin) toolbox = base.Toolbox() toolbox.register(\"expr\", gp.genHalfAndHalf, pset=pset, min_=1, max_=3) toolbox.register(\"individual\", tools.initIterate, creator.Individual, toolbox.expr) toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual) Data rng = np.random.default_rng(0) X = rng.uniform(-3, 3, size=200) y = 2 np.sin(X) + 0.1 * (X 2) def eval_ind(individual): func = gp.compile(expr=individual, pset=pset) y_hat = np.array([func(x) for x in X]) return ((y_hat - y) 2).mean(), toolbox.register(\"evaluate\", eval_ind) toolbox.register(\"select\", tools.selTournament, tournsize=5) toolbox.register(\"mate\", gp.cxOnePoint) toolbox.register(\"mutate\", gp.mutUniform, expr=toolbox.expr, pset=pset) toolbox.decorate(\"mate\", gp.staticLimit(key=len, max_value=17)) toolbox.decorate(\"mutate\", gp.staticLimit(key=len, max_value=17)) pop = toolbox.population(n=200) hof = tools.HallOfFame(1) algorithms.eaSimple(pop, toolbox, cxpb=0.8, mutpb=0.2, ngen=30, halloffame=hof, verbose=False) print(hof[0]) ` 장점과 단점 장점 표현력: 모델 형태 자체를 탐색하므로 비선형/상호작용을 유연하게 포착 해석성: 결과가 수식/규칙이면 설명과 지식 발견에 유리 도메인 priors 통합: operators/grammar 제약으로 도메인 지식 주입 가능 단점 계산 비용: 탐색 공간이 매우 크고 세대 반복이 필요 Bloat/과적합: 제약과 패널티 없으면 표현이 비대해지고 노이즈 적합 위험 재현성: 확률적 탐색 특성상 난수 시드, 설정에 민감 언제 유용한가 식 형태가 중요한 과학/공학 모델링(equation discovery) 복잡한 feature interaction을 설명 가능한 형태로 포착하고 싶을 때 규칙/프로그램 합성이 필요한 컨트롤/휴리스틱 설계 Feature engineering(SymbolicTransformer)로 해석 가능한 새 특징 생성 실무 팁 Function set 최소화: 도메인 친화적 operators만 두고 시작 → 탐색 공간 축소, 해석성 향상 안전한 연산자: protected division/log/sqrt, bound-check로 수치 안정성 확보 복잡도 제약: max depth/size`, parsimony penalty, size-fair crossover로 bloat 제어 Validation 관리: train/validation 분리, Pareto set에서 단순한 식 우선 검토 Search budget 점증: generations/population을 점진적으로 늘리며 개선 추적 Seed 고정 및 로깅: reproducibility를 위해 random state와 결과 수식/길이/성능 기록 관련 키워드 Symbolic Regression, Interpretable ML, Program Synthesis 구현 라이브러리: DEAP, gplearn(SymbolicRegressor/Transformer), PySR",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/what-is-redis",
    "title": "Redis란?",
    "date": "2025-08-11",
    "excerpt": "Redis의 개념과 활용되는 사례에 대한 정리",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "statistics"
    ],
    "content": "인메모리 데이터 저장소: 데이터를 RAM에 저장해 마이크로초~밀리초 단위 응답을 제공 키-값 구조: 키로 빠르게 조회/갱신하는 단순하고 효율적인 데이터 모델 다양한 용도 캐싱: 자주 조회되는 결과를 저장해 DB 부하와 지연시간을 줄임 세션 관리: 로그인 세션/토큰 등 상태 정보를 TTL로 안전하게 관리 메시지 큐: 비동기 작업 처리와 Producer-Consumer 패턴 구현 NoSQL 데이터베이스: 스키마 유연성, 다양한 자료구조, 수평 확장 용이 오픈 소스: BSD 라이선스 기반으로 커뮤니티와 생태계가 활발 TTL이란? TTL(Time To Live)은 키에 설정하는 유효기간입니다. 만료 시간이 지나면 키는 자동으로 삭제됩니다. 설정: SET key val EX <sec> 또는 PX <ms>, EXPIRE key <sec>, PEXPIRE key <ms> 해제: PERSIST key (만료 제거) 조회: TTL key(초), PTTL key(ms). -1은 만료 없음, -2는 키 없음 동작: Redis는 지연(lazy) + 주기적(active) 스캔으로 키를 만료 처리 참고: 만료(expiration)는 시간 기반 삭제, eviction은 메모리 압박 시 정책에 따른 제거 ``bash SET token:123 abc EX 60 # 60초 후 만료 TTL token:123 # 남은 초 확인 PERSIST token:123 # 만료 제거 ` 한 줄 요약 매우 빠른 in-memory data store로서, 캐시/세션/큐/순위표/레이트 리미팅 등 실시간성이 중요한 문제에 탁월합니다. 영속성과 고가용성 기능을 갖췄지만, 전통적인 RDBMS의 완전한 대체재는 아닙니다. 개요 Redis는 in-memory key-value data structure store로, 매우 낮은 지연시간과 높은 처리량을 제공하는 캐시(Cache), 세션 저장소(Session Store), 메시지 브로커(Message Broker) 용도로 널리 쓰입니다. 단일 스레드 기반 event loop로 동작하지만 I/O multiplexing을 활용해 고성능을 달성합니다. 원하는 경우 디스크에 영속화(RDB/AOF)도 지원합니다. 주요 특징 In-memory: 메모리에 데이터를 보관해 마이크로초~밀리초 단위 응답 Rich data structures: String, List, Set, Sorted Set(ZSet), Hash, Stream, HyperLogLog, Bitmap, Geospatial TTL/Expiration: key 단위 만료와 다양한 eviction 정책(LRU/LFU 등) Persistence: RDB(snapshot), AOF(append only) 또는 혼합 사용 Scalability/HA: Replication, Sentinel(자동 failover), Cluster(16384 hash slots) Atomic ops & Lua: 단일 명령 원자성, EVAL을 통한 서버 측 Lua 스크립팅 핵심 자료구조 한눈에 String: 카운터, 토큰, 플래그 등 단순 값 저장 Hash: 사용자 프로필 같은 field-value 맵 List: 큐/스택, 작업 대기열, 로그 버퍼 Set: 고유 집합, 중복 제거, 교집합/합집합 연산 Sorted Set (ZSet): 점수 기반 정렬(리더보드, 순위표) Stream: 로그/이벤트 스트림, 소비자 그룹 기반 처리 대표 활용 시나리오 Cache: DB 질의/렌더링 결과 캐싱, TTL과 invalidation 전략 결합 Session Store: 웹 세션/인증 토큰 저장, 만료/연장 관리 Queue & Pub/Sub: 간단한 작업 큐(List/BRPOP) 또는 실시간 메시징(Stream, Pub/Sub) Rate Limiting: 요청 빈도 제어(카운터, 토큰 버킷), 분산 락(SET key val NX PX) Leaderboard/Ranking: ZSet으로 상위 N, 스코어 범위 질의 운용 포인트 Persistence: RDB: 주기적 스냅샷, 복구 빠름, 데이터 손실 가능성 있음(마지막 스냅샷 이후) AOF: 모든 write 로그, 내구성 우수, 파일 커질 수 있어 리라이트 필요 Eviction 정책: noeviction, allkeys-lru, allkeys-lfu 등 워크로드에 맞게 선택 Cluster: 키 해시 슬롯 기반 샤딩. 멀티키 연산은 같은 슬롯(= 같은 hash tag)에서만 가능 모듈: RediSearch(검색), RedisJSON(JSON 문서), RedisBloom(확률 자료구조), TimeSeries(시계열) 간단한 명령 예시 `bash String SET page:home \"rendered_html\" EX 60 GET page:home List queue (producer/consumer) LPUSH jobs \"task-1\" BRPOP jobs 5 Sorted Set leaderboard ZADD leaderboard 100 user:alice 120 user:bob ZREVRANGE leaderboard 0 1 WITHSCORES ` Python 예시(redis-py) `python import redis client = redis.Redis(host=\"localhost\", port=6379, decode_responses=True) cache set/get with TTL client.set(\"user:42:profile\", \"{\\\\\"name\\\\\": \\\\\\\"Alice\\\\\\\"}\", ex=300) profile = client.get(\"user:42:profile\") rate limiting (simple counter) key = \"rl:ip:203.0.113.1:60s\" pipe = client.pipeline() pipe.incr(key) pipe.expire(key, 60) count, _ = pipe.execute() if count > 100: print(\"Too many requests\") leaderboard client.zadd(\"leaderboard\", {\"user:alice\": 100, \"user:bob\": 120}) top = client.zrevrange(\"leaderboard\", 0, 9, withscores=True) print(top) `` 언제 피할까? 강한 트랜잭션 일관성과 복잡한 조인/쿼리가 필요한 경우(전통 RDBMS 권장) 대용량 장기 보관의 주 데이터베이스로 쓰기에는 비용/내구성 트레이드오프 고려 필요 Redis, 이러한 프로젝트에 적용하시면 유용합니다. 인메모리 데이터베이스로 작동하는 Redis는 데이터를 빠르게 처리하며 실시간 응답이 중요한 프로젝트에서 뛰어난 성능을 발휘하는데요, Redis를 활용하면 좋은 대표적인 프로젝트를 4가지로 정리해 보았습니다. 세션 관리 Redis는 사용자 세션 데이터를 메모리에 저장하여 빠르고 효율적인 세션 관리를 지원합니다. 사용자 인증 속도 향상: 로그인 시 사용자 데이터를 즉각적으로 조회할 수 있어 인증 속도를 크게 높입니다. 세션 만료 관리: TTL(Time to Live)을 설정해 불필요한 데이터가 메모리를 차지하지 않도록 자동으로 만료시킵니다. 이를 통해 메모리 사용을 효율적으로 관리할 수 있습니다. 다중 서버 환경 지원: Redis를 중앙 세션 저장소로 사용하면 여러 웹 서버 간 세션 데이터를 쉽게 공유할 수 있어 로드밸런싱 환경에서도 안정적인 세션 관리가 가능합니다. 캐싱 시스템 Redis는 자주 조회되는 데이터를 메모리에 캐싱 하여 데이터베이스의 부하를 줄이고 빠른 응답 속도를 제공합니다. 읽기 부하 분산: 제품 정보, 페이지 콘텐츠 같은 고빈도 읽기 요청을 Redis에 캐싱 해 데이터베이스의 읽기 부하를 감소시킵니다. 복잡한 연산 속도 향상: 상품 추천이나 검색 결과처럼 계산이 많은 데이터를 Redis에 저장해 클라이언트 요청 시 빠르게 반환할 수 있습니다. 만료 정책 지원: 캐싱 데이터에 TTL을 설정해 오래된 데이터를 자동으로 제거, 최신 상태를 유지할 수 있습니다. 실시간 분석 실시간 데이터 처리가 중요한 환경에서 빠른 데이터 조회와 집계 기능을 제공합니다. 대시보드 업데이트: 트래픽, 사용자 행동 데이터 같은 정보를 Redis에 저장해 실시간으로 대시보드에 반영할 수 있습니다. 실시간 통계 계산: Redis의 Sorted Set이나 Hash 구조를 활용해 사용자 활동 집계나 순위 계산을 즉시 처리할 수 있습니다. 이벤트 처리: 실시간으로 발생하는 이벤트 데이터를 저장하고 처리해 이상 징후 탐지, 트랜잭션 모니터링 같은 시스템에 활용됩니다. 메시지 큐 Redis의 Pub/Sub 기능은 실시간 메시지 전송과 수신을 간단하고 효율적으로 처리합니다. 실시간 알림 시스템: Redis를 활용하면 이메일 알림, 푸시 알림을 실시간으로 전송할 수 있습니다. 채팅 애플리케이션: 사용자 간 메시지를 Redis를 통해 즉시 전달해 끊김 없는 실시간 채팅 환경을 구현할 수 있습니다. 스트리밍 데이터 관리: Redis Streams 기능을 사용하면 실시간으로 데이터 스트리밍을 관리하고, 여러 소비자(Consumer)에게 데이터를 분배할 수 있어 확장성이 뛰어납니다. Redis는 인 메모리 데이터베이스로서 뛰어난 데이터 처리 능력을 자랑합니다. 높은 처리량과 영속성 옵션 덕분에 세션 관리, 캐싱 시스템, 실시간 분석, 메시지 큐와 같은 프로젝트에서 활용하면 더욱 효과적인 결과를 얻을 수 있습니다. 이어지는 내용에서는 Redis를 더욱 효과적으로 활용하는 방법에 대해 알아보겠습니다. Redis를 사용하기 좋은 경우 세션 관리: 로그인 세션/토큰을 빠르게 조회하고 TTL로 자동 만료가 필요할 때 캐싱 시스템: 자주 조회되는 데이터로 DB 부하를 줄이고 응답 시간을 단축해야 할 때 실시간 분석: 대시보드/통계/활동 집계를 ms~s 단위로 갱신해야 할 때 메시지 큐·스트리밍: 간단한 작업 큐나 실시간 알림/채팅/이벤트 스트림이 필요할 때 레이트 리미팅·쿼터: 사용자/ IP별 요청 빈도 제한을 원자 카운터+TTL로 구현할 때 랭킹·순위표: 점수 기반 상위 N, 범위 질의가 잦을 때(ZSet 적합) 임시 데이터: OTP/인증코드/단기 캐시 등 만료가 명확한 데이터를 다룰 때 초저지연 읽기: DB 앞단 캐시 계층으로 p95 응답 시간을 낮춰야 할 때",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/how-to-evaluate-model-in-selection",
    "title": "선택 문제에서 모델을 어떻게 평가할 것인가",
    "date": "2025-08-11",
    "excerpt": "선택 문제에서 모델을 평가하는 방법에 대한 정리",
    "category": "Causal Inference",
    "tags": [
      "evaluation"
    ],
    "content": "문제정의 <figure> <img src=\"/post/Causal_Inference/선택문제_평가_고민.png\" alt=\"선택 문제 평가 고민\" /> <figcaption>그림1. 선택의 문제에서 평가에 대한 고민</figcaption> </figure> 선택의 문제에서 모델의 예측 결과를 평가하는 일은 꽤나 어려운 일입니다. 왜냐하면 고객은 여러가지 선택 중에서 하나의 선택만 하기 때문이죠. 그림1은 제가 풀고 있는 문제를 시각화한 자료입니다. 시력교정 도메인에서 의사는 고객에게 최적화된 렌즈 크기를 선택하게 됩니다. 이때 선택에 도움을 주기 위해서 결과를 추정하는 모델을 개발하고 있습니다. 모델의 결과 추정치를 평가하는데 한 가지 이슈가 있습니다. 하나의 예측값에 대한 실제 값만을 알 수 있기 때문입니다. 그림1에서는 output 2에 대한 실제 결과를 확인할 수 있고, output1과 output3에 대한 실제 결과는 확인할 수 없습니다. 이런 선택의 문제에서 모델의 성능은 어떻게 평가할 수 있을까요? 처치별 환자의 분포가 겹친다면, RCT 상황에서 생각해보기 *RCT(Randomized Controlled Trial) : 무작위 배정 시험. 무작위로 처치에 배정된 경우를 생각해보겠습니다. 무작위로 배정되었기 때문에 처치별 환자의 분포가 비슷하게 형성되었을 것입니다. A라는 고객은 12.1로 수술을 받았지만, A와 비슷한 A'는 12.6으로 수술을 받았을 것이고, A와 비슷한 A\"는 13.2로 수술을 받았을 것입니다. 따라서 A에 대해서는 12.1 밖에 수술 결과가 없지만, 나머지 12.6과 13.2에 대한 예측결과를 간접적으로 확인할 수 있습니다. 다르게 표현하면, A라는 고객이 각 처치를 받을 확률이 비슷하다면, 그 처치에 대한 예측값을 간접적으로 평가할 수 있고 신뢰할 수 있습니다. 인과추론에서는 이를 다음과 같이 표현합니다. > positivity/overlap assumption and the common support region. 처치별 고객의 분포가 겹치지 않는다면, 실제 상황에서 생각해보기 당연하게도 실제 상황은 RCT 와 다릅니다. 의사는 고객의 상태를 보고 처치를 결정하기 때문입니다. <figure> <img src=\"/post/what-is-statistical-bias/처치별_Feature_DensityPlot.png\" alt=\"처치별 Feature Density Plot\"> <figcaption>그림 2: 처치별 Feature 분포 비교</figcaption> </figure> 특히 눈의 크기(ata)를 통해 렌즈 크기의 범위를 결정합니다. 그림 2를 통해 이를 확인할 수 있습니다. 12.1을 선택하는 눈의 크기 (Cas_ata) 분포와 13.2를 선택하는 눈의 크기 분포는 꽤나 다릅니다. 거의 겹쳐져 있지 않습니다. 이렇게 13.2를 받을 확률이 매우 낮은 고객의 13.2에 대한 모델의 예측값은 간접적으로도 평가할 수 없고, 이를 신뢰하기 어려울 것입니다. 겹치는 정도, 처치를 선택할 확률을 구하는 방법 특정 고객이 어떤 처치를 받을 확률은 모델을 통해 추정할 수 있습니다. X를 고객의 눈의 상태로, Y를 처치로 두고 모델을 학습시키면 됩니다. <figure> <img src=\"/post/Causal_Inference/선택문제_확률추정모델결과_1.png\" alt=\"확률추정결과\"> <figcaption>그림 3: 12.1과 12.6이 겹치는 고객 (처치 : size, 나머지는 고객 눈의 조건)</figcaption> </figure> <figure> <img src=\"/post/Causal_Inference/선택문제_확률추정모델결과_2.png\" alt=\"확률추정결과\"> <figcaption>그림 4: 12.6과 13.2가 겹치는 고객 (처치 : size, 나머지는 고객 눈의 조건)</figcaption> </figure> 그림3과 그림4는 확률추정 모델을 만든 후, 모델을 통해 확률값을 구해 필터링한 결과입니다. 그림3의 경우 12.1과 12.6을 받을 확률이 0.4 이상인 고객들입니다. 이런 고객들의 경우 12.1과 12.6에 대한 예측값을 간접적으로 평가할 수 있고, 신뢰할 수 있습니다. 13.2의 경우에는 신뢰할 수 없을 것입니다. 예측의 불확실성을 표현하는 방법 그렇다면 이런 직관을 수학적으로 표현할 수 있을까요? 예측값의 불확실성을 어떻게 표현할 수 있을까요? GPT-5 의견 신뢰도(confidence) = Overlap × Predictive Uncertainty 반영 Overlap 기반 신뢰도: 추천 후보 상위 2개 처치 t1, t2에 대해 min(P_hat(t1 x), P_hat(t2 x))가 높을수록 신뢰도↑ 예측 불확실성: 앙상블/MC Dropout 분산, 또는 Quantile Regression으로 예측구간 산출 표시 방법: 신뢰도 낮은 처치는 회색 처리 또는 주석(“데이터 부족/overlap 부족”) 신뢰도와 함께 예측구간 표시 $$ \\text{Confidence}(x, t) \\;=\\; \\underbrace{\\min_{t \\in \\mathcal{C}(x)} \\hat{P}(t \\mid x)}_{\\text{overlap}} \\;\\times\\; \\underbrace{g\\!\\left(\\frac{1}{1 + \\sigma^2_{\\hat{y}_t}(x)}\\right)}_{\\text{uncertainty}} $$ 여기서 $\\mathcal{C}(x)$는 추천 후보 처치 집합, $sigma^2_{\\hat{y}_t}(x)$는 (앙상블/MC Dropout/분위수 회귀 등으로 추정한) 예측 분산이며, $g(\\cdot)$는 0–1 정규화 함수입니다. 내 생각, 분산으로 표현하기 제가 원하는 방향은 prediction interval을 구하는 것입니다. 그리고 여기에 overlap 정도가 반영되어서, 앞서 언급한 예시처럼 13.2를 받을 확률이 매우 낮은 고객의 13.2에 대한 분산은 매우 크게 계산 됐으면 좋겠습니다. 이를 어떻게 정의하고 나타낼 수 있을지는 아직 찾아보면서 고민하고 있습니다.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/장효진강의/6강_하복부만 조이는 감각을 통해 횡경막 컨트롤하기",
    "title": "6강 하복부만 조이는 감각을 통해 횡경막 컨트롤하기",
    "date": "2025-08-10",
    "excerpt": "하복부만 조이는 감각을 통해 횡경막 컨트롤하기. 관련 연습법 정리",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "장효진 강의",
      "연습방법"
    ],
    "content": "유료 인강 링크 가슴은 서브. 가슴은 최대한 안쓰는게 좋음. 갈비뼈 아래 부분만 조이는 느낌 아래 위쪽으로 > 처음으로 분리하는 연습!",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/소문및가십/08-10-듣기로는다시만나고싶어한대",
    "title": "듣기로는 다시 만나고 싶어 한대",
    "date": "2025-08-10",
    "excerpt": "소문 및 가십 파트",
    "category": "English Study",
    "tags": [
      "SPEAK",
      "소문 및 가십"
    ],
    "content": "From what I've heard, he wants to get back together. I knew it 오늘 배운 것들 내가 들은 바로는 내가 아는 바로는 내가 이해하는 범위 > From what I've haard From what I've heard, Steve wants to get back together. (다시 만나고 싶더라) From what I've heard, Andrew is seeing someone else. (다른사람을 만나고 있다) > From what I know : (틀릴 수도 있는데) 내가 아는 바로는 know 자체가 상태를 나타내기 때문에 현재형으로 쓸 수 있음 From what I know, Sean is from New York. From what I know, Sue used to work at Google. From what I know, Alex is from Hawai. > As far as I understand: 내가 이해하는 범위 내에서 As far as I understand, Julie doesn't want to get back together. have a huge crash on someone 간단한 작문 (배운 표현들 사용해서) I had a blind date yesterday. (I went on a blind date yesterday.) I heared that from matchmaker, from what she know, my blind date has really good personality. So I expected that our date would be fun. After date, I have a good emotion to her. As far as I unserstand, she is so 무해하다 and cute. I wanna know more about her. --- 수정된 버전 I went on a blind date yesterday. I heard that from the matchmaker. According to what she knows, my blind date has a really good personality. So I expected that our date would be fun. After the date, I have good feelings toward her. As far as I understand, she is so harmless and cute. I want to know more about her. --- 피드백 및 수정사항 문법적 오류 수정: \"heared\" → \"heard\" (hear의 과거형) \"matchmaker\" → \"the matchmaker\" (관사 필요) \"she know\" → \"she knows\" (3인칭 단수 현재형) \"my blind date has really good personality\" → \"my blind date has a really good personality\" (관사 필요) \"After date\" → \"After the date\" (관사 필요) \"I have a good emotion to her\" → \"I have good feelings toward her\" (더 자연스러운 표현) \"I wanna know\" → \"I want to know\" (공식적인 작문에서는 완전한 형태 사용) 더 자연스러운 표현: \"According to what she knows\" - \"from what she know\"보다 더 자연스러움 \"good feelings toward her\" - \"good emotion to her\"보다 더 자연스러운 표현 \"harmless\" - \"무해하다\"의 적절한 영어 번역",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-information-entropy",
    "title": "Information Entropy란?",
    "date": "2025-08-10",
    "excerpt": "Information Entropy 개념정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1: 공돌이의 수학정리노트 2: 위키피디아 Information content 정보란? > In information theory, the information content, self-information, surprisal, or Shannon information is a basic quantity derived from the probability of a particular event occurring from a random variable. > It can be thought of as an alternative way of expressing probability, much like odds or log-odds, but which has particular mathematical advantages in the setting of information theory. surprisal, information content 이런 단어들은 확률을 변형해서 표현하는 것들. 놀라움 정도를 양적으로 표현한 것 같다. > The Shannon information can be interpreted as quantifying the level of \"surprise\" of a particular outcome. > As it is such a basic quantity, it also appears in several other settings, such as the length of a message needed to transmit the event given an optimal source coding of the random variable. $$I(x) = -log_b(P(X))$$ $b$: $2, e, 10$ 왜 log를 붙여서 정의했을까? 확률값에 반비례 해야 하기 때문에 두 사건의 정보량을 합치면 각 사건의 정보량을 합친 것과 같아야 하기 때문에 엔트로피 (entropy) : 평균 정보량 $$H(X) = E[I(X)] = - \\sum_{i=1}^n P(x_i) log_b (P(x_i))$$",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-data-drift",
    "title": "Data Drift란?",
    "date": "2025-08-10",
    "excerpt": "Data Drift의 개념과 활용되는 사례에 대한 정리",
    "category": "Data Science",
    "tags": [
      "data-drift",
      "machine-learning",
      "statistics",
      "data-engineering"
    ],
    "content": "참고한 글 1: 링크드인 - Machine Learning Community (Moderated) --- How to choose Data Drift metrics in ML Production (참조 1)? Jensen–Shannon(JS) Distance 장점: symmetric적이고 값이 0~1로 bounded라 해석이 쉽고 threshold 설정이 안정적. noise·경미한 변동에 robust. 단점: computational cost가 상대적으로 큼(특히 high-dimensional). 언제 쓰나: 해석 용이성과 stable monitoring이 중요할 때. Kullback–Leibler(KL) Divergence 장점: distribution의 subtle differences까지 sensitive하게 감지(early warning에 유리). 단점: outlier·작은 probability mass에 sensitive해 spike/false alarm 가능, support set mismatch 시 발산. 언제 쓰나: early detection이 최우선이거나 높은 sensitivity가 필요한 시스템. Wasserstein Distance 장점: 큰 distribution 변화도 smoothly tracking, reference·current distribution이 겹치지 않아도 동작. 단점: computational cost 큼(large-scale/real-time, high-dimensional에서 특히 부담). 언제 쓰나: large/structural changes의 stable tracking이 필요할 때. 한 줄 가이드 interpretability·threshold stability: JS early detection of subtle changes: KL smooth and robust tracking: Wasserstein <figure> <img src=\"/post/DataScience/DataDrift_plot_1.gif\" alt=\"Data Drift Plot\" /> <figcaption>그림 1. Data Drift Plot</figcaption> </figure> --- 지표별 파이썬 구현 예시 (JS/KL/Wasserstein) 연속형 feature의 distribution을 동일한 구간으로 discretize한 뒤, KL/JS를 계산하고 Wasserstein distance를 직접 계산하는 간단한 예시입니다. ``python import numpy as np from scipy.stats import entropy, wasserstein_distance def _normalize_hist(values: np.ndarray, bins: np.ndarray, epsilon: float = 1e-8) -> np.ndarray: counts, _ = np.histogram(values, bins=bins) probs = counts.astype(float) probs = probs / max(probs.sum(), 1.0) probs = np.clip(probs, epsilon, None) probs = probs / probs.sum() return probs def kl_divergence_from_samples(reference: np.ndarray, current: np.ndarray, num_bins: int = 30, base: int = 2) -> float: \"\"\"KL(P Q): 기준(reference)=P, 현재(current)=Q\"\"\" bins = np.histogram_bin_edges(reference, bins=num_bins) p = _normalize_hist(reference, bins) q = _normalize_hist(current, bins) return float(entropy(p, qk=q, base=base)) def js_distance_from_samples(reference: np.ndarray, current: np.ndarray, num_bins: int = 30, base: int = 2) -> float: \"\"\"JS distance: sqrt(JS divergence), 0~1 범위(base=2)\"\"\" bins = np.histogram_bin_edges(reference, bins=num_bins) p = _normalize_hist(reference, bins) q = _normalize_hist(current, bins) m = 0.5 (p + q) js_div = 0.5 entropy(p, qk=m, base=base) + 0.5 * entropy(q, qk=m, base=base) return float(np.sqrt(js_div)) def wasserstein_from_samples(reference: np.ndarray, current: np.ndarray) -> float: return float(wasserstein_distance(reference, current)) 예시 데이터 ref = np.random.normal(0.0, 1.0, size=10_000) cur = np.random.normal(0.3, 1.2, size=2_000) metrics = { \"KL(P Q)\": kl_divergence_from_samples(ref, cur), \"JS distance\": js_distance_from_samples(ref, cur), \"Wasserstein\": wasserstein_from_samples(ref, cur), } print(metrics) ` 참고: 범주형 feature는 각 category의 relative frequency를 probability로 normalize하여 KL/JS를 계산하면 됩니다(동일한 category 순서와 smoothing 필요). 데이터 드리프트 지표 비교표 Metric 값 범위 symmetry distribution overlap 필요 sensitivity(subtle changes) 장점 단점 computational cost 권장 사용 --- --- --- --- --- --- --- --- --- JS distance 0~1 symmetric 불필요 중간 interpretability·threshold 설정 용이, noise robust high-dimensional에서 computational cost 증가 중 stable monitoring/threshold 운영 KL divergence 0~∞ asymmetric 부분 overlap 권장 높음 subtle changes early detection outlier/sparse probability에 sensitive, false alarm 가능 중 early warning이 중요한 시스템 Wasserstein ≥0 symmetric 불필요 중간 large/structural changes도 smoothly tracking, interpretation 직관적 large-scale/real-time·high-dimensional에서 cost 큼 높음 robust change tracking, overlap 적은 distribution --- 지표 상세 설명 Kullback–Leibler(KL) Divergence relative entropy, I-divergence. > A simple interpretation of the KL divergence of P from Q is the expected excess surprisal from using Q as a model instead of P when the actual distribution is P. P대신 Q를 썼을 때 정보량의 변화정도가 KL divergence. $$ D_{KL}(P Q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} = H(P, Q) - H(P) \\tag{3} $$ distance의 속성을 만족하지 못해서 distance라고 표현할 수는 없다. squared distances의 속성은 만족한다고 한다. 직관: 진짜가 P인데 Q로 coding하려고 할 때 “추가로 내야 하는 information cost”. directionality이 중요해 KL(P Q)와 KL(Q P)는 다른 값을 준다. 수식 $$ KL(P\\,\\ \\,Q) = \\sum\\limits_{x} p(x)\\,\\log\\frac{p(x)}{q(x)}\\quad(\\text{연속형은 적분}) $$ 특징: asymmetric, lower bound 0·upper bound 없음. Q(x)=0이면서 P(x)>0이면 발산(infinity). sparse probability·outlier에 매우 sensitive. 계산법: histogram/KDE로 p,q 추정 후 epsilon smoothing(예: 1e-6). 운영에서는 KL(P\\ \\ Q), KL(Q\\ \\ P) 둘 다 보거나 symmetricization(KL sym) 혹은 JS로 대체. threshold 힌트: spike가 잦으므로 moving average·quantile 기반 경보, permutation(label shuffle) null로 p-value 추정 권장. 사용처: early 미세 변화 탐지, sensitive도 최우선 경보 시스템, 리스크가 큰 영역의 세밀 monitoring. 흔한 함정: sample 적을 때 unstable, bin 선택/bandwidth sensitive, log 수치unstable → smoothing/clipping·normalization 필요. --- Jensen–Shannon(JS) Divergence information radius(IRAD) or total divergence > It is based on the Kullback-Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value. square root of Jensen-Shannon divergence == Jensen-Shannon distance 직관: 두 distribution의 “중간 distribution” M과의 information difference를 평균해 측정하고, square root을 취해 실제 distance로 만든 값. symmetric이고 0~1 범위라 threshold을 정하기 쉽다. 수식 $$ JS(P, Q) = \\tfrac{1}{2}\\,KL(P\\,\\ \\,M) + \\tfrac{1}{2}\\,KL(Q\\,\\ \\,M)\\\\ M = \\tfrac{1}{2}(P + Q)\\\\ JS\\_\\text{dist}(P, Q) = \\sqrt{JS(P, Q)}\\quad(\\text{log base}=2\\Rightarrow 0\\le JS\\_\\text{dist}\\le 1) $$ 특징: symmetric, bounded(0~1), support set mismatch에도 finite. rare event에 over-sensitive하지 않아 운영 monitoring에 안정적. 계산법: 연속형은 공통 bin 또는 KDE로 확률추정 후 계산, 범주형은 범주 집합을 합집합으로 정렬해 smoothing(e.g., 1e-6) 후 계산. 표본 적으면 분산↑ → 윈도우 크기 확대. threshold 힌트: 기준 기간에서 distribution를 부트스트랩해 경험적 distribution를 만든 뒤 상위 quantile로 경보선 설정(조직별 캘리브레이션 권장). 사용처: 해석 용이·안정적 스케일이 필요한 대시보드, 다수 피처의 일관된 threshold 운영. 흔한 함정: bin 개수/경계, KDE bandwidth에 sensitive. 범주 희박도 높을 때 값이 과소평가될 수 있음 → smoothing/병합 고려. --- Wasserstein Distance (Earth Mover's Distance, 1-Wasserstein) 직관: 한 distribution의 질량을 다른 distribution로 옮길 때 필요한 “최소 작업량”. 값의 단위가 원 변수와 같아 해석이 직관적이다. 수식(1차원) $$ W\\_1(P, Q) = \\int\\limits_{0}^{1} \\big F\\_P^{-1}(u) - F\\_Q^{-1}(u)\\big \\,du $$ 특징: symmetric, distribution 겹침이 없어도 정의 가능, support set 경계에 강함. 위치·스케일 이동을 부드럽게 반영. 계산법: 1D는 scipy.stats.wasserstein_distance`로 정렬 기반 O(n log n). 다변량은 표준화 후 Sinkhorn(엔트로피 normalization) 등으로 근사(POT 등 라이브러리). threshold 힌트: 원 단위 그대로 해석하거나 IQR/표준편차로 normalization해 무단위 distance로 비교. KPI와 직접 연결해 threshold 세팅 용이. 사용처: 큰/구조적 이동 추적, 기준·현재 distribution가 거의 겹치지 않는 상황, 비즈니스 단위 해석이 중요한 경우. 흔한 함정: 고차원 계산량 부담·메모리 사용 증가, 피처 스케일 mismatch 시 왜곡 → 표준화 필수. 다변량 OT는 구현·튜닝 난이도 높음. 직관적 이해 🚚 \"흙을 옮기는 비용\" 개념 한 분포의 \"질량\"을 다른 분포로 옮기는 최소 작업량을 의미합니다 마치 건설 현장에서 흙을 한 곳에서 다른 곳으로 옮기는 것과 비슷합니다 각 위치에서 필요한 흙의 양과 옮겨야 할 거리를 고려하여 총 작업량을 계산합니다 간단한 예시 분포 P: [0, 0, 1, 0, 0] (중앙에만 질량) 분포 Q: [0, 0, 0, 1, 0] (오른쪽으로 한 칸 이동) Wasserstein Distance = 1 (한 칸 옮기면 됨) KL divergence = ∞ (겹치는 부분이 없음) 데이터 드리프트에서의 활용 데이터 드리프트를 감지할 때: 시간에 따른 분포 변화를 정량적으로 측정 모델 성능 저하의 원인 분석 데이터 품질 모니터링에 활용 이 방법은 특히 연속형 변수나 순서가 있는 범주형 변수에서 분포 변화를 측정할 때 매우 유용합니다. ---",
    "public": true
  },
  {
    "id": "Data Science/Statistics/what-is-KL-divergence",
    "title": "KL Divergence란?",
    "date": "2025-08-10",
    "excerpt": "KL Divergence 개념정리",
    "category": "Data Science",
    "tags": [
      "statistics"
    ],
    "content": "참고자료 1: 공돌이의 수학정리노트 entropy entropy 개념 정리 $$ H(x) = - \\sum_{i=1}^n p(x_i)log(p(x_i)) \\tag{1} $$ cross entropy 예측과 달라서 생기는 깜놀도 (즉 정보량) $$ H(p, q) = - \\sum_{i=1}^n p(x_i)log(q(x_i)) \\tag{2} $$ KL divergence > A simple interpretation of the KL divergence of P from Q is the expected excess surprisal from using Q as a model instead of P when the actual distribution is P. $$ D_{KL}(p q) = \\sum_{x \\in X} p(x) \\log \\frac{p(x)}{q(x)} \\tag{3} $$ $$ = \\sum_{x \\in X} p(x) \\log p(x) - \\sum_{x \\in X} p(x) \\log q(x) \\tag{4} $$ $$ D_{KL}(p q) = H(p, q) - H(p) \\tag{5} $$",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/소문및가십/08-09-결국-둘이-잘될줄-알았어",
    "title": "08-09 결국 둘이 잘될줄 알았어",
    "date": "2025-08-09",
    "excerpt": "소문및가십 08-09 결국 둘이 잘될줄 알았어",
    "category": "English Study",
    "tags": [
      "SPEAK",
      "소문및가십"
    ],
    "content": "I knew they would end up together. 배운 것들 No wonder : 놀랍지 않아. No wonder he didn't call. No wonder he's great with kids. I knew it : 그럴 줄 알았어. I knew they would end up together. I knew they would break up. would have naver p.p : 절대 ~하지 못했을거야. I would have naver guessed it. I would have naver thought it.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/장효진강의/5강_마지막까지 뱉는 방법을 통한 복부가 조이는 감각 익히기",
    "title": "5강 마지막까지 뱉는 방법을 통한 복부가 조이는 감각 익히기",
    "date": "2025-08-07",
    "excerpt": "마지막까지 뱉는 방법을 통해 복부가 조이는 감각 익히기. 관련 연습법 정리",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "장효진 강의",
      "연습방법"
    ],
    "content": "유료 인강 링크 4강 헀던 상황에서 조금만 틈을 벌리기 휘파람 모양 만들기 마지막까지 뱉기!! 주먹으로 막기 피지컬 훈련!! 많이 연습하면 연습할수록 빨리 늠. 다음 수업을 듣더라도 계속 해주기. 가수들은 노래를 잘하니까 기본만 더 많이 함. 더 빡세게.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/장효진강의/4강_입을 막아서 뱉을 때 쓰는 압력 느끼기",
    "title": "4강 입을 막아서 뱉을 때 쓰는 압력 느끼기",
    "date": "2025-08-07",
    "excerpt": "입을 막아서 뱉을 때 쓰는 압력 느끼기. 관련 연습법 정리",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "장효진 강의",
      "연습방법"
    ],
    "content": "유료 인강 링크 3강 훈련. 아침 점심 저녁 기준으로 15번은 할 수 있을 때 넘어오기. 입을 막고 압력을 느껴보는 훈련 나갈 길이 없기 때문에 압축에 가까움. 압축을 느껴보는 훈련. 강하게 뱉어보기 뱉다가 막아보기 저항이 생김. 이 느낌. 힘을 꾹 주고 있는 느낌. 막은 걸 뚫고 나가겠다는 힘을 주기 배가 안으로 밀려 올라가는 느낌. 손으로. 최대한 많이. 압력 = 저항법",
    "public": true
  },
  {
    "id": "Data Science/Statistics/confidence_interval_for_ML",
    "title": "ML의 신뢰구간을 구하는 방법",
    "date": "2025-08-06",
    "excerpt": "ML의 신뢰구간은 어떻게 구할까?",
    "category": "Data Science",
    "tags": [
      "machine-learning",
      "confidence-interval"
    ],
    "content": "참고자료 1: Confidence Intervals for Machine Learning 2: Don’t Forget Confidence Intervals for Your ML Product 3: Creating Confidence Intervals for Machine Learning Classifiers 4: Confidence vs Prediction Intervals: Understanding the Difference --- 신뢰구간의 정의 > A confidence interval provides a range of values within which a population parameter is likely to fall, with a certain level of confidence. 신뢰구간은 모집단의 모수(parameter)가 특정 범위 내에 있을 가능성을 나타내는 통계적 구간 --- metric에 대한 신뢰구간 (참고1) 보통 ML의 신뢰구간은 metric에 대한 신뢰구간을 의미하는 것 같다. <figure> <img src=\"/post/DataScience/CI_for_ML/CI_example1.png\" alt=\"confidence_interval_for_ML\" style=\"width: 50%;\" /> <figcaption>그림1. confidence_interval_for_ML (참고1)</figcaption> </figure> > For example, a 95% confidence interval of [85%, 90%] for model accuracy means that if we repeated the process multiple times, about 95% of the intervals would contain the true accuracy. This helps in assess model reliability and make informed decisions. 정확도와 같은 평가지표를 신뢰구간으로 제시하면 데이터가 바뀌었을 때에 대한 고려도 할 수 있고, 이러한 이유 떄문에 선택에 더 도움이 됨. --- 신뢰구간의 중요성 (참고1) 신뢰구간은 머신러닝의 다양한 영역에서 가치 있는 정보를 제공합니다: 모델 성능 평가 정확도나 F1-score와 같은 단일 지표에만 의존하는 대신, 신뢰구간은 서로 다른 샘플에서의 성능 변화를 이해하는 데 도움을 줍니다. 예시: 모델의 정확도가 85%이고 95% 신뢰구간이 [82%, 88%]라면, 새로운 데이터에서 테스트할 때 모델의 정확도가 이 범위 내에서 변동할 수 있음을 의미합니다. 회귀 계수 해석 선형 회귀 모델에서 신뢰구간은 추정된 계수의 신뢰성을 평가하는 데 도움을 줍니다. 예시: 특정 특성의 계수가 2.5이고 95% 신뢰구간이 [1.8, 3.2]라면, 해당 특성이 양의 효과를 가진다고 합리적으로 확신할 수 있습니다. 예측의 불확실성 신뢰구간은 모델 예측에 대한 불확실성 측정을 제공하며, 특히 확률적 모델에서 유용합니다. 예시: 주택 가격을 [200K, 250K] 범위로 95% 신뢰수준에서 예측한다면, 실제 가격이 이 범위 내에 있을 가능성이 높음을 의미합니다. A/B 테스트 및 가설 검정 두 모델이나 특성을 비교할 때, 신뢰구간은 차이가 통계적으로 유의한지 판단하는 데 도움을 줍니다. 예시: 모델 A의 정확도가 90% [88%, 92%]이고 모델 B가 87% [85%, 89%]라면, 모델 A가 우수하다는 증거가 있습니다. --- 신뢰구간을 구하는 법 (참고2) > So how can we create and add confidence intervals to our models? As the name implies, to build confidence intervals (or any other measure of confidence) you first need to define what your metric represents and what confidence means to you and your user. > There are many ways to derive a measure of confidence but all depend on some sort of knowledge which you can use to validate and test your predictions. --- 신뢰구간을 구하는 법 (참고3) > Confidence intervals are no silver bullet, but at the very least, they can offer an additional glimpse into the uncertainty of the reported accuracy and performance of a model. 여러 글에서 신뢰구간을 제시하는게 소통을 하는데 도움을 준다고 이야기하네. 공감이 된다. 어떤 추정값을 이야기할 때 불확실성에 대한 이야기는 꼭 포함이 되어야 한다고 생각한다. <figure> <img src=\"/post/DataScience/CI_for_ML/with_CI_without_CI.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>그림2. with_CI_without_CI</figcaption> </figure> 확실히 포함된게 더 보기 좋음 이 블로그에서 CI를 구하는 다양한 방법들을 소개하고 있다. 기본적인 컨셉만 이해하고, 정리하다가 너무 많아서 패스. Confidence Intervals in a Nutshell > A confidence interval is a method that computes an upper and a lower bound around an estimated value. The actual parameter value is either insider or outside these bounds. 추정값에 대한 불확실성을 정량화한게 신뢰구간이라고 생각해볼 수 있겠다. <figure> <img src=\"/post/DataScience/CI_for_ML/95_percent_CI.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>그림3. 95% 신뢰구간의 의미</figcaption> </figure> 95% 신뢰구간의 의미는? > “95% 신뢰구간”이란, 동일한 방법으로 표본을 여러 번 뽑아서 신뢰구간을 계속 계산한다면, 그중 약 95%의 신뢰구간이 모집단의 참값을 포함하게 된다는 의미 오해와 정확한 해석 > 많은 사람들이 “이 신뢰구간 안에 진짜 평균이 95% 확률로 들어 있다”라고 생각하지만, 엄밀하게는 틀린 해석이야. 참값은 이미 고정된 값이니까, 확률로 해석하지 않아. 올바른 해석은 “이 방식으로 구한 신뢰구간이 모집단의 평균을 95% 수준으로 포함한다”는 것. 원하는 건 모델의 성능에 대한 값들. 신뢰구간은 위에서 언급한 것처럼 모델이 추정값에 대한 얼만큼의 불확실성을 가지고 있는지를 의미. A Note About Statistical Significance 통계적 유의성이란? > 통계적 유의성은 관찰된 결과가 단순한 우연이나 확률적 변동이 아닌, 실제로 의미 있는 차이나 관계가 있다고 판단할 수 있는 정도를 나타냅니다. <figure> <img src=\"/post/DataScience/CI_for_ML/ci-overlap.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>그림4. 신뢰구간의 중복을 통한 판단</figcaption> </figure> group1과 group2가 다른지를 신뢰구간을 통해 확인해볼 수 있다. 당연하게도 완전히 겹치지 않으면 확실히 다른 것이라고 판단할 수 있다. 겹친다면 겹친 분포를 그리고 여기서 mu = 0이 신뢰구간에 포함되는지를 통해 확인할 수 있다. Method 1 : Normal Approximation Interval Based on a Test Set > In our case, the sample mean $\\bar x$ is test set accuracy $\\text{ACC}_{test}$, a proportion of success (in the context of a Binomial proportion confidence interval). Binomial proportion confidence interval? > 성공/실패와 같은 이항 분포를 따르는 비율(proportion)에 대한 신뢰구간 ML에서의 적용: 분류 모델의 정확도(accuracy)는 이항 분포를 따름 각 예측이 맞음(성공) 또는 틀림(실패)의 두 가지 결과만 가짐 전체 예측 중 맞은 비율이 정확도 예시: 테스트셋에서 100개 샘플 중 85개를 맞췄다면 정확도 = 85/100 = 0.85 (85%) 이 85%라는 비율에 대한 신뢰구간을 구하는 것이 이항 비율 신뢰구간 계산 방법: 정규근사법 (Normal Approximation) Wilson 방법 Clopper-Pearson 방법 수식 정의 standard error : $\\sqrt{\\frac{1}{n} \\text{ACC}_{test} (1 - \\text{ACC}_{test})}$ > You can find a description of this method in section 1.7 Confidence Intervals via Normal Approximation of my “Model Evaluation, Model Selection, and Algorithm Selection in Machine Learning”. <figure> <img src=\"/post/DataScience/CI_for_ML/normal-approx.png\" alt=\"confidence_interval_for_ML\" style=\"width: 80%;\" /> <figcaption>그림5. 정확도에 대한 신뢰구간 표시</figcaption> </figure> Method 2: Bootstrapping Training Set - Setup Step > If we only have one estimate, like the accuracy from a single test set, we need to make assumptions about the distribution of this accuracy value. > For example, we may assume that the accuracy values (that we would compute from different samples) are normally distributed. 부트스트랩 기반으로 신뢰구간을 구한다는건 정규분포를 가정하는건가? > No! 부트스트랩은 정규분포 가정 없이도 신뢰구간을 구할 수 있는 강력한 비모수적 방법입니다. 이것이 바로 부트스트랩의 가장 큰 장점 중 하나입니다. Confidence vs Prediction Intervals: Understanding the Difference (참고4) > The root cause of this uncertainty is the sampling process. > It is unrealistic to consider the entire population when conducting a statistical analysis. 좋은 문장이다. > The true value of the population parameter is usually not exactly equal to the value estimated from the sample - this difference is the standard error. > To account for this error, it is conventional to estimate an expected value and then specify a range that is expected to contain the actual value. 추정값은 실제값과 차이가 있을 것이고, 이를 범위로 표시한다. What is a Prediction Interval? > A prediction interval is the range that is expected - with some level of confidence, to contain the true value of an individual data point, based on a prediction made using regression analysis. 내가 원하는 개념이 prediction interval이구나. 개별 추정값에 대한 오차 범위. > However, you must account for the increased variability of individual data points because you are predicting an individual value (and not an average). > Thus, the prediction interval is larger than the confidence interval.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/sharding_vs_partitioning",
    "title": "sharding vs partitioning",
    "date": "2025-08-05",
    "excerpt": "DB관리할 때 sharding과 partitioning의 차이에 대한 개념 정리",
    "category": "Data Science",
    "tags": [
      "DataBase"
    ],
    "content": "참고한 글 링크드인 게시물 - Ashish Pratap Singh <figure> <img src=\"/post/DataScience/sharding_vs_partitioning.gif\" alt=\"Sharding vs Partitioning 차이점\" /> <figcaption>Sharding vs Partitioning</figcaption> </figure> 구분 Partitioning Sharding ------ ------------- ---------- 범위 단일 서버 내 여러 서버 간 목적 성능 최적화 수평적 확장 복잡도 상대적으로 단순 복잡한 분산 시스템 사용 시기 단일 머신 내 최적화 단일 머신 한계 초과 시 Partitioning이란? > 하나의 큰 테이블을 더 작은 청크(파티션)로 나누는 것 목적: 성능 향상과 유지보수 단순화 특징: 논리적으로는 하나의 테이블: 개발자가 쿼리할 때는 마치 하나의 테이블처럼 사용 (파티션을 신경 쓸 필요 없음) 물리적으로는 여러 파일로 분산: 실제로는 각 파티션이 별도의 파일/세그먼트로 저장됨 쿼리 성능 향상 (특정 파티션만 스캔) 데이터 관리 용이성 (오래된 파티션 삭제 등) ``sql -- 파티셔닝된 테이블을 조회할 때 SELECT FROM logs WHERE log_date >= '2024-01-01'; -- 개발자는 마치 하나의 테이블을 조회하는 것처럼 사용 -- 하지만 DB 엔진은 내부적으로 2024년 파티션만 스캔함 ` 파티셔닝은 대부분의 현대적인 DBMS에서 지원하는 기능 지원하는 DBMS MySQL: Range, List, Hash, Key 파티셔닝 지원 PostgreSQL: Range, List, Hash 파티셔닝 지원 Oracle: Range, List, Hash, Composite 파티셔닝 지원 SQL Server: Range, List, Hash 파티셔닝 지원 BigQuery: Date, Integer Range 파티셔닝 지원 Snowflake: 다양한 파티셔닝 전략 지원 지원하지 않는 DBMS SQLite: 기본적으로 파티셔닝 미지원 일부 NoSQL DB: 파티셔닝 대신 샤딩 개념 사용 MySQL 파티셔닝 예시 `sql -- 월별 Range 파티셔닝 CREATE TABLE logs ( id INT, log_date DATE, message TEXT ) PARTITION BY RANGE (YEAR(log_date) 100 + MONTH(log_date)) ( PARTITION p202401 VALUES LESS THAN (202402), PARTITION p202402 VALUES LESS THAN (202403), PARTITION p202403 VALUES LESS THAN (202404), PARTITION p_future VALUES LESS THAN MAXVALUE ); ` Sharding이란? > 데이터를 여러 데이터베이스 서버에 분산 배치 목적: 수평적 확장(horizontal scaling)을 통한 대용량 처리 특징: 각 서버(샤드)가 전체 데이터셋의 일부를 보유 단일 머신의 한계를 넘어서는 확장성 제공 복잡한 분산 시스템 관리 필요 예시 `sql -- 샤딩된 환경에서 사용자 조회 -- 사용자 ID를 4로 나눈 나머지로 샤드 결정 shard_number = user_id % 4 -- 사용자 ID가 123인 경우: 123 % 4 = 3 → shard_3에서 조회 SELECT * FROM users WHERE user_id = 123; -- 실제로는 shard_3 서버에서 실행됨 ` 수평적 확장(Horizontal Scaling)이란? > 수평적 확장은 \"더 많은 서버를 추가\"하는 방식으로 시스템을 확장하는 것을 의미 수직적 확장 vs 수평적 확장 구분 수직적 확장 (Vertical Scaling) 수평적 확장 (Horizontal Scaling) ------ ------------------------------- ---------------------------------- 방식 기존 서버의 성능을 향상 새로운 서버를 추가 예시 CPU 4코어 → 16코어, RAM 8GB → 64GB 서버 1대 → 서버 10대 장점 구현이 단순 무제한 확장 가능 단점 물리적 한계 존재, 비용 증가 복잡한 분산 시스템 관리 필요 샤딩에서의 수평적 확장 ` 초기 상태: 서버 1대 (모든 데이터 보유) ├── 사용자 1~1000 ├── 사용자 1001~2000 └── 사용자 2001~3000 샤딩 후: 서버 3대 (데이터 분산) ├── 서버1: 사용자 1~1000 ├── 서버2: 사용자 1001~2000 └── 서버3: 사용자 2001~3000 더 많은 사용자가 늘어나면: ├── 서버1: 사용자 1~1000 ├── 서버2: 사용자 1001~2000 ├── 서버3: 사용자 2001~3000 ├── 서버4: 사용자 3001~4000 ← 새로 추가 └── 서버5: 사용자 4001~5000 ← 새로 추가 `` RDBMS MySQL: ProxySQL, MySQL Router 등 미들웨어 필요 PostgreSQL: 기본 버전은 샤딩 미지원 Oracle: RAC는 있지만 샤딩과는 다름 SQL Server: 기본 버전은 샤딩 미지원 샤딩 구현 방법 샤딩은 DBMS에서 직접 지원하는 경우가 적어서, 다양한 방법으로 구현해야 합니다. 3.1 DBMS별 샤딩 지원 현황 내장 샤딩 지원 DBMS MongoDB: 자동 샤딩 지원 (shard key 기반) Cassandra: 파티션 키 기반 샤딩 Redis Cluster: 해시 슬롯 기반 샤딩 Elasticsearch: 인덱스 기반 샤딩 샤딩 미지원 DBMS (수동 구현 필요) MySQL: ProxySQL, MySQL Router 등 미들웨어 필요 PostgreSQL: 기본 버전은 샤딩 미지원 (Citus 확장으로 가능) Oracle: RAC는 있지만 샤딩과는 다름 SQL Server: 기본 버전은 샤딩 미지원 3.2 샤딩 구현 방식 애플리케이션 레벨 샤딩 개념: 개발자가 직접 애플리케이션 코드에서 샤딩 로직을 구현 방식: 사용자 ID나 다른 키를 기반으로 어떤 샤드에 접근할지 결정 장점: 완전한 제어 가능, 세밀한 튜닝 가능 단점: 개발 복잡도 증가, 유지보수 부담 미들웨어를 통한 샤딩 개념: 데이터베이스와 애플리케이션 사이에 중간 계층을 두어 샤딩 처리 방식: 쿼리를 받아서 적절한 샤드로 라우팅 장점: 애플리케이션 코드 변경 최소화, 전문적인 샤딩 관리 단점: 추가적인 인프라 복잡도, 성능 오버헤드 클라우드 서비스 활용 개념: 클라우드 제공업체가 관리하는 샤딩 서비스 사용 방식: 설정만으로 자동 샤딩 구성 장점: 관리 부담 최소화, 자동 확장 단점: 벤더 종속성, 비용 증가 3.3 샤딩 전략 Hash 기반 샤딩 개념: 데이터의 해시값을 기반으로 샤드 결정 특징: 데이터가 균등하게 분산됨 예시: 사용자 ID를 해시해서 샤드 번호 결정 Range 기반 샤딩 개념: 데이터의 범위를 기반으로 샤드 결정 특징: 범위 쿼리 성능이 좋음 예시: 날짜별, ID 범위별로 샤드 할당 Directory 기반 샤딩 개념: 매핑 테이블을 사용해서 샤드 결정 특징: 유연한 샤드 할당 가능 예시: 특정 사용자를 특정 샤드에 고정 배치 3.4 샤딩 시 고려사항 장점 무제한 수평적 확장 가능 각 샤드별 독립적인 성능 최적화 장애 격리 (한 샤드 장애가 전체에 영향 없음) 단점 복잡한 분산 시스템 관리 크로스 샤드 쿼리의 성능 이슈 데이터 일관성 보장의 어려움 트랜잭션 관리의 복잡성",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/datapipeline_linkedin_shared",
    "title": "링크드인에서 공유된 Data Pipeline 구조 정리",
    "date": "2025-08-05",
    "excerpt": "링크드인에서 공유된 Data Pipeline 참고해서 그림 그려보고, 이해하는 과정",
    "category": "Data Science",
    "tags": [
      "Data Pipeline"
    ],
    "content": "참고한 글 링크드인 - Aurimas Griclunas <figure> <img src=\"/post/DataScience/Datapipeline_linkedin.gif\" alt=\"Data Pipeline LinkedIn\" /> <figcaption>Data Pipeline LinkedIn</figcaption> </figure> 원본 글 정리 ML 시스템의 Data Pipeline 구조 ML 시스템에서 Data Quality와 Integrity를 보장하는 것은 매우 중요합니다. 특히 ML Training 및 Inference Pipeline의 upstream에서 이를 확보해야 하며, downstream 시스템에서 이를 시도하면 대규모 작업 시 불가피한 실패를 야기할 수 있습니다. Data Lake 또는 LakeHouse layer에서 수행해야 할 작업이 많습니다. 아래는 production-grade end-to-end data flow의 예시 아키텍처입니다. Data Flow 단계별 설명 Schema 관리 Schema changes는 version control에서 구현 승인 후 Applications, Databases, 중앙 Data Contract Registry에 푸시 Applications는 생성된 Data를 Kafka Topics로 전송 Data 수집 2단계: Application Services에서 직접 발생하는 Events IoT Fleets 및 Website Activity Tracking 포함 2.1단계: CDC streams을 위한 Raw Data Topics Data 검증 및 처리 3단계: Flink Application(s)이 Raw Data streams을 소비하고 Contract Registry의 schemas에 대해 검증 4단계: contract를 충족하지 않는 Data는 Dead Letter Topic으로 전송 5단계: contract를 충족하는 Data는 Validated Data Topic으로 전송 6단계: Validated Data Topic의 Data는 추가 Validation을 위해 object storage로 전송 Data Warehouse 및 변환 7단계: Object Storage의 Data는 schedule에 따라 Data Contracts의 추가 SLAs에 대해 검증되고, analytical purposes를 위해 transformed 및 modeled되어 Data Warehouse로 전송 Feature Store 및 ML Pipeline 8단계: Modeled 및 Curated data는 추가 Feature Engineering을 위해 Feature Store System으로 전송 8.1단계: Real Time Features는 Validated Data Topic(5단계)에서 직접 Feature Store로 ingested SLA checks가 어려워 Data Quality 보장이 복잡함 9단계: High Quality Data는 ML Training Pipelines에서 사용 10단계: 동일한 Data는 Inference에서 Feature Serving에 사용 주의사항 ML Systems는 Data Drift와 Concept Drift와 같은 다른 Data 관련 문제로 고통받습니다. 이는 silent failures이며 monitoring할 수 있지만 Data Contract에 포함할 수 없습니다. --- 이 구조는 LLM 기반 시스템에서도 사용할 수 있습니다. CDC streams? > CDC(Change Data Capture) Stream은 데이터베이스의 변경사항(INSERT, UPDATE, DELETE)을 실시간으로 감지하고 스트리밍하는 기술입니다. 데이터 동기화에 사용되는 개념? 실시간 데이터 동기화 아키텍처: Kafka & CDC 기반 설계 경험 Kafka? > Apache Kafka는 고성능 분산 스트리밍 플랫폼입니다. 실시간 데이터 파이프라인과 스트리밍 애플리케이션을 구축하기 위한 오픈소스 플랫폼입니다. 핵심 개념: Producer: 데이터를 Kafka로 전송하는 애플리케이션 Consumer: Kafka에서 데이터를 읽어가는 애플리케이션 Topic: 데이터가 저장되는 카테고리 (메시지 큐와 유사) Broker: Kafka 서버 (클러스터로 구성) Partition: Topic을 여러 개로 나눈 단위 (병렬 처리)",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/소문및가십/08-04-너한테_작업_걸었다니_말도안돼",
    "title": "너한테 작업 걸었다니 말도 안돼",
    "date": "2025-08-04",
    "excerpt": "믿을 수 없다는 듯 다시 묻는 표현들. 그럴리 없다는 이야기",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 정리 Are you telling me that John got plastic surgery? There's no way : 그럴리가 없어 There's no way that Mark is cheating on his girlfriend. hit on makes fun of someone : ~~를 비웃다.",
    "public": true
  },
  {
    "id": "Self_Development/Career/how-to-get-job",
    "title": "이직하는 방법 정리",
    "date": "2025-08-04",
    "excerpt": "이직하는 법 찾아보면서 하나씩 정리한 것들",
    "category": "Career",
    "tags": [
      "이직관련 정리"
    ],
    "content": "잡플래닛 이직 자료",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-04-하이퍼커넥트",
    "title": "하이퍼커넥트 ML Engineer",
    "date": "2025-08-04",
    "excerpt": "하이퍼커넥트 ML Engineer 적어보기",
    "category": "Career",
    "tags": [
      "하이퍼커넥트 채용공고"
    ],
    "content": "하이퍼커넥트 ML Engineer 채용공고 채용공고 링크: LinkedIn Job Posting 회사 정보 Legal Entity 회사명: Hyperconnect 브랜드: Match Group AI Match Group AI 소개 Match Group AI는 중앙 AI 조직으로서, AI 기술을 활용하여 Tinder와 Hinge를 비롯한 Match Group의 다양한 제품군에 기여하고 있습니다. 기존의 기술로는 접근하기 어렵지만 AI 기술을 통해 풀 수 있는 문제들을 찾아내고 사용자 경험을 혁신합니다. 주요 역할: 사용자들이 자신을 더 잘 표현할 수 있도록 돕는 도구 개발 의미 있는 만남을 발견하는 과정에서 더 만족스러운 경험을 제공하는 기능 개발 ML Team 소개 ML Team은 Match Group의 다양한 서비스에 AI 기술을 적용하는 ML Engineer로 구성된 팀입니다. 이들은 최신 AI 기술을 연구하고 적용하는 과학자이자, 실제 서비스 환경에 맞게 모델 및 시스템을 설계하고 최적화하여 실질적인 임팩트를 만들어내는 엔지니어이기도 합니다. 업무 프로세스 문제 발굴 및 정의: 실제 서비스 현장에서 발생하는 문제를 발굴하고 정의 모델 개발: 문제 해결에 가장 적합한 SotA(State-of-the-Art) 모델을 개발하거나 재현 배포: 완성된 모델을 모바일과 서버 환경에 안정적이고 효율적으로 배포 모니터링 및 개선: 지속적인 모니터링과 개선을 통해 서비스의 AI Flywheel 구축 협업 백엔드/프론트엔드/DevOps 엔지니어, 데이터 분석가, PM 등 다양한 전문 조직과 긴밀히 협업하며 실제 사용자에게 임팩트를 주는 AI 경험을 만들어갑니다. 참고 자료 How AI Lab Works - Head of AI - Shurain 인터뷰 AI in Social Discovery(Blending Research and Production) 연구 성과 업무 성과 중 일부는 논문 또는 오픈소스 코드로 외부에 공개하기도 합니다. 지금까지 약 20여 건의 대외적 연구 성과를 거둘 수 있었습니다. 주요 연구 성과 2024년 CUPID: 1:1 소셜 디스커버리 플랫폼을 위한 실시간 세션 기반 상호 추천 시스템 (ICDM Workshop 발표) 2023년 TiDAL: 효율적인 학습 과정의 모델 행동에 기반한 액티브 러닝 기법 (ICCV 2023 게재) 모더레이션 환경에서 여러 분류 기준을 동시에 만족하기 위한 문턱값을 잡는 연구 (WSDM 2023 게재) 2022년 대화 생성에서의 의미적 다양성을 높이는 연구 (EMNLP 2022 게재) 레이블 노이즈가 심한 환경에서 효과적으로 학습하는 방법 (ECCV 2022 게재) 타깃 캐릭터의 몇가지 발화만을 이용하여 타깃 캐릭터를 모방하는 챗봇연구 (NAACL 2022 게재) 대화 생성 모델에서 예시를 활용하여 성능을 높이는 연구 (ACL 2022 Workshop 발표) 모바일 환경에서 오디오 분류를 위한 distillation 기술 연구 (ICASSP 게재) 인프라 및 환경 딥러닝 클러스터 총 160대 A100 GPU 40대 H100 GPU 데이터 파이프라인 데이터 수집, 전처리를 포함한 자체적인 데이터 파이프라인 클라우드 서비스를 활용하여 구축 및 운영 협업 환경 다양한 software engineer(백엔드/프론트엔드/DevOps/MLSE)와 함께 작업 담당 업무 Match Group AI의 ML Engineer는 Tinder, Hinge 등 매치그룹의 글로벌 팀들과 협업하며, 생성형 AI, 온디바이스 AI, 추천 시스템 등 다양한 AI 기술을 활용해 데이팅 도메인 사용자 경험을 혁신하는 다양한 프로젝트를 수행합니다. 주요 역할 아이디어 구상부터 프로토타이핑, 실제 사용자에게 도달하기까지 전 과정을 주도적으로 이끌기 필요시 직무 역할과 도메인, 기술 스택의 경계를 넘나들며 문제 해결 다루는 문제 영역 다음과 같은 문제들 중 일부에 대한 경험이 있거나, 아직 경험해보지 않은 문제라도 빠르게 학습하고 주도적으로 대응할 수 있는 분을 찾고 있습니다: Multi-modal 데이터 활용 제품에서 발생하는 multi-modal data를 효과적으로 활용 극도로 불균형하거나 노이즈가 많은 데이터 처리 서로 다른 도메인에서 수집된 데이터 간 차이를 극복할 수 있는 domain adaptation 기법 Meta Learning 모델 요구사항이 자주 변경되거나 초기 데이터가 충분하지 않은 상황에도 유연하게 대응할 수 있는 meta learning 접근법 모바일 최적화 모바일 환경에서 짧은 응답 시간(latency)을 유지하면서도 높은 성능을 달성할 수 있도록, 모델을 경량화하고 최적화하는 방법 대규모 모델 활용 대규모 언어 모델(large language models)이나 멀티모달 모델(multimodal models)을 활용해 새로운 기능을 개발 특정 기능을 위해 효과적으로 모델 성능을 끌어올리는 방법 서비스 운영 대규모 모델을 실제 서비스 환경에서 안정적으로 운영하기 위한 모델링, 최적화 및 경량화 기법 편향 해결 서비스, 시스템, 모델에 의해 발생하는 다양한 데이터 편향을 해결하기 위한 방법 Requirements 필수 요구사항 AI/ML에 대한 기본 지식과 적어도 한 개 이상의 특정 도메인에 대한 깊이 있는 지식을 갖추고, 관련 프로젝트 경험이 있으신 분 Exploratory Data Analysis(EDA)를 통해, 데이터의 통계적 특성과 패턴을 발견하고 이를 ML 모델에 반영하실 수 있는 분 공개된 벤치마크 데이터 셋을 이용해 모델의 테스트 성능을 올리기 위해 여러 가지 모델링을 해본 경험이 있으신 분 구현체가 공개되지 않은 논문을 읽고, 빠르고 정확하게 구현할 수 있는 구현 역량을 갖추신 분 Tensorflow, PyTorch, JAX 등 오픈소스 프레임워크 및 전반적인 파이썬 개발에 능숙하신 분 ML 모델 학습 및 서비스 배포에 필요한 엔지니어링 역량을 갖추신 분 AI 기술의 서비스화에 관심이 많으신 분 학위/국적 무관, 영어로 기초적인 의사소통이 가능하고, 한국어로 원활한 의사소통이 가능한 분 Preferred Qualifications 우대사항 AI/ML 도메인 전반에 대한 이해를 갖추신 분 ML 관련 탑티어 학회 및 저널(NeurIPS, ICLR, ICML, CVPR, ICCV/ECCV, KDD, …) 게재 실적 혹은 AI 관련 대회 수상 실적이 있으신 분 ML 관련 오픈 소스 개발에 참여해 본 경험이 있으신 분 ML 워크플로우의 자동화 경험이 있으신 분(예: AutoML, hyperparameter optimization, 데이터 및 학습 파이프라인 구성 등) 클라이언트(Android, iOS), 백엔드를 포함해 AI/ML 분야 외 프로젝트 개발 경험이 있으신 분 실제 서비스에 AI 기술을 통합하고 주요 지표를 유의미하게 향상시켜 본 경험이 있으신 분 A/B 테스트 실험 기획 및 타겟 KPI 지표를 정의하고, SQL기반 데이터 분석을 진행한 경험이 있으신 분 영어로 비즈니스 의사소통이 가능하신 분 Hiring Process 채용 정보 고용 형태: 정규직 지원 서류: 자유 양식의 상세 경력기반 영문이력서(PDF) 채용 절차 서류전형 코딩테스트, 과제전형 1차 면접 Recruiter Call 2차 면접 최종합격 필요 시 전형이 변경 진행될 수 있습니다. 서류 전형의 경우 합격자에 한하여 개별 안내드릴 예정입니다. 특별 안내사항 이 포지션은 전문연구요원 현역 편입/전직, 전문연구요원 보충역 편입/전직 채용이 가능합니다. 병역특례요원의 경우, 병역특례 관련법에 따라 복무관리를 진행합니다. 주의사항 제출해 주신 내용 중 허위 사실이 있거나 관련법 상 근로제공에 결격사유가 있는 경우 채용이 취소될 수 있으며, 필요시 사전에 안내된 채용 절차 외에도 추가 전형 및 서류 확인이 진행될 수 있습니다. 국가보훈대상자는 관계 법령에 따라 우대하오니, 해당되시는 분께서는 지원 시 고지해주시고 채용 시 증빙서류를 제출해주시기 바랍니다. 하이퍼커넥트가 채용하는 포지션에 지원하는 경우, 개인정보 처리에 관하여서는 본 개인정보처리방침이 적용됩니다: https://career.hyperconnect.com/privacy",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-04-토스ML엔지니어-간편지원",
    "title": "Toss - ML Engineer 간편지원",
    "date": "2025-08-04",
    "excerpt": "Toss ML Engineer 간편지원 적어보기",
    "category": "Career",
    "tags": [
      "토스 채용공고"
    ],
    "content": "https://toss.im/builder/to/59b74ab2bee94d93b41025c8bd7afb49?utm_source=linkedin&utm_medium=display&utm_campaign=2507_ML 🤖 ML 엔지니어의 다음 커리어, 부담 없이 시작하세요 ML 엔지니어로 3년 이상의 경력을 가지고 있다면, 이력서 없이 가볍게 지원해 보세요. 본 간편지원 이벤트는 지원 이력에 남지 않아요. 3개의 질문을 통해 여러분의 경험과 잘 맞는 포지션을 찾아드릴게요. 기술적인 고민을 즐기고, 문제를 풀어온 경험이 있다면 충분해요. 📆 지원 마감 ~8/6(수) 오후 12시까지 결과는 마감일 이후 순차적으로 안내해 드려요. 🗺️ 진행 절차 간편 지원 → 합격할 경우 채용팀의 포지션 제안 → 직무 인터뷰 경력과 방향에 맞는 포지션을 함께 검토해 드려요. 🔎 토스의 ML 포지션이 궁금하다면, 다음 링크를 통해 자세히 살펴보실 수 있어요. JD 바로 가기 *문의: 토스 채용팀 (recruit@toss.im) Q1. 토스에 지원하는 이유를 알려주세요. Q2. 본인이 가장 깊이 관여했던 프로젝트는 무엇인가요? 해당 프로젝트에서 다뤘던 문제와 도메인을 간단히 소개해주세요. Q3. 해당 프로젝트에서 본인이 정의한 문제와 선택한 접근 방식에 대해 구체적으로 설명해주세요. 프로젝트 결과가 조직이나 서비스에 어떤 영향을 주었나요? 가능하다면 정량적 성과(AUC, CVR, latency 등)로 작성해주세요. (e.g. 어떤 데이터를 어떻게 활용했는지, 어떤 모델이나 기법을 적용했는지 등) 한 번 제출한 내용은 수정할 수 없으니, 제출 전 마지막으로 확인해 주세요.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-08-04-지마켓-데이터과학자",
    "title": "Gmarket 데이터 사이언티스트 (Data Scientist)",
    "date": "2025-08-04",
    "excerpt": "Gmarket 데이터 사이언티스트 (Data Scientist) 적어보기",
    "category": "Career",
    "tags": [
      "지마켓 채용공고"
    ],
    "content": "팀 소개 Foundation AI팀은 지마켓의 핵심 자산인 데이터를 기반으로, e-commerce 비즈니스의 다양한 문제를 AI 기술로 해결하는 모델 및 시스템을 개발합니다. 범용 모델이 아닌, 실제 커머스 데이터 기반의 자체 학습 Foundation 모델을 구축하고 있으며, 이를 다양한 비즈니스 과제에 적용하고 있습니다. 특히, 고객 행동 이력 데이터를 활용한 개인화 타겟팅 및 CRM 쿠폰 최적화 모델링을 통해 고객 경험을 향상시키고, 데이터 기반의 마케팅 전략 고도화에 기여하고 있습니다. 주요 업무 ■ CRM 쿠폰 타겟팅 최적화를 위한 개인화 모델 개발 고객 상황과 문맥 기반의 관심 상품 예측 및 세분화(Contextual Segmentation) 쿠폰/마케팅 캠페인 효율 분석 및 CRM 성과 최적화를 위한 분석 모델 설계 ■ 이커머스 도메인 내 데이터 융합 분석 및 AI 모델링 다양한 서비스에서 발생하는 대규모 데이터를 통합·분석하여 핵심 인사이트 도출 비즈니스 성과 개선을 위한 AI/ML 기반 모델 개발 및 운영 자격 요건 데이터 사이언티스트 또는 유사 직무 경력 3년 이상 9년 이하 PyTorch, TensorFlow 등 주요 ML 프레임워크 사용 경험이 있는 분 Python 등 데이터 분석 및 ML 모델링 관련 언어에 능숙하신 분 NoSQL / SQL 기반 데이터 처리 및 분석 역량 보유한 분 머신러닝/딥러닝 적용을 위한 기본적인 통계 및 알고리즘을 이해하는 분 (우대) E-commerce 또는 유사 플랫폼 도메인에서 데이터 분석 및 ML 모델링 실무 경험이 있는 분 (우대) Spark, Hive 등 분산 컴퓨팅 환경에서의 대용량 데이터 처리 경험이 있는 분 (우대) 데이터 분석 관련 분야 석사 학위 이상을 소지하신 분 공통 요건 해외여행 결격 사유가 없는 분 남성의 경우, 병역필 또는 면제자 국가보훈 대상자와 장애인은 관련 법에 따라 우대합니다 국가유공자 가산점 부여받기 위해 \"취업 지원 대상자 증명서\" 회사에 제출 필요 지원 방법 및 전형 절차 모집기간: 2025년 08월 4일 23시 59분 까지 지마켓 채용홈페이지 (https://careers.gmarket.com) 통해 지원 전형 절차: 서류전형 > 1차 면접(코딩테스트 포함) > 2차 면접 > 최종 합격 면접전형의 세부절차 및 진행방식은 상황에 따라 변동될 수 있습니다. 자세한 내용은 합격 시 개별 안내 예정입니다. 본 채용은 수시 모집으로 적합한 분이 있으실 경우 조기 마감될 수 있습니다. 최종 제출 후에는 지원서 수정이 불가하오니 이 점 유의하여 주시기 바랍니다. 기타사항 기본정보(성명, 전화번호, 이메일, 이력서, 유관 경력 기간)는 필수 제출 필요, 이외 정보는 선택하시어 작성 부탁드립니다. 시스템상 반복되는 오류가 있으실 경우, 크롬으로 접속 부탁드립니다. 지원서 내용이 사실과 다르거나, 허위 서류를 제출하신 경우 채용이 취소될 수 있습니다. 기타 문의 사항은 FAQ 참조 또는 지원 후 안내해 드리는 메일, 카카오톡 내 '문의하기' 이용 부탁드립니다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/what-is-meta-learning",
    "title": "meta-learning이란?",
    "date": "2025-08-04",
    "excerpt": "meta-learning의 개념과 활용되는 사례에 대한 정리",
    "category": "Data Science",
    "tags": [
      "meta-learning",
      "machine-learning",
      "statistics"
    ],
    "content": "참고자료 Bot Penguin - Meta Learning 논문세미나 007 - Meta-Learning의 개념 및 연구 분야 (김도형 연구원) meta-learning이란? 개념적 정의 --- “배우는 방법을 배우는 것” (learning to learn) > meta-learning (메타 러닝)은 기계학습에서 점점 더 중요한 개념이 되고 있으며, 특히 few-shot learning, online adaptation, 개인화 모델링 등에서 매우 강력한 도구입니다. > Meta-learning, also known as \"learning how to learn,\" is a cutting-edge approach in machine learning that focuses on algorithms that learn from their experiences and adapt to new data more effectively 개념만으로는 크게 와닿진 않네. <figure> <img src=\"/post/DataScience/meta_learning_concept_1.webp\" alt=\"Meta-learning 개념도\" /> <figcaption>그림 1. Meta-learning의 기본 개념</figcaption> </figure> > Meta-learning enhances traditional machine learning methods by adding a meta-layer to optimize learning algorithms. 과거의 학습 경험 (데이터나 파라미터)를 바탕으로 새로운 문제에 대해서 빠르게 학습해나가는 방법 <figure> <img src=\"/post/DataScience/A-taxonomy-of-meta-learning-Meta-learning-strategies-are-divided-into-two-main.png\" alt=\"Meta-learning strategies\" /> <figcaption>그림 2. Meta-learning strategies</figcaption> </figure> 기본 아이디어 : 일반적인 머신러닝에서는 하나의 작업(task)에 대해 모델을 학습합니다. 하지만 메타 러닝은 여러 개의 작업들을 보고, 새로운 작업에 빠르게 적응할 수 있는 학습 전략을 학습하는 것이 목표입니다. 일반 머신러닝: 학습: 하나의 데이터셋에서 파라미터를 최적화 테스트: 같은 도메인의 unseen 데이터에 적용 메타 러닝: 학습: 여러 task를 보면서, 모델이 \"어떻게 빠르게 적응할 수 있을지\"를 학습 테스트: 새로운 task에 소수의 샘플(few-shot)만으로 빠르게 적응 Meta-Classifiers and Regressors > Meta-classifiers play a central role in ensemble machine learning techniques. They act as a higher-level classifier that takes the predictions of multiple base-level classifiers to generate a final prediction. 여러 값을 하나로 합치는 역할을 하는 모델. stacking 방법에서 맨 마지막에 쓰이는 모델, 모델의 예측값을 바탕으로 예측값을 생성하는 모델. random-forest도 어떻게 보면 meta-classifier구나. decision-tree 예측값을 voting해서 결과내는 방식이니까.",
    "public": true
  },
  {
    "id": "Data Science/Clean Code Principles/github-notebook-preview",
    "title": "GitHub에서 Jupyter Notebook 리뷰하기 - ReviewNB",
    "date": "2025-08-04",
    "excerpt": "GitHub에서 Jupyter Notebook 코드 리뷰를 쉽게 할 수 있는 ReviewNB 도구 소개",
    "category": "Data Science",
    "tags": [
      "github",
      "jupyter-notebook",
      "code-review",
      "tools"
    ],
    "content": "GitHub에서 Jupyter Notebook 리뷰하기 문제 상황 주피터 노트북을 GitHub에서 관리할 때 겪는 어려움: 기존 방식의 한계 JSON diff: 노트북 파일이 JSON 형태로 표시되어 가독성 떨어짐 리뷰 어려움: 실제 코드와 결과를 확인하기 어려움 검증 복잡: 로컬 환경이나 Colab에 포팅해서 validation 필요 실제 문제점 ``json { \"cells\": [ { \"cell_type\": \"code\", \"execution_count\": 1, \"metadata\": {}, \"outputs\": [], \"source\": [ \"import pandas as pd\\n\", \"df = pd.read_csv('data.csv')\" ] } ] } ` 이런 식으로 JSON이 보이면 실제 코드를 이해하기 어려움! 해결책: ReviewNB What is ReviewNB? > \"Say Goodbye to messy JSON diffs!\" ReviewNB는 GitHub에서 Jupyter Notebook 코드 리뷰를 실제 노트북이 렌더링된 상태에서 직관적으로 할 수 있도록 도와주는 도구입니다. 주요 특징 직관적인 리뷰 ✅ 실제 노트북 형태로 렌더링 ✅ 코드와 결과를 함께 확인 ✅ 셀 단위로 리뷰 가능 무료 사용 ✅ Public 저장소: 무제한 무료 사용 ✅ Private 저장소: 제한적 무료 사용 편리한 기능 ✅ Side-by-side diff 확인 ✅ 과거 PR 검토 가능 ✅ 실시간 코멘트 작성 설치 및 사용법 설치 과정 ReviewNB 웹사이트 방문: https://www.reviewnb.com/ \"Install GitHub App\" 버튼 클릭 GitHub 마켓플레이스에서 Free 버전 설치 저장소 권한 설정 사용 방법 PR 리뷰하기 ` GitHub PR 페이지 방문 ReviewNB 탭 확인 노트북 파일 선택 렌더링된 상태에서 리뷰 ` 코멘트 작성 셀 단위로 코멘트 추가 가능 코드 라인별 코멘트 작성 마크다운 지원 실제 사용 예시 Before (기존 방식) ` JSON diff만 보임 실제 코드 내용 파악 어려움 결과 확인 불가능 ` After (ReviewNB 사용) ` 실제 노트북 형태로 표시 코드와 결과를 함께 확인 직관적인 리뷰 가능 `` 장점과 활용 사례 장점 가독성 향상: JSON 대신 실제 노트북 형태 효율성 증대: 별도 환경 구축 불필요 협업 개선: 팀 리뷰 프로세스 간소화 품질 향상: 더 정확한 코드 검토 활용 사례 데이터 사이언스 프로젝트 머신러닝 모델 코드 리뷰 데이터 전처리 과정 검토 시각화 결과 확인 교육 및 튜토리얼 노트북 튜토리얼 리뷰 교육 자료 품질 관리 학습 자료 검증 연구 프로젝트 연구 노트북 공유 실험 결과 검토 재현성 확인 대안 도구들 nbviewer GitHub 노트북 렌더링 단순 뷰어 기능 Binder 인터랙티브 노트북 실행 환경 설정 복잡 Google Colab 클라우드 기반 실행 GitHub 연동 제한 결론 ReviewNB는 GitHub에서 Jupyter Notebook을 관리하는 팀에게 필수적인 도구입니다. 핵심 가치 간편함: 설치 후 즉시 사용 효율성: JSON diff 문제 해결 무료: Public 저장소 무제한 사용 직관성: 실제 노트북 형태로 리뷰 추천 대상 데이터 사이언스 팀 머신러닝 프로젝트 관리자 교육 기관 연구 프로젝트 팀 링크: https://www.reviewnb.com/ --- 이 도구를 사용하면 GitHub에서 Jupyter Notebook 리뷰가 훨씬 효율적이고 직관적이 됩니다!",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/lenze-size-recommendation-problem-definition",
    "title": "렌즈 사이즈 추천 문제에서 문제 정의에 대한 고민",
    "date": "2025-08-04",
    "excerpt": "렌즈 사이즈 추천에서 인과추론을 도입할 때 문제 정의를 어떻게 할 것인가",
    "category": "Causal Inference",
    "tags": [
      "problem definition"
    ],
    "content": "렌즈 삽입술에서 Y는 vault로 정의가 됨 (지금 풀려고 하는 문제에서는). vault는 렌즈 삽입술 후에 측정되는 값으로, 렌즈 후면과 수정체 전면까지의 거리를 의미함. 여기서 고민사항은 $T=0$인 것을 어떻게 정의해야하는지. $T=0$ 인 상황은 무엇일까? --- 수술 받기 전 상태 수술(처치) 받기 전 상태라고 정의하게 되면 vault값이 존재하지 않는다. 왜냐하면 vault의 정의 자체가 렌즈 후면과 수정체 전면까지의 거리라서. 존재하지 않는 값이지만 계산을 위해 0으로 정의해놔도 괜찮을까? $\\text{vault} = Y(T=0, X=x) = 0$ 가장 작은 크기의 렌즈를 받은 상태 현재 삽입하는 렌즈 크기에는 [12.1, 12.6, 13.2]가 있다. 이때 가장 작은 렌즈를 삽입한 상태를 $T=0$ 처치를 받은 상태로 정의해도 될까? $Y(T=0, X=x) = Y(T=12.1, X=x)$ 인과추론에서 T=0 의 정의가 뭐지? > 처치(treatment)가 적용되기 전의 시점, 즉 control 상태. T=0: 처치가 시작되기 직전의 시점 (baseline), 대조군에 해당하는 값 T=1: 처치가 적용된 후의 시점 (post-treatment) 예를 들어: 의약품 효과 연구: 약을 먹기 전(T=0)과 약을 먹은 후(T=1)의 건강 상태 비교 교육 프로그램 평가: 프로그램 참여 전(T=0)과 참여 후(T=1)의 성적 비교 마케팅 캠페인: 캠페인 시작 전(T=0)과 시작 후(T=1)의 매출 비교 대조군(control group)과 처치군(treatment group)의 개념처럼 어떤 처치의 효과를 추정하려고 할 때 비교대상이 되는 값. 이 개념에 따르면 2번 정의는 말이 안된다. 하지만 이렇게 정의해보는 건 어떨까? > 처치효과는 가장 작은 렌즈 대비 렌즈의 크기가 결과에 미치는 효과 혹은 영향도 처치를 받기 전에 Y값이 없는 경우는 어떻게 하지? (정의가 안될 떄) AI랑 ideation 해보면서 나온 것들 정리. 대안적 결과 변수 정의: vault 대신 다른 결과 변수 사용: 수술 전후 모두 측정 가능한 변수 예: 시력 개선도, 안압 변화, 환자 만족도 등 vault의 변화율: 수술 전후 vault 변화량을 결과 변수로 사용 T=12.1mm 렌즈를 기준(control)으로 정의 — 상대 비교: 완전 무처치는 존재하지 않음 -> 최소 크기 렌즈를 기준점으로 설정: $T=12.1$을 기준으로 $CATE(T=t, X=x) = E[Y T=t, X=x] - E[Y T=12.1, X=x]$ 이는 \"최소 크기 대비 다른 크기의 효과\"를 측정 렌즈 크기 자체가 treatment level인 multi-valued treatment setting $T \\in {12.1, 12.6, 13.2}$ 기준점: $T_0 = 12.1$ 이전 렌즈 대비 개선도: 환자가 이전에 사용하던 렌즈 대비 개선 효과 장점: 인과추론의 틀에서 정의 가능한 treatment comparison 단점: 절대값이 아닌 상대값만 추정 가능하며, baseline vault가 \"충분히 작은가?\"에 대한 해석적 고민 필요 Vault를 0으로 정의 — \"절대 효과로 바꾸기\": vault = 0 가정: 수술 전 vault를 0으로 가정 $CATE(T=t, X=x) = E[Y T=t, X=x] - 0 = E[Y T=t, X=x]$ 이는 \"절대적 vault 효과\"를 측정 CATE가 아닌 ATE와 유사한 구조로 해석됨 특정 렌즈를 넣었을 때 vault값이 얼마나 나오는지를 절대적 관점에서 해석 🧠 결론 및 제안 임상 현실 + 인과추론 이론을 동시에 고려할 때, 가장 자연스러운 정의는: $T=12.1$mm 렌즈를 기준으로 상대적 효과(CATE)를 정의하고, 절대효과(=vault 값 자체)는 최적화 목적의 타겟으로 사용 인과추론: $CATE(T=t, X=x) = E[Y T=t, X=x] - E[Y T=12.1, X=x]$ 추천 목적: $\\arg\\max_t E[Y T=t, X=x]$ 측정하지 않은 값에 대한 예측값은 어떻게 평가할 것인가 --- <figure> <img src=\"/post/Causal_Inference/렌즈추천_평가고민.jpeg\" alt=\"how-to-evalutate-counterfactual\" /> <figcaption>그림1. 측정하지 않은 값에 대한 예측값은 어떻게 평가할 것인가</figcaption> </figure> 내가 생각하는 문제는 결국 그림1과 같은 상황이다. 3가지의 렌즈 크기 중 고객은 하나에 대해서 수술을 받게 되는데, 그러면 나머지 2개에 대한 결과값이 없는 상황에서, 예측모델의 성능은 어떻게 판단할 수 있을까? 여기서 사용되는 개념이 양수성 (positivity). 이는 $ 0 < P(T=t X=x) < 1$ 을 만족하는 것을 의미함. 즉 어떤 고객이 각 처치를 받을 확률이 0보다 크다는 것. 따라서 A는 12.1을 받았지만, A랑 비슷한 다른 고객은 12.6을 받았기 때문에 성능을 확인해볼 수 있다는 것. 처치 간의 분포가 겹쳐있으면 반사실에 대해서도 어느정도 추정이 가능하다. > 이는 \"비슷한 고객들이 다양한 처치를 받았는가?\"라는 질문과 직결됨. $P(T=t X=x)$로 모델의 신뢰도를 정의하는 법 직관적으로 생각해보면 P(T=12.1 X)가 낮다면 해당 고객 같은 경우에는 12.1을 받을 확률이 낮으니까, 12.1에 대한 예측값의 신뢰도가 낮을 것이다. 이를 수학적으로 정의할 수 있을까?",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/how-to-define-confidence-score",
    "title": "렌즈 사이즈 추천 문제에서 모델의 신뢰도를 어떻게 정의할 것인가",
    "date": "2025-08-04",
    "excerpt": "렌즈 사이즈 추천에서 모델의 신뢰도를 어떻게 정의할 것인가",
    "category": "Causal Inference",
    "tags": [
      "confidence score"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/소문및가십/08-03-못믿겠지만탐쿠르즈봤어",
    "title": "못 믿겠지만 탐크루즈 봤어",
    "date": "2025-08-03",
    "excerpt": "믿지 못할만한 소문이나 소식을 전할 때 쓰는 표현들",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 Believe it or not, I bumped into a Tom Cruise. Believe it or not : 너가 믿기 힘들겠지만, 믿든 말든. Believe it or not, Joe and Mary are getting married. Believe it or not, Joe and Mary are engaged. engaged : 약혼하다 divorce : 이혼하다 This may be a surprise : 놀랄만한 일일수도 있는데, 예상치 못했겠지만 This may be a surprise, but Joe is getting divorced. This may be a surprise, but Lenny passed bar exam. bar exam : 변호사 시험 You may not know this : 모를수도 있는데 You may not know this, but Drake is Canadian. 간단한 작문 Believe it or not, I will have love metting(?소개팅, 영어로 뭐지). Someone who is joined in same project suggest it to me. I like her style, maybe she has lovely smile I guess in the picture. You may not know this, my 이상형 is someone who has lovely smile and isn't short (more than 165cm). 피드백이랑 더 자연스러운 작문 Believe it or not, I'm going on a blind date this weekend! Someone from the same project suggested it to me. I really like her style, and from her picture, I can see she has a lovely smile. You might not know this about me, but my ideal type is someone with a beautiful smile and who isn't too short (preferably over 165cm). I'm actually quite nervous about it. I haven't been on a blind date in ages, and I'm not sure what to expect. But I'm trying to stay positive and just be myself. The person who set us up said we have similar interests and personalities, so hopefully we'll hit it off. I'm planning to meet her at a nice café downtown - nothing too fancy, just somewhere comfortable where we can talk and get to know each other. Wish me luck! I'll let you know how it goes. 피드백 원본 작문의 문제점들: 문법 오류들: \"I will have love metting\" → \"I'm going on a blind date\" \"Someone who is joined\" → \"Someone who joined\" \"suggest it to me\" → \"suggested it to me\" 어휘 선택: \"love meeting\" → \"blind date\" (소개팅의 정확한 표현) \"이상형\" → \"ideal type\" 또는 \"type\" 자연스러운 표현: \"I guess in the picture\" → \"from her picture, I can see\" \"isn't short\" → \"isn't too short\" 개선된 점들: 더 자연스러운 문장 구조 감정과 상황 설명 추가 (긴장감, 계획 등) 대화체 톤 유지 (친근하고 자연스러운 느낌) 구체적인 세부사항 (카페에서 만나기, 비슷한 관심사 등) 학습 포인트: 소개팅 = blind date \"hit it off\" = 잘 맞다, 사이가 좋다 \"in ages\" = 오랫동안 \"set us up\" = 소개해주다",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/그래프/합격자가되는모의테스트",
    "title": "합격자가 되는 모의 테스트 - 그래프",
    "date": "2025-08-03",
    "excerpt": "그래프 탐색 파이썬 구현",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "그래프",
      "알고리즘"
    ],
    "content": "--- title: \"합격자가 되는 모의 테스트 - 그래프\" date: \"2025-08-03\" category: \"Codility Study\" tags: [\"코딩 테스트 합격자 되기\", \"Codility Study\", \"그래프\", \"알고리즘\"] excerpt: \"그래프 탐색 파이썬 구현\" --- # 문제 42: 게임 맵 최단 거리 https://school.programmers.co.kr/learn/courses/30/lessons/1844 5x5 크기의 게임맵에서 (1, 1) -> (5, 5)까지 가는 최단거리를 구하는 문제 제약조건 maps는 n x m 크기의 게임 맵의 상태가 들어 있는 2차원 배열. 1 <= n, m <= 100 n과 m은 같거나 다를 수 있음 maps는 0과 1로 구성됨 0 : 벽 1 : 벽이 없는 자리 def solution(maps: list) -> int: # 관련 값들 정의 start_x, start_y = 0, 0 end_x, end_y = len(maps[0]) - 1, len(maps[1]) -1 distance_list = [] def bfs(x, y, distance): print(x, y, distance) if x == end_x and y == end_y: distance_list.append(distance) return if 0 < x + 1 < end_x and 0 < y < end_y and maps[x + 1][y] == 1: maps[x + 1][y] = 0 bfs(x + 1, y, distance + 1) elif 0 < x - 1 < end_x and 0 < y < end_y and maps[x -1][y] == 1: maps[x - 1][y] = 0 bfs(x - 1, y, distance + 1) elif 0 < x < end_x and 0 < y - 1 < end_y and maps[x][y - 1] == 1: maps[x][y - 1] = 0 bfs(x, y - 1, distance + 1) elif 0 < x < end_x and 0 < y + 1 < end_y and maps[x][y + 1] == 1: maps[x][y + 1] = 0 bfs(x, y + 1, distance + 1) bfs(start_x, start_y, 0) return distance_list solution_1 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,1],[0,0,0,0,1]]) print(solution_1) assert solution_1 == 11 solution_2 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,0],[0,0,0,0,1]]) print(solution_2) assert solution_2 == -1 from collections import deque def solution(maps: list) -> int: # 1. 이동할 수 있는 방향 정의 move = [[-1, 0], [0, -1], [0, 1], [1, 0]] # 2. 맵의 크기 저장 n = len(maps) m = len(maps[0]) # 3. 거리를 저장하는 배열 초기화 dist = [[-1] * m for _ in range(n)] # 4. bfs def bfs(start): q = deque([start]) dist[start[0]][start[1]] = 1 while q: here = q.popleft() for direct in move: row, column = here[0] + direct[0], here[1] + direct[1] if row < 0 or row >= n or column < 0 or column >= m: continue if maps[row][column] == 0: continue if dist[row][column] == -1: q.append([row, column]) dist[row][column] = dist[here[0]][here[1]] + 1 return dist bfs([0, 0]) return dist[n - 1][m - 1] solution_1 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,1],[0,0,0,0,1]]) print(solution_1) assert solution_1 == 11 solution_2 = solution(maps=[[1,0,1,1,1],[1,0,1,0,1],[1,0,1,1,1],[1,1,1,0,0],[0,0,0,0,1]]) print(solution_2) assert solution_2 == -1",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/그래프/몸풀기문제",
    "title": "뭄풀기 문제 - 그래프",
    "date": "2025-08-03",
    "excerpt": "그래프 탐색 파이썬 구현",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "그래프",
      "알고리즘"
    ],
    "content": "--- title: \"뭄풀기 문제 - 그래프\" date: \"2025-08-03\" category: \"Codility Study\" tags: [\"코딩 테스트 합격자 되기\", \"Codility Study\", \"그래프\", \"알고리즘\"] excerpt: \"그래프 탐색 파이썬 구현\" --- # 문제 38 : 깊이 우선 탐색 순회 입출력의 예 graph : 인접리스트 / start / return : 리스트 def solution(graph: list[list[str]], start: str) -> list[str]: def dfs(node): # 방문했던 거면 pass if visited[node]: return # 방문처리 visited[node] = True visited_list.append(node) if node in graph_list: for adj_nodes in graph_list[node]: dfs(adj_nodes) graph_list = {} node_set = set() for start_node, end_node in graph: if start_node in graph_list: graph_list[start_node].append(end_node) else: graph_list[start_node] = [end_node] node_set.add(start_node) node_set.add(end_node) visited = {key: False for key in node_set} # 노드가 숫자인 경우에는 [False] len(node) 이런식으로 가능 visited_list = [] dfs(start) return visited_list # 답안지 풀이 from collections import defaultdict adj_list = defaultdict(list) visited = set() result = [] def dfs(node): visited.add(node) result.append(node) for adj_node in adj_list.get(node, []): if adj_node not in visited: dfs(adj_node) def solution(graph, start): for u, v in graph: adj_list[u].append(v) dfs(start) return result solution_1 = solution(graph=[['A', 'B'], ['B', 'C'], ['C', 'D'], ['D', 'E']], start='A') print(solution_1) assert solution_1 == [\"A\", \"B\", \"C\", \"D\", \"E\"] solution_2 = solution(graph=[['A', 'B'], ['A', 'C'], ['B', 'D'], ['B', 'E'], ['C', 'F'], ['E', 'F']], start='A') print(solution_2) assert solution_2 == [\"A\", \"B\", \"D\", \"E\", \"F\", \"C\"] 시간복잡도 계산하기 : 노드 개수 N, 간선 개수 E라고 하면, 인접리스트 생성 : O(E) 탐색시 모든 노드 방문 : O(N) 따라서 O(N+E) # 문제 39: 너비 우선 탐색 순회 from queue import deque from collections import defaultdict def solution(graph: list, start:int) -> list: adj_list = defaultdict(list) visited = set() for start_node, end_node in graph: adj_list[start_node].append(end_node) visited_list = [] # 헷갈릴 수 있는 포인트가, bfs는 바로 방문할거니까 넣으면서 방문했다는 표시 해주기 queue = deque() queue.append(start) visited.add(start) visited_list.append(start) while queue: next_node = queue.popleft() for adj_node in adj_list[next_node]: if adj_node not in visited: queue.append(adj_node) visited.add(adj_node) visited_list.append(adj_node) return visited_list solution_1 = solution(graph=[(1, 2), (1, 3), (2, 4), (2, 5), (3, 6), (3, 7), (4, 8), (5, 8), (6, 9), (7, 9)], start=1) print(solution_1) assert solution_1 == [1, 2, 3, 4, 5, 6, 7, 8, 9] solution_2 = solution(graph=[(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 0)], start=1) print(solution_2) assert solution_2 == [1, 2, 3, 4, 5, 0] 시간복잡도 : adj_list 생성할 때 : O(E) 탐색할 때 : O(N) # 문제 40: 다익스트라 알고리즘 시작노드에서 각 노드까지 최단거리 구하는 알고리즘 from collections import defaultdict def solution(start:int, numNodes:int, edges:list) -> list: adj_list = defaultdict(list) check_nodes = set() for start_node, end_node, weight in edges: adj_list[start_node].append((end_node, weight)) # 1. 최단거리 결과를 담을 자료구조 INF = 1e20 result = [INF] len(adj_list) result[start] = 0 check_nodes.add(0) for end_node, weight in adj_list[start]: if result[end_node] > weight: result[end_node] = weight # 2. 가장 작은 노드부터 greedy하게 갱신하기 # 가장 작은 거리인 노드를 찾는것도 짜줘야하는건가? min_node = None for i, tmp in arange(result): if i not in check_nodes # 우선 순위 큐 (heap) 자료구조로 최단 거리를 관리 import heapq from collections import defaultdict, deque from turtle import distance INF = 1e10 def solution(start:int, numNodes:int, edges:list) -> list: # 1. 그래프 초기화 graph = defaultdict(list) for start_node, end_node, weight in edges: graph[start_node].append((end_node, weight)) # 2. 최단경로 길이 및 방문 기록 초기화 distances = [INF] numNodes visited = [False] numNodes distances[start] = 0 # 3. 우선순위 큐 priority_queue = [(0, start)] # (거리, 노드) while priority_queue: # 4. 현재 노드 찾기 current_distance, current_node = heapq.heappop(priority_queue) # 5. 이미 방문한 노드는 무시 if visited[current_node]: continue # 6. 현재 노드 방문 처리 visited[current_node] = True # 7. 인접 노드에 대한 거리 업데이트 for adj_node, weight in graph[current_node]: new_distance = distances[current_node] + weight if new_distance < distances[adj_node]: distances[adj_node] = new_distance heapq.heappush(priority_queue, (new_distance, adj_node)) return distances solution_1 = solution(start=0, numNodes=3, edges=[[0, 1, 9], [0, 2, 3], [1, 0, 5], [2, 1, 1]]) print(solution_1) assert solution_1 == [0, 4, 3] solution_2 = solution(start=0, numNodes=4, edges=[[0, 1, 1], [1, 2, 5], [2, 3, 1]]) print(solution_2) assert solution_2 == [0, 1, 6, 7] 시간복잡도 : 인접 리스트 생성 : O(E) 최소 비용 찾는 부분 : O(VlogV) 최종 : O(E + VlogV) # 벨만-포드 알고리즘 음의 가중치가 있을 때 최소거리 구하는 방법 from collections import defaultdict import dis INF = 1e15 def solution(num_vertices:int, edges:list, source: int) -> list int: # 1. adj list adj_list = defaultdict(list) for start_node, end_node, weight in edges: adj_list[start_node].append((end_node, weight)) # 2. distasnce 초기화 distance = [INF] num_vertices distance[source] = 0 # 3. 정점의 개수 -1만큼 최소 비용을 갱신 for _ in range(num_vertices - 1): for u in range(num_vertices): for v, weight in adj_list[u]: if distance[u] + weight < distance[v]: distance[v] = distance[u] + weight # 4. 음의 순환이 있는지 확인 for u in range(num_vertices): for v, weight in adj_list[u]: if distance[u] + weight < distance[v]: return [-1] return distance solution_1 = solution(num_vertices=5, edges=[[0, 1, 4], [0, 2, 3], [0, 4, -6], [1, 3, 5], [2, 1, 2], [3, 0, 7], [3, 2, 4], [4, 2, 2]], source=0) print(solution_1) assert solution_1 == [0, -2, -4, 3, -6] solution_2 = solution(num_vertices=4, edges=[[0, 1, 5], [0, 2, -1], [1, 2, 2], [2, 3, -2], [3, 0, 2], [3, 1, 6]], source=0) print(solution_2) assert solution_2 == [-1] # 음의 순환이 있을 때 시간복잡도 = O(N E)",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/자료구조/heapq",
    "title": "heapq 개념정리",
    "date": "2025-08-03",
    "excerpt": "heapq에 대한 설명과 어떻게 사용되는지 정리",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "자료구조"
    ],
    "content": "heapq와 우선순위 큐 (Priority Queue) <figure> <img src=\"/post/CodilityStudy/heapq.png\" alt=\"heapq 개념도\" /> <figcaption>heapq 그림</figcaption> </figure> heapq란? Python의 내장 모듈로, 최소 힙(Min Heap)을 구현하는 모듈입니다. ``python import heapq 빈 힙 생성 heap = [] 요소 추가 heapq.heappush(heap, 5) heapq.heappush(heap, 2) heapq.heappush(heap, 8) heapq.heappush(heap, 1) print(heap) # [1, 2, 8, 5] - 최솟값이 루트에 위치 ` 기본 메서드들 요소 추가 `python heapq.heappush(heap, item) # 힙에 요소 추가 ` 최솟값 제거 및 반환 `python min_value = heapq.heappop(heap) # 최솟값 제거하고 반환 ` 리스트를 힙으로 변환 `python numbers = [5, 2, 8, 1, 9] heapq.heapify(numbers) # 리스트를 힙으로 변환 print(numbers) # [1, 2, 8, 5, 9] ` 최솟값 확인 (제거하지 않음) `python min_value = heap[0] # 힙의 루트 노드 확인 ` 우선순위 큐 구현 기본 우선순위 큐 `python import heapq class PriorityQueue: def __init__(self): self.heap = [] def push(self, item): heapq.heappush(self.heap, item) def pop(self): return heapq.heappop(self.heap) def peek(self): return self.heap[0] if self.heap else None def is_empty(self): return len(self.heap) == 0 사용 예시 pq = PriorityQueue() pq.push(5) pq.push(2) pq.push(8) pq.push(1) print(pq.pop()) # 1 (가장 작은 값) print(pq.pop()) # 2 ` 튜플을 이용한 우선순위 큐 `python (우선순위, 데이터) 형태로 저장 pq = [] heapq.heappush(pq, (3, \"task3\")) heapq.heappush(pq, (1, \"task1\")) heapq.heappush(pq, (2, \"task2\")) print(heapq.heappop(pq)) # (1, \"task1\") - 우선순위가 가장 높은 것 ` 최대 힙 구현 방법 1: 음수 변환 `python 최대 힙을 원할 때는 음수로 변환 numbers = [5, 2, 8, 1, 9] max_heap = [-x for x in numbers] heapq.heapify(max_heap) 최댓값 추출 max_value = -heapq.heappop(max_heap) print(max_value) # 9 ` 방법 2: 튜플 사용 `python (-우선순위, 데이터) 형태 max_heap = [] heapq.heappush(max_heap, (-5, \"task5\")) heapq.heappush(max_heap, (-2, \"task2\")) heapq.heappush(max_heap, (-8, \"task8\")) priority, task = heapq.heappop(max_heap) print(f\"우선순위: {-priority}, 작업: {task}\") # 우선순위: 8, 작업: task8 ` heapq를 사용하는 알고리즘들 5.1 다익스트라 알고리즘 (Dijkstra's Algorithm) 목적: 가중 그래프에서 최단 경로 찾기 `python import heapq def dijkstra(graph, start): distances = {node: float('inf') for node in graph} distances[start] = 0 pq = [(0, start)] while pq: current_dist, current_node = heapq.heappop(pq) if current_dist > distances[current_node]: continue for neighbor, weight in graph[current_node].items(): distance = current_dist + weight if distance < distances[neighbor]: distances[neighbor] = distance heapq.heappush(pq, (distance, neighbor)) return distances ` 5.2 프림 알고리즘 (Prim's Algorithm) 목적: 최소 신장 트리(MST) 찾기 `python def prim(graph, start): mst = [] visited = set() pq = [(0, start, None)] # (가중치, 노드, 부모) while pq: weight, node, parent = heapq.heappop(pq) if node in visited: continue visited.add(node) if parent is not None: mst.append((parent, node, weight)) for neighbor, edge_weight in graph[node].items(): if neighbor not in visited: heapq.heappush(pq, (edge_weight, neighbor, node)) return mst ` 5.3 A* 알고리즘 목적: 휴리스틱을 사용한 최단 경로 찾기 `python def a_star(graph, start, goal, heuristic): pq = [(0, start, [start])] # (f_score, node, path) g_score = {start: 0} f_score = {start: heuristic(start, goal)} while pq: current_f, current, path = heapq.heappop(pq) if current == goal: return path for neighbor, weight in graph[current].items(): tentative_g = g_score[current] + weight if neighbor not in g_score or tentative_g < g_score[neighbor]: g_score[neighbor] = tentative_g f_score[neighbor] = tentative_g + heuristic(neighbor, goal) heapq.heappush(pq, (f_score[neighbor], neighbor, path + [neighbor])) return None ` 5.4 허프만 코딩 (Huffman Coding) 목적: 데이터 압축을 위한 가변 길이 인코딩 `python from collections import Counter def huffman_coding(text): # 빈도수 계산 freq = Counter(text) # 힙에 모든 문자 추가 heap = [[freq[char], char] for char in freq] heapq.heapify(heap) # 허프만 트리 구성 while len(heap) > 1: left = heapq.heappop(heap) right = heapq.heappop(heap) # 새로운 노드 생성 new_node = [left[0] + right[0], left, right] heapq.heappush(heap, new_node) return heap[0] # 루트 노드 ` 5.5 Top K 문제 목적: 배열에서 상위 K개의 요소 찾기 `python def find_top_k(nums, k): # 최소 힙을 사용해서 상위 K개 유지 heap = [] for num in nums: heapq.heappush(heap, num) if len(heap) > k: heapq.heappop(heap) # 가장 작은 값 제거 return sorted(heap, reverse=True) # 내림차순 정렬 사용 예시 nums = [3, 2, 1, 5, 6, 4] print(find_top_k(nums, 2)) # [6, 5] - 상위 2개 ` 5.6 중간값 찾기 (Median) 목적: 스트리밍 데이터에서 중간값 유지 `python class MedianFinder: def __init__(self): self.max_heap = [] # 왼쪽 절반 (최대 힙) self.min_heap = [] # 오른쪽 절반 (최소 힙) def addNum(self, num): # 최대 힙에 추가 heapq.heappush(self.max_heap, -num) # 균형 맞추기 if self.max_heap and self.min_heap and -self.max_heap[0] > self.min_heap[0]: val = -heapq.heappop(self.max_heap) heapq.heappush(self.min_heap, val) # 크기 균형 맞추기 if len(self.max_heap) > len(self.min_heap) + 1: val = -heapq.heappop(self.max_heap) heapq.heappush(self.min_heap, val) elif len(self.min_heap) > len(self.max_heap): val = heapq.heappop(self.min_heap) heapq.heappush(self.max_heap, -val) def findMedian(self): if len(self.max_heap) > len(self.min_heap): return -self.max_heap[0] return (-self.max_heap[0] + self.min_heap[0]) / 2 ` 5.7 병합 K개 정렬된 리스트 목적: K개의 정렬된 리스트를 하나로 병합 `python def merge_k_sorted_lists(lists): pq = [] result = [] # 각 리스트의 첫 번째 요소를 힙에 추가 for i, lst in enumerate(lists): if lst: heapq.heappush(pq, (lst[0], i, 0)) while pq: val, list_idx, element_idx = heapq.heappop(pq) result.append(val) # 다음 요소가 있으면 힙에 추가 if element_idx + 1 < len(lists[list_idx]): heapq.heappush(pq, (lists[list_idx][element_idx + 1], list_idx, element_idx + 1)) return result ` 시간복잡도 삽입: O(log n) 삭제: O(log n) 최솟값 확인: O(1) 힙 생성: O(n) 코딩테스트 팁 자주 사용되는 패턴 `python 최소값/최대값 유지 min_heap = [] max_heap = [] 우선순위와 함께 데이터 저장 pq = [(priority, data)] 여러 정보를 튜플로 저장 pq = [(cost, node, path)] 음수 변환으로 최대 힙 구현 max_heap = [-x for x in numbers] ` 주의사항 튜플 비교: 첫 번째 요소부터 순서대로 비교 같은 우선순위: 두 번째 요소로 구분 최대 힙: 음수 변환 또는 (-priority, data) 형태 중복 방문: 방문 체크 필수 (다익스트라 등) 핵심 포인트: heapq`는 최소 힙을 구현 우선순위 큐의 기본 구현체 그래프 알고리즘에서 필수적 실시간 데이터 처리에 유용 메모리 효율적인 자료구조",
    "public": true
  },
  {
    "id": "Self_Development/Singing/장효진강의/3강_깊게 마시고 뱉기를 통한 호흡 익히기",
    "title": "3강 깊게 마시고 뱉기를 통한 호흡 익히기",
    "date": "2025-08-02",
    "excerpt": "깊게 마시고 뱉는 법을 통해 내복근 느낌 잡는 법. 관련 연습법 정리",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "장효진 강의",
      "연습방법"
    ],
    "content": "점진적 과부하 공기를 다 뺐다고 느낄 때, 그 떄 떠 뺼떄 내복근이 더 많이 쓰임 똑같은 힘으로 끝까지 뱉는 느낌 허리펴고. 같은 힘으로. > 마지막에 마지막에 마지막까지 뱉어주기",
    "public": true
  },
  {
    "id": "Self_Development/Singing/장효진강의/2강_내복근_컨트롤",
    "title": "2강 내복근 컨트롤",
    "date": "2025-08-02",
    "excerpt": "2강 내복근 컨트롤 , 에 대한 내용정리",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "장효진 강의"
    ],
    "content": "유로인강 유튭 링크 호흡을 내뱉는 법. 패시브가 되어야 함. 횡경막의 역할. 나온 배를 다시 배만 정위치로 돌려놓는 느낌. \"배만 움직이는 느낌\" \"피지컬의 중요성\" 가슴도 안써야됨. 배로만 컨트롤하는 느낌.",
    "public": true
  },
  {
    "id": "Self_Development/Singing/장효진강의/1강_개념정리 및 본질",
    "title": "1강 개념정리 및 본질",
    "date": "2025-08-02",
    "excerpt": "1강 개념정리 및 본질 , 에 대한 내용정리",
    "category": "Singing",
    "tags": [
      "Singing Practice",
      "장효진 강의"
    ],
    "content": "전체적인 목차에 대한 설명 호흡에 대한 자세한 설명 발성의 시작. 소리를 만드는 법 성대 컨트롤과 관련된 내용들 정~~말 많은 내용이 있구나. 꾸준히 하나씩 들어보자고. > 시키는대로만 하세요. 묻지도 따지지도 말고 영상에서 하라는대로.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/그래프/그래프_최단경로_구하기",
    "title": "그래프 최단 경로 구하기",
    "date": "2025-08-02",
    "excerpt": "그래프 최단 경로 구하기",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "그래프",
      "알고리즘"
    ],
    "content": "> 최단 경로 (shortest path)는 그래프의 종류에 따라 그 진의가 다르게 해석될 수도 있는 주제입니다. 가중치의 여부. 알고리즘 목적 장단점 및 특징 시간 복잡도 --------- ------ ---------------- ------------- 다익스트라 알고리즘 출발 노드로부터 도착 노드들까지의 최단 경로 찾기 음의 가중치를 가지는 그래프에서 최단 경로를 구할 수 없음 (그리디 방식) O(V^2), 우선순위 큐로 개선하면 O(E logV) 벨만-포드 알고리즘 출발 노드로부터 도착 노드들까지의 최단 경로 찾기 음의 가중치를 가지는 그래프에서 최단 경로를 구할 수 있고, 음의 순환도 감지할 수 있음 O(VE) 다익스트라(dijkstra) 알고리즘 --- 모든 가중치가 양수일 때. > 다익스트라 알고리즘의 목적은 그래프 내에서 특정 노드를 기준으로 각 노드까지 최단 경로 또는 최단 경로의 길이를 찾는 것에 있습니다. > 다익스트라 알고리즘의 핵심은 현재까지 발견한 가장 적은 비용으로 갈 수 있는 노드를 경유해서, 각 노드까지 최단 경로를 갱신하는 작업을 반복하는 것입니다. 시작 노드에서 각 노드까지 거리를 초기화 현재 가장 적은 비용으로 갈 수 있는 노드 선택 (각 노드는 한 번씩만 선택) 2에서 선택한 노드의 주변 노드 거리를 갱신 알고리즘 동작 순서 시작 노드를 설정하고 시작 노드로부터 특정 노드까지의 최소 비용을 저장할 공간과 직전 노드를 저장할 공간을 마련합니다. 1-1. 최소 비용을 저장할 공간은 모두 매우 큰 값으로 초기화합니다. 1-2. 시작 노드의 최소비용은 0, 직전 노드는 자신으로 합니다. 현재까지 구한 각 노드까지 최소 비용을 기준으로 지금까지 선택하지 않은 노드 중 최소 비용이 가장 적은 노드를 선택합니다. 2-1. 해당 노드를 거쳐서 각 노드까지 가는 최소 비용과 현재까지 구한 최소 비용을 비교하여 작은 값을 각 노드의 최소 비용으로 갱신합니다. 2-2. 이때 직전 노드도 같이 갱신합니다. 노드 개수에서 1을 뺀 만큼 반복합니다. 벨만-포드(bellman-ford) 알고리즘 --- > 벨만-포드 알고리즘은 매 단계마다 모든 간선의 가중치를 다시 확인하여 최소 비용을 갱신하므로 음의 가중치를 가지는 그래프에서도 최단 경로를 구할 수 있습니다. 알고리즘 동작순서 시작노드를 설정한 다음 시작 노드의 최소 비용은 0, 나머지 노드는 INF로 초기화. 노드 개수 -1 만큼 다음 연산은 반복 2-1. 시작 노드에서 갈 수 있는 각 노드에 대하여 전체 노드 각각을 거쳐갈 때 현재까지 구한 최소 비용보다 더 적은 최소 비용이 있는지 확인하여 갱신. 과정2를 마지막으로 한번 더 수행하여 갱신되는 최소 비용이 있는지 확인. 있다면 음의 순환이 있음을 의미. 왜 정점개수 -1만큼 반복하는가? 매 연산마다 최단경로가 1개씩 확정되므로! 벨만-포드 알고리즘에서 정점 개수 - 1번만큼 반복하는 이유는 최단 경로의 특성 때문입니다. 핵심 원리 최단 경로는 최대 V-1개의 간선으로 구성됩니다 n개의 정점이 있을 때, 최단 경로는 최대 n-1개의 간선을 가질 수 있습니다 만약 n개 이상의 간선이 있다면, 그 경로에는 반드시 순환(cycle)이 포함되어 있습니다 예시로 이해하기 `` 정점: A, B, C, D (4개) 가능한 최단 경로: A → B → C → D (3개 간선, 최대) A → B → D (2개 간선) A → D (1개 간선) ` 왜 V-1번이면 충분한가? 1번째 반복: 출발점에서 1개 간선으로 도달 가능한 모든 정점의 최단 거리 확정 2번째 반복: 출발점에서 2개 간선으로 도달 가능한 모든 정점의 최단 거리 확정 ... (V-1)번째 반복: 출발점에서 (V-1)개 간선으로 도달 가능한 모든 정점의 최단 거리 확정 이후 더 반복해도 새로운 최단 경로를 찾을 수 없습니다. 왜 한번 더 연산을 반복하는가? 음의 순환을 찾기 위해! 벨만-포드 알고리즘의 마지막 단계에서 한 번 더 반복하는 이유는 음의 순환(negative cycle)을 감지하기 위해서입니다. 음의 순환이란? ` A → B → C → A (순환) 가중치: 2 → 3 → (-6) → 2 ` 이 경우 A에서 시작해서 A로 돌아올 때마다 비용이 -1씩 감소합니다. 즉, 무한히 작아질 수 있습니다. 감지 방법 V-1번 반복: 정상적인 최단 경로 계산 V번째 반복: 한 번 더 반복하여 거리 값이 갱신되는지 확인 만약 거리 값이 갱신된다면 → 음의 순환이 존재 만약 거리 값이 갱신되지 않는다면 → 음의 순환이 없음 예시 ` 정점: A, B, C 간선: A→B(2), B→C(3), C→A(-6) V-1번 반복 후: dist[A] = 0, dist[B] = 2, dist[C] = 5 V번째 반복 시: dist[A] = 5 + (-6) = -1 (갱신됨!) → 음의 순환 존재! `` 왜 V번째에서 감지되는가? V-1번 반복으로 모든 정상적인 최단 경로를 찾았습니다 V번째 반복에서 거리 값이 갱신된다면, 그것은 V개 이상의 간선을 사용하는 경로입니다 V개 이상의 간선을 사용하는 경로는 반드시 순환을 포함합니다 그 순환이 음의 가중치를 가진다면, 거리 값이 계속 감소하게 됩니다",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/08-01-최악의대비_리뷰",
    "title": "최악의 대비 리뷰 데이",
    "date": "2025-08-01",
    "excerpt": "최악의 대비 관련 표현들 리뷰 데이",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "리뷰하는 것들 What if I mess up? What if I fail the exam? What if it rains? What if the flight is delayed? What if I miss thre connecting flight? What if I miss my flight? What if I miss my train? What if you were in Seoul? What if you were in New York? What would you do if you were in Newyork? If it hadn't been for your help, I wouldn't have made it. Would you have moved to New York? Let me know if you're interested. I don't mind if we go clubbing. Even if it's inconvenient, I'll still wear a mask. No matter what happens, I'll always be there for you.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/현대오토에버코테",
    "title": "현대오토에버 코테 준비하기",
    "date": "2025-08-01",
    "excerpt": "현대오토에버 코테 준비하기",
    "category": "Codility Study",
    "tags": [
      "코테 준비"
    ],
    "content": "시험 기출 문제 유형 참고한 블로그 2025년 기출 문제 유형 백엔드 총 2문제 DFS 문제 그래프가 나뉘어져 있는 경우와 사이클인 경우를 고려해야 하는 문제. DP 유형의 문제 늘보코치의 IT 취업 코테 + 서류로 보통 1차 평가 예정 알고리즘 위주인데 시간복잡도를 줄여주는 알고리즘 선호 투포인터 (백준 1253번) 완전탐색으로 풀면 시간 초과 이분탐색 (백준 6236번) DP 기본문제 (백준 12865번), 메모이제이션 (백준 9184번) 우선순위 큐 (백준 1766번) 누적합 (백준 2143번)",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/그래프/그래프_탐색_노트북",
    "title": "그래프 탐색 파이썬 구현",
    "date": "2025-08-01",
    "excerpt": "그래프 탐색 파이썬 구현",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "그래프",
      "알고리즘"
    ],
    "content": "--- title: \"그래프 탐색 파이썬 구현\" date: \"2025-08-01\" category: \"Codility Study\" tags: [\"코딩 테스트 합격자 되기\", \"Codility Study\", \"그래프\", \"알고리즘\"] excerpt: \"그래프 탐색 파이썬 구현\" --- # DFS 구현 !DFS 그래프 그림 # 인접 리스트로 표시된 그래프 graph = { 1: [4, 5], 2: [3], 3: [], 4: [2, 3], 5: [4] } 방문 여부 visited = [False] (len(graph) + 1) def dfs(current_node): visited[current_node] = True print(current_node) for neighbor in graph[current_node]: if not visited[neighbor]: dfs(neighbor) dfs(1) # BFS 구현 from collections import deque 인접 리스트로 표시된 그래프 graph = { 1: [4, 5], 2: [3], 3: [], 4: [2, 3], 5: [4] } def bfs(start_node): # 방문 여부 visited = [False] (len(graph) + 1) queue = deque([start_node]) visited[start_node] = True while queue: node = queue.popleft() print(node) for adj_node in graph[node]: if not visited[adj_node]: queue.append(adj_node) visited[adj_node] = True bfs(1)",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/그래프/그래프_탐색",
    "title": "그래프 탐색 개념 정리",
    "date": "2025-08-01",
    "excerpt": "그래프 탐색 개념 정리",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "그래프",
      "알고리즘"
    ],
    "content": "구현 노트북 그래프 탐색 파이썬 구현 DFS / BFS 더 이상 탐색할 노드가 없을 때까지 일단 가봅니다. 그러다가 더 이상 탐색할 노드가 없으면 최근에 방문했던 노드로 되돌아간 다음 가지 않은 노드를 방문합니다. 현재 위치에서 가장 가까운 노드부터 모두 방문하고 다음 노드로 넘어갑니다. 그 노드에서 또 다시 가장 가까운 노드부터 모두 방문합니다. 최단경로찾기 일때는 보통 BFS, 나머지는 DFS인 경우가 많다. > DFS와 BFS가 방문처리 시점이 다른 이유 : > DFS는 stack에서 pop을 할 때 방문처리를 하지만, BFS는 queue에 push할 때 방문처리를 함. > 왜냐하면 DFS는 나중에 확인할 노드이고, BFS는 바로 확인할 노드이기 때문에. DFS(Deep-First Search) : 깊이 우선 탐색 > 최대 깊이 노드까지 방문한 다음에는 이전에 방문한 노드를 거슬러 올라가며 해당 노드와 연결된 노드 중 방문하지 않은 노드로 다시 최대 깊이까지 차례대로 방문합니다. stack 자료구조 활용 <figure> <img src=\"/post/CodilityStudy/stack.png\" alt=\"stack\" style=\"width: 100%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">그림1. stack 자료구조 : FILO(First In Last Out) 구조</figcaption> </figure> sudo code : 스택이 비었는지 확인. 스택이 비었다면 -> 모든 노드 방문 완료 -> 종료 스택에서 노드를 팝. 팝한 노드는 최근에 스택에 푸쉬한 노드 팝한 노드의 방문 여부 확인 아직 방문 안했으면 -> 노드를 방문처리 방문한 노드와 인접한 모든 노드를 확인 아직 방문하지 않은 노드를 스택에 푸쉬. 고려사항 : 탐색할 노드가 없을 때까지 간선을 타고 내려갈 수 있어야 함 가장 최근에 방문한 노드를 알아야 함 이미 방문한 노드인지 확인할 수 있어야 함 > 가장 깊은 노드까지 방문한 후에 더 이상 방문할 노드가 없으면 최근 방문한 노드로 돌아온 다음, 해당 노드에서 방문할 노드가 있는지 확인한다. back tracking : 탐색하고 있는 방향의 역방향으로 되돌아가는 동작 스택은 최근에 푸쉬한 노드를 팝 연산으로 확인할 수 있으므로 특정 노드를 방문하기 전에 최근 방문노드를 팝 연산으로 쉽게 확인할 수 있음. 스택을 활용한 구현 / 재귀를 활용한 구현 스택을 활용한 깊이 우선 탐색 : <figure> <img src=\"/post/CodilityStudy/DFS_그려보기.png\" alt=\"DFS\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">그림2. DFS stack 구현</figcaption> </figure> 재귀 함수를 활용한 깊이 우선 탐색 : > 재귀 함수를 호출할 때마다 호출한 함수는 시스템 스택이라는 곳에 쌓이므로 깊이 우선 탐색에 활용할 수 있는 것입니다. <figure> <img src=\"/post/CodilityStudy/DFS_재귀.png\" alt=\"DFS\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">그림3. DFS 재귀 구현</figcaption> </figure> stack으로 구현하는 것과 논리는 같음. 스택에 쌓아주는 대신 이웃노드에 대한 재귀함수를 실행시켜주는 방식. BFS(Breadth-First Search) : 너비 우선 탐색 Queue 자료구조 활용 <figure> <img src=\"/post/CodilityStudy/queue.webp\" alt=\"queue\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">그림4. queue 자료구조 : FIFO(First In First Out) 구조</figcaption> </figure> <figure> <img src=\"/post/CodilityStudy/BFS_큐.png\" alt=\"BFS\" style=\"width: 50%; height: auto; display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\">그림5. BFS 큐 구현</figcaption> </figure>",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/코딩테스트합격자되기/그래프/그래프_개념정리",
    "title": "그래프 개념정리",
    "date": "2025-08-01",
    "excerpt": "그래프 개념정리",
    "category": "Codility Study",
    "tags": [
      "코딩 테스트 합격자 되기",
      "Codility Study",
      "자료구조",
      "그래프"
    ],
    "content": "그래프란? 정의 : > 그래프는 노드 (vertex)와 간선 (edge)을 이용한 비선형 데이터 구조 데이터의 관계를 표현하는데 사용 그래프의 특징과 종류 : 방향성 존재 여부 방향 그래프 (directed graph) : 간선에 방향성이 있는 그래프 무방향 그래프 (undirected graph) : 간선에 방향성이 없는 그래프 간선의 가중치 존재 여부 가중치 그래프 (weighted graph) : 간선에 가중치가 있는 그래프 무가중치 그래프 (unweighted graph) : 간선에 가중치가 없는 그래프 순환 여부 순환 그래프 (cyclic graph) : 간선을 따라 이동하면 출발점으로 돌아오는 경로가 존재하는 그래프 비순환 그래프 (acyclic graph) : 간선을 따라 이동하면 출발점으로 돌아오는 경로가 존재하지 않는 그래프 그래프 구현 : 그래프의 표현 방법 인접 행렬 (adjacency matrix) : 2차원 배열로 그래프의 연결 관계를 표현 인접 리스트 (adjacency list) : 연결 리스트로 그래프의 연결 관계를 표현 > 입력값이 적거나 특정 정점 사이의 간선 여부를 빠르게 판단해야 할 때는 인접 행렬을, 그렇지 않으면 인접 리스트를 사용하면 좋습니다. 인접행렬 노드 간의 관계를 행렬로 표현하는 것 인접 리스트 [정점(v), 가중치(w), 다음 노드(next)] 로 구성된 리스트로 그래프를 표현. 노드에 대한 배열이 있고, 노드별로 인접 리스트를 가진다. 장단점 메모리 사용 시간복잡도 기타 --- --- --- --- 인접 행렬 $O(N^2)$ $O(1)$ 구현이 상대적으로 쉬움 인접 리스트 $O(N+E)$ $O(N)$ 인접 행렬 인접 리스트의 장점 : 정보를 확인할 떄 시간 복잡도가 O(1)로 좋음 인접 행렬의 단점 : 인접 행렬로 희소 그래프를 표현하는 경우 -> 굉장히 비효율적. 메모리 낭비. 노드들의 값의 차이가 매우 큰 그래프를 표현하는 경우 -> 가장 큰 값을 기준으로 인접 행렬의 크기를 잡아야 하니까. 인접 리스트 (인접 행렬과 반대) 인접 리스트의 장점 : 메모리 사용량이 적음 노드들의 값의 차이가 매우 큰 그래프를 표현하는 경우 메모리 낭비가 적음 인접 리스트의 단점 : 정보를 확인할 떄 시간 복잡도가 O(n)으로 비효율적.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-31-좀불편해도마스크쓸거야",
    "title": "좀 불편해도 마스크 쓸 거야",
    "date": "2025-07-31",
    "excerpt": "좀 불편해도 마스크 쓸 거야",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 What if I catch the flu? Even if it's inconvinient, I will still wear a mask. Even if you mess up, we'll still believe in you. No matter what happens, I'll always believe in you. be there for you. 겁나 로맨틱하게 느껴지. Failures are okay as long as youy learn from then. 간단한 작문 Even if I'm tired, I'll practice English. No matter what happens, I'll always be there for you.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-30-고깃집에가도난상관없어",
    "title": "고깃집에 가도 난상관없어",
    "date": "2025-07-30",
    "excerpt": "고깃집에 가도 난상관없어",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 Sharing is caring. 관심 있으면 알려줘 그렇게 해도 상관없어 Let me know. 알려줘. 연락해. Let me know if you're interested. 관심 있으면 알려줘. change your mind. hange out : 같이 시간을 보내다. I don't mind if we go clubbing. 간단한 작문 I totally agree about the sentence \"Sharing is caring.\" I'm willing to share my insights or knowledge with my pioneers and friends. Today I learned the phares that is used in the conversation. Let me know if you wonder my codes. Then I will explain it. And my food type (식성) is so normal. I like almost foods. So I always say to my friends that I don't mind if we have chinese foods. 피드백 문법 오류 \"pioneers\" → \"peers\" (동료를 의미할 때) \"phares\" → \"phrases\" (철자 오류) \"that is\" → \"that are\" (phrases는 복수) \"wonder my codes\" → \"wonder about my code\" (code는 보통 단수형 사용) \"almost foods\" → \"almost all foods\" (거의 모든 음식) \"chinese foods\" → \"Chinese food\" (음식은 보통 단수형) 표현 개선 \"I totally agree about\" → \"I totally agree with\" \"I'm willing to share\" → 더 자연스러운 표현으로 개선 \"Let me know if you wonder\" → 더 자연스러운 표현으로 개선 개선된 버전 I totally agree with the saying \"Sharing is caring.\" I'm always happy to share my insights and knowledge with my peers and friends. Today I learned some useful phrases that are commonly used in conversations. If you're curious about my code, feel free to ask. I'd be happy to explain it to you. As for my food preferences, I'm quite flexible. I enjoy almost all types of food. That's why I always tell my friends that I don't mind going to Chinese restaurants.",
    "public": true
  },
  {
    "id": "Self_Development/Career/site-recommendation",
    "title": "취업 준비 사이트 추천",
    "date": "2025-07-30",
    "excerpt": "취업 준비 사이트 추천",
    "category": "Career",
    "tags": [
      "취업"
    ],
    "content": "취업 준비 사이트 추천 zighang offercent bzpp 링크드인 채용공고나 채용하는 피드 참고하기",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-29-너아니었으면",
    "title": "너 아니었으면",
    "date": "2025-07-29",
    "excerpt": "너 아니었으면, 너라면 어떻게 했겠어?",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운거 정리 match maker 문법. 현재와 다른 어떤 상황을 가정할 때는 과거완료를 사용. If it hadnt been for you, I wouldn't have made it. would've -> /wʊdəv/ (woul-duv) wouldn't -> /wʊdnt/ (woul-dnt) wou-ov 발음 기호를 추가했습니다! 📝 발음 가이드 would've: /wʊdəv/ (woul-duv) wʊ = \"woul\" (wool과 비슷) dəv = \"duv\" (duh + v 소리) wouldn't: /wʊdnt/ (woul-dnt) wʊ = \"woul\" (wool과 비슷) dnt` = \"dnt\" (d + n + t를 빠르게 연결) 실제로는 \"would have\"와 \"would not\"이 축약되어 더 짧고 빠르게 발음되는 형태입니다. Would you have moved to New York?",
    "public": true
  },
  {
    "id": "Self_Development/Career/how-to-write-resume",
    "title": "이력서, 경력기술서 작성 방법 정리",
    "date": "2025-07-29",
    "excerpt": "이력서, 경력기술서 적는 방법 정리",
    "category": "Career",
    "tags": [
      "이력서"
    ],
    "content": "참고링크: 유튜브 이형 - 경력기술서 작성법 총정리 이직이 처음이라면 이것부터 보세요 이직바이블 137p ~ 187p 경력기술서 작성방법 : 최근경력 - 지원직무에 맞게 먼저 제출 직무와 무관한 경험에 대해서는 과감하게 삭제 연차별 1칸씩은 필수 가급적 10개 미만 경력기술서의 내용 : 구분 작성 방법 --- --- 기간 프로젝트 중심으로 설명 성과 (1줄 이내) How + Result -> KPI 역할 (5줄 이내) 1. 문제 해결 관점: 핵심 문제 해결 과정에서의 역할<br>2. 생산성 향상 관점: 업무 효율성 개선에서의 역할<br>3. 의사소통 관점: 팀 협업 및 커뮤니케이션에서의 역할 기술 1. 도구/툴: 사용한 기술 스택<br>2. 프로세스 이해: 업무 프로세스에 대한 이해도 물경력 진단 리스트: 숫자로 설명할 수 있는 성과가 없다. 내 직무 KPI가 무엇인지 모른다 진행한 프로젝트/업무의 Before/After가 명확하지 않다. 성과를 달성하기 위해 구체적으로 어떤 역할을 수행했는지 설명할 수 없다. 연속 근무 기간이 1년 미만이다. 최근 3년 내 2곳 이상으로 이직했다. 동기나 동일 연차에 비해 승진이 늦다. 경력 기술서 작성 프로세스 : 경력 기술서를 위한 소재 준비하기 : 경험 리스트업 경험 리스트업 템플릿을 활용하여 주요 활동 정리하기 KPI 매칭하기 경력 기술서 작성하기 프로젝트명 기간 성과 역할 기술 > 간혹 경력 기술서를 구체적이고 길게 서술하는 경우가 있는데, 경력 기술서의 목적 자체가 핵심을 간결하게 보기 위함이기 때문에 > 우리는 면접관이 궁금해야할 내용만 작성하자. 예시 직무 : MD 프로젝트명/업무명 : 명절 시즌 라이브 판매 방송으로 매출 154% 견인 기간 : 2010년 9월 ~ 2020년 10월 (2개월) 성과 : 5000명의 관심고객 CRM 분석을 통한 명절 판매 방송으로 라이브 매출 1억 달성 역할 : 경쟁사 100개 상품조살르 바탕으로 3만원 대 세트 상품을 제안해 객단가 50% 증가 5000명 관심고객 대상으로 CRM 마케팅 진행으로 유입 300% 증가 100개 벤더사 분석을 통해 벤더사 선정으로 기획 단가 30% 절감 기술 : Jira, Data Studio, Slack 라이브 커머스 프로세스에 대한 이해 직무: 품질관리 프로젝트명/업무명 : 대만 공장 생산 가동 기간 : 2019.10 ~ 2020.10 (1년) 성과 : 신규 품질관리 시스템 구축을 통해 대만 공장 생산 제품 불량률 80% 감소 역할 : 불량품 100% 전수검사를 통해 완제품 결함 비용 30% 감소 반복되는 접착 결함 이슈 프로세스 개선을 통한 검사 시간 10% 감소 유효기간 만료된 완제품을 결함 테스트용으로 사용하여 악성 재고 70% 처리 불량품 데이터 100개 분석을 통해 생산 메뉴얼 개발 및 교육 불량 발생 원인 파악을 통해 신규 장비 구매로 생산비 3% 절감 기술 : QMS, ISO 9001 대만 법률에 대한 이해 직무: 재무회계 프로젝트명/업무명 : 분기 재무 실사 기획 및 분석 보고 기간 : 2022년 8월 ~ 2022년 10월 (3개월) 성과: 분기별 재고조사 총 10회 실시를 통해 재무 데이터 일치율 10%p 상승 역할 현장 재고조사를 통해 자료 오차율 0% 달성 실사 내역 분석을 통해 세무 증빙 업무 프로세스 변경 및 메뉴얼 교육 잘못 기재된 계정 70% 수정 재고관리 프로세스 개선을 통해 재고 관리 비용 10% 절감 기술 : ERP 자산 유형별 재무관리 법률에 대한 이해 경력 기술서 탬플릿 사용 가이드 기간 : 되게 중요한 부분. 꼭 번거롭지 않게 몇개월 한건지를 적어주자. 기간 : 2022.08 ~ 2022.10 (3개월) 성과 : > 경력기술서는 나의 소속과 직무를 적는 칸이 아니다. > 면접관은 근거를 토대로 지원자들의 합불을 가른다. > 그 근거로 결정적인 요소가 바로 성과이다. > 앞에서 아마추어와 프로페셔널의 차이를 노력을 강조하는가 결과를 강조하는가라고 설명했다. > 경력기술서의 성과를 작성하는게 바로 결과를 강조하는 행위이다. > 성과를 보고 지원자를 뽑고 싶은지를 결정한다고 해도 과언이 아니다. 성과는 간결하게 작성할수록 좋다. 반드시 한 줄로. How + Result. 아쉬운 예시 신규브랜드 런칭 100만원 달성 좋은 예시 브랜드 콜라보레이션을 통한 신재품 출시로 객단가 1.5배 상승 및 매출 300% 상승 수요 에측 오류 발견 및 로직 변경 통한 판매 순위 100위권 내 forecast accuracy 5% 상승 가장 중요한 부분. > 지원자들이 자주 하는 실수는 너무 많은 내용을 담으려는 것이다. > 성과는 자세함보다 핵심을 잘 표현했는가가 중요하다. 역할 : 어떻게 성과를 달성했는지를 나타내는 부분. > 역할이라고 적었지만 사실은 전략이라고 표현하는 것이 더 적합하다. 문제를 해결한 액션 생산성을 높인 액션 의사소통을 잘한 액션 > 역할을 훨씬 엣지있게 작성하는 방법이 있다. > 각 액션들에 수치화를 가미하는 것이다. 아쉬운 예시 역할 : 제품 콘셉트와 스토리보드 도출, 시장분석 -> 당연히 해야하는 일들을 적고 있음 성과 : 신제품 출시를 통해 고객만족도 100% 상승 역할 : 해외 플랫폼 시장 마케팅 방안 조사 경쟁사 콘텐츠를 벤치마킹해 제품 랜딩 페이지 리뉴얼 및 광고 소재 기획 마케팅 성과 분석 -> 신제품 출시라는 액션이 모호하기 때문에, 역할이 성과와 연결되어 보이지 않는다. 각 역할도 수치화가 되지 않아서 당연한 업무 활동으로 해석된다. 역할에 기술된 내용이 성과에 어떤 영향을 주었는지 수치로 설명해보자. 좋은 예시 역할 : 기존 프로세스 분석을 통한 생산관리 프로세스 로직을 설게하여 불량품 검증 자동화 기능 개발 불량품 검증 자동화 시스템 개발을 통해 불량품 검사 시간 300% 단축 제품별, 고객사별 100가지 불량품 케이스 분석을 통해 테스트 데이터를 만들어 불량품 검사 비용 30% 절감 -> 문제를 해결한 핵심 액션과 생산성을 높인 액션을 숫자를 근거로 정확하게 나타냈다. 역할 : 신규 유입자 30%가 랜딩 페이지 체류시간이 10초 이내인 점을 분석 홈페이지 유입자 5000명 데이터 분석으로 랜딩 페이지 개선안 도출 A/B 테스트로 개선안 디자인 검증을 통해 클릭률 높은 문구, 버튼 위치 파악 변경된 랜딩 페이지로 연계 매출 100% 증가 -> 히트맵 분서기라는 분명한 액션이 있고 역항에 기술된 내용이 연결된다. 역할에 기술된 내용이 성과에 어떤 영향을 주었는지 숫자로 잘 표현되어 있다. > 역할을 적을 때 가장 피해야 할 일은, 당연히 해야 하는 일들을 나열하는 것이다. > 목표와 상관없이 반복하는 일들은 적을 필요가 없다. 기술 : > 내가 지원하는 회사의 정보를 파악하여 그에 맞게 작성하는 것 필수적인 건 아님 경력기술서 체크리스트 기관, 성과, 역할, 획득 기술을 작성하였는가? 성과를 서술형이 아닌 개조식으로 작성하였는가? 핵심적인 내용 외에 다른 내용이 많지는 않은가? 해석이 필요 없는 객관적이고 직관적인 숫자 혹은 결과물로 작성하였는가? 성과를 달성하기 위한 나의 역할을 3-5가지로 기재했는가? 지원하는 직무, 산업, 회사와 일치도가 떨어지는 경력 내용을 제거하였는가? 경력 기술서에 기술한 경력수가 총 경력기간 / 2 보다 많은가?",
    "public": true
  },
  {
    "id": "Self_Development/Career/Resumes/25-07-29-hyundai-autoever-ml-engineer-resume",
    "title": "[탈락!] Hyundai Autoever - ML Engineer Resume",
    "date": "2025-07-29",
    "excerpt": "현대자동차 이력서",
    "category": "Career",
    "tags": [
      "현대자동차 채용공고",
      "이력서"
    ],
    "content": "채용공고와 분석내용 필요한 경험만 살려서 짧고 굵게 쓰기. 2장정도 분량 필살기? 유사경험 🔍 이런 분과 함께 하고 싶어요 -> 검색 / 챗봇 관련 데이터 구축 및 서비스 개발 경험 프로젝트명: 챗봇 시스템 개발 및 서비스 운영 기간: 2024.11 ~ 2025.02 (4개월) 성과: RAG 파이프라인 구현 및 최적화로 기존 상담 중 65% 대체 및 사용자 만족도 90% 이상 달성 역할: 쿼리 분해 및 재작성, 하이브리드 검색 기반 RAG 파이프라인 구축으로 검색 정확도 90% 이상 달성 질문 분류 모델 도입으로 불완전 응답률을 낮춰 사용자 불만 70% 감소 프롬프트 엔지니어링(Instruct, Role, Context 기반) 최적화로 상담 톤의 자연스러운 응답 생성 및 답변 정확도 90% 이상 달성 Qdrant 벡터 데이터베이스 구축 및 운영, 메타데이터 기반 필터링 시스템 구현으로 검색 성능 40% 향상 LLM 기반 대화 품질 자동 평가 시스템 구축으로 모니터링 비용 80% 감소 카카오톡 채널 연동을 통한 실제 병원 상담 서비스 운영으로 기존 상담 중 65% 챗봇 대체 기술: Python, FastAPI, Transformers, Qdrant, Docker, AWS RAG 시스템 아키텍처, 벡터 데이터베이스 구축 및 운영, 챗봇 서비스 품질 관리 프로세스에 대한 이해 --- 데이터 품질 검증 프로세스 경험자 프로젝트명: OCR 파이프라인 시스템 구축 및 데이터 품질 검증 프로세스 개발 기간: 2024.08 ~ 2024.10 (3개월) 성과: OCR 파이프라인 최적화로 정확도 99% 달성 및 에러율 1% 미만 운영, 자동화된 데이터 검증 시스템 구축으로 모니터링 자동화 역할: 이미지 위치 고정 특성을 활용한 전처리 및 text recognition 특화 모델 도입, 후처리 로직 구현으로 OCR 정확도 99% 달성 이미지 송수신부터 OCR 처리, DB 적재까지 각 단계별 에러 처리 시스템 구축 및 객체 지향 설계를 통해 에러율 1% 미만 달성 및 유지보수 효율 10배 증가 모델 정확도 및 데이터 품질 모니터링 자동화를 위해 통계적 분포 모니터링 시스템 구축으로 품질 검증 프로세스 개발 고객번호 등 일관성 검증 로직 개발 및 데이터 무결성 검증 프로세스 구축 자동 모니터링 시스템 개발로 각 객체별 실시간 품질 모니터링 자동화 기술: Python, Transformers, OpenCV, gRPC, FastAPI, Triton Server, Docker OCR 데이터 품질 검증 프로세스, 실시간 모니터링 시스템 운영, 단계별 에러 처리 및 객체 지향 설계를 통한 시스템 아키텍처에 대한 이해 --- 딥러닝 모델링 경험자 프로젝트명: 당뇨병성 망막병증 병변 탐지 모델링 및 서비스 개발 기간: 2024.04 ~ 2024.07 (4개월) 성과: Contrastive Learning 및 Multi-Task Learning 적용으로 기존 모델 대비 일반화 성능 10% 이상 향상 및 Dice Score 0.8 달성 역할: U-Net 기반 segmentation 모델 설계 및 최적화를 통해 기존 대비 성능 10-15% 향상 CLAHE 전처리 기법 적용으로 데이터 특징 명확화을 통해 민감도 10% 향상 Contrastive Learning과 Multi-Task Learning을 활용해 제한된 데이터(1,500개) 환경에서 모델 일반화 성능 10%이상 개선 Triton Server를 통한 고성능 모델 서빙 시스템 구축으로 추론 속도 20% 향상 기술: U-Net, OpenCV, CLAHE, PyTorch, Contrastive Learning, Multi-Task Learning, Triton Server, TensorRT, gRPC 제한된 데이터 환경에서의 일반화 성능 향상 방법론 및 모델 서빙 시스템에 대한 이해 --- 사전질문 (선택) 이력서 내용 외에 추가적으로 설명하거나 어필하고자 하는 내용이 있으시면 자유롭게 작성 부탁드립니다. (최대 1,000자) 안녕하세요! 저는 데이터과학자로서 일하면서 챗봇 개발부터 수술 경과 예측 모델 개발까지 다양한 데이터를 다루며 다양한 서비스를 개발하고 운영하는 경험을 하면서 즐겁게 일을 하고 있습니다. 서비스를 개발하고 모델을 만들면서 가장 많이 시간을 쓰는 부분은 역시 데이터를 수집하고 정제하는 과정인 것 같습니다. 특히 데이터 품질을 관리하는 일은 가장 많은 노력을 쏟는 일입니다. 이상치는 어떻게 정의할 것이며 어떻게 처리할 것인지와 데이터 안에 존재하는 노이즈는 어떻게 관리해야하는지 등등 많은 고민과 시행착오 끝에 하나씩 해결해 나가는 과정을 경험했습니다. 그리고 제가 구축한 OCR 파이프라인을 통해서 수집한 데이터와 그 외에 수집되는 데이터들을 데이터 웨어하우스에 적재하고 이를 정제해 데이터 마트와 피처 스토어를 개발하는 경험을 하며 데이터를 효율적으로 다루기 위한 시스템을 설계하는 법을 배울 수 있었습니다. 새로운 기술이나 개념을 배워 서비스에 적용하며 문제를 해결하는 일은 꽤나 설레는 일이라고 생각합니다. segmentation 모델을 개발하면서 데이터 부족 문제를 해결하기 위해 contrast learning과 multi-task learning에 대한 개념을 공부하고 논문들을 참고해 모델을 개발하고 최적화하는 경험을 했습니다. 그리고 최적의 렌즈 사이즈를 추천해주는 서비스를 개발하면서 예측 모델의 선택 문제에 가지는 한계점을 극복하기 위해 인과추론에 대해 공부했고, 이를 서비스에 적용한 경험을 했습니다. 그 외에도 다른 프로젝트에서도 프로젝트를 진행하면서 막히는 문제를 해결해나가는 경험은 꽤나 즐겁고 재미있었습니다. 이처럼 저는 길지 않은 기간이지만 다양한 프로젝트를 경험하며 많은 문제를 마주했고 새로운 기술을 이해하고 활용하면서 이를 해결하는 경험을 하고 있습니다. 후기 떨리는 구만... 일단 기대는 0... 떨어지면 떨어지는거지 뭐 ㅎ.ㅎ 결과 -> 탈락 1차에서 계속 떨어지는구만.. 왜 떨어지는걸까? [Tech] Machine Learning Engineer - AI 데이터 사이언티스트 서류전형 결과를 안내드립니다. 안녕하십니까, 현대오토에버입니다. 당사 채용에 관심을 가지고 귀한 시간을 내주셔서 감사드립니다. 김민규님께서는 아쉽게도 이번 전형에 불합격하셨습니다. 다음번에 더 좋은 인연으로 만나뵙길 희망하며, 앞으로 하시는 일 모두 건승하시길 바랍니다. 감사합니다.",
    "public": false
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-29-hyundai-autoever-ml-engineer",
    "title": "Hyundai Autoever - ML Engineer",
    "date": "2025-07-29",
    "excerpt": "현대자동차 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "현대자동차 채용공고"
    ],
    "content": "채용공고 <details> <summary>📋 <strong>채용공고 상세정보</strong></summary> 링크 [Tech] Machine Learning Engineer - AI 데이터 사이언티스트 📃 누구나 마음 속에 이력서 한 장은 있으니까, ⚡ 1분 컷 지원으로 현대오토에버로의 여정을 시작하세요. (이력서 자율양식) ✅ AI ​데이터 ​사이언티스트 🚀 합류하실 ​팀을 ​소개해요 인공지능기술실은 ​AI 신기술을 바탕으로 ​전사 ​AI 주제를 리드하며 ​공통 ​활용 ​기술에 대해 다룹니다. ​연구개발을 통해 ​'살아있는 ​AI' 기술을 ​만들어 사내 ​주요 ​서비스 및 제품이 ​더욱 경쟁력 ​있도록 만들어 갑니다. 음성/언어, 비전/인지 등 폭넓은 AI 주제 전반을 다루고 있으며, 개발 결과는 우리 회사 및 그룹의 중요 서비스에 적용되고 있습니다. AI 신기술에 대한 이해와 AI 모델 연구개발 역량을 바탕으로 미래 모빌리티 및 유관 분야의 주인공이 될 개발자/연구자 분들을 모십니다. 💻 합류하시면 함께 할 업무예요 AI 대화형 서비스를 위한 데이터 분석 및 설계 프롬프트 엔지니어링 / 데이터 증강 AI 대화형 서비스 데이터 관련 운영 및 품질 관리 🔍 이런 분과 함께 하고 싶어요 검색 / 챗봇 관련 데이터 구축 및 서비스 개발 경험 NLP 관련 Task 데이터 구축 및 설계 경험 Python 개발 경력 1년 이상 혹은 그에 준하는 실력 🔍 이런 분이라면 더욱 좋아요 전산언어학/자연어처리 전공 석사 이상 LLM 기반 프롬프트 엔지니어링 경험자 LLM / GPT 기반 데이터 증강 경험자 딥러닝 모델링 경험자 데이터 품질 검증 프로세스 경험자 ⌛ 이렇게 합류해요 서류 접수 → 서류 검토 → 직무역량테스트(코딩 또는 과제테스트) 및 인성검사 → 1차면접 → 2차면접 → 처우협의 및 채용검진 → 최종 합격🎉 📍 만나게 될 근무지는 여기예요 서울 강남 📣 동료의 한 마디 인공지능기술실의 기둥 L책임입니다. 우리 팀은 자율성이 특징이에요. 서비스에서부터 제조/생산까지, 기술이 적용되는 분야가 다양하고 또 팔로업 해야 하는 주제가 광범위하다 보니 자율성을 기반으로 성장을 지원하고 있어요. 전반적인 분야를 잘 아는 멘토들이 있어 많은 도움을 받고 있고, 또 각 분야별 주제에 따라 활발한 자율적 학습 그룹 구성을 통해 업무 외에 역량적 성장을 꾸준히 하고 있답니다 :) 연구개발 결과는 그룹 서비스나 제품에 반영되기도 하고, 관련 학회나 행사 등에 초대를 받아 발표하기도 해요. 사무실은 삼성역 인근에 있고, 롯데타워가 조망되는 뷰도 환경적 장점이랍니다! 함께 신기술을 공부하고 구체적 결과가 있는 실질적 기술 응용 경험을 통해 커리어적으로 성장하고 싶은 분들을 기다리고 있어요! (팀 리더) 인공지능기술실은 신기술에 대한 이해와 활용을 통해 문제를 해결하는 열정을 가진 사람들로 구성되어 있습니다. AI를 위한 인프라, 데이터, 분석 모델링 및 알고리즘, 기술 기반 공통 서비스 개발까지 다양한 분야를 다루며, 이런 노력을 바탕으로 그동안 의미 있고 결과들을 만들어내고 있습니다. 주로 주요 각 분야 AI 프레임워크를 이용해 개발을 하며 Python 등의 언어를 주로 사용합니다. 우리 팀은 멀티모달 기반의 궁극적 AI 개발을 지향하고, 이를 통해 회사의 성장과 각 구성원의 커리어 성장을 함께 이룰 수 있습니다. 관심을 가지고 성공의 여정을 함께해 주세요! Join Us! 📌 참고해 주세요 채용 시 마감되는 상시 채용 공고로 운영되며, 채용 절차와 일정은 변동될 수 있어요. 사회적 배려 대상자(보훈 취업지원대상자, 장애인)는 관계 법령과 내규에 따라 우대해요. 모집 분야 및 담당 업무에 따라 영어 구술평가, 레퍼런스 체크, 또는 기타 전형이 실시될 수 있어요. 지원자의 경험과 역량을 고려하여 다른 포지션이 더 적합하다고 판단되는 경우 지원 분야가 변경될 수 있어요. 배치 부서 및 근무지는 회사 사정에 따라 변경될 수 있어요. 아래의 경우, 합격이 취소되거나 전형 진행에서 불이익을 받으실 수 있어요. 지원서가 사실과 다르거나 증빙이 불가할 경우 해외여행 결격 사유가 있는 경우 (남성의 경우, 회사가 지정한 입사일까지 병역 필 또는 면제 필요) 최종 합격 후 회사가 지정하는 입사일에 입사 불가한 경우 </details> 이력서 전략 필요경험 검색 / 챗봇 관련 데이터 구축 및 서비스 개발 경험 -> 챗봇 개발하고 서비스 운영해본 경험이 있음 선호 경험 딥러닝 모델링 경험자 데이터 품질 검증 프로세스 경험자 NLP task는 아니지만 segmentation 모델 개발 경험이랑 --- 🎯 필요 경험 (Must Have) 검색 / 챗봇 관련 데이터 구축 및 서비스 개발 경험 ✅ Chatbot 프로젝트: RAG 기반 지능형 챗봇 시스템 개발 및 서비스 운영 ✅ 실제 서비스: 카카오톡 채널을 통한 병원 상담 자동화 서비스 🌟 선호 경험 (Nice to Have) 딥러닝 모델링 경험자 ✅ Computer Vision: OCR Pipeline에서 TrOCR 모델 활용 ✅ NLP: Chatbot에서 BGE-m3 임베딩 모델, GPT 기반 답변 생성 ✅ 모델 서빙: Triton Server, BentoML을 통한 프로덕션 배포 데이터 품질 검증 프로세스 경험자 ✅ 자동화된 품질 평가: LLM 기반 대화 품질 자동 평가 시스템 ✅ 실시간 모니터링: Slack 기반 에러 알림 및 작업 요약 ✅ 품질 지표 정의: 답변 출처 적합성, 질문-문서 연관성 평가 ✅ 임계치 기반 관리: 품질 점수 임계치 이하 시 자동 알림 ✅ 데이터 검증 로직: OCR 결과 정제, 수기 입력 데이터 완전성 검증 📋 포트폴리오 매칭도 요구사항 프로젝트 경험 수준 --------- --------- ---------- 챗봇 서비스 개발 Chatbot ⭐⭐⭐⭐⭐ 데이터 품질 검증 Chatbot + OCR ⭐⭐⭐⭐⭐ 딥러닝 모델링 OCR + Chatbot ⭐⭐⭐⭐ Python 개발 모든 프로젝트 ⭐⭐⭐⭐⭐ 🎯 강조 포인트 AI 대화형 서비스: 실제 병원 상담 챗봇 서비스 운영 경험 데이터 품질 관리: 자동화된 모니터링 시스템으로 품질 관리 프로세스 구축 프로덕션 배포: Docker, AWS, 온프레미스 환경에서 실제 서비스 운영 성과 지표: 사용자 만족도 90%+, 답변 정확도 95%+ 달성",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-28-나망치면어떡하지",
    "title": "나망치면 어떡하지?",
    "date": "2025-07-28",
    "excerpt": "나망치면 어떡하지?",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "--- 배운거 ~하면 어쩌지? ~면 어떻게 할거야. What if I mess up? What if the flight is delayed? What if + 과거형 -> 현재와 일치하고 있지 않은 상황을 가정 What if you were in Seoul? What if he weren't married? (현재와 다른 가정을 할 때는 무조건 were을 씀) What would you do if you were in Seoul? anxious 간단한 작문 I think that what if questions are good to check value-thinking (가치관). What if I got a lot of money? Will I work continue? It then, it means I love my job. What would I do If I were fluent in English? This imaginations make me more 명확하게. --- 수정된 버전 I think that \"what if\" questions are good for checking values and priorities. What if I got a lot of money? Would I continue working? If so, it means I love my job. What would I do if I were fluent in English? These imaginations help me think more clearly. --- 피드백 문법 수정사항: \"what if questions\" → \"what if\" questions (따옴표 추가) \"check value-thinking\" → \"check values and priorities\" (더 자연스러운 표현) \"Will I work continue?\" → \"Would I continue working?\" (조건문에서는 would 사용) \"It then\" → \"If so\" (더 자연스러운 표현) \"What would I don\" → \"What would I do\" (오타 수정) \"This imaginations\" → \"These imaginations\" (복수형에 맞는 지시사) \"make me more 명확하게\" → \"help me think more clearly\" (영어로 완전히 표현) 표현 개선사항: \"value-thinking\" 대신 \"values and priorities\" 사용 \"It then\" 대신 \"If so\" 사용하여 더 자연스럽게 연결 마지막 문장을 더 구체적이고 명확하게 표현 전체적인 피드백: 조건문을 사용한 가치관 탐구라는 좋은 아이디어입니다! 문법적으로 몇 가지 수정이 필요했지만, 핵심 메시지는 잘 전달되고 있습니다. 특히 \"What if\" 질문을 통해 자신의 진짜 가치관을 파악하는 방법은 매우 효과적인 학습법입니다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/회사에서하는고민_효율적인ML구조",
    "title": "효율적인 ML 구조 설계는 어떻게 하는걸까?",
    "date": "2025-07-28",
    "excerpt": "효율적인 ML 구조 설계에 대한 고민",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design"
    ],
    "content": "사수가 없는 회사에서 일을 하다보니 실제로 서비스를 운영하면서 맞닥뜨리는 문제들을 어떻게 해결하는게 좋은지에 대한 고민이 늘 생긴다. 이를 해결하기 위해 Designing Machine Learning Systems 책을 읽고 공부하면서 적용해보려고 한다. 읽을 책들 Designing Machine Learning Systems Machine Learning System Design Interview AI Engineering Designing Data-Intensive Applications 회사의 use case별로 ML을 어떻게 배포하고 있는지. 아키텍처 어떻게 설계할지. 고민했던 부분 포트폴리오로도 녹여내자.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch3_데이터엔지니어링기초",
    "title": "데이터 엔지니어링 기초",
    "date": "2025-07-28",
    "excerpt": "데이터 엔지니어링 기초",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> 프로덕션용 ML 시스템을 구축하려면 증가하는 데이터를 수집, 처리, 저장, 검색 및 처리하는 방벙ㅂ을 숙지해야 합니다. > 시스템 관점의 데이터 엔지니어링을 더 자세히 알고 싶다면 마틴 클레프만의 \"데이터 중심 애플리케이션 설계\"를 읽어보기 바랍니다. 데이터 중심 애플리케이션 설계 데이터 소스 --- > ML 시스템은 다양한 소스에서 온 데이터로 작동합니다. > 데이터마다 특성, 목적, 처리 방법이 다르며 데이터 소스를 파악하면 데이터를 보다 효율적으로 사용하는데 도움이 됩니다. user input data system-generated data 데이터 포맷 --- JSON CSV (행 우선) Parquet (열 우선) 행 우선인 데이터에 행으로 접근하면 빠르고, 열 우선 데이터에 열로 접근하면 빠르다. 데이터 모델 --- 데이터가 어떻게 표현되는지 관계형 모델 : > 관계형 모델에서는 데이터는 관계 (relation)로 구성되며 각 관계는 튜플의 집합입니다. > 관계는 순서가 없습니다. 관계에서 행의 순서나 열의 순서를 섞더라도 여전히 동일한 관계입니다. 정규화하는 편이 좋다. 정규화를 통해 데이터 중복을 줄이고 데이터 무결성을 향상 다만, 데이터가 분산되어 관리되고 조인 비용 등이 증가. > 관계형 데이터 모델을 기반으로 구축된 데이터베이스를 관계형 데이터베이스라고 합니다. 쿼리 언어. SQL. > 주목해야 할 점은 SQL이 선언적 언어라는 사실입니다. > 명령형 언어인 파이썬과 대비되죠. 선언적 언어 (Declarative Language) -> 무엇을 원하는지를 표현하는 언어 (시스템이 자동으로 최적화 수행) 명령형 언어 (Imperative Language) -> 어떻게 원하는 것을 달성할지를 표현하는 언어 선언적 데이터 시스템에서 선언적 ML 시스템으로 : > 선언적 데이터 시스템이 성공한 데서 영감을 받은 많은 사람들은 선언적 ML을 기대해왔습니다. > 선언적 ML 시스템을 사용하면 사용자는 피처의 스키마와 작업만 선언하면 됩니다. > 그러면 시스템은 주어진 피처로 해당 작업을 수행하는데 가장 적합한 모델을 찾아냅ㄴ디ㅏ. ALudwig, AutoML. 모델 개발 부분은 많이 추상화되는 중. 그렇게 어려운 작업이 아니다. NoSQL : 스미카 관리에 대한 불만. > 비관계형 모델의 주요 유형 두 가지는 문서 모델과 그래프 모델입니다. 문서 모델 그래프 모델 > 데이터 모델에 따라 수행하기 쉬운 쿼리가 있고 어려운 쿼리가 있습니다. > 따라서 애플리케이션에 적합한 데이터 모델을 선택하는 것이 바람직하죠. 정형 데이터 vs 비정형 데이터 정형 데이터는 스키마를 따른다. > 정형 데이터를 저장하는 저장소를 데이터 웨어하우스라고 하며 비정형 데이터를 저장하는 저장소를 데이터 레이크라고 합니다. > 데이터 레이크는 읿나적으로 처리 전 원시 데이터를 저장하는데 사용하며 데이터 웨어하우스는 사용 가능한 형식으로 처리된 데이터를 저장하는데 사용됩니다. 데이터 스토리지 엔진 및 처리 --- 트랜잭션 처리와 분석 처리 : transaction : 온갖 종류의 작업 생성될 때 삽입되고 변경될 때 업데이트 되면 필요하지 않으면 삭제됨. OLTP (Online Transaction Processing) > 트랜잭션 데이터베이스는 온라인 트랜잭션을 처리하고 낮은 레이턴시와 고가용성 요구 사항을 충족하도록 설계됐습니다. ACID (Atomicity, Consistency, Isolation, Durability) Atomicity(원자성) : > 트랜잭션의 모든 단계가 하나의 그룹으로서 성공적으로 완료되도록 보장합니다. Consistency(일관성) : > 들어오는 모든 트랜잭션이 미리 정의된 규칙을 따라야 함을 보장합니다. Isolation(격리성) : > 두 트랜잭션이 마치 격리된 것처럼 동시에 발생하도록 보장합니다. Durability(지속성) : > 트랜잭션이 커밋된 후에는 시스템 장애가 발생하더라도 커밋된 상태를 유지하도록 보장합니다. 트랜잭션 데이터베이스는 행 우선일 때가 많음 -> 분석에 적절하지 않을 수 있음 OLAP (Online Analytical Processing) 분석에 더 적절한 database. 열 조회에 최적화 됨. 요즘은 구분해서 이야기안함. 둘 다 지원하는 경우가 많아서. ETL: Extract, Transform, Load : > ETL은 데이터를 범용 처리 및 원하는 모양과 포맷으로 집계함을 의미 <figure> <img src=\"/post/DesigningMachineLaerningSystem/ETL.webp\" alt=\"ETL Process\" /> <figcaption> 출처: <a href=\"https://www.getdbt.com/blog/etl-vs-elt\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.getdbt.com/blog/etl-vs-elt</a> </figcaption> </figure> ELT: Extract, Load, Transform > 데이터를 먼저 스토리지에 적재한 뒤 나중에 처리하는 프로세스 <figure> <img src=\"/post/DesigningMachineLaerningSystem/ELT.webp\" alt=\"ELT Process\" /> <figcaption> 출처: <a href=\"https://www.getdbt.com/blog/etl-vs-elt\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.getdbt.com/blog/etl-vs-elt</a> </figcaption> </figure> 데이터플로 모드 --- > 데이터가 한 프로세스에서 다른 프로세스로 전다로딜 때 데이터가 한 프로세스에서 다른 프로세스로 흐른다고 합니다. > 즉, 데이터 플로가 생깁니다. 데이터베이스를 통한 데이터 전달 서비스를 통한 데이터 전달 (REST, RPC API에서 제공하는 요청) 실시간 전송을 통한 데이터 전달 (아파치 카프카, 아마존 키네시스)",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch2_머신러닝시스템설계소개",
    "title": "머신러닝 시스템 설계 소개",
    "date": "2025-07-28",
    "excerpt": "머신러닝 시스템 설계 소개",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "ML 시스템 설계는 어떻게 하는걸까? MLOps에 시스템으로 접근 > 시스템을 비즈니스용으로 개발한다면 비즈니스 목적에 따라 업무를 추진해야 하며, 비즈니스 목적은 다시 ML 목적으로 바꿔서 ML 모델 개발의 이정표로 삼아야 합니다. 가장 중요하게 생각하는 부분. 비즈니스 목적을 이해하기 이에 맞게 ML 목적을 세우는 과정. 신뢰성, 확장성, 유지보수성, 적응성 비즈니스와 머신러닝의 목적 --- > 사실 기업들은 대부분 이런 화려한 ML 지표에 크게 관심이 없습니다. > 비즈니스 지표에 영향을 주지 않는 한 모델 정확도를 94%에서 94.2%로 높이는 일 따위에 신경쓰지 않죠. > 비즈니스 조직 내에서 ML 프로젝트가 성공하려면 ML 시슴템 성과를 전체적인 비즈니스 성과와 연결해야 합니다. > 신규 ML 시스템은 광고 수익, 월간 활성 사용자 수 등 어떤 비즈니스 성과 지표에 영향을 미칠까요? > 많은 회사에서 비즈니스 지표를 ML 지표에 연결하기 위해 자체 지표를 만듭니다. 넷플릭스 - 추천시스템. 채택률 (take-rate) 영상 재생 횟수 / 사용자가 본 추천 썸내일 개수 추천시스템의 채택률을 다른 비즈니스 지표, 예컨대 총 스트리밍 시간이나 구독 취소율의 맥락에서도 고려. 실제 비즈니스에 어떤 영향을 미치는지 확인하기 위한 시도들. -> 넷플릭스는 새로운 평가지표를 계산 직접적으로 파악하는건 꽤나 어렵고 많은 실험이 필요함 대안으로 A/B test를 진행해 어떻게 연관되는지는 모르지만, 비즈니스 목표 관점에서만 분석해볼 수도 있음 > 많은 기업에서 'AI 기반'이라는 문구가 고객 유치에 도움이 되기 때문에 ML을 사용한다고 이야기합니다. > AI가 실제로 유용한지는 상관없이 말이죠. ML이 마법같은 변화를 가져올 수는 있지만, 하루아침에 되는건 절대 아니다. 비즈니스 관점으로 ML이 가져올 수익에 대해서 현실적으로 생각해야 함. > ML 관련한 투자 수익은 도입 이후 성숙 단계에 다다라서야 눈에 띄기 시작합니다. > ML을 도입한 기간이 길수록 파이프라인이 효율적으로 실행되고 개발 주기가 빨라지며 엔지니어링 시간과 클라우드 비용이 줄어들어 수익이 높아지게 됩니다. 머신러닝 시스템 요구 사항 --- ML 시스템이 갖춰야 하는 요구사항 신뢰성 : > ML 시스템은 '시스템 동작의 올바름'을 판단하기 쉽지 않습니다. 실제 상황에서는 정답이 없는데, 틀렸다는 사실을 어떻게 파악할 수 있을까? 확장성 : > ML 시스템은 다양한 방식으로 확장 가능합니다. > 시스템 복잡도가 증가하든, 트래픽 양이 증가하든, ML 모델 개수가 증가하든 상관없이 규모 증가를 처리할 합리적인 방법이 필요합니다. resource scaling. artfiact managment. 유지보수성 : > 워크로드를 구조화하고 인프라를 설정하는 일이 중요합니다. 적응성 : > 시스템은 변화하는 데이터 분포와 비즈니스 요구 사항에 적응할 수 있어야 합니다. > 그러려면 성능 향상에 영향을 주는 요소를 찾아낼 수 있어야 하며 서비스 중단 없이 업데이트가 가능해야 합니다. > ML 시스템은 반은 코드이고 반은 데이터입니다. > 데이터는 빠르게 변화하므로 ML 시스템 또한 자체적으로 빠르게 진화할 수 있어야 하죠. 반복 프로세스 --- > ML 시스템 개발은 반복적이며 대부분 끝이 없는 프로세스입니다. > 시스템을 프로덕션 환경에 배포하면 지속적으로 모니터링하고 업데이트해야 합니다. 프로젝트 범위 산정 범위를 산정하고 목표, 목적과 제약사항을 설정하는 일 데이터 엔지니어링 ML 모델 개발 배포 > ML 시스템 개발은 글쓰기와 같습니다. 시스템은 결코 완벽히 완성되지 않습니다. > 하지만 어느 시점에는 시스템을 외부에 내보여야 합니다. 모니터링과 연속 학습 > 모델을 프로덕션 환경에 배포한 뒤에는 지속적으로 성능 저하를 모니터링하고 변화하는 환경과 요구 사항에 적응하도록 유지 관리해야 합니다. 비즈니스 분석 > 모델 성능을 비즈니스 목표 관점에서 분석해 비즈니스 인사이트를 추출합니다. 머신러닝 문제 구조화하기 --- > 고객 지원 속도가 느리다는 점은 문제지만 ML로 바로 풀 수 있는 문제는 아닙니다. > ML 문제는 입력과 출력, 학습 프로세스를 이끌어나가는 목적 함수로 정의됩니다. 분류 vs 회귀 이진분류 vs 다중클래스 분류 다중 클래스 vs 다중 레이블 분류 문제를 구조화하는 다양한 방법 : 문제: 사용자가 다음으로 열어볼 앱 예측하기 구조화 방법: 분류 -> 앱이 추가될 때마다 재학습을 해야하는 이슈 문제: 사용자가 다음으로 열어볼 앱 예측하기 구조화 방법: 회귀 입력으로 앱관련 피처를 넣어주고 열어볼 확률을 예측하는 문제로 정의하면, 다시 학습해야하는 소요가 줄어듬 지성 (mind) vs 데이터 (data) --- > 데이터보다 지성이 위위라고 생각하는 진영에는 튜링상 수상자인 주데아 펄 박사가 있습니다. > 펄은 인과 관계 추론과 베이즈 네트워크에 대한 연구로 잘 알려졌으며 그의 저서 인과성에 대해에 실린 서문 제목이 '데이터보다 우위인 지성'입니다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/Designing Machine Learning Systems/ch1_머신러닝시스템개요",
    "title": "머신러닝 시스템 개요",
    "date": "2025-07-28",
    "excerpt": "머신러닝 시스템 개요",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> '머신러닝 시스템'이라고 하면 많은 사람이 로지스틱 회귀나 다양한 유형의 신경망 등 ML 알고리즘만 떠올립니다. > 그러나 프로덕션 환경에서 알고리즘은 ML 시스템의 일부일 뿐입니다. > 시스템은 ML 프로젝트의 출발점이 된 비즈니스 요구 사항, 사용자와 개발자가 시스템과 상호 작용하는 인터페이스, 데이터 스택, 모델을 개발 및 모니터링하고 업데이트하기 위한 로직은 물론 해당 로직을 전달할 수 있는 인프라를 포함합니다. ML을 사용해야하는 경우 (use case)는 언제인가? 사용하기 위해 필요한 것들은? 연구와 프로덕션 상황에서 머신러닝을 개발하는게 어떤 차이가 있는지에 대한 내용들. 연구는 연구 목표를 달성하기 위해 성능을 높이는 데에 목표가 있지만, 서비스를 운영할 때는 이해관계를 만족하고 비즈니스 목표에 맞는 모델을 개발하는 것이 목표. > ML 시스템은 다양한 요소로 구성된 복잡한 시스템입니다. > 프로덕션에서 ML 시스템을 작업하는 데이터 과학자와 ML 엔지니어는 ML 알고리즘에만 집중하는 걸로는 절대 충분치 않다는 사실을 깨달을 겁니다. > 알고리즘 외에 시스템의 다른 측면, 예컨대 데이터 스택, 배포, 모니터링, 유지 관리, 인프라에 관해 아는 것이 중요합니다. 내가 서비스를 구성한 후 이를 설명할 때도 내가 디자인한 시스템에 대해서, 이런 부분을 고려해 설명할 필요가 있겠다. 연구용 머신러닝 vs 프로덕션용 머신러닝 연구용 ML 프로덕션용 ML --- --- --- 요구사항 벤치마크 데이터셋에서 최적의 모델 성능 달성하기 이해관계자마다 다름 계산 우선순위 빠른 훈련, 높은 스루풋 빠른 추론, 낮은 레이턴세 데이터 정적임 끊임없이 변동함 공정성 중요하지 않은 경우가 많음 반드시 고려해야 함 해석 가능성 중요하지 않은 경우가 많음 반드시 고려해야 함 다양한 이해관계자와 요구사항 : > ML 시스템을 프로덕션에 적용하는 과정에는 많은 이해관계자가 얽혀 있으며 각각 요구 사항이 다릅니다. > 요구 사항들은 서로 충돌할 때도 많으므로 모든 요구 사항을 충족하는 ML 모델을 설계, 개발, 선택하기는 어려울 수 있습니다. 사용자 목표를 이해하고 목표가 여러개라면 쪼개서 각각의 모델을 만들 필요가 있다. 사용자 목표를 만족하지만 모델의 목표가 다를 수도 있고. > ML 프로젝트를 개발할 때 ML 엔지니어는 모든 이해관계자의 요구 사항을 파악하고 각각이 얼마나 엄격한지 이해해야 합니다. > 프로덕션용 ML은 연구용 ML과 요구사항이 다릅니다. 계산 우선순위 : > ML 시스템을 배포한 경험이 없는 분들은 시스템을 설계할 때 종종 모델 개발에만 너무 집중하고 배포와 유지 관리를 소홀히 하는 실수를 합니다. 연구에서는 학습단계가 병목이지만 배포 단계에서는 추론이 병목이다. > 한가지 결론은 연구에서는 높은 스루풋 (throughput)을 우선시 하는 반면에 프로덕션에서는 낮은 레이턴시를 우선시한다는 점입니다. > 레이턴시는 쿼리를 수신하고 결과를 반환하는데 걸리는 시간을 의미하며 스루풋은 특정 기간 내에 처리된 쿼리 수를 의미합니다. 스루풋(Throughput): 특정 시간 내에 처리할 수 있는 작업의 총량 레이턴시(Latency): 하나의 요청을 처리하는데 걸리는 시간 > 실험 단계에서 대량의 실험이 필요함. ``markdown 연구 시나리오: 하이퍼파라미터 튜닝: 1000개 조합 시도 모델 아키텍처 실험: 50가지 구조 테스트 데이터 증강 실험: 10가지 방법 비교 목표: 가능한 많은 실험을 빠르게 완료 → 스루풋이 중요 (시간당 처리할 수 있는 실험 수) ` > 프로덕션에서는 사용자 경험을 개선하고 비즈니스 임팩트를 줘야함. `markdown 실시간 서비스 시나리오: 추천 시스템: 사용자가 페이지 로드 후 100ms 내에 추천 제공 챗봇: 사용자 메시지에 2초 내에 응답 자율주행: 센서 데이터를 50ms 내에 처리 목표: 개별 요청을 빠르게 처리 → 레이턴시가 중요 (응답 시간) 예시: 이커머스 추천 시스템 레이턴시 100ms → 전환율 5% 레이턴시 500ms → 전환율 3% (40% 감소) 레이턴시 1초 → 전환율 1% (80% 감소) 결과: 1초 지연 = 매출 80% 감소 `` > 레이턴시는 개별 수치가 아닌 분포임을 기억합시다. 데이터 : > 프로덕션 단계의 데이터는 훨씬 더 복잡합니다. 잡음이 많고 비정형일 수 있으며 끊임없이 변화하죠. > 게다가 데이터가 편향됐을 수 있는데 어떻게 편향됐는지 모를 가능성이 큽니다. 진짜 raw data부터 다뤄본 경험이 꽤 많은데, 다 진짜 너무 더러웠다 ㅎ.ㅎ.. 공정성 : > 여러분 혹은 주변에 있는 누군가는 자신도 모르는 사이에 편향된 수학 알고리즘의 희생자일 수 있습니다. 해석 가능성 : > 산업계 대부분의 ML 유스 케이스에서 해석 가능성은 선택이 아닌 필수입니다. 머신러닝 시스템 vs 전통적인 소프트웨어 > 기존 SWE에서는 코드 테스트 및 버전 관리에만 집중하면 되지만 ML을 사용하면 데이터 또한 테스트하고 버전을 지정해야 합니다. > 프로덕션 환경에서 이러한 모델을 모니터링하고 디버깅하는 일 또한 사소하지 않습니다. > ML 모델이 복잡해지고 작업에 대한 가시성이 떨어짐에 따라 무엇이 잘못됐는지 파악하거나 문제가 발생했을 떄 신속하게 경고를 받기가 어렵습니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-27-선태의상황에서_ReviewDay",
    "title": "선택의 상황에서 쓰는 표현들 Review Day",
    "date": "2025-07-27",
    "excerpt": "선택의 상황에서",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 One one hand One the other hand On one hand the note has really big screen. On the other the iPhone has a really good camera. On one hand, I havent had chinees food in a while. On the other hand, I'm craving fried chicken. Whether we eat Chinese food or fried chicken, we should order dilivery. The best thing about iPhone is its design. When it comes to online shopping, Amazon might be the best option. Although its a bit pricey, the bar has amazing view at night. Even though i`m not a big fan of animated films, I love Frozen. The dress is too expensive. It's pretty though. It's not about design. It's about performance. In which case, I'll probably go with the new Galaxy.",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/20250726_좋은밤좋은꿈",
    "title": "좋은밤 좋은꿈 인트로 연습",
    "date": "2025-07-26",
    "excerpt": "좋은밤 좋은꿈 인트로 부분 핑거스타일 아르페지오 연습",
    "category": "Practice Log",
    "tags": [
      "기타연습",
      "Guitar Practice",
      "Arpeggio"
    ],
    "content": "@audio좋은밤 좋은꿈 인트로 연습 뭔가 맛이 없다 ㅋㅋ F랑 Fm가 완벽하게 안누림 특히 3번줄이.",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/20250726_고백_스트로크",
    "title": "델리스파이스 고백 스트로크 연습",
    "date": "2025-07-26",
    "excerpt": "기타연습일지",
    "category": "Practice Log",
    "tags": [
      "기타연습",
      "Guitar Practice",
      "Strokes"
    ],
    "content": "@audio델리스파이스 고백 스트로크 연습 C#7이 잘 안잡힘. 특히 G#m -> C#7이 잘 안됨. 기타 톤은 낫배드한 듯? AC clean + front pickup + 싱싱싱",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/20250726_love",
    "title": "웨이브 투어스 사랑으로 인트로 커버",
    "date": "2025-07-26",
    "excerpt": "기타연습일지",
    "category": "Practice Log",
    "tags": [
      "기타연습",
      "Guitar Practice"
    ],
    "content": "@audio웨이브 투어스 사랑으로 인트로 커버 확실히 녹음해서 들어보니까 끊기는 부분이 많이 들림. 박자 나가는 것도 보이고.",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/vDB_code_notebook",
    "title": "vDB관련 샘플 코드들",
    "date": "2025-07-25",
    "excerpt": "vDB관련 샘플 코드들",
    "category": "Data Science",
    "tags": [
      "VectorDB",
      "Qdrant",
      "ChromaDB",
      "FAISS",
      "Milvus"
    ],
    "content": "--- title: \"vDB관련 샘플 코드들\" date: \"2025-07-25\" category: \"Data Science\" tags: [\"VectorDB\", \"Qdrant\", \"ChromaDB\", \"FAISS\", \"Milvus\"] excerpt: \"vDB관련 샘플 코드들\" --- # Qdrant 파이썬 클라이언트 설치 필요 pip install qdrant-client from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np Qdrant 서버에 연결 (로컬 서버 예시) client = QdrantClient(host=\"localhost\", port=6333) 컬렉션 생성 (벡터 차원 1536, cosine 거리) client.recreate_collection( collection_name=\"my_collection\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) 벡터 데이터 삽입 vectors = np.random.rand(3, 1536).tolist() # 3개 벡터, 1536차원 payloads = [ {\"doc_id\": \"A1\", \"text\": \"문서1\"}, {\"doc_id\": \"A2\", \"text\": \"문서2\"}, {\"doc_id\": \"A3\", \"text\": \"문서3\"}, ] points = [ PointStruct(id=i, vector=vectors[i], payload=payloads[i]) for i in range(3) ] client.upsert(collection_name=\"my_collection\", points=points) 유사도 검색 (임의 쿼리 벡터) query_vector = np.random.rand(1536).tolist() results = client.search( collection_name=\"my_collection\", query_vector=query_vector, limit=2, ) for hit in results: print(f\"ID: {hit.id}, Score: {hit.score}, Payload: {hit.payload}\") from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np client = QdrantClient(host=\"localhost\", port=6333) 회사A 컬렉션 생성 client.recreate_collection( collection_name=\"companyA_docs\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) 회사B 컬렉션 생성 client.recreate_collection( collection_name=\"companyB_docs\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) 회사A 데이터 삽입 vectors_a = np.random.rand(2, 1536).tolist() payloads_a = [{\"doc_id\": \"A1\"}, {\"doc_id\": \"A2\"}] points_a = [PointStruct(id=i, vector=vectors_a[i], payload=payloads_a[i]) for i in range(2)] client.upsert(collection_name=\"companyA_docs\", points=points_a) 회사B 데이터 삽입 vectors_b = np.random.rand(2, 1536).tolist() payloads_b = [{\"doc_id\": \"B1\"}, {\"doc_id\": \"B2\"}] points_b = [PointStruct(id=i, vector=vectors_b[i], payload=payloads_b[i]) for i in range(2)] client.upsert(collection_name=\"companyB_docs\", points=points_b) 회사A 데이터만 검색 query_vector = np.random.rand(1536).tolist() results = client.search( collection_name=\"companyA_docs\", query_vector=query_vector, limit=1, ) for hit in results: print(f\"회사A 결과: {hit.payload}\") 회사B 데이터만 검색 results = client.search( collection_name=\"companyB_docs\", query_vector=query_vector, limit=1, ) for hit in results: print(f\"회사B 결과: {hit.payload}\") # Multi tenancy 를 잘 구현하는 법 # 1. 컬렉션 네이밍 컨벤션 from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np client = QdrantClient(host=\"localhost\", port=6333) 테넌트별 컬렉션 네이밍 패턴 def get_collection_name(tenant_id: str, data_type: str = \"docs\") -> str: \"\"\"테넌트별 컬렉션 이름 생성\"\"\" return f\"{tenant_id}_{data_type}\" 예시: 회사별 컬렉션 생성 tenants = [\"company_a\", \"company_b\", \"company_c\"] data_types = [\"docs\", \"images\", \"products\"] for tenant in tenants: for data_type in data_types: collection_name = get_collection_name(tenant, data_type) client.recreate_collection( collection_name=collection_name, vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) print(f\"Created collection: {collection_name}\") # 2. 멀티 테넌시 매니저 클래스 class QdrantMultiTenantManager: def __init__(self, client: QdrantClient): self.client = client self.tenant_configs = { \"company_a\": { \"collections\": [\"docs\", \"images\"], \"vector_size\": 1536, \"distance\": Distance.COSINE }, \"company_b\": { \"collections\": [\"docs\", \"products\"], \"vector_size\": 1536, \"distance\": Distance.COSINE } } def get_collection_name(self, tenant_id: str, collection_type: str) -> str: \"\"\"테넌트별 컬렉션 이름 생성\"\"\" return f\"{tenant_id}_{collection_type}\" def ensure_collection_exists(self, tenant_id: str, collection_type: str): \"\"\"컬렉션이 없으면 생성\"\"\" if tenant_id not in self.tenant_configs: raise ValueError(f\"Unknown tenant: {tenant_id}\") collection_name = self.get_collection_name(tenant_id, collection_type) config = self.tenant_configs[tenant_id] # 컬렉션 존재 여부 확인 collections = self.client.get_collections() collection_names = [c.name for c in collections.collections] if collection_name not in collection_names: self.client.create_collection( collection_name=collection_name, vectors_config=VectorParams( size=config[\"vector_size\"], distance=config[\"distance\"] ), ) print(f\"Created collection: {collection_name}\") def upsert_for_tenant(self, tenant_id: str, collection_type: str, vectors, payloads, ids): \"\"\"테넌트별 데이터 삽입\"\"\" self.ensure_collection_exists(tenant_id, collection_type) collection_name = self.get_collection_name(tenant_id, collection_type) points = [ PointStruct(id=id_, vector=vector, payload=payload) for id_, vector, payload in zip(ids, vectors, payloads) ] return self.client.upsert( collection_name=collection_name, points=points ) def search_for_tenant(self, tenant_id: str, collection_type: str, query_vector, limit=10, kwargs): \"\"\"테넌트별 검색\"\"\" collection_name = self.get_collection_name(tenant_id, collection_type) return self.client.search( collection_name=collection_name, query_vector=query_vector, limit=limit, kwargs ) def get_tenant_collections(self, tenant_id: str): \"\"\"테넌트의 모든 컬렉션 조회\"\"\" collections = self.client.get_collections() tenant_prefix = f\"{tenant_id}_\" return [ c.name for c in collections.collections if c.name.startswith(tenant_prefix) ] def delete_tenant_data(self, tenant_id: str): \"\"\"테넌트의 모든 데이터 삭제\"\"\" collections = self.get_tenant_collections(tenant_id) for collection_name in collections: self.client.delete_collection(collection_name) print(f\"Deleted collection: {collection_name}\") # 3. 실제 사용 예시 매니저 초기화 manager = QdrantMultiTenantManager(client) 회사A에 문서 데이터 삽입 vectors_a = np.random.rand(3, 1536).tolist() payloads_a = [ {\"doc_id\": \"A1\", \"title\": \"회사A 문서1\", \"tenant\": \"company_a\"}, {\"doc_id\": \"A2\", \"title\": \"회사A 문서2\", \"tenant\": \"company_a\"}, {\"doc_id\": \"A3\", \"title\": \"회사A 문서3\", \"tenant\": \"company_a\"} ] ids_a = [\"A1\", \"A2\", \"A3\"] manager.upsert_for_tenant(\"company_a\", \"docs\", vectors_a, payloads_a, ids_a) 회사B에 제품 데이터 삽입 vectors_b = np.random.rand(2, 1536).tolist() payloads_b = [ {\"product_id\": \"B1\", \"name\": \"회사B 제품1\", \"tenant\": \"company_b\"}, {\"product_id\": \"B2\", \"name\": \"회사B 제품2\", \"tenant\": \"company_b\"} ] ids_b = [\"B1\", \"B2\"] manager.upsert_for_tenant(\"company_b\", \"products\", vectors_b, payloads_b, ids_b) 테넌트별 검색 query_vector = np.random.rand(1536).tolist() 회사A 문서 검색 results_a = manager.search_for_tenant(\"company_a\", \"docs\", query_vector) print(\"회사A 검색 결과:\", [hit.payload for hit in results_a]) 회사B 제품 검색 results_b = manager.search_for_tenant(\"company_b\", \"products\", query_vector) print(\"회사B 검색 결과:\", [hit.payload for hit in results_b]) # 백업 복구 import json import os class TenantBackup: def __init__(self, manager: QdrantMultiTenantManager): self.manager = manager def backup_tenant(self, tenant_id: str, backup_path: str): \"\"\"테넌트별 백업\"\"\" collections = self.manager.get_tenant_collections(tenant_id) for collection_name in collections: # 컬렉션 데이터 내보내기 points = self.manager.client.scroll( collection_name=collection_name, limit=10000 # 적절한 배치 크기 )[0] # 백업 파일에 저장 backup_file = f\"{backup_path}/{collection_name}_backup.json\" with open(backup_file, 'w') as f: json.dump(points, f, indent=2) print(f\"Backed up {collection_name} to {backup_file}\") def restore_tenant(self, tenant_id: str, backup_path: str): \"\"\"테넌트별 복구\"\"\" collections = self.manager.get_tenant_collections(tenant_id) for collection_name in collections: backup_file = f\"{backup_path}/{collection_name}_backup.json\" if os.path.exists(backup_file): with open(backup_file, 'r') as f: points_data = json.load(f) # 데이터 복구 self.manager.client.upsert( collection_name=collection_name, points=points_data ) print(f\"Restored {collection_name} from {backup_file}\")",
    "public": true
  },
  {
    "id": "Data Science/Data Engineering/compare-vDB-libraries",
    "title": "VectorDB에 사용되는 라이브러리들 비교",
    "date": "2025-07-25",
    "excerpt": "챗봇 구현하면서 RAG 부분에 사용할 벡터화된 문서들을 관리할 DB들을 비교",
    "category": "Data Science",
    "tags": [
      "vector-database",
      "chatbot",
      "RAG"
    ],
    "content": "Qdrant vs ChromaDB vs FAISS vs Milvus DB 주요 특징 장점 단점 ------------ ---------------------------------------- -------------------------------------------------------------- -------------------------------------------------------------- Qdrant Rust 기반, 하이브리드 인덱스, API 다양 빠른 쿼리, 확장성, 메타데이터 필터링, 클라우드 지원 클러스터 관리 필요, 일부 고급 기능은 미흡 ChromaDB Python 기반, 내장형, LangChain 연동 쉬운 설치, 빠른 프로토타이핑, 파이썬 친화적, 실시간 검색 대규모 데이터/분산 미지원, 메모리 누수 이슈, 문서화 부족 FAISS Facebook 개발, 라이브러리, GPU 지원 매우 빠른 검색, 다양한 인덱스, 대규모 벡터 처리, 오픈소스 DB 아님(저장/운영 직접 구현 필요), 확장성/운영 자동화 미흡 Milvus 분산형, 클라우드/온프레미스, MLOps 연동 대규모 분산처리, 다양한 인덱스, RBAC 등 엔터프라이즈 기능 복잡한 배포(쿠버네티스 등), 리소스 요구 높음, 러닝커브 있음 --- Qdrant 특징: Rust로 개발된 오픈소스 벡터 DB. 하이브리드 인덱스(벡터+속성), 다양한 거리 측도(유클리드, 코사인 등), REST/gRPC API, 클라우드 및 온프레미스 지원. 장점: 빠른 쿼리 성능(HNSW, PQ 등 고급 인덱스) 메타데이터 기반 필터링(속성+벡터 동시 검색) 클러스터 확장성, 하이브리드 클라우드 지원 문서화 및 커뮤니티 활발 단점: 클러스터 관리/운영 복잡성 일부 고급 기능(예: 자동 클러스터링, 쿼리 리밸런싱 등) 미흡 추천 사용처: RAG, 추천시스템, 대규모 검색, 속성+벡터 동시 필터링이 필요한 서비스 ChromaDB 특징: Python 기반 내장형 벡터 DB. LangChain 등 LLM/RAG 파이프라인에 최적화. 내장 임베딩 모델, SQLite/ClickHouse 스토리지 지원. 장점: 설치/사용이 매우 간단, 빠른 프로토타이핑 파이썬 친화적(ML/AI 워크플로우와 연동 용이) 실시간 검색, 동적 메타데이터 필터링 LangChain, OpenAI API 등과 연동 쉬움 단점: 대규모 데이터/분산 미지원(싱글노드 한계) 메모리 누수 및 서비스 크래시 이슈(2024년 기준) 문서화/엔터프라이즈 기능 부족 추천 사용처: 소규모 RAG, LLM 실험, 빠른 프로토타입, 파이썬 기반 프로젝트 FAISS 특징: Facebook AI가 개발한 벡터 검색 라이브러리. 다양한 인덱스(HNSW, IVF, PQ 등), GPU 지원, 대규모 벡터 처리에 최적화. 장점: 매우 빠른 검색 속도(특히 GPU 활용 시) 다양한 인덱스/거리 측도 지원 대규모 벡터 데이터 처리에 강점 오픈소스, 커뮤니티 활발 단점: DB가 아니라 라이브러리(저장/운영 직접 구현 필요) 분산/확장성, 운영 자동화 미흡 메타데이터 필터링 등 DB적 기능 부족 추천 사용처: 대규모 벡터 검색 엔진 직접 구축, ML/AI 실험, GPU 기반 대용량 검색 Milvus 특징: 분산형 오픈소스 벡터 DB. 클라우드/온프레미스, 다양한 인덱스(HNSW, IVF 등), RBAC, MLOps 연동, 대규모 데이터 처리에 최적화. 장점: 대규모 분산처리(수억~수십억 벡터) 다양한 인덱스, GPU 가속 지원 엔터프라이즈 기능(RBAC, 감사로그, GDPR 등) MLOps, Grafana/Prometheus 등과 연동 단점: 복잡한 배포(쿠버네티스 등 인프라 필요) 높은 리소스 요구, 러닝커브 있음 소규모/단일 노드에는 과한 선택일 수 있음 추천 사용처: 대규모 RAG, 엔터프라이즈 검색, 보안/감사/확장성 중요한 서비스 --- 여러 회사/문서 그룹 구분 관리: 컬렉션/네임스페이스/파티션 비교 DB 데이터 분리 지원 분리 방식(컬렉션/네임스페이스 등) 멀티테넌시/권한관리 추천도(회사별 구분) ------------ ------------------ ------------------------------- -------------------- -------------------- Qdrant O 컬렉션(collection) O ★★★★☆ ChromaDB O 컬렉션(collection) X ★★☆☆☆ FAISS X(직접구현) 없음(코드로 인덱스 분리) X ★☆☆☆☆ Milvus O 컬렉션/파티션/네임스페이스 O ★★★★★ --- 실무/운영 관점에서의 추천 Qdrant: 컬렉션 단위로 회사A, 회사B 등 완전히 분리된 데이터 관리 가능. API/권한관리도 우수. 운영/확장성 모두 강점. Milvus: 컬렉션, 파티션, 네임스페이스 등 다양한 분리 방식 제공. 대규모 멀티테넌시, RBAC 등 엔터프라이즈 환경에 최적. ChromaDB: 컬렉션으로 구분은 가능하나, 싱글노드/내장형이라 대규모 운영에는 한계. 실험/소규모에 적합. FAISS: DB가 아니라 라이브러리라 데이터 분리/권한관리 직접 구현 필요. 운영/확장성 목적에는 비추천. 결론: 회사별, 프로젝트별 등 여러 그룹의 문서를 하나의 벡터 DB에서 안전하게 구분/운영하려면 Qdrant 또는 Milvus가 가장 적합합니다. (컬렉션/네임스페이스/권한관리 등 지원) ChromaDB는 소규모/실험에, FAISS는 직접 구현이 필요하므로 대규모 운영에는 부적합합니다. 멀티테넌시란? 멀티테넌시(Multi-tenancy)는 하나의 데이터베이스(혹은 시스템) 안에서 여러 사용자 그룹(회사, 조직, 프로젝트 등)이 서로의 데이터에 영향을 주지 않고 안전하게 분리된 공간에서 데이터를 관리할 수 있는 구조를 의미합니다. 테넌트(tenant): 하나의 고객, 회사, 조직, 프로젝트 등 독립적으로 데이터를 관리해야 하는 단위 멀티테넌시: 여러 테넌트(회사A, 회사B, …)가 한 DB 인스턴스를 공유하지만, 각 테넌트의 데이터는 완전히 분리되어 있고, 서로의 데이터에 접근하거나 영향을 줄 수 없음 벡터 DB에서의 멀티테넌시 예시 회사A와 회사B가 같은 Qdrant/Milvus 서버를 사용하지만, 회사A는 자신의 컬렉션/네임스페이스/파티션에만 접근 가능 회사B도 마찬가지로 자신의 데이터만 접근 가능 서로의 데이터는 절대 보거나 수정할 수 없음 권한 관리(RBAC), API 인증 등으로 보안도 보장 왜 중요할까? 운영비 절감: 회사마다 DB를 따로 띄우지 않아도 됨 보안/격리: 데이터가 섞이지 않고, 실수로도 접근 불가 확장성: 테넌트가 늘어나도 하나의 시스템에서 관리 가능 정리: 멀티테넌시는 “여러 회사/조직이 하나의 DB를 쓰더라도, 각자의 데이터가 완전히 분리·격리되어 안전하게 관리되는 구조”를 의미합니다. (SaaS 서비스에서 매우 중요한 개념!) 무료 서비스/오픈소스 여부 DB 오픈소스 자체 서버 무료 공식 클라우드 무료 플랜 비고 ------------ ---------- --------------- ------------------------ --------------------------- Qdrant O O O (Qdrant Cloud Free) 오픈소스, 클라우드 무료 티어(제한 있음) ChromaDB O O X 오픈소스, 자체 서버만 지원 FAISS O O X 오픈소스 라이브러리, 클라우드 없음 Milvus O O O (Zilliz Cloud Free) 오픈소스, Zilliz Cloud 무료 티어(제한 있음) --- 상세 설명 Qdrant 오픈소스(무료)로 직접 서버 운영 가능 공식 Qdrant Cloud에서 무료 플랜(제한적 용량/쿼리) 제공 ChromaDB 오픈소스(무료)로 직접 서버 운영만 가능 공식 클라우드 서비스/무료 플랜 없음 FAISS 오픈소스(무료) 라이브러리 클라우드 서비스 없음(직접 서버/코드로만 사용) Milvus 오픈소스(무료)로 직접 서버 운영 가능 공식 Zilliz Cloud에서 무료 플랜(제한적 용량/쿼리) 제공 --- 정리: 4개 모두 오픈소스라 직접 서버를 띄우면 무료로 사용 가능 Qdrant, Milvus는 공식 클라우드에서 무료 플랜도 제공 ChromaDB, FAISS는 클라우드 서비스가 없고, 직접 설치/운영만 가능 --- Qdrant vs ChromaDB 컬렉션의 차이 항목 Qdrant 컬렉션 ChromaDB 컬렉션 --------------------- ------------------------------------- ---------------------------------- 분리 수준 완전 분리(물리적/논리적) 논리적 분리(내장 DB 내 테이블) 확장성 분산/클러스터 지원, 대규모 운영 가능 싱글노드, 대규모 운영 한계 보안/권한관리 컬렉션 단위 권한/RBAC 지원 컬렉션 단위 권한관리 미흡 운영 신뢰성 장애 격리, 데이터 손상 최소화 전체 DB 장애 시 모든 컬렉션 영향 API/관리도구 REST/gRPC 등 공식 API, 관리도구 풍부 파이썬 API 위주, 관리도구 제한적 백업/복구 컬렉션 단위 백업/복구 지원 전체 DB 단위 백업/복구 멀티테넌시 엔터프라이즈급 멀티테넌시 구현 용이 실험/소규모 멀티테넌시에 적합 상세 설명 Qdrant 컬렉션이 완전히 분리된 데이터베이스처럼 동작합니다. 각 컬렉션은 별도의 인덱스, 설정, 권한, 백업/복구가 가능합니다. 대규모 서비스, 멀티테넌시, 보안이 중요한 환경에 적합합니다. 장애가 나도 한 컬렉션만 영향받고, 나머지는 정상 동작합니다. ChromaDB 컬렉션은 논리적 분리(테이블 수준)로, 하나의 내장 DB 파일에 여러 컬렉션이 들어갑니다. 싱글노드 기반이라 대규모 운영, 장애 격리, 권한 분리에는 한계가 있습니다. 실험, 소규모 서비스, 빠른 프로토타입에는 충분히 유용합니다. Qdrant vs Milvus 비교 항목 Qdrant Milvus --------------------- ---------------------------------------------- --------------------------------------------- 개발 언어/엔진 Rust 기반, 경량/고성능 C++/Go 기반, 대규모 분산 아키텍처 아키텍처 싱글노드/클러스터 모두 지원, 경량화 분산형(클러스터 전용), 대규모 확장성 설치/운영 설치/운영 간단, 로컬/클라우드 모두 용이 쿠버네티스 등 인프라 필요, 복잡한 배포 확장성 중~대규모까지 무난, 클러스터 지원 초대규모(수억~수십억 벡터) 분산처리 최적화 API/언어 지원 REST/gRPC, Python 등 다양한 언어 지원 REST/gRPC, Python/Java/Go 등 다양한 언어 지원 인덱스/검색 HNSW, PQ 등 고성능 인덱스, 하이브리드 검색 HNSW, IVF, DiskANN 등 다양한 인덱스 메타데이터/필터 벡터+속성 동시 필터링(하이브리드 검색 강점) 벡터+속성 동시 필터링, 복잡한 쿼리 지원 권한/보안 컬렉션 단위 권한, RBAC(클라우드) RBAC, 감사로그, GDPR 등 엔터프라이즈 기능 멀티테넌시 컬렉션 단위 완전 분리, 멀티테넌시 용이 네임스페이스/컬렉션/파티션 등 분리, 엔터프라이즈급 클라우드 서비스 Qdrant Cloud(무료/유료 플랜) Zilliz Cloud(무료/유료 플랜) 커뮤니티/문서 활발, 문서/가이드 풍부 활발, 엔터프라이즈 문서/사례 풍부 추천 사용처 RAG, 추천시스템, 중~대규모 서비스, 빠른 구축 초대규모 검색, 엔터프라이즈, 보안/감사 필수 환경 실무적 선택 포인트 Qdrant 설치/운영이 쉽고, 중~대규모 서비스에 적합 빠른 프로토타이핑, RAG, 추천시스템, 속성+벡터 동시 필터링이 중요한 서비스 클라우드/온프레미스 모두 유연하게 사용 가능 대규모 분산이 꼭 필요하지 않다면 Qdrant가 더 간편 Milvus 초대규모(수억~수십억 벡터) 데이터, 분산처리, 엔터프라이즈급 보안/감사/권한이 필요한 환경 쿠버네티스 등 인프라 구축이 가능하고, 복잡한 운영을 감수할 수 있을 때 MLOps, 대규모 AI/검색 플랫폼, 보안/감사/규제 준수가 중요한 기업 환경 결론: 빠른 구축, 중~대규모, 운영 편의성 → Qdrant 추천 초대규모, 엔터프라이즈, 보안/감사/분산 필수 → Milvus 추천 둘 다 무료/클라우드 플랜이 있으니, 실제 PoC(파일럿)로 테스트해보고 결정하는 것도 좋습니다. 샘플 코드들 compare-vDB-libraries",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-24-물건에대한분석",
    "title": "물건에 대한 분석",
    "date": "2025-07-24",
    "excerpt": "물건에 대한 분석",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 It's not about performance. It's about design. It's not about customizabiltiy. It's about compatibility. In which case, I'll probably go with the new Galaxy. go with : 선택하다",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/how-can-evaluate-casual-models",
    "title": "how to evaluate CATE estimation model",
    "date": "2025-07-24",
    "excerpt": "CATE estimation model 평가 방법에 대해 정리",
    "category": "Causal Inference",
    "tags": [
      "인과추론",
      "metrics"
    ],
    "content": "참고한 자료 Causal Inference for The Brave and True > When Prediction Fails chapter, they generate data on both $Y_{0i}$ and $Y_{1i}$ so that they can check if their model is correctly capturing the treatment effect $Y_{1i} - Y_{0i}$. > That’s fine for academic purposes, but in the real world, we don’t have that luxury. CATE estimation model은 어떻게 평가할 수 있을까요? 학문적으로는 임의로 데이터를 생성해서 실제로 이를 잘 추정하는지 확인해볼 수 있을 것 같습니다. > The quality of your model will have to be grounded on something more concrete than a beautiful theory. > Unfortunately, it isn’t obvious at all how we achieve anything like a train-test paradigm in the case of causal inference. > That’s because causal inference is interested in estimating an unobservable quantity, $\\frac{\\delta y}{\\delta t}$ 하지만 실제 데이터에서는 CATE를 직접 확인할 수 없으니, 다른 방법을 써야합니다. 회사에 인과추론을 적용하면서 가장 많이 고민헀던 부분이기도 합니다. 내가 만든 모델의 성능을 어떻게 설명하고 설득할 수 있을까요? 머신러닝은 train-test paradigm을 통해 성능을 평가할 수 있고, 이는 꽤나 직관적으로 모델의 성능을 증명해주고 있습니다. CATE estimation model에도 이러한 방법이 존재할까요? > Is not a definitive one, but it works in practice and it has that concreteness, which I hope will approach causal inference from a train-test paradigm similar to the one we have with machine learning. > The trick is to use aggregate measurements of sensitivity. > Even if you can’t estimate sensitivity individually, you can do it for a group and that is what we will leverage here. 개개인의 CATE는 알 수 없지만 그룹의 ATE는 알 수 있으니 이를 활용해서 CATE를 평가해볼 수 있습니다. Sensitivity by Model Band --- > A good causal model should help us find which customers will respond better and worse to a proposed treatment. > They should be able to separate units into how elastic or sensitive they are to the treatment. CATE estimation의 목적은 처치에 대한 민감도를 개인별로 추정하여, 개인화된 처치를 하는데에 있습니다. 따라서 개인별로 민감도를 명확하게 구분하는 것이 중요합니다. > If that is the goal, it would be very useful if we could somehow order units from more sensitive to less sensitive. CATE estimation을 통해 개인별 민감도를 계산하고 이를 활용해 정렬을 할 수 있습니다. 정렬을 얼마나 잘하는지를 통해 모델의 성능을 평가해볼 수 있지 않을까요? > Sadly, we can’t evaluate that ordering on a unit level. > But, what if we don’t need to? > What if, instead, we evaluate groups defined by the ordering? > If our treatment is randomly distributed (and here is where randomness enters), estimating sensitivity for a group of units is easy. > All we need is to compare the outcome between the treated and untreated. <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/cate_metric_using_ate.png\" alt=\"cate_metric_using_ate\" /> <figcaption> 그림1. CATE 평가 방법에 대한 직관적 설명 (low: 비처치대상, high: 처치대상-가격을 높였을 때)</figcaption> </figure> 그림1이 의미하는 것은 추정된 cate로 정렬했을 때, 정렬된 것들을 기준으로 그룹을 나눠 ATE를 구하면 그 차이가 분명할 것이라는 것입니다. 맨 오른쪽에 있는, 랜덤하게 cate를 배정한 경우 ATE가 완전하게 같은 것을 확인할 수 있습니다. 반대로 model-2의 경우 ATE가 분명하게 차이가 나는 것을 확인할 수 있습니다. > Just by looking at these plots, you can get a feeling of which model is better. > The more ordered the sensitivities look like and the more different they are between bands, the better. > Here, model 2 is probably better than model 1, which is probably better than the random model. Cumulative Gain Curve 계속해서 가져가고 있는 컨셉은, CATE estimation은 개인별로 cate를 구분되게 추정하는 것이 목표이고, 이를 사용해 정렬한 후 그룹을 나눠 ATE를 구하면 차이가 나야한다는 것입니다. <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/cumulative_gain_curve.png\" alt=\"cumulative_gain_curve\" /> <figcaption> 그림2. Cumulative gain curve</figcaption> </figure> 그림2는 그림1의 그룹별 ATE결과를 바탕으로 높은 순서대로 정렬한 후에, 누적되는 ATE를 살펴본 것입니다. 비교대상인 랜덤한 경우를 보면, ATE 누적 분포가 직선의 모습을 보이고 있는 것을 확인할 수 있습니다. 반대로 정렬이 잘된 경우라면 이 랜덤한 모델이 그리는 직선보다 더 높게 휘어진 직선을 그리고 있는 것을 확인할 수 있습니다. > Once we have the theoretic random curve, we can use it as a benchmark and compare our other models against it. > All curves will start and end at the same point. > However, the better the model at ordering sensitivity, the more the curve will diverge from the random line in the points between zero and one. 이러한 방법을 통해 모델의 성능을 평가할 수 있습니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-23-뭐뭐하긴해도",
    "title": "뭐뭐하긴 해도",
    "date": "2025-07-23",
    "excerpt": "뭐뭐하긴 해도 쓰는 표현들",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 Although, its a bit pricey, the bar has an amazing view at night. Although there is a long wait, it has the best bubble tea in town. Even though, Im not as big fan of the movie, I still enjoyed it. I like chocolate, even though I dont like sweet. I love burgers. Its not healthy though. The coat is too expensive. but its pretty though. Fair enough : 인정 간단한 작문 Although seoul is confused, full of people, its a great place to live. Many 편의시설 in here. Even though Im not a big fan of anime, I love 귀멸의 칼날. I really love burgers. Its not healthy thought. 더 자연스러운 작문 & 피드백 Although Seoul is confusing and crowded, it's a great place to live. → 피드백: \"confused\" 대신 \"confusing and crowded\"로 자연스럽게 수정. \"Many 편의시설 in here.\"는 \"There are many convenient facilities here.\"로 바꾸면 좋아요. Even though I'm not a big fan of anime, I love Demon Slayer. → 피드백: \"귀멸의 칼날\"은 영어 제목 \"Demon Slayer\"로 쓰면 자연스러워요. I really love burgers. It's not healthy though. → 피드백: \"thought\"는 오타, \"though\"가 맞아요.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/law-of-total-expectaion",
    "title": "Law of total expectation",
    "date": "2025-07-23",
    "excerpt": "전체기대법칙에 대해 정리",
    "category": "Data Science",
    "tags": [
      "math"
    ],
    "content": "전체기대법칙, 진짜 생활 버전 > 핵심 아이디어 > “전체 평균 = ‘각 상황별 평균’들을 확률로 가중한 평균” --- 마트 계산대 이야기로 직관 잡기 마트에는 \\\\빠른 계산대(셀프)\\\\와 일반 계산대(직원) 두 줄이 있다. 셀프 계산대를 선택할 확률 70 %, 일반 계산대는 30 %. 셀프에서 걸리는 평균 시간은 2분, 일반 계산대는 6분. “전체 줄 서기 평균 시간”은? 조건부 평균: 셀프 = 2분, 일반 = 6분 가중 평균: $2 \\times 0.7 + 6 \\times 0.3 = 3.2$ 분 3.2 분이 Law of Total Expectation 결과다. ([Wikipedia][1]) > 한 줄 요약 ― “상황별 평균을 구한 뒤, 각 상황이 일어날 확률로 다시 한 번 평균을 내면 전체 평균이 된다.” --- 공식을 뜯어보면 $$ \\mathbb{E}[X] = \\mathbb{E}\\big[\\;\\mathbb{E}[X \\mid Y]\\;\\big] $$ $X$: 알고 싶은 대상(줄 서는 시간). $Y$: 상황을 나누는 열쇠(선택한 계산대). $\\mathbb{E}[X\\mid Y]$: “상황 Y 가 주어졌을 때 평균” — 셀프면 2분, 일반이면 6분. $\\mathbb{E}[\\cdot]$: 그 값들을 다시 평균 — 확률 0.7, 0.3으로 가중. ([Wikipedia][1]) --- ‘3단 요리법’으로 기억하기 단계 해야 할 일 결과 --------- -------------------------- --------- ① 상황 나누기 문제를 쉽게 쪼갤 변수·사건 $Y$ 고르기 ‘셀프 / 일반’ ② 상황별 기대값 $\\mathbb{E}[X\\mid Y=y]$ 계산 2분, 6분 ③ 가중 평균 ②에 $P(Y=y)$ 곱해 다 더하기 3.2분 > 어디에 쓰나? > > 고객 세그먼트별 구매액 산정 > - 보험료 예상(나이대·지역별) > - A/B 테스트 두 그룹 결과 합치기 --- 트리 그림으로 그려 보기 `` 전체 고객 / \\ (70%) (30%) 셀프 계산대 일반 계산대 평균 2분 평균 6분 `` 아래층 평균(2, 6)을 위층 확률(0.7, 0.3)로 끌어올려 위층 평균(3.2)을 만든다. 그래서 이 법칙을 \\\\Tower Rule(탑 규칙)\\\\이라고도 부른다. ([Mathematics Stack Exchange][2]) --- ‘조건을 한 번 더 주어도 OK’ $$ \\mathbb{E}\\!\\big[\\;\\mathbb{E}[X \\mid Y,Z]\\;\\big]=\\mathbb{E}[X] $$ 층을 여러 층 쌓아도, 맨 꼭대기로 올라가면 언제나 $X$의 원래 평균. 정보를 조금 알고(한 층 내려가고) 다시 모른 척(평균)해도, 결국 제자리다. ([The Book of Statistical Proofs][3]) --- 한눈에 끝내는 기억 장치 > “조건부 평균은 곧 ‘부분 평균표’, > 그 표를 다시 평균내면 전체 평균표.” --- 자주 묻는 Q\\&A 질문 답변 ------------------- ----------------------------------------------------------------- 왜 굳이 이렇게 돌려 계산? 복잡한 문제를 작은 블록으로 나눌 때 계산이 훨씬 쉽다. 확률 대신 비율·가중치 써도 되나? 총합이 1이면 어떤 가중치든 무방. 연속형 변수도 되나? 사건 합 대신 적분으로 바뀔 뿐 똑같이 성립한다. ([Everything Explained Today][4]) --- 필요하면 전체분산법칙(Var 버전)이나 베이지안 업데이트에서의 활용 예도 이어서 보여줄 수 있다. [1]: https://en.wikipedia.org/wiki/Law_of_total_expectation?utm_source=chatgpt.com \"Law of total expectation\" [2]: https://math.stackexchange.com/questions/41536/intuitive-explanation-of-the-tower-property-of-conditional-expectation?utm_source=chatgpt.com \"Intuitive explanation of the tower property of conditional expectation\" [3]: https://statproofbook.github.io/P/mean-tot.html?utm_source=chatgpt.com \"Law of total expectation The Book of Statistical Proofs\" [4]: https://everything.explained.today/Law_of_total_expectation/?utm_source=chatgpt.com \"Law of total expectation explained\"",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-r-learner",
    "title": "R-learner란?",
    "date": "2025-07-23",
    "excerpt": "R-learner의 개념에 대해 정리",
    "category": "Causal Inference",
    "tags": [
      "R-learner",
      "인과추론"
    ],
    "content": "R-learner는 CATE estimation에 사용되는 meta-learner 중 하나입니다. 이에 대해 하나씩 정리헀습니다. CATE estimation이란? --- 인과추론 (Causal Inference)의 목적은 처치의 효과를 추정하는데에 있습니다. 처치가 결과에 어떤 영향을 미치는지 확인하고 이를 통해 더 좋은 결정을 내릴 수 있습니다. 1.1 ATE란? > ATE(Average Treatment Effect)는 전체 집단에서 처치(예: 신약, 정책 등)가 미치는 평균적인 인과 효과를 의미합니다. $$ \\text{ATE} = \\mathbb{E}[Y^{(t)} - Y^{(0)}] $$ 여기서 $$Y^{(t)}$$은 처치를 받았을 때의 잠재적 결과 (Potential Outcome)이고, $$Y^{(0)}$$은 처치를 받지 않았을 때의 잠재적 결과입니다. 즉 처치를 받았을 때의 평균적인 인과효과를 의미합니다. 1.2 CATE란? > CATE(Conditional Average Treatment Effect)는 특정 조건에서 처치가 미치는 평균적인 인과 효과를 의미합니다. $$ \\text{CATE} = \\tau(x) = \\mathbb{E}[Y^{(t)} - Y^{(0)} X = x] $$ 여기서 $$X$$는 조건을 의미하고, $$Y^{(t)}$$는 처치를 받았을 때의 잠재적 결과, $$Y^{(0)}$$는 처치를 받지 않았을 때의 잠재적 결과를 의미합니다. ATE를 통해 처치의 평균적인 인과효과(처치가 결과에 미치는 영향)을 추정했습니다. 다만 개개인별로 인과효과가 다르다는 것은 직관적인데요, 이러한 개별 개체의 처치효과를 ITE라고 합니다. > ITE (Individual Treatment Effect) > - 정의: 개별 개체의 처치 효과 > - 수식: $$\\text{ITE}_i = Y_i^{(1)} - Y_i^{(0)}$$ > - 의미: 특정 개인이 처치를 받았을 때와 받지 않았을 때의 결과 차이 > - 특징: 실제로는 관찰 불가능 (fundamental problem of causal inference) 그리고 이를 추정하는 값이 CATE 입니다. > Estimating heteronenous treatment effects is fundamental in causal inference and provides insights into various fields. > > -> Towards R-learner with Continuous Treatment 개인별 처치효과를 추정하는 일은 다양한 분야에서 핵심적인 역할을 합니다. 학생별 교육효과를 추정해 개인화된 교육 프로그램을 실행하거나, 환자별 처치효과를 추정해 개인화된 처치를 실행할 수 있습니다. CATE estimation에 사용되는 meta-learner들 --- 메타러너 (meta-learner)는 기존 예측 머신러닝 알고리즘을 활용해서 처치효과를 추정하는 간단한 방법입니다. cate estimation을 위해 사용되는 meta-learner에는 대표적으로 T-learner, X-learner, S-learner가 있습니다. 2.1 T-learner 범주형 처치를 다룰 때 사용할 수 있는 메타러너입니다. $$ \\hat{\\mu}_0(x) = \\mathbb{E}[Y T=0, X = x] \\\\ \\hat{\\mu}_1(x) = \\mathbb{E}[Y T=1, X = x] \\\\ \\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x) $$ <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/T-learner.png\" alt=\"T-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> 그림 1 : T-learner 구조<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">출처: Causal Inference for The Brave and True</a> </figcaption> </figure> 그림1에서 보이는 것처럼, 처치별로 각각의 예측 모델을 만들고 이 모델의 예측결과를 비교해 CATE를 추정합니다. ``python m0 = LGBMRegressor(max_depth=2, min_child_samples=60) m1 = LGBMRegressor(max_depth=2, min_child_samples=60) m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y]) estimate the CATE t_learner_cate_train = m1.predict(train[X]) - m0.predict(train[X]) t_learner_cate_test = test.assign(cate=m1.predict(test[X]) - m0.predict(test[X])) ` 2.2 S-learner (aka, the Go-Horse Learner) S러너는 가장 기본적인 방식으로, 단일 머신러닝 모델을 사용하여 추정합니다. $$ \\hat{\\mu}(x) = E[Y X = x] \\\\ \\hat{\\tau}(x) = \\hat{\\mu}(x, T=1) - \\hat{\\mu}(x, T=0) $$ <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/S-learner.png\" alt=\"S-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> 그림 2 : S-learner 구조<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">출처: Causal Inference for The Brave and True</a> </figcaption> </figure> 그림2에서 보이는 것처럼 모든 처치에 대해 예측하는 모델을 만들고, 이 모델을 통해 CATE를 추정합니다. `python s_learner = LGBMRegressor(max_depth=3, min_child_samples=30) s_learner.fit(train[X+[T]], train[y]); s_learner_cate_train = (s_learner.predict(train[X].assign({T: 1})) - s_learner.predict(train[X].assign({T: 0}))) s_learner_cate_test = test.assign( cate=(s_learner.predict(test[X].assign({T: 1})) - # predict under treatment s_learner.predict(test[X].assign({T: 0}))) # predict under control ) ` 이 모델은 CATE estimation에 사용되는 메타러너 중에 가장 쉽게 적용해볼 수 있는 모델이지만, 편향에 가장 취약한 모델이기도 합니다. 2.3 X-learner <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/X-learner.png\" alt=\"X-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> 그림 3 : X-learner 구조<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">출처: Causal Inference for The Brave and True</a> </figcaption> </figure> X-learner는 2단계로 구성되어 있습니다. 1단계에서는 T-learner와 마찬가지로 처치별로 데이터를 나눠 모델을 학습시킵니다. $$ \\hat{M}_0(X) \\approx \\mathbb{E}[Y T=0, X] \\\\ \\hat{M}_1(X) \\approx \\mathbb{E}[Y T=1, X] $$ 2단계에서는 1단계에서 학습시킨 모델로 추정된 CATE를 계산합니다. $$ \\hat{\\tau}(X, T=0) = \\hat{M}_0(X) - Y_{T=0} \\\\ \\hat{\\tau}(X, T=1) = \\hat{M}_1(X) - Y_{T=1} $$ 그 후에 이를 예측하는 모델을 학습시킵니다. $$ \\hat{M}_{\\tau 0}(X) = E[\\hat\\tau(X) T = 0] \\\\ \\hat{M}_{\\tau 1}(X) = E[\\hat\\tau(X) T = 1] $$ 마지막으로 이 모델의 결과를 보완해주기 위해서 propensity score를 사용합니다. 학습을 위한 $\\hat{\\tau}(X, T=0)$ 을 추정할 때 $Y_1$ 이 없기 때문에 $\\hat{M}_1$을 사용해서 이를 추정했습니다. 이때 X가 0일 확률이 높다면 $\\hat{M}_1$의 예측값의 성능이 떨어질 것이고, 이로 인해 $\\hat{\\tau}(X, T=0)$의 추정값도 신뢰가 떨어질 것입니다. 반대로 \\hat{\\tau}(X, T=0)$의 추정값의 신뢰도는 올라갈 것입니다. 이러한 직관을 가지고 propensity score를 사용해서 보완합니다. $$ \\hat{\\tau}(X) = \\hat{M}_{\\tau 0}(X)\\hat e(x) - \\hat{M}_{\\tau 1}(X)(1-\\hat e(x)) $$ `python from sklearn.linear_model import LogisticRegression np.random.seed(123) first stage models m0 = LGBMRegressor(max_depth=2, min_child_samples=30) m1 = LGBMRegressor(max_depth=2, min_child_samples=30) propensity score model g = LogisticRegression(solver=\"lbfgs\", penalty='none') m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y]) g.fit(train[X], train[T]); ` `python d_train = np.where(train[T]==0, m1.predict(train[X]) - train[y], train[y] - m0.predict(train[X])) second stage mx0 = LGBMRegressor(max_depth=2, min_child_samples=30) mx1 = LGBMRegressor(max_depth=2, min_child_samples=30) mx0.fit(train.query(f\"{T}==0\")[X], d_train[train[T]==0]) mx1.fit(train.query(f\"{T}==1\")[X], d_train[train[T]==1]); ` `python def ps_predict(df, t): return g.predict_proba(df[X])[:, t] x_cate_train = (ps_predict(train,1)mx0.predict(train[X]) + ps_predict(train,0)mx1.predict(train[X])) x_cate_test = test.assign(cate=(ps_predict(test,1)mx0.predict(test[X]) + ps_predict(test,0)mx1.predict(test[X]))) `` R-learner란? 다른 meta-learner들과 비교 --- R-learner는 FWL-theorem을 meta-learner 형태로 구현한 모델입니다. FWL theorem이란? 조금 더 엄밀하게 이야기하면 double/debiased machine learning이라고 말할 수 있고, 여기에 propensity score를 함께 사용해 편향을 보정하는 모델이 R-learner입니다. 편향을 nuisance function을 통해 잔차화하여 제거하고, CATE를 추정합니다. 이때 nuisance function들은 기존의 예측 모델을 통해 쉽게 구현되며, CATE estimation에 사용되는 모델도 마찬가지입니다. 따라서 다중회귀모델과 다르게 비선형성을 포착할 수 있습니다. R-learner는 다른 meta-learner들과 다르게 이 모델은 CATE를 직접적으로 추정하고 있습니다. 따라서 다른 모델보다 CATE를 더 잘 추정한다고 알려져 있습니다. > CATE estimation을 평가하는 방법에는 2가지정도가 있는데, > 하나는 임의로 데이터셋을 만들어 실제로 잘 추정하는지 확인하는 것과 > 다른 하나는 cate로 정렬해 그룹별로 ATE를 구해 비교하는 것입니다. -> how to evaluate CATE estimation model 또한 회귀모델을 통한 잔차화와 propensity score를 통한 잔차화를 진행해 doubly robust 하다는 장점도 가지고 있습니다. Double robustness란? --- 편향을 제거하기 위한 방법에는 회귀를 통한 잔차화와 propensity score를 통한 방법이 있습니다. 회귀를 통한 잔차화는 FWL-theorem에 기반하고 있습니다. propensity score를 통한 잔차화의 컨셉은 X를 통제할 수 없다면 e(X)를 통제해서, X를 통제한 것과 같은 상태를 만들자는 것입니다. 참고자료 PropensityScoreNotebook.ipynb Causal Inference for The Brave and True - Propensity Score R-leaner는 이 2가지 모두를 사용해 편향을 없애고 있고, 이를 위한 2개의 모델 중 하나의 성능만 보장되면 나머지 하나의 성능이 부족하더라도 CATE estimation의 성능이 보장된다고 합니다. 4.1 ATE에서 doubly robust estimation $$ \\hat{\\text{ATE}} = \\frac{1}{N} \\sum (\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)} + \\hat \\mu_1(X_i)) - \\frac{1}{N} \\sum (\\frac{(1-T_i)(Y_i - \\hat \\mu_0(X_i))}{1 - \\hat P(X_i)} + \\hat \\mu_0(X_i)) \\tag{1} $$ 1번 수식에서 앞에 부분은 $\\hat E[Y X, T=1]$ 의 추정값이고, 뒤에 부분은 $\\hat E[Y X, T=0]$ 의 추정값입니다. $\\hat E[Y X, T=1]$에 대해서 doubly robust가 성립하는지 살펴보고 이를 기반해서 $\\hat E[Y X, T=0]$에 대해서는 유추해보록 하겠습니다. $$ \\hat E[Y_1] = \\frac{1}{N} \\sum (\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)} + \\hat \\mu_1(X_i)) \\tag{2} $$ $\\hat \\mu_1(X_i)$ 가 정확하고 propensity score가 부정확할 때. 식 2에서 $\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)}$ 부분이 0이 되기 때문에 결과가 보장됩니다. $$ \\begin{align} \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\frac{T_i \\hat{\\mu}_1(X_i)}{\\hat{P}(X_i)} + \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\left( \\frac{T_i}{\\hat{P}(X_i)} - 1 \\right) \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\left( \\frac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)} \\right) \\hat{\\mu}_1(X_i) \\right) \\end{align} \\tag{3} $$ propensity score가 정확하고 $\\hat \\mu_1(X_i)$ 가 부정확할 때. 식 3에서 $\\frac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)}$ 부분이 0이 되기 때문에 결과가 보장됩니다. 4.2 R-learner에서 doubly robust가 보장되는 방식 Quasi-Oracle Estimation of Heterogeneous Treatment Effects에서 이를 정의하고 있습니다. paper link paper review 기본적인 컨셉은 2개의 nuisance function이 이상적일 때의 error bound와 2개 중 1개만 이상적일 때의 error bound가 같다는 것을 증명하는 것입니다. R-loss가 정의되는 방식 --- Generalized R-loss는 Towards R-learner with Continuous Treatment 논문에서 제안된 손실함수입니다. paper link paper review $$ L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right] \\tag{generalized R-loss} $$ 이는 다음 수식들에서 유도됩니다. 이 수식들은 인과추론의 기본 가정인 unconfoundedness와 stable unit treatment value assumption을 가정하고 있습니다. full conditional outcome mean model $$ \\mu(x, t) = \\mathbb{E}[Y X = x, T = t] $$ conditional outcome mean $$ \\mu(x) = \\mathbb{E}[Y X = x] $$ generalized propensity score $$ \\varpi(x) = \\mathbb{E}[T X = x] $$ 수식 전개 $$ Y_i^{(T_i)} = \\mu(X_i, T_i) + \\epsilon_i = \\mu(X_i, 0) + \\tau(X_i, T_i) + \\epsilon_i \\\\ m(X_i) = E(Y_i^{(T_i)} X=X_i) = \\mu(X_i, 0) + E_{\\varpi}\\{\\tau(X, T) X=X_i\\} + \\epsilon_i \\\\ Y_i^{(T_i)} - m(X_i) = \\tau(X_i, T_i) - E_{\\varpi}\\{\\tau(X, T) X=X_i\\} + \\epsilon_i \\\\ $$ 마지막 식을 $\\epsilon_i$ 에 대해서 정리하면 loss function을 얻을 수 있습니다. 여기서 $h(X, t)$는 $tau(X, T)$를 추정하는 함수를 의미합니다. 5.1 Binary treatment에 대한 loss function > Quasi-Oracle Estimation of Heterogeneous Treatment Effects에서 정의한 binary treatment에 대한 loss function은 generalized R-loss의 특수한 경우입니다. $$ L_b(h) =E[Y - m(X) - {T-e(X)}h(X, 1)]^2 \\tag{binary treatment} $$ T가 0 또는 1이 존재하는 경우이기 떄문에 T를 풀어서 쓸 수 있고, $h(x, 0) = 0$이라는 가정을 loss function에 녹여낼 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/my-fail-experience-graduate-project",
    "title": "졸업 과제로 진행한 프로젝트에 대한 회고",
    "date": "2025-07-23",
    "excerpt": "졸업 과제로 진행한 프로젝트에 대한 회고",
    "category": "Causal Inference",
    "tags": [
      "인과추론",
      "프로젝트"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-22-선택의상황",
    "title": "선택의 상황",
    "date": "2025-07-22",
    "excerpt": "선택의 상황에서 쓰는 표현들",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "배운 것들 best-thing ~의 가장 좋은 점은 ~이다. The best thing about iPhone is its camera. best-thing -> 한번에 일기 aBOUT when it comes to ~에 대해서는, ~가 가장 좋을 것이다. When it comes to online shopping, Amazon might be the best option. 간단한 작문 When it comes to smart-watch, Galaxy Fit is the best option. The best thing about Galaxy Fit is price. It is ver cheap, but have good design and good quality. --- 더 자연스러운 버전 & 피드백 자연스러운 버전 When it comes to smartwatches, the Galaxy Fit is the best option. The best thing about the Galaxy Fit is its price. It is very cheap, but it also has a good design and high quality. 피드백 When it comes to smart-watch, Galaxy Fit is the best option. 수정: When it comes to smartwatches, the Galaxy Fit is the best option. 피드백: \"smart-watch\"는 보통 복수형 \"smartwatches\"로 쓰는 것이 자연스럽고, \"the\"를 붙여 특정 제품을 지칭하는 것이 더 명확합니다. The best thing about Galaxy Fit is price. 수정: The best thing about the Galaxy Fit is its price. 피드백: \"its price\"로 소유격을 써서 더 자연스럽게 표현할 수 있습니다. \"the\"를 붙여 제품을 명확히 지칭하세요. It is ver cheap, but have good design and good quality. 수정: It is very cheap, but it also has a good design and high quality. 피드백: \"ver\"는 오타로 \"very\"가 맞습니다. \"have\" 대신 \"has\"를 써야 하고, \"good design and good quality\" 대신 \"a good design and high quality\"로 더 자연스럽게 표현할 수 있습니다. \"also\"를 추가하면 문장이 더 부드럽게 연결됩니다.",
    "public": true
  },
  {
    "id": "Data Science/Statistics/bias-variance",
    "title": "Bias-Variance Tradeoff",
    "date": "2025-07-22",
    "excerpt": "Bias-Variance Tradeoff에 대한 간단한 설명",
    "category": "Data Science",
    "tags": [
      "bias-variance",
      "machine-learning",
      "statistics"
    ],
    "content": "Bias-Variance Tradeoff 머신러닝에서 모델의 성능을 평가할 때 가장 중요한 개념 중 하나가 Bias-Variance Tradeoff입니다. 이는 모델의 복잡도와 일반화 성능 사이의 균형을 설명하는 핵심 개념입니다. 기본 개념 Bias (편향) > 학습된 모델의 예측값 평균과 실제 값 간의 차이 정의: 모델이 실제 데이터 생성 과정을 얼마나 잘 근사하는지를 나타내는 지표 높은 Bias: 모델이 너무 단순해서 데이터의 패턴을 제대로 학습하지 못함 낮은 Bias: 모델이 복잡해서 데이터의 패턴을 잘 학습할 수 있음 Variance (분산) > 예측값이 얼마나 흩어졌는지를 나타내는 지표 정의: 모델이 훈련 데이터의 작은 변화에 얼마나 민감한지를 나타내는 지표 높은 Variance: 모델이 훈련 데이터에 과도하게 적합되어 새로운 데이터에 대해 성능이 떨어짐 낮은 Variance: 모델이 훈련 데이터의 노이즈에 덜 민감함 수학적 표현 예측 오차의 분해 $$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$ 여기서: $y$: 실제 값 $\\hat{f}(x)$: 모델의 예측값 $\\text{Bias}^2 = E[(\\hat{f}(x) - f(x))^2]$: 편향의 제곱 $\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$: 분산 $\\text{Irreducible Error}$: 데이터 자체의 노이즈로 인한 오차 편향과 분산의 계산 편향과 분산은 다음과 같이 계산됩니다: $$\\text{Bias} = E[\\hat{f}(x)] - f(x)$$ $$\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$ Bias-Variance Tradeoff 핵심 아이디어 모델이 단순할수록: 높은 편향, 낮은 분산 모델이 복잡할수록: 낮은 편향, 높은 분산 시각적 예시 <figure style=\"text-align: center;\"> <img src=\"/post/bias_variance/bias_variance_1.png\" alt=\"Bias-Variance Tradeoff\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto;\"/> <figcaption>그림 1: Bias-Variance 개념 설명 그림</figcaption> </figure> 그림1은 편향과 분산의 개념을 설명하는 그림입니다. 편향은 데이터의 패턴을 얼마나 잘 학습했는지를 의미합니다. 따라서 편향이 낮으면 패턴을 잘 학습해 파란 원 안에 안에 예측값이 모여있는 것을 확인할 수 있습니다. 분산은 예측값이 얼마나 흩어져있는지를 의미합니다. 따라서 분산이 낮으면 한 곳에 뭉쳐있고, 분산이 높으면 넓게 펴져있는 것을 확인할 수 있습니다. <figure style=\"text-align: center;\"> <img src=\"/post/bias_variance/bias_variance_2.png\" alt=\"Bias-Variance Tradeoff\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto;\"/> <figcaption>그림 2: 실제 예측에서의 Bias-Variance Tradeoff - 단순한 모델(높은 편향, 낮은 분산) vs 복잡한 모델(낮은 편향, 높은 분산)</figcaption> </figure> 그림2는 실제 예측에서의 Bias-Variance Tradeoff를 설명하는 그림입니다. 학습하는 데이터셋 안에서 패턴을 잘 찾으면 (편향이 낮을수록) 모델은 복잡해져야 하고, 이때 예측 값이 넓게 퍼져있게 됩니다 (분산이 높아짐) 반대로 학습하는 데이터셋 안에서 패턴을 잘 찾지 못하면 (편향이 높을수록) 모델은 단순해져야 하고, 이때 예측 값이 한 곳에 모여있게 됩니다 (분산이 낮아짐) `` 단순한 모델 (예: 선형 회귀) ├── 높은 Bias: 데이터의 복잡한 패턴을 놓침 └── 낮은 Variance: 새로운 데이터에 대해 안정적 복잡한 모델 (예: 고차 다항식, 딥러닝) ├── 낮은 Bias: 데이터의 복잡한 패턴을 잘 학습 └── 높은 Variance: 새로운 데이터에 대해 불안정 `` 실제 예시 선형 회귀 vs 다항 회귀 선형 회귀 (1차 다항식) 낮은 복잡도 → 높은 편향, 낮은 분산 데이터가 비선형 패턴을 가지고 있다면 성능이 떨어짐 고차 다항 회귀 (예: 10차 다항식) 높은 복잡도 → 낮은 편향, 높은 분산 훈련 데이터에는 완벽하게 적합하지만 새로운 데이터에는 성능이 떨어짐 모델 선택에서의 적용 Underfitting vs Overfitting Underfitting (과소적합) 높은 편향, 낮은 분산 모델이 너무 단순해서 데이터의 패턴을 학습하지 못함 해결책: 모델 복잡도 증가 Overfitting (과적합) 낮은 편향, 높은 분산 모델이 훈련 데이터에 과도하게 적합되어 일반화 성능이 떨어짐 해결책: 모델 복잡도 감소, 정규화, 더 많은 데이터 최적 모델 선택 교차 검증 (Cross-Validation) 훈련 데이터와 검증 데이터를 분리하여 모델의 일반화 성능 평가 편향과 분산의 균형점을 찾는 데 도움 정규화 (Regularization) Ridge Regression (L2 정규화): 분산을 줄여 과적합 방지 Lasso Regression (L1 정규화): 특성 선택을 통한 모델 단순화 실무적 고려사항 데이터 크기의 영향 작은 데이터셋: 분산이 높아 과적합 위험 증가 큰 데이터셋: 편향이 더 중요한 고려사항 특성 엔지니어링 적절한 특성 선택과 변환이 편향-분산 균형에 큰 영향 도메인 지식을 활용한 특성 생성이 중요 결론 Bias-Variance Tradeoff는 머신러닝 모델 설계의 핵심 원칙입니다. 최적의 모델은 주어진 데이터와 문제에 대해 편향과 분산이 적절히 균형을 이루는 모델입니다. 이를 위해서는: 데이터의 특성 이해: 데이터의 복잡도와 노이즈 수준 파악 모델 복잡도 조절: 문제에 적합한 모델 선택 정규화 기법 활용: 과적합 방지를 위한 기법 적용 교차 검증: 모델의 일반화 성능 평가 이러한 원칙을 바탕으로 실무에서 더 나은 머신러닝 모델을 구축할 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/ML Engineering/parametric-vs-non-parametric",
    "title": "Parametric vs Non-Parametric, GBDT는 왜 non-parametric일까?",
    "date": "2025-07-22",
    "excerpt": "Parametric과 Non-Parametric에 대한 간단한 설명",
    "category": "Data Science",
    "tags": [
      "parametric",
      "non-parametric",
      "machine-learning",
      "statistics"
    ],
    "content": "Parametric이란? 영어 한국어 설명 ------------------------- ----------- -------------------------------- parametric model 모수적 모델 고정된 수의 파라미터(모수)를 갖는 모델 parametric assumption 모수적 가정 데이터가 특정한 분포 형태(예: 정규분포)를 따른다는 가정 non-parametric model 비모수적 모델 파라미터 수나 함수 형태를 가정하지 않는 유연한 모델 데이터에서 패턴을 근사해내는게 Machine Learning의 핵심이라고 할 수 있습니다. 이때 데이터가 특정한 함수 형태를 따른다고 가정하거나, 모델의 구조(예: 선형 회귀처럼)가 고정되어 있는 경우를 모수적(parametric) 모델이라고 합니다. Parametric vs Non-Parametric 구분 Parametric Non-parametric ------------------- -------------- ------------------------ 모델 구조 사전에 고정됨 유연하고 데이터 기반 모수(parameter) 수 고정 데이터가 많아지면 증가함 복잡도 제어 파라미터 조정 모델 자체의 구조 변화로 조정 예시 선형 회귀, 로지스틱 회귀 GBDT, KNN, 랜덤포레스트, 커널 회귀 반대로 Non-parametric 모델은 모델의 형태나 구조를 사전에 가정하지 않고, 데이터를 통해 모델의 구조를 유연하게 만들어나가는 것을 의미합니다. 왜 GBDT는 non-parametric일까? 이유 설명 --------- --------------------------------- 파라미터 수 증가 데이터가 늘면 트리 개수나 깊이가 커져서 구조 자체가 변화함 구조 유연성 특정 함수 형태를 가정하지 않음 (예: 선형성) 자유도 과적합 위험도 있지만, 그만큼 복잡한 함수도 학습 가능 학습 대상 데이터가 가르쳐주는 방식에 따라 함수 구조 자체가 결정됨 GBDT는 잔차를 보완하는 방식으로 여러 개의 약한 결정 트리를 순차적으로 학습하고 누적하는 구조를 가지지만, 트리의 개수, 깊이, 노드 수, 분할 규칙 등은 데이터 양과 복잡도에 따라 계속 늘어나거나 바뀔 수 있습니다. 이런 점에서 GBDT는 non-parametric 모델이라고 할 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/Double-Debiased-Machine-Learning-for-Treatment-and-Structural-Parameters",
    "title": "[Paper Review] Double Debiased Machine Learning for Treatment and Structural Parameters",
    "date": "2025-07-22",
    "excerpt": "Double Debiased Machine Learning for Treatment and Structural Parameters",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link",
    "public": false
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-21-고민할때",
    "title": "고민하는 표현들",
    "date": "2025-07-21",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "On one hand, the Note has a really big screen. On one hand, I havent had Korean food in a while On one hand, taking a cab would be really convenient On the other hand, the new iPhone has a really good camera. On the other hand, Im craving fried chicken. On the other hand, I can walk there in 15 minute. Whether I buy the Note or the new iPhone, I should buy a phone tommorow.",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-21-남철님-clabi-커피챗",
    "title": "Clabi coffee chat with 남철",
    "date": "2025-07-21",
    "excerpt": "Clabi 채용공고 관련해서 남철님과 커피챗 진행",
    "category": "Career",
    "tags": [
      "Coffee Chat",
      "Clabi"
    ],
    "content": "채용공고 <details> <summary>📋 <strong>커피챗 상세정보</strong></summary> 클라비 기술연구소 AI 개발자 채용 (RAG, LLM, NLP, 인공지능) 모집부문 및 상세내용 공통 자격요건 학력: 무관 채용 프로세스 서류 지원 > 기술 면접(팀장) > 2차 면접 (연구소장님) > 3차 면접 (대표님) AIOps AI 개발자 모집 (기술연구소 0명) 주요업무 LLM 및 RAG 연구를 통한 AI 서비스 고도화 및 신규 기능 개발 프롬프트 엔지니어링을 활용한 챗봇 및 AI 모델 개발 AI 모델 성능 파인튜닝 및 최적화 RAG 파이프라인 구축 및 관련 애플리케이션 개발 대규모 데이터 수집 전처리 및 관리 근무조건 근무형태: 정규직(수습기간 3개월) 근무일시: 주 5일 (월-금) 10:00~17:00 근무지역: (05854) 서울 송파구 법원로 114 엠스테이트 B동 1108호(문정동) 서울 8호선 문정역에서 200m 이내 지원방법 이력서 및 포트폴리오 제출 지원 및 문의 : DM 또는 이메일(ncher@clabi.co.kr) </details> 커피챗 진행 방식 아이펠에서 공부할 때 퍼실님이었던 남철님과 채용관련 커피챗 진행. 온라인으로 1시간정도 진행함. 간단한 근황 이야기와 내가 진행했던 프로젝트에 대해서 설명드리고 회사에 대해 궁금한 점 질문. 유사 경험 (필살기) 설명 회사에서 하는 일과 내가 진행했던 일의 유사성을 설명. 유사 경험이 있고 회사에서 할 일과 비슷한 고민들을 했었다. 챗봇 프로젝트 Clabi의 주요 업무가 RAG SI인데 실제로 RAG를 통해 비즈니스 문제를 해결했던 경험이 있으니까 이를 강조. -> 긍정적인 반응. 아쉬운 점은 진행했던 일의 난이도가 그렇게 높지 않았다는 점. 만약 기술면접을 준비한다면 이 부분을 미리 공부하고 가면 좋을 것 같다. 내가 궁금했던 부분 RAG SI 업무는 진짜 전혀 안끌림. 그래서 이 부분을 벗어나서 확장하려는 부분이 있는지 -> 현재 수익으로 생성형 AI, SasS 부분으로 확장하고 있다고 함 대규모 데이터 처리. 빅데이터 플랫폼. 스마트팜 부분은 어떻게 진행되는지 -> 아마 외부업체가 참여하는 것 같다. 인터뷰하고 느낀 점은 내가 원하는 방향의 커리어는 아니다. 조금 더 데이터를 살펴보고 분석하면서 모델을 만드는, 데싸적인 느낌의 커리어가 나는 더 좋은 것 같다. 그리고 단순히 SI성 일들이 아닌, 좀 더 높은 수준의 난이도를 요구하는 일을 하고 싶다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-20-조언하는표현Review",
    "title": "Review day",
    "date": "2025-07-20",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "Review day"
    ],
    "content": "오늘 배운거 Try to ~~ Try to cut down on drinking. Try not to skip breakfast. Make sure to ~ Make sure not to eat too much junk foods. Im worried about ~ Im worried about my love life. Im little worried about my bills. Be on the lookout for Be on the lookout for pickpockets. Be careful with ~ Be careful with your phone. Be careful not to burn yourself. Be careful not to catch a cold. Be careful not to cut yourself. By the way By the way, I was the blind date? Well the things is Well the thing is Im not really attracted to her. 간단한 작문 (오늘 배운거 사용도 해보면서 작문하는 연습) Time is so fast. 정신을 차려보면 시간은 한참 지나있다. Acheving something like, get a good english, playing guitar well, 좋은 회사에 취직하는 것 ,... are needs many time in the days. If you cant focus on today looking farway and beding nervous, I never acheive somethings that needs days. You should know that today is so precious. You can do everthing, but you couldn`t do nothing if you don't move. So, make sure to focus on the day you are living. 더 자연스러운 작문 Time flies so fast. Before you know it, a lot of time has passed. Achieving things like getting good at English, playing guitar well, or getting a job at a good company requires many days of effort. If you can't focus on today and keep looking far ahead while getting nervous, you'll never achieve things that take time. You should know that today is precious. You can do anything, but you won't accomplish anything if you don't take action. So, make sure to focus on the day you're living. 피드백 문법 및 표현 수정사항 \"Time is so fast\" → \"Time flies so fast\" \"Time is fast\"는 문법적으로 맞지 않습니다. \"Time flies\"가 자연스러운 표현입니다. \"Acheving\" → \"Achieving\" 철자 오류입니다. \"get a good english\" → \"getting good at English\" \"English\"는 대문자로 시작해야 하며, \"good at English\"가 올바른 표현입니다. \"are needs many time\" → \"requires many days of effort\" 문법적으로 맞지 않는 구조입니다. \"require\"를 사용하고 \"time\" 대신 \"days of effort\"가 더 자연스럽습니다. \"If you cant focus\" → \"If you can't focus\" \"can't\"의 축약형에서 apostrophe가 빠졌습니다. \"looking farway\" → \"looking far ahead\" \"farway\"는 올바른 단어가 아닙니다. \"far ahead\"가 맞습니다. \"beding nervous\" → \"getting nervous\" \"beding\"은 \"being\"의 오타로 보이지만, 맥락상 \"getting nervous\"가 더 자연스럽습니다. \"I never acheive\" → \"you'll never achieve\" 주어를 일관성 있게 \"you\"로 통일하고, 미래 시제를 사용하는 것이 자연스럽습니다. \"somethings\" → \"things\" \"something\"은 셀 수 없는 명사이므로 복수형이 없습니다. \"You can do everthing\" → \"You can do anything\" \"everthing\"은 \"everything\"의 오타입니다. \"couldn't do nothing\" → \"won't accomplish anything\" 이중 부정을 피하고 더 명확한 표현을 사용합니다. \"if you don't move\" → \"if you don't take action\" \"move\"보다 \"take action\"이 더 구체적이고 자연스럽습니다. 오늘 배운 표현 활용도 ✅ \"Make sure to\" 표현을 잘 활용했습니다! \"Make sure to focus on the day you're living.\" 추가 제안사항 더 다양한 표현 사용: \"Try to\", \"I'm worried about\", \"Be careful with\" 등의 표현도 활용해보세요. 문장 연결: \"By the way\", \"Well, the thing is\" 같은 연결어를 사용하면 더 자연스러운 대화체가 됩니다. 구체적인 예시: 추상적인 내용보다 구체적인 상황이나 예시를 들어보세요.",
    "public": true
  },
  {
    "id": "Self_Development/Book/만들어진 신 (The God Delusion)/대단히종교적인불신자",
    "title": "대단히 종교적인 불신자",
    "date": "2025-07-20",
    "excerpt": "나는 인격신을 상상하려는 시도는 하지 않는다. 신은 우리의 불충분한 감각으로 세계의 구조를 이해할 수 있도록 함으로써 경외심을 품게 하는 정도면 충분하다. <알베르트 아인슈타인>",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "만들어진 신"
    ],
    "content": "과학자들이 말하는 신은 뭘까? > 나는 인격신을 상상하려는 시도는 하지 않는다. 신은 우리의 불충분한 감각으로 세계의 구조를 이해할 수 있도록 함으로써 경외심을 품게 하는 정도면 충분하다. > > <알베르트 아인슈타인> 과학자들이 말하는 신은 무엇일까? 같은 신이라는 단어로 표현되고 있지만 이를 구분해서 이해할 필요가 있을 것 같다. 아이슈타인은 신을 말하며 우주의 합리성과 질서에 대한 경외심을 들어내고 있다. 예수나 부처와 같은 신을 의미하는게 아닌. > 훨씬 더 불행한 혼란은 아인슈타인식의 종교와 초자연적인 종교를 구분하지 못함으로써 빚어진다. 알베르트 아인슈타인(Albert Einstein)은 종종 신이라는 명칭을 사용함으로써(그런 무신론자 과학자가 그만은 아니다), 그런 유명한 과학자가 자신들의 편이기를 너무나 바라는 초자연주의자들의 오해를 자초하곤 했다. > > <만들어진 신>, 리처드 도킨스 - 밀리의 서재 과학자들이 신을 이야기한다고 이들이 신을 믿는다고 생각하면 안될 듯 하다. > 우리 시대의 위대한 과학자들이 종교적인 말을 하는 듯이 보여도 그들의 신념을 더 깊이 파고 들어가면 대개 그렇지 않다는 사실이 드러난다. > > <만들어진 신>, 리처드 도킨스 - 밀리의 서재 다른 사람들은 몰라도 과학자들 중 신을 믿는 사람들은 왜 있는걸까? 고등학교때 친구를 따라 교회를 1~2년도 다녔던 적이 있다. 그때도 신이라는 존재를 어떻게 믿을 수 있는지에 대해 의문이 들어 주위에 물어봤을 때, 믿음을 믿는다는 이야기를 들었다. 즉 어떤 근거가 있어서 믿는다기 보다는 믿음 자체를 믿는다는 것이다. 그래서 믿을 수 없다는 사람에게는 믿음이 부족하다, 성경을 더 공부하다보면 믿게 된다는 이야기를 한다. 나는 이런 믿음이라는게 굉장히 위험하다고 생각한다. 넷플릭스 시리즈의 \"나는 신이다\"에서 소개되는 JMS 같은 사이비 종교들의 논리와 교회가 다를게 뭔가? 믿음을 우선시 하는 삶의 태도가 나는 도저히 납득이 안간다. 과학을 하는 사람들 중에 신을 믿는 사람들이 많은데, 이들도 종교에 있어서는 믿음을 우선시 하기 때문이 아닐까 싶다. 종교에 있어서는 논리적인 이야기가 필요하지 않은. [이기적 유전자 저자] 리처드 도킨스한테 까인 썰 풀어드림 : 종교는 없어져야 한다 / 최재천의 아마존 유튜브 링크 최재천 교수가 리처드 도킨스와의 인터뷰 기회를 만들어서 만들어진 신에 대해 이야기했던 이야기. 리처드 도킨스가 이야기하는 모태신앙. 이 영상에 있는 댓글 중 기억에 남는 것들 > '개인이 망상에 빠지면 미친 것이지만 집단이 망상에 빠지면 종교이다. > > '-리처드 도킨스 > '종교는 부족의 생존에 도움을 준댜. > > '- 에드워드 윌슨 > 수 백명의 사람이 몇 달동안 믿으면 fake news이지만 수 천만명의 사람들이 수 천년 동안 믿으면 종교이다. > > - 유발 하라리 리처드 도킨스 : 창조론을 반박하는 팩트 , 충코의 철학 유튜브 링크 유전자들 간의 유사성 패턴. 창조론 vs 진화론. 신을 믿는 사람에게는 논쟁을 할 수 없다. 과학적인 근거를 무시한채 성경을 믿으니까. 과학자와 신에 대한 정리 (GPT 시킴) 왜 하나의 ‘과학자 관점’이 존재하지 않는가 현대 연구자들은 개인적 신·비개인적(범-)신·\\\\무신(불가지론 포함)\\\\의 폭넓은 스펙트럼을 이루며, 통계조차 양극화를 보여 준다. 미국 과학자 2,000 여 명을 조사한 Pew Research Center 보고서에 따르면 \\\\과학자의 51 %\\\\가 어떤 형태로든 신적 존재를 인정하지만, \\\\41 %\\\\는 “신이나 영적 힘이 없다”고 응답했다. 일반 대중과 비교하면 무신론 비율이 10 배에 가깝다.([Pew Research Center][1]) --- “과학도 신을 믿는다”―유신론·기독교적 입장 대표 인물 핵심 생각 주요 저술·발언 ------------------------ ------------------------------ ------------------------------------------------------ 프랜시스 콜린스 (유전학자) “진화는 하나님의 창조 도구”―‘유신론적 진화’ The Language of God (2006)([위키백과][2]) 조르주 르메트르 (빅뱅 제안자·사제) “과학은 어떻게를, 신앙은 왜를 다룬다” NYT 인터뷰(1933)([The Society of Catholic Scientists][3]) 그들은 과학 방법론을 준수하면서도, 창조 목적·도덕법 등 ‘궁극적 의미’는 과학 밖(종교)의 범주라고 본다. --- “우주가 곧 신”―범신론·우주적 경외 알베르트 아인슈타인: “나는 개인적 신을 믿지 않는다”면서도 “우주의 신비에 대한 경외를 ‘종교적 감정’이라 부를 수 있다”([Learn Religions][4]) 칼 세이건: “우리는 \\\\별의 먼지(star-stuff)\\\\로 이루어져, 우주가 스스로를 인식하는 방식”이라 표현했다.([Goodreads][5]) 이 흐름은 범신론·범재신론(Spinoza적 신)으로 분류되며, 자연법칙 = 신적 이성과 동등시한다. --- “신이 설 자리는 없다”―무신론·자연주의 인물 주된 논지 ------------ --------------------------------------------------------------- 스티븐 호킹 “시간이 ‘빅뱅 이전’에 존재하지 않았으므로 창조주가 개입할 ‘순간’도 없었다”([Live Science][6]) 리처드 도킨스 “개인적 신은 ‘착각(delusion)’이며, 진화가 설계의 환상을 설명한다”([위키백과][7]) 로런스 크라우스 “양자장과 중력이면 ‘무(無)’에서도 우주가 자연 발생할 수 있다”([위키백과][8]) 이들은 존재론적 자연주의를 채택하며, 초월적 설명은 과잉가설로 본다. --- “판단 유보”―불가지론·과학적 회의 천체물리학자 닐 드그래스 타이슨처럼 “증거 없는 주장엔 확신하지 않는다”는 경험론적 불가지론도 확산 중이다.([A-Z Quotes][9]) --- 다리를 놓으려는 시도: NOMA 원칙 고(故) 스티븐 제이 굴드의 비중첩 교구(NOMA) 모델은 “과학은 사실, 종교는 가치·의미”라는 영역 분리로 충돌을 완화하려 한다.([위키백과][10]) --- 이론적 배경 ① — 방법론적 자연주의(MN) 과학은 실험 가능한 자연 원인만을 탐구해야 한다는 규범이다. 초자연 원인은 검증 불가하므로 연구 설계에서 배제된다. 이 점에서 MN은 “과학의 성공 경험”이 낳은 실용적 전략으로 해석된다.([스프링거링크][11]) > \\\\존재론적 자연주의(ON)\\\\와 달리, MN은 “신 존재 여부에 대해 침묵”하지만, 무신론자는 종종 MN → ON을 연역한다. 논쟁은 바로 여기서 발생한다. --- 이론적 배경 ② — 미세 조정·다중우주·설계 논증 물리 상수들이 생명 탄생에 “절묘하게” 맞춰져 있다는 \\\\미세 조정(fine-tuning)\\\\은 두 갈래 해석을 낳았다. 신적 설계 — 콜린스·폴 데이비스 등은 “우연이라 보기엔 확률이 극미”라고 본다. 다중우주 / 선택 효과 — 호킹·크라우스는 “관측 가능한 우주만 생명 친화적일 뿐”이라 반박한다.([위키백과][12]) --- 역사적 맥락 한눈에 보기 시대 과학 혁신 신 개념 변화 ---------- --------- -------------------------- 17C 뉴턴 기계적 우주관 ‘시계공(Deus ex machina)’ 20C 초 빅뱅(르메트르) “창조 = 시작?” 논쟁 재점화 21C 양자중력·다중우주 무신론·불가지론 강화 vs. 미세조정 논증 강화 --- 종합 · 시사점 다양성: 과학적 업적과 신앙 유무는 직결되지 않는다. 대화 모델: 갈등·독립·대화·융합 — 네 가지 틀 가운데 어느 쪽을 택하느냐에 따라 입장도 분화된다. 철학의 역할: 과학이 “어떻게”를 설명할수록, “왜”에 대한 질문은 철학·신학·윤리로 이동한다. > 결론: “과학자들은 신을 믿는가?”라는 단일 질문 대신, \\\\“어떤 ‘신’ 개념을, 어떤 방법론으로 논의하는가?”\\\\*를 물을 때 비로소 논쟁의 결이 선명해진다. 이 다층적 구도를 염두에 두면, 과학과 종교의 관계는 ‘적대’보다는 창조적 긴장과 상호 보완의 장으로 재해석될 수 있다. [1]: https://www.pewresearch.org/religion/2009/11/05/scientists-and-belief/ \"Scientists and Belief Pew Research Center\" [2]: https://en.wikipedia.org/wiki/The_Language_of_God \"The Language of God - Wikipedia\" [3]: https://catholicscientists.org/articles/lemaitre-follows-two-paths-to-truth-new-york-times-1933-interview-with-lemaitre/ \"\\\"Lemaître Follows Two Paths to Truth\\\" (1933 New York Times Interview with Fr. Lemaître, Father of the Big Bang Theory) - The Society of Catholic Scientists\" [4]: https://www.learnreligions.com/albert-einstein-quotations-249858 \"Albert Einstein Quotes on Science, God, and Religion\" [5]: https://www.goodreads.com/quotes/484665-the-cosmos-is-within-us-we-are-made-of-star-stuff?utm_source=chatgpt.com \"Quote by Carl Sagan: “The cosmos is within us. We are made of star-st...” - Goodreads\" [6]: https://www.livescience.com/63854-stephen-hawking-says-no-god.html \"Stephen Hawking's Final Book Says There's 'No Possibility' of God in Our Universe Live Science\" [7]: https://en.wikipedia.org/wiki/The_God_Delusion \"The God Delusion - Wikipedia\" [8]: https://en.wikipedia.org/wiki/A_Universe_from_Nothing \"A Universe from Nothing - Wikipedia\" [9]: https://www.azquotes.com/author/14904-Neil_deGrasse_Tyson \"TOP 25 QUOTES BY NEIL DEGRASSE TYSON (of 764) A-Z Quotes\" [10]: https://en.wikipedia.org/wiki/Non-overlapping_magisteria \"Non-overlapping magisteria - Wikipedia\" [11]: https://link.springer.com/article/10.1007/s10670-024-00790-y?utm_source=chatgpt.com \"Methodological Naturalism, Analyzed Erkenntnis - Springer\" [12]: https://en.wikipedia.org/wiki/Fine-tuned_universe \"Fine-tuned universe - Wikipedia\"",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Towards-Optimal Doubly-Robust-Estimation-of-Heterogeneous-Causal-Effects",
    "title": "[Paper Review] Towards Optimal Doubly-Robust Estimation of Heterogeneous Causal Effects",
    "date": "2025-07-20",
    "excerpt": "Doubly-Robust Estimation of Heterogeneous Causal Effects 논문 리뷰",
    "category": "Causal Inference",
    "tags": [
      "paper review"
    ],
    "content": "paper link Abstact",
    "public": false
  },
  {
    "id": "Self_Development/Current Affairs/중국문화대혁명과홍위병",
    "title": "중국 문화대혁명과 홍위병",
    "date": "2025-07-17",
    "excerpt": "문화대혁명과 홍위병에 대한 내용들",
    "category": "Current Affairs",
    "tags": [
      "중국",
      "문화대혁명",
      "홍위병"
    ],
    "content": "참고 자료 리뷰엉이 - 삼체에서 가장 중요한 사건 문화대혁명 문화대혁명이란? 문화대혁명(文化大革命, Cultural Revolution)은 정식 명칭이 \"무산계급 문화대혁명\"(無產階級文化大革命, Great Proletarian Cultural Revolution)으로, 1966년부터 1976년까지 10년간 중국에서 일어난 사회정치적 운동이다. 발생 배경 대약진운동의 실패 (1958-1962) 마오쩌둥이 주도한 경제개발 정책인 대약진운동이 참혹하게 실패 2천만 명 이상이 기근으로 사망 마오쩌둥의 권력과 영향력이 크게 약화됨 실용주의 노선의 등장 류샤오치(劉少奇), 저우언라이(周恩來), 덩샤오핑(鄧小平) 등이 실용적 경제 정책 도입 개인 농지 허용 등 시장경제 요소 도입으로 경제 회복 (1962-1965) 마오쩌둥은 이러한 정책이 공산주의 원칙에 위배된다고 판단 문화대혁명의 목표 마오쩌둥이 문화대혁명을 일으킨 주요 목적: 권력 회복: 대약진운동 실패로 잃은 정치적 영향력 되찾기 정적 제거: 류샤오치, 덩샤오핑 등 실용주의 노선 지도자들 축출 혁명 정신 부활: 관료화되고 있는 중국 공산당과 사회에 혁명 정신 주입 \"4구\"(四舊) 타파: 구사상(舊思想) - 옛 사상 구문화(舊文化) - 옛 문화 구풍속(舊風俗) - 옛 풍습 구습관(舊習慣) - 옛 습관 문화대혁명의 전개 과정 1단계: 발발과 혼란 (1966-1968) 1966년 5월: 시작 5.16 통지: 중국 공산당 중앙위원회가 문화대혁명 시작을 공식 선언 부르주아 분자들이 당, 정부, 군대, 문화계에 침투했다고 주장 홍위병(紅衛兵) 조직 주로 학생들로 구성된 준군사조직 마오쩌둥어록(모택동어록, \"빨간책\")을 들고 다니며 숭배 \"혁명 무죄, 조반 유리\"(革命無罪, 造反有理) - \"혁명은 죄가 없고, 반란은 이치가 있다\" 20~30대 학생들이 주를 이뤘고 이때에 엄청난 권력을 가지고 있었다고 한다. 사단장보다 더 높은 권력. 사소한 이유로 사람들을 무자비하게 폭행하고 죽였기 때문에 사람들이 행동을 엄청 조심했었다고 한다. 폭력과 파괴 지식인, 교사, 정부 관리, 문화재 등에 대한 무차별 공격 공자묘, 천단, 명십삼릉 등 문화유산 파괴 학교와 대학 폐쇄 2단계: 권력 투쟁과 군 개입 (1967-1969) 1967년: \"전면적 내전\" 홍위병 파벌 간 무력 충돌 각 지역에서 \"권력 탈취\" 운동 전개 광저우, 베이징 등에서 대규모 폭력 사태 인민해방군의 개입 린뱌오(林彪)가 이끄는 인민해방군이 질서 회복에 나섬 홍위병들을 농촌으로 추방하는 \"상산하향\"(上山下鄉) 운동 시작 3단계: 안정화와 내부 권력 투쟁 (1969-1976) 린뱌오 사건 (1971) 마오쩌둥의 후계자로 지명된 린뱌오가 정치적으로 몰락 마오 암살 시도 후 소련으로 망명 시도 중 몽골에서 비행기 추락사 많은 중국인들이 공산당 지도부에 대한 신뢰 상실 4인방(四人幫)의 등장 장칭(江青, 마오쩌둥의 부인), 장춘차오(張春橋), 야오원위안(姚文元), 왕훙원(王洪文) 문화대혁명 후기의 실질적 권력자들 극좌 노선 추진 문화대혁명의 주요 정책 상산하향 운동(上山下鄉運動) \"산과 시골로 가라\"는 의미 도시 청년 1천만 명 이상을 농촌으로 강제 이주 노동을 통한 \"재교육\" 명목 실제로는 도시 과밀화 해결과 홍위병 해체 목적 교육 제도 파괴 대학 입시 시험 폐지 (1966-1973) \"혁명적 순수성\"이 학력보다 중요시됨 지식인과 과학자들이 \"구지식분자\"로 탄압받음 문화 예술 통제 서구 문학, 음악, 연극 금지 셰익스피어 작품 등 서양 고전 금지 오직 혁명 주제만 허용 문화대혁명의 종료 마오쩌둥의 죽음 (1976년 9월 9일) 27년간 중국을 통치한 마오쩌둥 사망 전국적으로 애도 분위기 4인방 체포 (1976년 10월 6일) 화궈펑(華國鋒)이 4인방을 체포 문화대혁명의 공식적 종료로 간주 4인방을 \"반혁명 범죄\" 혐의로 기소 문화대혁명의 결과와 영향 몇천년의 역사와 문화유산을 다 부쉈다. 공자나 제갈량과 같은 선조들의 무덤을 파헤치고 시신을 불태웠다. 인명 피해 사망자: 약 150만 명에서 200만 명 (추정치는 학자마다 다름) 박해받은 인원: 수천만 명 광시성에서는 식인 행위를 포함한 학살 발생 경제적 피해 1968년 공업 생산량이 1966년 대비 12% 감소 전반적인 경제 발전 지연 사회적 파괴 전통 문화와 역사 유물 대량 파괴 가족, 친구 관계 파괴 교육 시스템 붕괴로 \"잃어버린 세대\" 등장 정치적 영향 중국 공산당의 권위와 신뢰도 크게 실추 1978년 덩샤오핑의 개혁개방 정책으로 이어짐 문화대혁명에 대한 평가 중국 공산당의 공식 입장 (1981년) > \"문화대혁명은 중화인민공화국 건국 이후 당, 국가, 인민이 겪은 가장 심각한 좌절이자 가장 무거운 손실을 초래한 사건이다.\" 현재 중국에서의 취급 \"10년 동란\"(十年動亂) 또는 \"10년 재난\"(十年浩劫)으로 불림 공개적 논의는 여전히 제한적 마오쩌둥의 명성을 보호하기 위해 4인방에게 주로 책임 전가 문화대혁명이 남긴 교훈 권력 집중의 위험성: 개인 독재의 폐해 대중 동원의 한계: 조직화되지 않은 대중 운동의 위험 이데올로기의 맹목성: 현실을 무시한 교조적 사고의 문제 지식과 문화의 중요성: 반지성주의의 참혹한 결과 문화대혁명은 20세기 중국 역사상 가장 비극적인 사건 중 하나로, 중국 사회에 깊은 상처를 남겼으며 그 영향은 오늘날까지도 지속되고 있다. 삼체와의 연결 삼체란? 삼체(三體, The Three-Body Problem)는 중국의 SF 소설가 류츠신(劉慈欣)이 2006년에 발표한 과학소설로, 《지구왕사》(地球往事, Remembrance of Earth's Past) 삼부작의 첫 번째 작품이다. 소설의 성취 2015년 휴고상 수상: 중국 소설 최초로 세계 최고 권위의 SF 문학상 수상 글로벌 베스트셀러: 2014년 켄 류(Ken Liu)의 영어 번역을 통해 전 세계적 인기 획득 \"중국의 스타워즈\"로 불리며 중국 SF 문학의 글로벌 진출 상징 삼체의 줄거리 기본 설정 소설은 지구 근처에 있는 삼성계(三星系, Three-Body System)에 살고 있는 외계 문명 삼체인(三體人, Trisolarians)과 지구의 접촉을 다룬다. 삼체인들은 세 개의 태양이 불규칙하게 움직이는 혼돈스러운 환경에서 살고 있어 멸종 위기에 처해 있다. 주요 인물 예원제(葉文潔, Ye Wenjie): 문화대혁명 시기 물리학자인 아버지를 홍위병에게 잃은 여성 인류에 대한 절망으로 외계 문명과의 접촉을 시도 이야기의 핵심 인물이자 비극적 영웅 영상화 작품들 중국판 드라마 (2023) 제작사: 텐센트(Tencent) 특징: 원작에 충실하되 문화대혁명 장면은 검열로 인해 삭제 평가: 중국 내에서 높은 시청률 기록 넷플릭스 드라마: \"3 Body Problem\" (2024) 제작진: 데이비드 베니오프(David Benioff) D.B. 와이스(D.B. Weiss) - 《왕좌의 게임》 제작진 알렉산더 우(Alexander Woo) 제작비: 1억 6천만 달러 (넷플릭스 사상 최고액 드라마 중 하나) 주요 변화: 배경을 중국에서 영국 런던으로 이전 캐릭터들의 인종 다양화 \"옥스퍼드 파이브\"라는 새로운 인물 관계 창조 문화대혁명과 삼체의 연결 소설 속 문화대혁명 트라우마의 근원: 예원제가 홍위병들에게 아버지를 잃는 장면으로 시작 인류에 대한 절망: 문화대혁명의 폭력성이 예원제로 하여금 인류 전체에 대한 신뢰를 잃게 만듦 외계 문명과의 접촉 동기: 인류보다 나은 문명에 대한 갈망 역사적 의미 과학의 탄압: 문화대혁명 시기 지식인과 과학자들에 대한 박해 이데올로기 vs 진리: 과학적 진실보다 정치적 올바름을 우선시한 결과 집단 광기: 홍위병의 폭력성이 보여주는 군중 심리의 위험성 삼체가 제기하는 철학적 질문들 문명의 충돌 서로 다른 문명이 만났을 때의 필연적 갈등 생존을 위한 도덕적 선택의 딜레마 과학과 정치 과학의 정치적 중립성에 대한 의문 지식인의 사회적 책임 인류의 미래 기술 발전과 도덕적 진보의 불일치 문명의 지속가능성에 대한 근본적 의문 글로벌 수용과 논란 긍정적 평가 과학적 상상력: 정교한 물리학 이론을 바탕으로 한 설정 철학적 깊이: 문명, 과학, 도덕에 대한 깊이 있는 성찰 중국 SF의 세계화: 서구 중심의 SF 장르에 동양적 관점 도입 비판과 논란 넷플릭스 버전에 대한 중국 내 반발: 문화대혁명 장면이 중국을 비하한다는 비판 중국적 요소의 희석에 대한 불만 정치적 해석: 중국 국가주의자들의 \"서구 문화 패권주의\" 비판 문화대혁명 묘사에 대한 검열 우려 삼체가 보여주는 문화대혁명의 영향 개인적 차원 트라우마의 세대 전승: 예원제를 통해 본 개인적 상처 신뢰의 붕괴: 인간과 사회에 대한 근본적 불신 사회적 차원 지식 체계의 파괴: 과학과 학문에 대한 탄압의 결과 도덕적 혼란: 선악의 기준이 무너진 사회의 모습 문명사적 차원 진보에 대한 회의: 인류 문명의 발전 가능성에 대한 의문 외부 구원에 대한 환상: 내부 개혁의 한계와 외부 개입 기대 삼체는 단순한 SF 소설을 넘어서, 문화대혁명이라는 역사적 비극이 어떻게 개인과 문명 전체에 영향을 미칠 수 있는지를 보여주는 우화적 작품이다. 예원제의 선택은 개인적 트라우마가 어떻게 인류 전체의 운명을 바꿀 수 있는지를 극적으로 보여주며, 문화대혁명의 상처가 여전히 중국 사회와 문화에 깊이 각인되어 있음을 시사한다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-ate",
    "title": "What is ATE?",
    "date": "2025-07-17",
    "excerpt": "What is ATE?",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "ATE란? --- Average Treatment Effect (ATE)는 모든 사람들이 받은 처치의 평균 효과를 말한다. 즉, 처치받은 결과와 받지 않은 결과의 차이의 평균이라고 이해할 수 있다. ATE 추정 --- $$ \\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) \\tag{1} $$ 정의에 따라 ATE는 (1)처럼 정의된다. 하지만 실제로는 모든 사람들이 처치를 받지 않기 때문에 (1)을 직접 계산할 수 없다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/double-robustness-explained",
    "title": "Double Robustness: 인과추론의 핵심 개념 완전 정복",
    "date": "2025-07-17",
    "excerpt": "Double Robustness의 이론적 기반부터 실용적 응용까지 완전 해부",
    "category": "Causal Inference",
    "tags": [
      "Double Robustness",
      "Causal Inference",
      "Theory",
      "R-learner"
    ],
    "content": "Double Robustness: 인과추론의 핵심 개념 완전 정복 Double Robustness(이중 강건성)는 현대 인과추론의 핵심 개념 중 하나로, R-learner와 같은 최신 방법론의 이론적 기반이 됩니다. 이 포스트에서는 Double Robustness의 모든 측면을 자세히 살펴보겠습니다. Double Robustness란 무엇인가? 1.1 직관적 이해 Double Robustness는 \"두 가지 모델 중 하나만 정확해도 편향 없는 추정이 가능한 성질\"입니다. 간단한 비유 자물쇠 두 개: 문을 열려면 둘 중 하나만 열면 됨 백업 시스템: 주 시스템이 실패해도 보조 시스템이 작동 이중 보험: 하나가 실패해도 다른 하나가 보장 1.2 인과추론에서의 의미 인과추론에서 우리가 추정해야 하는 두 가지 핵심 요소: Outcome Model (결과 모델): $\\mu(x, w) = \\mathbb{E}[Y X = x, W = w]$ Propensity Score Model (처치 확률 모델): $e(x) = \\text{Pr}(W = 1 X = x)$ Double Robustness: 이 둘 중 하나만 정확해도 처치효과를 편향 없이 추정할 수 있음! 수학적 이론: 핵심 메커니즘 2.1 Potential Outcomes Framework 기본 설정: 잠재결과: $Y_i(0), Y_i(1)$ (처치를 받지 않았을 때/받았을 때의 결과) 관찰결과: $Y_i = Y_i(W_i)$ 개별 처치효과: $\\tau_i = Y_i(1) - Y_i(0)$ 평균 처치효과: $\\tau = \\mathbb{E}[Y(1) - Y(0)]$ 근본적 문제: 같은 개인에 대해 $Y_i(0)$과 $Y_i(1)$을 동시에 관찰할 수 없음! 2.2 Unconfoundedness Assumption $$\\{Y(0), Y(1)\\} \\perp \\!\\!\\! \\perp W \\mid X$$ 의미: 공변량 $X$를 통제하면 처치 할당이 무작위와 같음 이 가정 하에서 다음이 성립: $$\\mathbb{E}[Y(w) X] = \\mathbb{E}[Y X, W = w] = \\mu(X, w)$$ 2.3 Double Robustness의 수학적 표현 핵심 항등식 Average Treatment Effect (ATE)에 대한 doubly robust 추정량: $$\\hat{\\tau}_{DR} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{W_i Y_i}{\\hat{e}(X_i)} - \\frac{(1-W_i) Y_i}{1-\\hat{e}(X_i)} + \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1-\\hat{e}(X_i))} \\left\\{ \\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0) \\right\\} \\right]$$ 더 간단한 형태 AIPW (Augmented Inverse Propensity Weighting) 추정량: $$\\hat{\\tau}_{AIPW} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0) + \\frac{W_i}{\\hat{e}(X_i)} \\{Y_i - \\hat{\\mu}(X_i, 1)\\} - \\frac{1-W_i}{1-\\hat{e}(X_i)} \\{Y_i - \\hat{\\mu}(X_i, 0)\\} \\right]$$ 2.4 왜 \"Double Robust\"인가? 핵심 정리 Theorem: 다음 조건 중 하나만 만족하면 $\\mathbb{E}[\\hat{\\tau}_{AIPW}] = \\tau$: Outcome model이 정확: $\\hat{\\mu}(x, w) = \\mu^(x, w)$ Propensity model이 정확: $\\hat{e}(x) = e^(x)$ 증명의 핵심 아이디어 AIPW 추정량을 다음과 같이 분해할 수 있습니다: $$\\hat{\\tau}_{AIPW} = \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\{\\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0)\\}}_{\\text{Outcome-based}} + \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{W_i}{\\hat{e}(X_i)} - \\frac{1-W_i}{1-\\hat{e}(X_i)}\\right] \\{Y_i - \\hat{\\mu}(X_i, W_i)\\}}_{\\text{IPW correction}}$$ 케이스 1: Outcome model이 정확한 경우 첫 번째 항이 정확한 $\\tau$ 제공 두 번째 항의 기댓값이 0 (잔차의 가중평균) 케이스 2: Propensity model이 정확한 경우 두 번째 항이 IPW 추정량으로 정확한 $\\tau$ 제공 첫 번째 항의 오차가 두 번째 항에 의해 상쇄 직관적 이해: 왜 작동하는가? 3.1 Visual Intuition `` 진실한 처치효과: τ = 5 시나리오 1: Outcome Model 정확, Propensity Model 부정확 ┌─────────────────┐ ┌─────────────────┐ │ Outcome Model │ │ Propensity Model│ │ ✅ τ̂ = 5.0 │ + │ ❌ 편향 있음 │ = τ̂ = 5.0 ✅ └─────────────────┘ └─────────────────┘ 시나리오 2: Outcome Model 부정확, Propensity Model 정확 ┌─────────────────┐ ┌─────────────────┐ │ Outcome Model │ │ Propensity Model│ │ ❌ 편향 있음 │ + │ ✅ 정확함 │ = τ̂ = 5.0 ✅ └─────────────────┘ └─────────────────┘ 시나리오 3: 둘 다 정확 ┌─────────────────┐ ┌─────────────────┐ │ Outcome Model │ │ Propensity Model│ │ ✅ τ̂ = 5.0 │ + │ ✅ 정확함 │ = τ̂ = 5.0 ✅ └─────────────────┘ └─────────────────┘ ` 3.2 보상 메커니즘 (Compensation Mechanism) Outcome Model이 정확한 경우 `python Outcome model이 완벽하면 mu_hat(x, 1) - mu_hat(x, 0) ≈ τ IPW correction term은 E[W/e_hat(X) - (1-W)/(1-e_hat(X))] {Y - mu_hat(X,W)} ≈ 0 (잔차의 가중평균은 0에 가까움) ` Propensity Model이 정확한 경우 `python Propensity model이 완벽하면 IPW가 정확한 추정 제공 IPW_term = E[WY/e(X) - (1-W)Y/(1-e(X))] = τ Outcome model의 오차는 IPW correction에 의해 상쇄됨 ` 3.3 실제 데이터 예시 시뮬레이션 설정 `python n = 1000 X = np.random.normal(0, 1, (n, 2)) e_true = expit(X[:, 0]) # true propensity W = np.random.binomial(1, e_true) True outcome model: complex nonlinear Y_0 = X[:, 0]*2 + X[:, 1] + noise Y_1 = Y_0 + 2 + X[:, 0] # treatment effect = 2 + X[:, 0] Y = W Y_1 + (1 - W) Y_0 true_ATE = 2 + np.mean(X[:, 0]) # ≈ 2.0 ` 시나리오별 결과 `python Scenario 1: Good outcome model, bad propensity model mu_hat_good = fit_complex_model(Y, X, W) # R² = 0.95 e_hat_bad = 0.5 # constant (wrong!) AIPW_1 = compute_AIPW(Y, W, X, mu_hat_good, e_hat_bad) Result: 2.03 (거의 정확! ✅) Scenario 2: Bad outcome model, good propensity model mu_hat_bad = fit_linear_model(Y, X, W) # R² = 0.3 (underfit) e_hat_good = fit_logistic_model(W, X) # very accurate AIPW_2 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_good) Result: 1.97 (여전히 정확! ✅) Scenario 3: Both models bad AIPW_3 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_bad) Result: 1.2 (편향됨 ❌) ` Double Robustness의 장점과 한계 4.1 핵심 장점 견고성 (Robustness) 모델 오지정에 대한 보험: 하나가 틀려도 안전 실무적 안정성: 완벽한 모델링이 어려운 현실에서 유용 효율성 (Efficiency) 둘 다 정확하면 최고 효율성: 최소 분산 달성 Semiparametric efficiency bound 달성 유연성 (Flexibility) 서로 다른 방법 조합: 각 모델에 최적화된 방법 사용 가능 기계학습 방법 활용: 복잡한 모델도 사용 가능 4.2 한계점 둘 다 틀리면 편향 `python 최악의 시나리오 if outcome_model_wrong and propensity_model_wrong: bias = f(error_outcome, error_propensity) # 편향 발생 ` 분산 증가 가능성 IPW term의 극단값: propensity score가 0 또는 1에 가까우면 분산 급증 Overlap 조건 필요: $0 < e(x) < 1$ for all $x$ 추정 복잡성 두 모델 모두 추정: 계산 부담 증가 교차검증 복잡성: 각 모델의 튜닝이 복잡 R-learner와의 연결 5.1 Robinson's Transformation과 Double Robustness R-learner는 Robinson's transformation을 사용합니다: $$Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i$$ 이를 다시 정리하면: $$Y_i = m^(X_i) + \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i$$ Double Robust Structure R-loss 함수: $$\\hat{L}_n\\{\\tau(\\cdot)\\} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\left\\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\right\\} - \\left\\{W_i - \\hat{e}^{(-q(i))}(X_i)\\right\\} \\tau(X_i) \\right]^2$$ 핵심: 이 손실함수도 double robust 성질을 가집니다! $\\hat{m}$이 정확하면 첫 번째 항이 정확한 잔차 제공 $\\hat{e}$가 정확하면 두 번째 항이 정확한 가중치 제공 5.2 왜 R-learner가 Quasi-Oracle인가? `python Oracle knows m(x) and e(x) oracle_loss = E[(Y - m(X) - {W - e(X)}τ(X))²] R-learner uses estimates r_loss = E[(Y - m̂(X) - {W - ê(X)}τ̂(X))²] Double robustness ensures: if rate(m̂ - m) rate(ê - e) → 0 faster than oracle_rate: rate(R-learner) ≈ oracle_rate # 🎯 ` 실용적 구현 가이드 6.1 Python Implementation `python import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict def doubly_robust_ate(Y, W, X, outcome_model=None, propensity_model=None): \"\"\" Doubly Robust ATE 추정 Parameters: ----------- Y : array-like, shape (n,) Outcome variable W : array-like, shape (n,) Treatment indicator (0 or 1) X : array-like, shape (n, p) Covariates outcome_model : sklearn estimator Outcome regression model propensity_model : sklearn estimator Propensity score model Returns: -------- ate_estimate : float Doubly robust ATE estimate \"\"\" n = len(Y) # Default models if outcome_model is None: outcome_model = RandomForestRegressor(n_estimators=100, random_state=42) if propensity_model is None: propensity_model = LogisticRegression(random_state=42) # Cross-fitting for outcome model mu_1_hat = np.zeros(n) mu_0_hat = np.zeros(n) # Estimate E[Y X,W=1] idx_1 = W == 1 if np.sum(idx_1) > 0: mu_1_hat[idx_1] = cross_val_predict( outcome_model, X[idx_1], Y[idx_1], cv=5 ) outcome_model.fit(X[idx_1], Y[idx_1]) mu_1_hat[~idx_1] = outcome_model.predict(X[~idx_1]) # Estimate E[Y X,W=0] idx_0 = W == 0 if np.sum(idx_0) > 0: mu_0_hat[idx_0] = cross_val_predict( outcome_model, X[idx_0], Y[idx_0], cv=5 ) outcome_model.fit(X[idx_0], Y[idx_0]) mu_0_hat[~idx_0] = outcome_model.predict(X[~idx_0]) # Cross-fitting for propensity score e_hat = cross_val_predict( propensity_model, X, W, cv=5, method='predict_proba' )[:, 1] # Clip propensity scores to avoid extreme weights e_hat = np.clip(e_hat, 0.01, 0.99) # AIPW estimator aipw_components = ( (mu_1_hat - mu_0_hat) + # Outcome-based estimate W (Y - mu_1_hat) / e_hat - # IPW correction for treated (1 - W) (Y - mu_0_hat) / (1 - e_hat) # IPW correction for control ) ate_estimate = np.mean(aipw_components) # Standard error (using influence function) influence_func = aipw_components - ate_estimate se_estimate = np.std(influence_func) / np.sqrt(n) return { 'ate': ate_estimate, 'se': se_estimate, 'ci_lower': ate_estimate - 1.96 se_estimate, 'ci_upper': ate_estimate + 1.96 se_estimate } Usage example def simulate_data(n=1000): \"\"\"Generate simulation data\"\"\" X = np.random.normal(0, 1, (n, 3)) # True propensity score (logistic) e_true = 1 / (1 + np.exp(-(X[:, 0] + 0.5 X[:, 1]))) W = np.random.binomial(1, e_true) # True outcome model (nonlinear) Y_0 = X[:, 0]*2 + X[:, 1] + 0.5 X[:, 2] + np.random.normal(0, 0.5, n) Y_1 = Y_0 + 2 + X[:, 0] # Heterogeneous treatment effect Y = W Y_1 + (1 - W) Y_0 return Y, W, X, 2.0 # True ATE ≈ 2.0 Run example Y, W, X, true_ate = simulate_data() result = doubly_robust_ate(Y, W, X) print(f\"True ATE: {true_ate:.3f}\") print(f\"DR Estimate: {result['ate']:.3f}\") print(f\"95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\") ` 6.2 Best Practices 모델 선택 가이드라인 `python Outcome Model High flexibility for complex relationships outcome_models = { 'linear': LinearRegression(), 'rf': RandomForestRegressor(n_estimators=200), 'xgb': XGBRegressor(), 'nn': MLPRegressor(hidden_layer_sizes=(100, 50)) } Propensity Model Focus on overlap and calibration propensity_models = { 'logistic': LogisticRegression(C=1.0), 'rf': RandomForestClassifier(n_estimators=200), 'calibrated': CalibratedClassifierCV(LogisticRegression()) } ` 진단 도구 `python def diagnose_overlap(e_hat, W): \"\"\"Check overlap assumption\"\"\" print(f\"Propensity score range: [{e_hat.min():.3f}, {e_hat.max():.3f}]\") print(f\"Extreme scores (< 0.1 or > 0.9): {np.mean((e_hat < 0.1) (e_hat > 0.9)):.1%}\") # Plot distributions import matplotlib.pyplot as plt plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.hist(e_hat[W==0], alpha=0.7, label='Control', bins=30) plt.hist(e_hat[W==1], alpha=0.7, label='Treated', bins=30) plt.xlabel('Propensity Score') plt.ylabel('Frequency') plt.legend() plt.title('Overlap Check') plt.subplot(1, 2, 2) plt.boxplot([e_hat[W==0], e_hat[W==1]], labels=['Control', 'Treated']) plt.ylabel('Propensity Score') plt.title('Distribution by Treatment') plt.tight_layout() plt.show() def check_balance(X, W, e_hat): \"\"\"Check covariate balance after weighting\"\"\" weights_1 = W / e_hat weights_0 = (1 - W) / (1 - e_hat) for j in range(X.shape[1]): mean_1 = np.average(X[W==1, j], weights=weights_1[W==1]) mean_0 = np.average(X[W==0, j], weights=weights_0[W==0]) print(f\"Variable {j}: Weighted difference = {mean_1 - mean_0:.4f}\") ` 최신 연구 동향과 확장 7.1 Machine Learning과의 결합 Targeted Maximum Likelihood Estimation (TMLE) One-step correction: 초기 추정값을 업데이트 Cross-validation: 최적 모델 선택 Causal Random Forests Honest splitting: 편향 없는 추정 Local centering: Double robustness 보장 Neural Network Approaches Representation learning: 공통 특성 학습 Domain adaptation: Treatment/control domain 정렬 7.2 고차원 데이터에서의 확장 Debiased Machine Learning (DML) `python Neyman orthogonality + Cross-fitting def debiased_ml_ate(Y, W, X): # Step 1: Estimate nuisance functions theta_0 = estimate_initial(Y, W, X) # Step 2: Compute orthogonal score psi = compute_orthogonal_score(Y, W, X, theta_0) # Step 3: Solve orthogonal equation theta_final = solve_orthogonal_equation(psi) return theta_final ` 7.3 연속 처치와 다중 처치 Continuous Treatment `python GPS (Generalized Propensity Score) + Outcome Model def continuous_dr(Y, T, X): # Estimate treatment density: f(T X) gps_hat = estimate_gps(T, X) # Estimate dose-response: E[Y T,X] outcome_hat = estimate_outcome(Y, T, X) # Double robust estimand dr_estimate = compute_dr_continuous(Y, T, X, gps_hat, outcome_hat) return dr_estimate `` 결론: Double Robustness의 의의 8.1 이론적 기여 편향 없는 추정의 보장: 모델 오지정에도 robust 효율성: 최적 조건에서 semiparametric efficiency bound 달성 유연성: 다양한 기계학습 방법과 결합 가능 8.2 실용적 가치 현실적 해결책: 완벽한 모델링이 어려운 현실에서 안전망 제공 방법론 발전의 기반: R-learner, TMLE, DML 등의 이론적 토대 산업 응용: A/B 테스트, 개인화, 정책 평가 등에서 널리 활용 8.3 미래 전망 Double Robustness는 앞으로도 인과추론 방법론 발전의 핵심 원리로 작용할 것입니다: 딥러닝과의 결합: 표현 학습 기반 double robust 방법 실시간 추론: 온라인 학습에서의 double robustness 복잡한 처치 구조: 네트워크, 시계열에서의 확장 Double Robustness를 이해하는 것은 현대 인과추론의 핵심을 파악하는 것과 같습니다. 이 개념을 바탕으로 더 robust하고 효율적인 인과 추론 방법들이 계속 발전할 것입니다. 🎯 --- 참고문헌: Robins, J.M., Rotnitzky, A., & Zhao, L.P. (1994). Estimation of regression coefficients when some regressors are not always observed. Bang, H. & Robins, J.M. (2005). Doubly robust estimation in missing data and causal inference models. Chernozhukov, V., et al. (2018). Double/debiased machine learning for treatment and structural parameters. Kennedy, E.H. (2020). Towards optimal doubly robust estimation of heterogeneous causal effects.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects",
    "title": "[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects",
    "date": "2025-07-17",
    "excerpt": "R-leaner 방법 소개와 이게 가지는 quasi-oracle property에 대해 설명",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link 논문 리뷰 Abstract > Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. 개인화된 처치효과를 유연하게 추정하는 것은 많은 분야에서 통계적 문제의 핵심입니다. 개인화된 처치효과를 알고 있으면 개인화된 약처방이나 교육정책 결정, 자원 분배 등 많은 문제에서 좋은 결정을 내릴 수 있습니다. 하지만 이를 추정하는 일은 꽤나 복잡한 일이라서 논문에서는 이를 유연하게 추정할 수 있는 방법을 제시하고 있습니다. 이 논문에서는 marginal effects와 treatment propensity라는 두 가지 nuisance component를 추정하여 개인화된 처치효과를 추정하는 방법을 제시합니다. > we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. 이러한 방법은 quasi-oracle property를 가지고 있다고 합니다. <small> quasi-oracle property : marginal effects와 treatment propensity의 추정이 정확하지 않더라도, 마치 이 두 nuisance component를 미리 알고 있는 것과 같은 오차 경계를 달성할 수 있는 성질</small> Introduction 기존 연구의 한계점: 방법론적 불일치: 관찰 연구에서 기계학습 방법을 처치효과 추정에 어떻게 적용해야 하는지에 대한 포괄적인 답이 아직 확립되지 않음 개발 과정의 복잡성: 인과관계 기계학습 방법을 개발하는 과정이 노동집약적이며, 전문 연구자들의 참여가 필수적 이론적 근거 부족: 대부분의 방법들이 수치적 실험으로만 검증되고, 형식적인 수렴 보장이나 오차 경 이 논문의 새로운 접근법: 자동화된 프레임워크: 임의의 손실 최소화 절차를 통해 개인화된 처치효과 추정기를 완전 자동으로 명세할 수 있는 프레임워크 제공 Oracle 수준의 성능: 데이터 생성 분포에 대한 모든 정보를 알고 있는 oracle 방법과 비교 가능한 오차 경계 달성 이론적 기반: 이중 강건 추정(double robust estimation), oracle 부등식, 교차 검증을 결합하여 일반적인 기계학습 도구로 원리적 통계 추정을 위한 손실 함수 개발 이 논문은 기존의 실용적 한계와 이론적 부족함을 모두 해결하는 새로운 접근법을 제시하고 있습니다. A Loss Function for Treatment Effect Estimation > We formalize our problem in terms of the potential outcomes framework (Neyman, 1923; Rubin, 1974). potential outcomes framework를 사용해 문제를 정의합니다. 데이터 구조: 관찰 데이터: $(X_i, Y_i, W_i)$ for $i = 1, ..., n$ $X_i \\in \\mathcal{X}$: 개인별 특성 (features) $Y_i \\in \\mathbb{R}$: 관찰된 결과 (observed outcome) $W_i \\in \\{0, 1\\}$: 처치 할당 (treatment assignment) 잠재 결과 (Potential Outcomes): 잠재 결과: $\\{Y_i(0), Y_i(1)\\}$ $Y_i(0)$: 처치를 받지 않았을 때의 결과 $Y_i(1)$: 처치를 받았을 때의 결과 관찰된 결과와의 관계: $Y_i = Y_i(W_i)$ 조건부 평균 처치효과 (CATE): 목표 함수: $\\tau^(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]$ 특성 $X = x$인 개인들의 평균 처치효과 식별 조건: Unconfoundedness: 처치 할당이 관찰되지 않은 confounding variable에 의해 영향을 받지 않는다는 가정 이 수식들은 인과추론의 표준적인 잠재결과 프레임워크를 따르며, 개인화된 처치효과 추정을 위한 기본 구조를 정의합니다. > In order to identify $\\tau^(x)$, we assume unconfoundedness, i.e., the treatment assignment is randomized once we control for the features $X_i$ (Rosenbaum and Rubin, 1983). CATE를 추정하기 위해서는 무작위 할당처럼 unconfoundedness 가정을 만족해야 합니다. 이를 위해서 X를 통제할 수 있습니다. Assumption 1 : The treatment assignment $W_i$ is unconfounded, $\\{Y_i(0), Y_i(1)\\} \\perp \\!\\!\\! \\perp W_i \\mid X_i$ Treatment Propensity : $e^(x) = \\Pr(W = 1 \\mid X = x)$ Conditional Response Surfaces : $\\mu^{(w)}(x) = \\mathbb{E}[Y(w) \\mid X = x]$ for $w \\in \\{0, 1\\}$ Error Term : $\\varepsilon_i(w) := Y_i(w) - \\{\\mu^{(0)}(X_i) + w\\tau^(X_i)\\}$ 성질 : unconfoundedness 하에서 $\\mathbb{E}[\\varepsilon_i(W_i) \\mid X_i, W_i] = 0$ Conditional Mean Outcome : $m^(x) = \\mathbb{E}[Y \\mid X = x] = \\mu^{(0)}(x) + e^(x)\\tau^(x)$ $$ Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i \\tag{1} $$ (1)에서 *propensity score $e^(X_i)$를 통해 X를 통제합니다. $W_i - e^(X_i)$: 실제 처치 할당에서 예측된 처치 확률을 뺀 값 이는 X를 통제한 후의 \"처치 할당의 편차\"를 나타냄 마치 X가 같은 그룹 내에서 무작위 할당된 것처럼 만듦 따라서 이 등식은 propensity score를 통한 X 통제를 구현한 것입니다. > The goal of this paper is to study how we can use the Robinson’s transfomation (1) for flexible treatment effect estimation that builds on modern machine learning approaches such as boosting or deep learning. 이 논문의 목표는 부스팅이나 딥러닝과 같은 현대적인 기계학습 방법을 기반으로 하는 유연한 처치효과 추정을 위해 Robinson's transfomation (1)을 어떻게 사용할 수 있는지 연구하는 것입니다. > Our main result is that we can use this representation to construct a loss function that captures heterogeneous treatment effects, and that we can then accurately estimate treatment effects—both in terms of empirical performance and asymptotic guarantees—by finding regularized minimizers of this loss function. 이 논문의 주요 결과는 이 표현을 사용하여 개인화된 처치효과를 포착하는 손실 함수를 구성할 수 있으며, 이 손실 함수의 정규화된 최소화 해를 찾아 처치효과를 정확하게 추정할 수 있다는 것입니다. $$ \\tau^(\\cdot) = \\arg\\min_{\\tau} \\mathbb{E}\\left[\\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\} \\tau(X_i)\\right]^2 \\tag{2} $$ $$ \\tilde{\\tau}(\\cdot) = \\arg\\min_{\\tau} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( \\{ Y_i - m^(X_i) \\} - \\{ W_i - e^(X_i) \\} \\tau(X_i) \\right)^2 + \\Lambda_n[\\tau(\\cdot)] \\right\\} \\tag{3} $$ where the term $\\Lambda_n[\\tau(\\cdot)]$ is interpreted as a regularizer on the complexity of the $\\tau(\\cdot)$ function > This regularization could be explicit as in penalized regression, or implicit, e.g., as provided by a carefully designed deep neural network. 정규화항은 모델의 설계에 맞춰 적용한다고 합니다. > The difficulty, however, is that in practice we never know the weighted main effect function $m^(x)$ and usually don’t know the treatment propensities $e^(x)$ either, and so the estimator (3) is not feasible. 하지만 어려운 점은 당연하게도 $m^(x)$와 $e^(x)$를 알 수 없다는 점입니다. > Given these preliminaries, we here study the following class of two-step estimators using cross-fitting (Chernozhukov et al., 2018; Schick, 1986) motivated by the above oracle procedure: Cross-fitting을 이용한 2단계 추정 방법: Step 1: 데이터 분할 및 Nuisance Component 추정 데이터를 Q개(보통 5 또는 10)의 균등한 크기의 fold로 분할 $q(\\cdot)$: $i = 1, \\ldots, n$ 샘플 인덱스를 Q개의 fold에 매핑하는 함수 Cross-fitting을 통해 $\\hat{m}$과 $\\hat{e}$를 최적 예측 정확도를 위해 조정된 방법으로 추정 Step 2: Plug-in 추정 (3)의 plug-in 버전을 통해 처치효과 추정 $\\hat{e}^{(-q(i))}(X_i)$ 등은 i번째 훈련 예제가 속한 fold를 사용하지 않고 만든 예측값 $$ \\hat{\\tau}(\\cdot) = \\arg\\min_{\\tau} \\left\\{ \\hat{L}_n[\\tau(\\cdot)] + \\Lambda_n[\\tau(\\cdot)] \\right\\} \\tag{4} $$ $$ \\hat{L}_n[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\} - \\{W_i - \\hat{e}^{(-q(i))}(X_i)\\} \\tau(X_i) \\right]^2 \\tag{4a} $$ > In other words, the first step learns an approximation for the oracle objective, and the second step optimizes it. We refer to this approach as the R-learner in recognition of the work of Robinson (1988) and to emphasize the role of residualization. We will also refer to the squared loss $L_b^n[\\tau(\\cdot)]$ as the R-loss. 1단계: 오라클 목적함수(이론적으로 최적임을 보장하는 함수)의 근사값을 학습 2단계: 그 근사 목적함수를 실제로 최적화 이 전체 과정을 R-learner라 부르고, 손실함수를 R-loss라 부름 논문의 주요 기여: 다양한 방법 적용 및 성능 R-learner를 페널티 회귀, 커널 릿지 회귀, 부스팅 등 다양한 방법에 적용하여 기존 방법들보다 좋은 성능을 보임. 이론적 보장 커널 회귀의 경우, 실제 추정기(plug-in estimator)의 오차 경계가 오라클(이론적 최적) 방법과 거의 일치함을 증명. 특히, nuisance component($m^(x)$, $e^(x)$)의 추정 오차가 충분히 빠르게 줄어들면, 최종 처치효과 추정기의 수렴 속도는 오직 $\\tau^(x)$의 복잡도에만 의존함. 실용적 장점 R-learner는 처치확률과 결과 예측의 상관관계를 손실함수 구조로 분리하여, 두 작업(상관관계 제거, 처치효과 추정)을 명확히 분리함. 이로 인해 다양한 기계학습 도구(예: glmnet, XGBoost, TensorFlow 등)를 손쉽게 활용할 수 있고, 손실함수(R-loss)만 잘 최소화하면 됨. 복잡한 교차검증 없이도 손실함수 기반의 간단한 튜닝이 가능함. Related Work Regularization bias: > However, the fact that both $\\hat{\\beta}^{(0)}$ and $\\hat{\\beta}^{(1)}$ are regularized towards 0 separately may inadvertently regularize the treatment effect estimate $\\hat{\\beta}^{(1)} - \\hat{\\beta}^{(0)}$ away from 0, even when $\\tau^(x) = 0$ everywhere CATE(조건부 평균 처치효과)는 $\\tau^(x) = \\mu^{(1)}(x) - \\mu^{(0)}(x)$로 쓸 수 있음. 흔히 $\\mu^{(1)}(x)$와 $\\mu^{(0)}(x)$를 각각 따로 추정한 뒤, 그 차이로 처치효과를 구하는데, 이때 두 함수를 별도로 정규화(regularization)하면 regularization bias가 발생할 수 있음. 예를 들어, 라쏘(lasso) 회귀를 각각의 집단(처치/비처치)에 따로 적용하면, 두 추정치 모두 0에 가까워지도록 정규화되어 실제로는 처치효과가 없을 때도 $\\hat{\\tau}(x) = \\hat{\\mu}^{(1)}(x) - \\hat{\\mu}^{(0)}(x)$가 0에서 멀어질 수 있음. 특히, 처치군과 대조군의 샘플 수가 다를 때 이 현상이 더 심해짐. $$ \\begin{align} \\hat{\\beta}^{(w)} = \\arg\\min_{\\beta^{(w)}} \\left\\{ \\sum_{i:W_i=w} \\left( Y_i - X_i^\\top \\beta^{(w)} \\right)^2 + \\lambda^{(w)} \\ \\beta^{(w)}\\ _1 \\right\\} \\tag{5} \\end{align} $$ 간단하게 대조군과 통제군을 따로 학습시키고, 그 결과를 비교해서 처치효과를 얻으면 되는거 아니야? 라고 생각할 수 있지만 이렇게 하면 regularization bias라고 하는, 과하게 정규화되어 처치효과가 0일 때도 0에서 멀어질 수 있는 문제가 발생합니다. Regularization Bias를 피하는 최근 방법들: 최근 연구들은 regularization bias 문제를 피하기 위해 다양한 구조적(machine learning 구조 자체의) 개선 방법을 제안함. 예를 들어, Imai & Ratkovic (2013)은 식 (5)처럼 처치군/대조군을 따로 학습하는 대신, 아래와 같이 하나의 라쏘(lasso) 회귀로 동시에 학습하는 방법을 제안함: $$ \\begin{align} \\hat{b}, \\hat{\\delta} = \\arg\\min_{b, \\delta} \\left\\{ \\sum_{i=1}^n \\left( Y_i - X_i^\\top b + (W_i - 0.5) X_i^\\top \\delta \\right)^2 + \\lambda_b \\ b\\ _1 + \\lambda_\\delta \\ \\delta\\ _1 \\right\\} \\tag{6} \\end{align} $$ 여기서 최종 처치효과 추정은 $\\hat{\\tau}(x) = x^\\top \\hat{\\delta}$로 계산함. 이 방법은 $\\delta$에만 희소성(sparsity)을 강제하여, 처치효과의 구조적 특성을 더 잘 반영할 수 있음. 이외에도 신경망 등 다양한 기계학습 방법에서 처치효과 이질성(heterogeneity)을 잘 추정하도록 구조를 설계하는 연구들이 진행되고 있음(예: Shalit et al., 2017). Loss Function(손실함수) 변경을 통한 접근: > Here, instead of trying to modify the algorithms underlying different machine learning tools to improve their performance as treatment effect estimators, we focus on modifying the loss function used to training generic machine learning methods. 최근 연구들은 모델 구조 자체를 바꾸는 대신, 손실함수(R-loss)를 바꿔서 처치효과 추정 성능을 높이는 방법에 주목하고 있음. 이 접근은 van der Laan과 Dudoit(2003) 등에서 시작된 연구 흐름을 기반으로 하며, 이들은 doubly robust objective(이중 강건 목적함수)에 대한 교차검증을 통해 최적의 통계적 규칙을 선택하는 방법을 제안함. Luedtke & van der Laan(2016) 등은 이러한 목적함수를 이용해 개별화 처치규칙(individualized treatment rules)이나 이질적 처치효과(heterogeneous treatment effects)를 학습하는 다양한 유효한 목적함수(oracle loss 등)의 성질을 분석함. 본 논문의 기여는, R-loss를 활용해 범용 기계학습(generic machine learning)으로 처치효과를 추정하는 방법을 제시하고, 커널 힐버트 공간에서의 정규화 회귀 등 널리 쓰이는 비모수적 방법에 대해 강한 이론적 오차 경계(수렴률 보장)를 제공한 것임. meta-learning 방법들: meta learner 관련 설명자료 X-learner (Künzel et al., 2019): 먼저 $\\hat{\\mu}^{(w)}(x)$를 비모수 회귀 방법으로 추정 처치 관찰값에 대해 pseudo-effects $D_i = Y_i - \\hat{\\mu}^{(-i)}{(0)}(X_i)$를 정의하고, 이를 이용해 $\\hat{\\tau}^{(1)}(X_i)$를 비모수 회귀로 추정 대조군에 대해서도 유사하게 $\\hat{\\tau}^{(0)}(X_i)$를 구하고, 두 추정기를 다음과 같이 결합: $$ \\hat{\\tau}(x) = \\{1 - \\hat{e}(x)\\} \\hat{\\tau}^{(1)}(x) + \\hat{e}(x) \\hat{\\tau}^{(0)}(x) \\tag{7} $$ U-learner (Künzel et al., 2019): $U_i = \\frac{Y_i - m^(X_i)}{W_i - e^(X_i)}$에 대해 $\\mathbb{E}[U_i \\mid X_i = x] = \\tau(x)$임을 이용 $U_i$를 $X_i$에 대해 범용 기계학습 방법으로 회귀 Propensity Score 가중 방법들: Athey & Imbens (2016), Tian et al. (2014) 등이 제안 결과나 공변량을 propensity score로 가중하여 처치효과 추정 예: $Y_i\\{W_i - e^(X_i)\\}/\\{e^(X_i)(1-e^(X_i))\\}$를 $X_i$에 대해 회귀 본 논문의 기여: R-learner 방법을 제안하여 다양한 설정에서 baseline보다 의미있는 개선을 제공 Quasi-oracle 오차 경계를 제공하여 $\\hat{\\tau}$의 오차가 $\\hat{e}$나 $\\hat{m}$의 오차보다 빠르게 감소할 수 있음을 이론적으로 보장 관련 연구와 본 논문의 차별점: 가장 유사한 기존 연구: Zhao, Small, and Ertefaie (2017): Robinson 변환과 라쏘(lasso)를 결합해 고차원 선형모형에서 효과 수정(effect modification)에 대한 유효한 사후선택 추론(post-selection inference)을 제공 하지만 일반적인 기계학습 맥락에서 손실함수로 Robinson 변환을 활용한 것은 본 논문이 처음 이론적 기반: 본 논문의 이론적 결과는 Robinson(1988) 등에서 발전된 준모수적 효율성(semiparametric efficiency)과 직교 모멘트(orthogonal moments) 이론에 기반 알고리즘적으로는 Targeted Maximum Likelihood Estimation(TMIE)와 유사: nuisance component를 비모수적으로 추정 이를 활용해 likelihood(또는 손실함수)를 최적화 Cross-fitting(홀드아웃 예측)은 최근 준모수적 추정에서 널리 쓰이는 방법 본 논문의 차별점: 기존 연구들은 주로 단일(또는 저차원) 파라미터 추정에 초점 본 논문은 복잡한 함수적 객체(즉, $\\tau^(\\cdot)$ 전체 함수)를 추정하는 데 초점 최적 처치 할당 규칙(optimal treatment allocation rule) 추정과도 관련 있지만, 목적함수(손실)가 다름 추가 논의: 본 논문은 모집단에서 무작위 추출된 샘플을 가정 엄격한 무작위화 추론(randomization inference) 하에서의 비모수적 처치효과 추정에 대한 추가 연구도 흥미로운 주제임 The R-Learner in Action 4.1 Application to a Voting Study 투표에 관한 연구에 이를 적용해본 사례를 이야기하고 있습니다. 연구 배경 원본 연구: Arceneaux, Gerber, and Green (2006) 연구 주제: 유료 투표 독려 전화가 투표율에 미치는 효과 실제 결과: 투표 독려 전화의 효과가 거의 없음 (1% 미만) 실험 설계 데이터 구성 전체 샘플: 1,895,468개 관측치 처치 그룹:59,264개 (전화 받은 사람) 분석 샘플: 148160개 (모든 처치 그룹 + 랜덤 대조 그룹) 처치 비율: 2/5 (40 데이터 분할 훈련 세트: 1000개 테스트 세트: 25,0개 홀드아웃 세트: 나머지 인공적 처치효과 추가 실제로는 처치효과가 거의 없으므로, 인공적으로 처치효과를 추가: $$ \\tau^(X_i) = -\\frac{\\text{VOTE00}_i}{2 + \\frac{100}{\\text{AGE}_i}} $$ VOTE00_i: 200투표 여부 AGE_i: 나이 목적: 이질적 처치효과 추정을 위한 의미있는 작업 생성 R-learner 적용 과정 Step 1: Nuisance Components 추정 부스팅과 라쏘 두 방법으로 $\\hat{e}(\\cdot)$와 $\\hat{m}(\\cdot)$ 추정 교차검증으로 최적 모델 선택 결과: 부스팅이 두 nuisance components 모두에서 더 나은 성능 Step2 R-loss 최적화 라쏘: R-loss = 00.1816 (훈련), 0.1781 (홀드아웃) 부스팅: R-loss = 00.1818 (훈련), 0.1783홀드아웃) 최종 선택: 라쏘 기반 $\\hat{\\tau}(\\cdot)$ 성능 비교 Oracle Test Set MSE (Mean Squared Error) 방법 MSE ------ ----- R-learner (라쏘) 00.47 ×10⁻³ R-learner (부스팅) 1.2310 Single Lasso 00.61 ×10⁻³ BART 40.05 ×10⁻³ 성능 분석 R-learner (라쏘)가 최고 성능2. 라쏘 vs 부스팅: 라쏘: 편향 있지만 분산 낮음 부스팅: 노이즈 많지만 편향 적음 편향-분산 트레이드오프에서 라쏘가 유리 핵심 발견 하이브리드 접근법의 우수성 Nuisance components: 비모수적 방법 (부스팅) 사용 처치효과 추정: 간단한 방법 (라쏘) 사용 이유: 각 단계에 최적화된 방법 선택 기존 방법들의 한계 Single Lasso: 모든 곳에서 선형 모델링 → 모델 오지정 가능성 BART: 모든 곳에서 비모수적 모델링 → 불안정한 $\\tau(\\cdot)$ 추정 R-learner의 장점 유연성: 각 단계에 적합한 방법 선택 가능 안정성: 교차검증을 통한 안정적인 모델 선택 성능: 기존 방법들보다 우수한 추정 정확도 실용적 교훈 단계별 최적화: 각 단계에 가장 적합한 방법 선택 교차검증의 중요성: 과적합 방지와 모델 선택 편향-분산 트레이드오프: 데이터 크기와 복잡성 고려 실제 데이터 적용: 이론적 방법의 실제 성능 검증 4.2 Model Averaging with the R-Learner 여러 모델을 합치는 stacking 방법을 적용한 시도를 이야기하고 있습니다. 배경과 동기 앞선 섹션에서는 R-learner의 각 단계에서 사용할 추정 전략을 세심하게 고려했습니다. 하지만 실제로는 분석의 시작점으로 기성품(off-the-shelf) 처치효과 추정기를 사용하는 것을 선호하는 경우가 많습니다. 이 섹션에서는 stacking의 변형을 통해 합의 처치효과 추정을 구축하는 R-learning 접근법을 논의합니다. Stacking 기반 합의 추정 기본 설정 $k = 1, \\ldots, K$개의 서로 다른 처치효과 추정기 $\\hat{\\tau}_k$ 보유 훈련 세트에서 out-of-fold 추정치 $\\hat{\\tau}_k^{(-i)}(X_i)$ 접근 가능 신뢰할 수 있는 out-of-fold 추정치 $\\hat{e}^{(-i)}(X_i)$와 $\\hat{m}^{(-i)}(X_i)$ 보유 합의 추정 방법 R-loss에 따라 $\\hat{\\tau}_k(\\cdot)$의 최적 양의 선형 결합을 취하여 합의 추정 $\\hat{\\tau}(\\cdot)$를 구축: $$ \\hat{\\tau}(x) = \\hat{c} + \\sum_{k=1}^K \\alpha_k \\hat{\\tau}_k(x) \\tag{8a} $$ $$ (\\hat{b}, \\hat{c}, \\hat{\\alpha}) = \\arg\\min_{b, c, \\alpha} \\left\\{ \\sum_{i=1}^n \\left[ \\left(Y_i - \\hat{m}^{(-i)}(X_i)\\right) - b - \\left(c + \\sum_{k=1}^K \\alpha_k \\hat{\\tau}_k^{(-i)}(X_i)\\right) \\left(W_i - \\hat{e}^{(-i)}(X_i)\\right) \\right]^2 : \\alpha \\geq 0 \\right\\} \\tag{8b} $$ 수식의 구성 요소 $\\hat{c}$: 상수 처치효과 항 (유연성을 위해 자유롭게 조정) $\\hat{b}$: 절편 ($\\hat{m}$의 잠재적 편향 흡수) $\\alpha_k$: 각 추정기의 가중치 (비음 제약) 실험 설정 데이터 생성 과정 다음과 같은 데이터 생성 분포에서 실험을 수행했습니다: $$ X_i \\sim N(0, I_{d \\times d}), \\quad W_i \\sim \\mathrm{Bernoulli}(0.5) \\tag{9a} $$ $$ Y_i \\mid X_i, W_i \\sim N\\left(\\frac{3}{1 + e^{X_{i3} - X_{i2}}} + (W_i - 0.5) \\tau^(X_i), \\sigma^2\\right) \\tag{9b} $$ 실험 조건: 샘플 크기: $n = 10,000$ (무작위 연구 설계) 차원: $d = 10$ 다양한 $\\tau^(\\cdot)$와 $\\sigma$ 선택 처치효과 함수 두 가지 유형의 처치효과 함수를 고려: 연속 함수: $\\tau^(X_i) = \\frac{1}{1 + e^{X_{i1} - X_{i2}}}$ 불연속 함수: $\\tau^(X_i) = \\frac{\\mathbf{1}\\{X_{i1} > 0\\}}{1 + e^{-X_{i2}}}$ 비교 방법 추정 방법들: BART (Bayesian Additive Regression Trees) Causal Forests (인과관계 숲) Stacking: 식 (8)을 이용한 두 방법의 결합 실험 조건: 데이터가 무작위화되었다는 것을 실험자가 알고 있다고 가정 필요한 곳에서는 $\\hat{e}(x) = 0.5$ 사용 Stacking용 $\\hat{m}(\\cdot)$는 random forest로 추정 실험 결과 (Figure 2) 연속 처치효과 함수의 경우 BART: Causal Forests보다 약간 우수한 성능 Stacking: 노이즈 수준 $\\sigma$가 매우 클 때까지 개별 방법보다 우수 고노이즈 환경: 모든 방법이 상수 처치효과 추정기와 비슷한 수준 불연속 처치효과 함수의 경우 Causal Forests: 낮은 노이즈 수준에서 특히 유리 Stacking: 더 정확한 기본 학습자의 성능을 자동으로 매칭 핵심 발견 Stacking의 자동 적응성 각 설정에서 더 나은 기본 학습자를 자동으로 식별 수동 선택 없이도 최적 성능에 근접 방법론적 유연성 기존 처치효과 추정기들을 그대로 활용 복잡한 알고리즘 수정 없이 성능 개선 실용적 장점 자동화: 복잡한 모델 선택 과정 단순화 견고성: 다양한 데이터 특성에 대한 적응력 효율성: 기존 도구들의 강점을 효과적으로 결합 A Quasi-Oracle Error Bound 이 섹션에서는 R-learner의 핵심 이론적 기여인 quasi-oracle property에 대해 설명합니다. 5.1 Oracle과 Quasi-Oracle의 개념 Oracle Estimator 이상적인 oracle은 다음을 미리 알고 있다고 가정합니다: True marginal effect: $m^(x) = \\mathbb{E}[Y \\mid X = x]$ True propensity score: $e^(x) = \\Pr(W = 1 \\mid X = x)$ Oracle이 이 정보를 알고 있다면, 다음 손실함수를 직접 최소화할 수 있습니다: $$ \\mathcal{L}_n^{\\text{oracle}}[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\} \\tau(X_i) \\right]^2 $$ R-learner (Quasi-Oracle) 실제로는 $m^(x)$와 $e^(x)$를 모르므로, 추정값 $\\hat{m}(\\cdot)$과 $\\hat{e}(\\cdot)$을 사용: $$ \\hat{\\mathcal{L}}_n[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\} - \\{W_i - \\hat{e}^{(-q(i))}(X_i)\\} \\tau(X_i) \\right]^2 $$ 5.2 Main Theoretical Result 논문의 핵심 이론적 결과는 다음과 같습니다: Theorem (Informal) 적절한 조건 하에서, R-learner $\\hat{\\tau}$는 다음 성질을 만족합니다: $$ \\mathbb{E}\\left[\\ \\hat{\\tau} - \\tau^\\ ^2\\right] \\lesssim \\text{Oracle Rate} + \\text{Nuisance Rate} $$ 여기서: Oracle Rate: oracle이 달성할 수 있는 최적 수렴률 Nuisance Rate: nuisance components 추정 오차의 곱 Quasi-Oracle Property 만약 nuisance components의 추정이 충분히 빠르게 수렴한다면: $$ \\text{Nuisance Rate} = \\ \\hat{m} - m^\\ \\cdot \\ \\hat{e} - e^\\ \\to 0 $$ 이 경우 R-learner는 oracle rate를 달성합니다: $$ \\mathbb{E}\\left[\\ \\hat{\\tau} - \\tau^\\ ^2\\right] \\lesssim \\text{Oracle Rate} $$ 5.3 Why Does This Work? Cross-fitting의 역할 Overfitting 방지: 같은 데이터로 추정과 예측을 하지 않음 Bias 감소: $\\mathbb{E}[\\hat{m}^{(-i)}(X_i) - m^(X_i)] \\approx 0$ Doubly Robust Structure R-loss는 doubly robust 구조를 가집니다: $$ \\mathbb{E}\\left[\\left\\{Y_i - m^(X_i)\\right\\} - \\left\\{W_i - e^(X_i)\\right\\} \\tau^(X_i) \\mid X_i\\right] = 0 $$ 이는 다음을 의미합니다: $m^(x)$ 또는 $e^(x)$ 중 하나만 정확해도 편향이 없음 두 추정의 오차가 곱의 형태로 나타남 Product Form of Error 최종 오차는 다음과 같이 분해됩니다: $$ \\text{Total Error} \\approx \\text{Oracle Error} + \\ \\hat{m} - m^\\ \\times \\ \\hat{e} - e^\\ $$ 따라서 두 nuisance components가 모두 $o(1)$ 속도로 수렴하면, 곱은 더 빠르게 수렴합니다. 5.4 Practical Implications Flexible Method Choice 각 단계에서 서로 다른 기계학습 방법 사용 가능 $\\hat{m}$에는 random forests, $\\hat{e}$에는 logistic regression 등 Robustness 한 nuisance component가 부정확해도 다른 것이 보완 완벽한 추정이 필요하지 않음 Rate Optimality 적절한 조건 하에서 minimax optimal 달성 이론적으로 최선의 성능 보장 논문에서 궁금한 점들 정리 그래서 quasi-oracle property는 어떻게 보장되는건데? 핵심 메커니즘 Robinson's Transformation의 마법 Robinson's transformation (1988)이 핵심입니다: $$ Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i $$ 이 변환의 특별한 성질: Orthogonality: $\\mathbb{E}[\\varepsilon_i \\mid X_i, W_i] = 0$ Causal Isolation: 처치효과만 분리됨 Cross-fitting의 핵심 역할 `` Step 1: 데이터를 K개 fold로 분할 Step 2: k번째 fold를 제외하고 m̂, ê 추정 Step 3: k번째 fold에서 τ̂ 추정 Step 4: 모든 fold에 대해 반복 ` 왜 이게 중요한가? Independence: 추정에 사용된 데이터와 예측 데이터가 독립 Bias Reduction: $\\mathbb{E}[\\hat{m}^{(-i)}(X_i) \\mid X_i] \\approx m^(X_i)$ Doubly Robust의 수학적 구조 핵심은 다음 decomposition입니다: $$ \\hat{\\mathcal{L}}_n - \\mathcal{L}_n^{\\text{oracle}} = \\text{Bias Term} + \\text{Variance Term} $$ Bias Term: $$ \\frac{2}{n} \\sum_{i=1}^n \\left[\\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\}\\tau^(X_i)\\right] \\times \\left[\\{\\hat{m}^{(-i)} - m^\\}(X_i) - \\{\\hat{e}^{(-i)} - e^\\}(X_i)\\tau^(X_i)\\right] $$ Cross-fitting에 의해 이 term의 기댓값이 0에 가까워집니다. Variance Term: $$ \\left\\ \\{\\hat{m} - m^\\} - \\{\\hat{e} - e^\\}\\tau^\\right\\ ^2 $$ 이는 $\\ \\hat{m} - m^\\ \\times \\ \\hat{e} - e^\\ $의 order입니다. 수렴률의 곱셈 구조 만약: $\\ \\hat{m} - m^\\ = O_p(r_m)$ $\\ \\hat{e} - e^\\ = O_p(r_e)$ 그러면: $$ \\ \\hat{\\tau} - \\tau^\\ = O_p(\\max\\{r_{\\text{oracle}}, r_m \\times r_e\\}) $$ 예시: $r_m = r_e = n^{-1/4}$ (느린 수렴) $r_m \\times r_e = n^{-1/2}$ (빠른 수렴!) Oracle rate $r_{\\text{oracle}} = n^{-1/3}$이면 최종 rate = $\\max\\{n^{-1/3}, n^{-1/2}\\} = n^{-1/3}$ ✨ 실제 동작 예시 시나리오 1: 완벽한 Oracle `python Oracle knows true m(x) and e(x) oracle_loss = R_loss(Y, W, X, tau, m_true, e_true) oracle_rate = n^(-1/3) # optimal rate for τ complexity ` 시나리오 2: R-learner with Imperfect Estimates `python Step 1: Estimate nuisances with cross-fitting m_hat = estimate_m_with_crossfit(Y, X) # rate: n^(-1/4) e_hat = estimate_e_with_crossfit(W, X) # rate: n^(-1/4) Step 2: Optimize R-loss tau_hat = optimize_R_loss(Y, W, X, m_hat, e_hat) Achieved rate: max(n^(-1/3), n^(-1/4) × n^(-1/4)) = n^(-1/3) Same as oracle! 🎉 `` 왜 이게 \"Quasi-Oracle\"인가? Oracle과 같은 수렴률: $n^{-1/3}$ 달성 하지만 Oracle이 아님: $m^, e^*$를 실제로는 모름 Automatic: 추가적인 튜닝 없이 자동으로 달성 Robust: nuisance 추정이 완벽하지 않아도 작동 이것이 바로 \"quasi-oracle property\"의 핵심입니다! 🎯",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/PropensityScoreNotebook",
    "title": "Propensity Score Notebook",
    "date": "2025-07-17",
    "excerpt": "Propensity Score Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "--- title: \"Propensity Score Notebook\" date: \"2025-07-17\" excerpt: \"Propensity Score Notebook\" category: \"Causal Inference\" tags: [\"Causal Inference\"] --- Propensity Score # The Psychology of Growth 치료(treatment)와 결과(outcome) 변수 외에도, 이 연구에서는 다음과 같은 다른 특성들도 기록했습니다: schoolid: 학생의 학교 식별자; success_expect: 미래 성공에 대한 자가 보고 기대치, 사전 성취도의 대용 지표로, 무작위 배정 전에 측정됨; gender: 학생이 식별한 성별의 범주형 변수; frst_in_family: 학생의 1세대 대학생 지위에 대한 범주형 변수, 즉 가족 중 대학에 진학한 첫 번째 사람; school_urbanicity: 학교의 도시화 정도에 대한 학교 수준의 범주형 변수, 즉 농촌, 교외 등; school_mindset: 학생들의 고정된 사고방식의 학교 수준 평균, 무작위 배정 전에 보고됨, 표준화됨; school_achievement: 학교 성취 수준, 이전 4개 학년 학생들의 시험 점수와 대학 준비도를 측정한 것으로, 표준화됨; school_ethnic_minority: 학교 인종/민족 소수자 구성, 즉 흑인, 라티노, 또는 아메리카 원주민인 학생 비율, 표준화됨; school_poverty: 학교 빈곤 집중도, 즉 연방 빈곤선 이하 소득을 가진 가정 출신 학생들의 비율, 표준화됨; school_size: 학교의 모든 4개 학년 학생들의 총 수, 표준화됨. import pandas as pd pd.set_option(\"display.max_columns\", None) data = pd.read_csv(\"./data/learning_mindset.csv\") data.sample(5, random_state=5) > Although the study was randomised, it doesn’t seem to be the case that this data is free from confounding > So, although the opportunity to participate was random, participation itself is not. We are dealing with a case of non-compliance here. 마인드셋을 바꾸는 프로그램 참여 여부는 랜덤하게 주어졌지만, 실제로 참석하는데에는 학생들의 개별적인 요인의 영향이 있으니까 data.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) > One evidence of this is how the student’s success expectation is correlated with the participation in the seminar. > Students with higher self-reported success expectation are more likely to have joined the growth mindset seminar. 스스로 더 높은 기대가 있는 학생들이 실제로 더 많이 세미나에 참석하더라 -> confounding bias가 존재한다. > Still, let’s see what the difference in means looks like. This will be a useful baseline to compare against. $$ \\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) = E[Y_i(1)] - E[Y_i(0)] $$ import statsmodels.formula.api as smf smf.ols(\"achievement_score ~ intervention\", data=data).fit().summary().tables[1] 처치가 0또는 1이기 때문에 처치만을 가지고 결과를 예측하는 회귀식을 통해 $E[Y_i(1)]$과 $E[Y_i(0)]$을 추정할 수 있다. 처치 혹은 개입이 있는 경우 없는 경우보다 평균적으로 0.4723정도 높다. 근데 이정도 차이면 큰건가? > But is this big or small? I know that interpreting standardized outcomes can be challenging, but bear with me for a moment. > I think it is worth going through this because it won’t be the last time you will encounter standardized scores. score가 정규화되었기 때문에 이 단위는 std다. 따라서 0.4723이란 의미는 약 0.5std 정도 차이가 있다는 의미. > Since the treated group has an average standardized score of about 0.5, this means that they fall above 70% in terms of individual achievement. > Or, in other words, they are in the top 30% who achieve more. Here is what this looks like in a picture. import matplotlib.pyplot as plt plt.hist(data[\"achievement_score\"], bins=20, alpha=0.3, label=\"All\") plt.hist(data.query(\"intervention==0\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C2\") plt.hist(data.query(\"intervention==1\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C3\") plt.vlines(-0.1538, 0, 300, label=\"Untreated\", color=\"C2\") plt.vlines(-0.1538+0.4723, 0, 300, label=\"Treated\", color=\"C3\") plt.legend() > Of course, we still think this result is biased. > The difference between treated and untreated is probably smaller than this, because we think the bias is positive 과연 이정도 차이 나는게 맞을까? 앞서 확인한 것처럼 편향이 존재한다는 것을 확인했다. 비교군 사이에 스스로 기대하는 정도 (success_expect)가 차이가 있었고, 이는 성공확률에 영향을 미치는 요인이 될 수 있다. # Propensity Score > Propensity score comes from the realisation that you don’t need to directly control for confounders X to achieve conditional independence $(Y_1, Y_0) \\perp T X$ > Instead, it is sufficient to control for a balancing score $E(T X)$ 편향을 제거하고 조건부 독립성을 만족하기 위해서는 X를 통제해야하는데, 이게 어려우니까 이를 대신해서 X일때 T일 확률($e(x)$, propensity score)을 통제하자는 컨셉 $$(Y_1, Y_0) \\perp T e(x)$$ import graphviz as gr g = gr.Digraph() g.edge(\"T\", \"Y\") g.edge(\"X\", \"Y\") g.edge(\"X\", \"e(x)\") g.edge(\"e(x)\", \"T\") g > If I know what e(x) is, X alone tells me nothing more that can help me learn what T would be. > Which means that controlling for e(x) acts the same way as controlling for X directly 데이터의 편향을 생각해보면, success expect가 높은 학생은 본인 일도 열심히하고 세미나에 참석할 확률도 높을 것이다. 따라서 랜덤으로 기회를 줬지만 실제로는 원래 성공할 확률이 높은 학생이 세미나에 참석할 확률이 더 높음. > Think about it. If they have the exact same probability of receiving the treatment, the only reason one of them received it and the other did not is pure chance. > Holding the propensity score constant acts in a way of making the data look as good as random. 참석할 확률을 같게 만든다면, 원래 목적처럼 참석할 확률이 랜덤하게 된다. # Propensity Weighting $$ E[Y X, T = 1] - E[Y X, T = 0] = E[\\frac{Y}{e(x)} X, T = 1]P(T) - E\\frac{Y}{(1 - e(x))} X, T = 0) $$ 처치를 받은 것 중에 처치를 받지 않을 것 같은 샘플 ($e(x)$가 낮은 경우)는 큰 가중치를 주고, 반대로 처치를 받지 않은 것 중에 처치를 받을 것 같은 샘플 ($e(x)$가 높은 경우)는 큰 가중치를 주는. 이러한 가중치는 직관적인 생각과 일치한다. 왜냐하면 처치를 받은 것 중에 처치를 받을 확률이 높은 건 랜덤에 가깝게 하려면 낮춰야 하고, 처치를 받을 확률이 낮은 건 랜덤에 가깝게 하려면 높여야하기 때문이다. > This estimator is called the Inverse Probability of Treatment Weighting (IPTW), since it scales each unit by the inverse probability of the treatment it received. !iptw 적용한 예제",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/PropensityScore",
    "title": "Propensity Score",
    "date": "2025-07-17",
    "excerpt": "Propensity Score",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "Propensity Score 이 책의 저자가 참 글을 잘쓴다. 되게 매력적인 주제로 이 내용을 풀어간다. Growth Mindset vs Fixed Mindset 심리학자들은 마인드셋이 인생에 큰 영향을 미친다. 내 능력이 고정되어있지 않고 충분히 변화할 수 있다고 믿는 사람들은, 내 능력이 고정되어있다고 믿는 사람보다 더 성공한다. 나는 이 부분에 크게 공감하고, 성장형 마인드셋을 가지려고 한다. 다만 인과추론을 다루는 입장에서는 이에 다음과 같은 의문을 던져볼 수 있다. > Is it that a growth mindset causes people to achieve more? Or is simply the case that people who achieve more are prone to develop a growth mindset as a result of their success? 진짜 성장형 사고방식이 사람들을 더 성공하게 했나? 아니면 성공한 사람들이 그런 마인드셋을 가지고 있을 확률이 높은건가? 되게 재밌는 질문이다. 이 부분을 위한 실험이 진행된게 있구나. > To settle things, researchers designed the The National Study of Learning Mindsets. It is a randomised study conducted in U.S. public high schools which aims at finding the impact of a growth mindset. > The real data on this study is not publicly available in order to preserve students’ privacy. However, we have a simulated dataset with the same statistical properties provided by Athey and Wager, so we will use that instead. 노트북 참고: Propensity Score Notebook",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimationNotebook",
    "title": "Doubly Robust Estimation Notebook",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "--- title: \"Doubly Robust Estimation Notebook\" date: \"2025-07-17\" excerpt: \"Doubly Robust Estimation Notebook\" category: \"Causal Inference\" tags: [\"Causal Inference\"] --- Doubly Robust Estimation import warnings warnings.filterwarnings('ignore') import pandas as pd import numpy as np from matplotlib import style from matplotlib import pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression, LinearRegression %matplotlib inline style.use(\"fivethirtyeight\") pd.set_option(\"display.max_columns\", None) # 학생들의 마인드셋을 향상시키기 위한 세미나를 진행했는데, 이게 어느정도 영향이 있었는지 가상의 데이터 세미나 참석 여부 -> intervention 얼마나 성공했는지 혹은 성공할 확률 -> success_expect data = pd.read_csv(\"./data/learning_mindset.csv\") data.sample(5, random_state=5) # 성공한 정도 (success expect)가 높을수록 세미나 참석 여부(intervention)가 높은지 data.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) 학생의 요인에 따라 참석 여부가 결정되고, 또 이 요인이 성공확률을 결정한다. -> confounding variable. 즉, 단순히 참석 여부에 따라 성공확률을 비교해 세미나의 효과를 추정하기엔 어렵다. 학생의 다른 요인들이 영향을 미치고 있으니까 # regression을 통해 편향을 제거하고 세미나의 효과 보기 이를 위해 categorical features를 더미 변수로 변환 categ = [\"ethnicity\", \"gender\", \"school_urbanicity\"] cont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"] data_with_categ = pd.concat([ data.drop(columns=categ), # dataset without the categorical features pd.get_dummies(data[categ], columns=categ, drop_first=False) # categorical features converted to dummies ], axis=1) print(data_with_categ.shape)",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimation",
    "title": "Doubly Robust Estimation",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "Doubly Robust Estimation",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-16-그나저나소개팅어땠어",
    "title": "그나저나 소개팅 어땠어?",
    "date": "2025-07-16",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "고민 및 조언하는 표햔"
    ],
    "content": "배운 것 How was it? 그나저나, 그런데 By the way, how was the blind date? BTW (By The Way) 친구 사이에서 가볍게 쓰는 말 By the way, how was the dinner yesterday? Blind date 소개팅 Well, the thing is he is a vegeterian. 글쎄, 문제는 out of budget to make it somewhere : 어디든 가다 간단한 작문. Simple writing or essay Daily talk to myself as usual. By the way, How was your day? Did you do your best? Well, the thing is condition. Today, I was so tired. So I can fully concentrate. So I think I should have a sleep early. 수정된 버전 (Improved Version) Just having my usual daily conversation with myself. By the way, how was your day? Did you do your best? Well, the thing is my condition wasn't great. Today, I was so tired that I couldn't fully concentrate. So I think I should go to sleep early tonight. 피드백 (Feedback) 주요 수정 사항: \"Daily talk to myself\" → \"Having my usual daily conversation with myself\" 더 완전한 문장 구조로 수정 \"Well, the thing is condition\" → \"Well, the thing is my condition wasn't great\" 불완전한 문장을 완전한 문장으로 수정 소유격 \"my\" 추가 및 구체적인 설명 추가 \"So I can fully concentrate\" → \"So I couldn't fully concentrate\" 맥락상 부정문이 맞음 (피곤해서 집중할 수 없었다는 의미) \"I should have a sleep early\" → \"I should go to sleep early tonight\" \"go to sleep\"이 더 자연스러운 표현 \"tonight\" 추가로 시간 명시 문법 포인트: \"so + 형용사/부사 + that + 주어 + 동사\" 구문 사용 시제 일치 (과거 상황을 설명할 때는 과거형 사용) 자연스러운 영어 표현 사용",
    "public": true
  },
  {
    "id": "Self_Development/Book/만들어진 신 (The God Delusion)/책을읽기시작하면서",
    "title": "책을 읽기 시작하면서",
    "date": "2025-07-16",
    "excerpt": "책을 읽기 전에 어떤 생각을 가지고 있는지 정리",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "만들어진 신"
    ],
    "content": "나는 확실한 무신론자다. 부모님의 영향도 있겠지만, 친구따라 교회에 다녔던 경험, 다양한 매체에서 접한 현상들 등등이 지금 나의 생각을 만들었다. 신이 있는게 많이 되나? 나의 사고방식은 직업적인 영향 (개발자 or 데이터과학자) 때문인지 몰라도 근거에 지배적이다. 근거가 명확하다면 지금도 당장 바뀔 수 있고, 근거가 없다면 백날 설득을 하려해도 전혀 소용이 없을 것이다. 내가 이 책을 읽게 된 이유는 신이 없는 이유를 알고 싶다기 보다는, 사람들이 왜 신을 믿는지가 더 궁금한게 크다. 어쨌든 책에서는 사람들이 신을 믿는 이유를 언급하고 이를 반박할테니까. 이 책의 영어 제목은 되게 파격적이다. 헛웃음이 나올정도로. Delusion, 망상이라는 단어를 쓰다니... > 누군가 망상에 시달리면 정신 이상이라고 한다. > 다수가 망상에 시달리면 종교라고 한다. > -> 로버트 퍼시그 사실 나도 비슷한 생각이긴 하다. 내가 생각하는 신은 공통적으로 그리고 있는 허상이라고 생각한다. 예전에 모시던 호랑이 신과 예수는 사실 같다고 보여진다. 사람들의 편의를 위해 어떤 허상의 개념을 만들고, 이를 함께 믿는 것. 주위에 종교를 가진 친구들이 많은데 그들을 무시하는 생각은 진짜 전혀없다. 그냥 나의 생각이 이럴 뿐. 또 허상이면 어떻나 삶에 도움이 되면 그게 좋은거지 라는 생각도 한다. 어쨌든 책을 읽어나가면서 나의 생각은 어떻게 바뀌고, 또 어떤 관점이 열릴지 기대가 된다. 쭉 읽어봐야겠다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/tikhonov-regularization-and-convex-optimization",
    "title": "Tikhonov Regularization과 Convex Optimization: 연속형 처치 R-learner의 이론적 배경",
    "date": "2025-07-16",
    "excerpt": "Ill-posed problem을 해결하는 Tikhonov regularization과 convex optimization의 원리",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Optimization",
      "Regularization"
    ],
    "content": "개요 --- 연속형 처치에서 R-learner를 적용할 때 발생하는 non-identification 문제를 해결하기 위해 Tikhonov regularization과 convex optimization 이론이 핵심적인 역할을 합니다. 이 글에서는 이러한 수학적 도구들의 원리와 응용을 자세히 설명합니다. Ill-posed Problem과 Non-identification --- 1.1 Well-posed vs Ill-posed Problem Well-posed problem (잘 정의된 문제) 의 3가지 조건: 존재성 (Existence): 해가 존재한다 유일성 (Uniqueness): 해가 유일하다 안정성 (Stability): 입력의 작은 변화가 해의 작은 변화를 가져온다 Ill-posed problem은 이 중 하나 이상의 조건을 만족하지 않는 문제입니다. 1.2 연속형 처치에서의 Non-identification 연속형 처치 R-learner에서는 유일성 조건이 위반됩니다: $$L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right]$$ 이 손실함수를 최소화하는 해집합은: $$S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\}$$ > 문제: 무한히 많은 해가 존재하여 유일한 CATE 함수 $\\tau(x, t)$를 식별할 수 없음 Tikhonov Regularization --- 2.1 기본 원리 Tikhonov regularization은 ill-posed problem을 well-posed problem으로 변환하는 방법입니다. 원래 문제: $$\\min_{h} L(h)$$ 정규화된 문제: $$\\min_{h} L(h) + \\rho \\ h\\ ^2$$ 여기서: $\\rho > 0$: 정규화 매개변수 $\\ h\\ ^2$: 정규화 항 (일반적으로 $L_2$ norm) 2.2 연속형 처치 R-learner에서의 적용 $$ \\begin{equation} \\tau_\\rho = \\arg \\min_{h \\in L_2^P(X,T)} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right] \\end{equation} $$ 정규화 항의 역할: $\\ h\\ _{L_2^P}^2 = E\\{h^2(X, T)\\}$ 함수의 \"크기\"를 제한하여 과적합 방지 무한히 많던 해를 유일한 해로 수렴시킴 2.3 Tikhonov 정리 정리: 정규화 매개변수 $\\rho > 0$가 주어졌을 때, 정규화된 문제는 유일한 해 $\\tau_\\rho$를 가진다. 증명 아이디어: $L_c(h)$는 convex하지만 strictly convex하지 않음 $\\rho \\ h\\ _{L_2^P}^2$는 strictly convex함 둘의 합은 strictly convex가 되어 유일한 최솟값을 가짐 Convex Optimization --- 3.1 Convex Function의 정의 함수 $f: \\mathbb{R}^n \\to \\mathbb{R}$가 convex하다는 것은: $$f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1-\\lambda) f(y)$$ 모든 $x, y \\in \\text{dom}(f)$와 $\\lambda \\in [0, 1]$에 대해 성립함을 의미합니다. 3.2 Strictly Convex Function 함수 $f$가 strictly convex하다는 것은: $$f(\\lambda x + (1-\\lambda) y) < \\lambda f(x) + (1-\\lambda) f(y)$$ $x \\neq y$이고 $\\lambda \\in (0, 1)$일 때 부등호가 엄격함을 의미합니다. 3.3 Convex Function의 중요한 성질 성질 1: 유일한 최솟값 Convex function: 지역 최솟값 = 전역 최솟값 Strictly convex function: 최솟값이 유일함 성질 2: 최적화의 용이성 경사하강법이 전역 최솟값으로 수렴 보장 수치적 안정성 성질 3: Jensen's Inequality $$E[f(X)] \\geq f(E[X])$$ 3.4 R-learner에서의 Convexity 원래 손실함수 $L_c(h)$: Convex하지만 strictly convex하지 않음 따라서 최솟값이 유일하지 않음 정규화된 손실함수 $L_c(h) + \\rho \\ h\\ ^2$: $\\rho \\ h\\ ^2$가 strictly convex함 전체 함수가 strictly convex가 됨 유일한 최솟값 보장 수학적 증명 --- 4.1 $\\ h\\ _{L_2^P}^2$가 Strictly Convex임을 증명 정리: $\\ h\\ _{L_2^P}^2 = E\\{h^2(X, T)\\}$는 strictly convex하다. 증명: 임의의 $h_1 \\neq h_2$와 $\\lambda \\in (0, 1)$에 대해: $$ \\begin{align} \\ \\lambda h_1 + (1-\\lambda) h_2\\ _{L_2^P}^2 &= E\\{[\\lambda h_1(X,T) + (1-\\lambda) h_2(X,T)]^2\\} \\\\ &= E\\{\\lambda^2 h_1^2(X,T) + 2\\lambda(1-\\lambda) h_1(X,T)h_2(X,T) + (1-\\lambda)^2 h_2^2(X,T)\\} \\\\ &= \\lambda^2 E\\{h_1^2(X,T)\\} + 2\\lambda(1-\\lambda) E\\{h_1(X,T)h_2(X,T)\\} + (1-\\lambda)^2 E\\{h_2^2(X,T)\\} \\end{align} $$ Cauchy-Schwarz 부등식에 의해: $$E\\{h_1(X,T)h_2(X,T)\\} \\leq \\sqrt{E\\{h_1^2(X,T)\\}} \\sqrt{E\\{h_2^2(X,T)\\}}$$ $h_1 \\neq h_2$일 때 부등호가 strict하므로: $$\\ \\lambda h_1 + (1-\\lambda) h_2\\ _{L_2^P}^2 < \\lambda \\ h_1\\ _{L_2^P}^2 + (1-\\lambda) \\ h_2\\ _{L_2^P}^2$$ 4.2 정규화된 손실함수의 Strict Convexity 결론: $L_{c,\\ell_2}(h \\mid \\rho) = L_c(h) + \\rho \\ h\\ _{L_2^P}^2$는 $\\rho > 0$일 때 strictly convex하다. 이유: $L_c(h)$는 convex $\\rho \\ h\\ _{L_2^P}^2$는 strictly convex Convex + Strictly convex = Strictly convex 실제 응용과 의미 --- 5.1 Two-step Procedure Step 1: Tikhonov regularization으로 중간 함수 추정 $$\\tilde{\\tau}_\\rho = \\arg \\min_{h} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right]$$ Step 2: Zero-constraining operator로 최종 CATE 추정 $$\\tau(x, t) = \\text{ZeroConstraint}(\\tilde{\\tau}_\\rho(x, t))$$ 5.2 정규화 매개변수 $\\rho$의 역할 $\\rho$가 작을 때: 원래 문제에 가까움 Bias는 작지만 variance가 클 수 있음 $\\rho$가 클 때: 정규화 효과가 강함 Variance는 작지만 bias가 클 수 있음 최적 $\\rho$ 선택: Cross-validation Information criteria (AIC, BIC) Theoretical convergence rate 5.3 수렴 속도와 점근적 성질 정규화된 추정량 $\\hat{\\tau}_\\rho$는 다음과 같은 우수한 성질을 가집니다: Consistency: $\\hat{\\tau}_\\rho \\to \\tau$ as $n \\to \\infty$ Convergence rate: $O_p(n^{-r})$ for some $r > 0$ Asymptotic normality: $\\sqrt{n}(\\hat{\\tau}_\\rho - \\tau) \\xrightarrow{d} N(0, \\Sigma)$ 다른 정규화 방법과의 비교 --- 6.1 Ridge vs Lasso vs Tikhonov 방법 정규화 항 특징 ------ ----------- ------ Ridge $\\lambda \\ \\beta\\ _2^2$ 계수 축소, 변수 선택 X Lasso $\\lambda \\ \\beta\\ _1$ 변수 선택, 희소성 Tikhonov $\\rho \\ h\\ _{L_2^P}^2$ 함수 공간에서의 정규화 6.2 다른 해결 방법들 Alternative 1: Truncated SVD 특이값 분해 후 작은 특이값 제거 수치적 방법 Alternative 2: Iterative methods Landweber iteration Conjugate gradient methods Alternative 3: Bayesian approach Prior distribution 설정 Posterior inference 결론 --- Tikhonov regularization의 핵심 장점: 이론적 보장: Strictly convex optimization으로 유일한 해 보장 수치적 안정성: Well-conditioned 문제로 변환 일반성: 다양한 ill-posed problem에 적용 가능 구현 용이성: 기존 최적화 알고리즘 활용 가능 연속형 처치 R-learner에서의 의미: Non-identification 문제의 근본적 해결 실용적인 CATE 추정 방법 제공 이론적 분석과 실제 구현의 교량 역할 이러한 수학적 도구들은 단순히 기술적 문제를 해결하는 것을 넘어서, 인과추론의 근본적인 어려움을 이해하고 극복하는 방법을 제시합니다. --- 참고 문헌: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization Boyd, S., & Vandenberghe, L. (2004). Convex optimization Wahba, G. (1990). Spline models for observational data Engl, H. W., Hanke, M., & Neubauer, A. (2000). Regularization of inverse problems",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/DefinitionOfArt",
    "title": "Definition of Art",
    "date": "2025-07-15",
    "excerpt": "TOEFL Reading 문제 공부 - 예술의 정의",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원"
    ],
    "content": "Definition of Art 예술의 정의 Paragraph 1 <button class=\"toggle-button translations\" data-type=\"paragraph1\">🔍 AI 해석 가리기</button> <div class=\"paragraph1-content\"> > How art is defined has varied throughout history, leading to a wide range of sometimes-contradictory meanings. <span class=\"translation\">예술이 어떻게 정의되는지는 역사를 통틀어 변화해왔으며, 때로는 상반되는 의미들의 광범위한 스펙트럼을 만들어냈다.</span> > In the ancient world, the term was used in a broad sense that was not strictly related to visual arts. <span class=\"translation\">고대 세계에서 이 용어는 시각 예술에 엄격하게 국한되지 않는 광범위한 의미로 사용되었다.</span> > The application of the word \"art\" exclusively to works in the form of paintings, sculptures, and prints came about only in the Renaissance, and later, in the eighteenth century, the use of the term expanded to include music and poetry. <span class=\"translation\">\"예술\"이라는 단어를 회화, 조각, 판화 형태의 작품들에만 독점적으로 적용하는 것은 르네상스 시대에 비로소 나타났으며, 이후 18세기에는 이 용어의 사용이 음악과 시를 포함하도록 확장되었다.</span> > Subsequently, in modern times, a new meaning came into fashion, obscuring the distinction between what is art and what is not. <span class=\"translation\">그 후 현대에 들어서는 새로운 의미가 유행하게 되었고, 무엇이 예술이고 무엇이 예술이 아닌지의 구별을 모호하게 만들었다.</span> </div> Paragraph 2 <button class=\"toggle-button translations\" data-type=\"paragraph2\">🔍 AI 해석 가리기</button> <div class=\"paragraph2-content\"> > In the ancient world, there was no clear difference between art and crafts. <span class=\"translation\">고대 세계에서는 예술과 공예 사이에 명확한 차이가 없었다.</span> > In Greek and Latin, the terms techne and ars both translated as \"art,\" and they stood for any activity that had rules and required expertise. <span class=\"translation\">그리스어와 라틴어에서 테크네(techne)와 아르스(ars)라는 용어들은 둘 다 \"예술\"로 번역되었으며, 규칙을 가지고 전문성을 요구하는 모든 활동을 의미했다.</span> > They were thus used in reference to a wide range of fields, from medicine and law to blacksmithing. <span class=\"translation\">따라서 이 용어들은 의학과 법률에서부터 대장장이 일까지 광범위한 분야들을 지칭하는 데 사용되었다.</span> > Those engaged in the production of art were viewed as craftsmen, more similar to manual laborers than artists. <span class=\"translation\">예술 제작에 종사하는 사람들은 장인으로 여겨졌으며, 예술가보다는 육체 노동자에 더 가까웠다.</span> > To the ancients, art was a pursuit necessitating skills that were learned according to rules, but that were believed to require no unique inspiration. <span class=\"translation\">고대인들에게 예술은 규칙에 따라 습득하는 기술을 필요로 하는 추구였지만, 독특한 영감을 요구하지는 않는다고 여겨졌다.</span> > Painting and sculpture had no special place in higher learning, as only music and poetry were taught as academic subjects. <span class=\"translation\">회화와 조각은 고등 학문에서 특별한 자리를 차지하지 못했는데, 음악과 시만이 학문 과목으로 가르쳐졌기 때문이다.</span> > The visual arts were viewed simply as manual crafts, and because the ancients looked down on manual labor, artisans occupied the bottom of the social hierarchy. <span class=\"translation\">시각 예술은 단순히 수공 기술로 여겨졌으며, 고대인들이 육체 노동을 업신여겼기 때문에 장인들은 사회 계층의 맨 아래를 차지했다.</span> </div> Paragraph 3 <button class=\"toggle-button translations\" data-type=\"paragraph3\">🔍 AI 해석 가리기</button> <div class=\"paragraph3-content\"> > The medieval world inherited this view of art, but things began to change during the Renaissance. <span class=\"translation\">중세 세계는 이러한 예술관을 물려받았지만, 르네상스 시대에 상황이 변화하기 시작했다.</span> > At the forefront of this transformation was the elevated position of painters, sculptors, and architects. <span class=\"translation\">이 변혁의 최전선에는 화가, 조각가, 건축가들의 지위 향상이 있었다.</span> > Renaissance artists increasingly employed scientific methods, and mathematics became central to their trade. <span class=\"translation\">르네상스 예술가들은 점점 더 과학적 방법을 사용했으며, 수학이 그들 기예의 중심이 되었다.</span> > Because their work was not simply mechanical but also intellectual, artists argued that they were superior to mere craftsmen, and their call for respect began to sway public opinion. <span class=\"translation\">그들의 작업이 단순히 기계적인 것이 아니라 지적이기도 했기 때문에, 예술가들은 자신들이 단순한 장인들보다 우월하다고 주장했으며, 존경을 요구하는 그들의 외침이 여론을 움직이기 시작했다.</span> > Artists gradually gained acceptance as creative geniuses, and their reputation rose to be on par with that of poets. <span class=\"translation\">예술가들은 점차 창조적 천재로 인정받게 되었으며, 그들의 명성은 시인들과 동등한 수준으로 올라갔다.</span> > Painting, sculpture, and architecture were given their own place in academies, and art became synonymous with the label \"classical art.\" <span class=\"translation\">회화, 조각, 건축은 학원에서 자신들만의 자리를 부여받았으며, 예술은 \"고전 예술\"이라는 라벨과 동의어가 되었다.</span> > During the 1700s, the definition was slightly expanded as the concept of fine art emerged. <span class=\"translation\">1700년대에는 순수 예술의 개념이 등장하면서 정의가 약간 확장되었다.</span> > Collectively, the classical arts, along with music and poetry, came to represent the fine arts. <span class=\"translation\">종합적으로, 고전 예술들은 음악과 시와 함께 순수 예술을 대표하게 되었다.</span> </div> Paragraph 4 <button class=\"toggle-button translations\" data-type=\"paragraph4\">🔍 AI 해석 가리기</button> <div class=\"paragraph4-content\"> > It was also in the Renaissance that the term \"artisan\" first appeared. <span class=\"translation\">\"장인\"이라는 용어가 처음 등장한 것도 르네상스 시대였다.</span> > The word was used to clearly distinguish between arts and crafts. <span class=\"translation\">이 단어는 예술과 공예를 명확히 구별하기 위해 사용되었다.</span> > Art required academic training, and its goal was to create perfect and idealized representations of reality that followed established principles of beauty. <span class=\"translation\">예술은 학문적 훈련을 요구했으며, 그 목표는 확립된 아름다움의 원칙을 따르는 현실의 완벽하고 이상화된 표현을 창조하는 것이었다.</span> > This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes. <span class=\"translation\">이는 구슬 세공이나 바구니 만들기와 같은 공예와 대조되는데, 이들은 주로 일상적인 목적을 위해 제작되었다.</span> > Craftsmen whose production consisted of such practical, or merely decorative, items were called artisans. <span class=\"translation\">그러한 실용적이거나 단순히 장식적인 물품들을 생산하는 장인들은 아티잔이라고 불렸다.</span> > They often learned their trade from master craftsmen, but they did not study art in the academies. <span class=\"translation\">그들은 종종 숙련된 장인들로부터 기술을 배웠지만, 학원에서 예술을 공부하지는 않았다.</span> > The clear division between arts and crafts saw its peak in the eighteenth century. <span class=\"translation\">예술과 공예 사이의 명확한 구분은 18세기에 절정에 달했다.</span> </div> Paragraph 5 <button class=\"toggle-button translations\" data-type=\"paragraph5\">🔍 AI 해석 가리기</button> <div class=\"paragraph5-content\"> > In the nineteenth century, modernists sought to promote the idea of \"art for art's sake\" and rejected classical definitions. <span class=\"translation\">19세기에 모더니스트들은 \"예술을 위한 예술\"이라는 사상을 촉진하려 했으며 고전적 정의들을 거부했다.</span> > Artists felt hindered by the conventions of classicism and began to ignore form in favor of stylistic expression. <span class=\"translation\">예술가들은 고전주의의 관습에 의해 방해받는다고 느꼈으며 형태를 무시하고 양식적 표현을 선호하기 시작했다.</span> > At the same time, they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society. <span class=\"translation\">동시에, 그들은 예술이 근본적으로 실용적이고 사회에 유익해야 한다고 주장하는 일부 동시대인들의 신념을 거부했다.</span> > Modernists denied that purpose, whether moral or political, was an important criterion for art to have merit. <span class=\"translation\">모더니스트들은 도덕적이든 정치적이든 목적이 예술이 가치를 갖기 위한 중요한 기준이라는 것을 부인했다.</span> > Accordingly, they believed all art had an intrinsic value of its own. <span class=\"translation\">따라서, 그들은 모든 예술이 그 자체로 내재적 가치를 가진다고 믿었다.</span> > In addition, artists experimented with new techniques and subject matter. <span class=\"translation\">또한, 예술가들은 새로운 기법과 주제를 실험했다.</span> > They began to include more impressionistic and abstract designs, emphasizing experimental over learned techniques. <span class=\"translation\">그들은 더 인상주의적이고 추상적인 디자인을 포함하기 시작했으며, 배운 기법보다 실험적인 것을 강조했다.</span> </div> Paragraph 6 <button class=\"toggle-button translations\" data-type=\"paragraph6\">🔍 AI 해석 가리기</button> <div class=\"paragraph6-content\"> > This new perception of art as an end in itself eventually erased the fragmentation between artist and artisan. <span class=\"translation\">그 자체가 목적인 예술에 대한 이 새로운 인식은 결국 예술가와 장인 사이의 분열을 지워버렸다.</span> > In a clear mockery of the classical tradition, the notion of art was extended to include anything a person creates. <span class=\"translation\">고전적 전통에 대한 명백한 조롱으로, 예술의 개념은 사람이 창조하는 모든 것을 포함하도록 확장되었다.</span> > Therefore, it effectively eliminated the distinction between crafts and the so-called fine arts. <span class=\"translation\">따라서, 그것은 공예와 소위 순수 예술 사이의 구별을 효과적으로 제거했다.</span> > A person who produced a basket and a person who painted a canvas in classical style were no different; they were both using their skills to create something. <span class=\"translation\">바구니를 만든 사람과 고전적 양식으로 캔버스에 그림을 그린 사람은 다르지 않았다; 그들은 둘 다 무언가를 창조하기 위해 자신의 기술을 사용하고 있었다.</span> > This meant that any creative work could be viewed as genuine and prestigious. <span class=\"translation\">이것은 모든 창조적 작업이 진정하고 권위 있는 것으로 여겨질 수 있음을 의미했다.</span> > Further, art moved beyond the realm of productive creativity. <span class=\"translation\">더 나아가, 예술은 생산적 창조성의 영역을 넘어섰다.</span> > Some artists randomly collected everyday objects and displayed them, completely unmodified, as \"creative sculpture.\" <span class=\"translation\">일부 예술가들은 일상적인 물건들을 무작위로 수집하여 완전히 수정하지 않은 채로 \"창조적 조각\"으로 전시했다.</span> </div> Paragraph 7 <button class=\"toggle-button translations\" data-type=\"paragraph7\">🔍 AI 해석 가리기</button> <div class=\"paragraph7-content\"> > Conceptions of art have changed remarkably and created much confusion. <span class=\"translation\">예술에 대한 개념들은 현저히 변화했으며 많은 혼란을 만들어냈다.</span> > Today there is no consensus on the meaning of the term, and it is applied very subjectively. <span class=\"translation\">오늘날 그 용어의 의미에 대한 합의가 없으며, 그것은 매우 주관적으로 적용된다.</span> > Evidence can be seen in art museums and galleries devoted to everything from primitive artifacts to video productions to commercial graphics. <span class=\"translation\">증거는 원시 유물에서부터 비디오 제작물, 상업적 그래픽에 이르기까지 모든 것을 다루는 미술관과 갤러리에서 볼 수 있다.</span> > Certainly, the question of what constitutes art has become largely subjective. <span class=\"translation\">확실히, 무엇이 예술을 구성하는가라는 질문은 대체로 주관적인 것이 되었다.</span> </div> --- 📚 핵심 단어 사전 <button class=\"toggle-button vocabulary\" data-type=\"vocabulary\">🔍 단어 뜻 가리기</button> <div class=\"vocabulary-content\"> 🎨 예술사 관련 용어 contradictory <span class=\"meaning\">(상반되는, 모순적인) - 서로 반대되는</span> exclusively <span class=\"meaning\">(독점적으로, 오직) - 다른 것을 배제하고</span> Renaissance <span class=\"meaning\">(르네상스) - 14-16세기 유럽 문예부흥시대</span> obscure <span class=\"meaning\">(모호하게 하다) - 불분명하게 만들다</span> techne <span class=\"meaning\">(테크네) - 그리스어로 \"기술, 기예\"</span> ars <span class=\"meaning\">(아르스) - 라틴어로 \"기술, 예술\"</span> expertise <span class=\"meaning\">(전문성) - 전문적 기술이나 지식</span> blacksmithing <span class=\"meaning\">(대장장이 일) - 철을 다루는 기술</span> craftsmen <span class=\"meaning\">(장인) - 수공업에 종사하는 사람</span> manual laborers <span class=\"meaning\">(육체 노동자) - 손으로 일하는 사람들</span> 🏛️ 학문/교육 관련 용어 academic subjects <span class=\"meaning\">(학문 과목) - 학교에서 가르치는 정규 과목</span> higher learning <span class=\"meaning\">(고등 학문) - 대학 수준의 교육</span> social hierarchy <span class=\"meaning\">(사회 계층) - 사회의 등급 구조</span> academies <span class=\"meaning\">(학원, 아카데미) - 예술이나 학문을 가르치는 기관</span> classical art <span class=\"meaning\">(고전 예술) - 전통적인 양식을 따르는 예술</span> fine arts <span class=\"meaning\">(순수 예술) - 실용성보다 미적 가치를 추구하는 예술</span> 🔧 기술/공예 관련 용어 artisan <span class=\"meaning\">(아티잔, 수공업자) - 숙련된 수공업 종사자</span> beadwork <span class=\"meaning\">(구슬 세공) - 구슬을 이용한 공예</span> basket making <span class=\"meaning\">(바구니 만들기) - 바구니 제작 기술</span> decorative <span class=\"meaning\">(장식적인) - 꾸미는 목적의</span> master craftsmen <span class=\"meaning\">(숙련 장인) - 기술을 완전히 익힌 장인</span> 💭 철학/사상 관련 용어 modernists <span class=\"meaning\">(모더니스트) - 현대주의자들</span> art for art's sake <span class=\"meaning\">(예술을 위한 예술) - 예술의 자율성을 주장하는 사상</span> conventions <span class=\"meaning\">(관습) - 전통적으로 따르는 방식</span> classicism <span class=\"meaning\">(고전주의) - 고전적 양식을 따르는 예술 사조</span> stylistic expression <span class=\"meaning\">(양식적 표현) - 개인적 스타일의 표현</span> utilitarian <span class=\"meaning\">(실용주의적인) - 실용성을 중시하는</span> intrinsic value <span class=\"meaning\">(내재적 가치) - 그 자체가 가진 가치</span> impressionistic <span class=\"meaning\">(인상주의적인) - 인상주의 양식의</span> abstract <span class=\"meaning\">(추상적인) - 구체적이지 않은</span> 🌍 현대 예술 관련 용어 fragmentation <span class=\"meaning\">(분열, 파편화) - 나뉘어진 상태</span> mockery <span class=\"meaning\">(조롱) - 비웃음</span> genuine <span class=\"meaning\">(진정한) - 가짜가 아닌</span> prestigious <span class=\"meaning\">(권위 있는) - 명성이 높은</span> realm <span class=\"meaning\">(영역) - 분야나 범위</span> unmodified <span class=\"meaning\">(수정되지 않은) - 변화시키지 않은</span> consensus <span class=\"meaning\">(합의) - 의견의 일치</span> subjectively <span class=\"meaning\">(주관적으로) - 개인적 견해에 따라</span> primitive artifacts <span class=\"meaning\">(원시 유물) - 고대의 인공 제작물</span> constitutes <span class=\"meaning\">(구성하다) - 이루다, 형성하다</span> </div> --- 💬 중요한 영어 표현들 <button class=\"toggle-button expressions\" data-type=\"expressions\">🔍 표현 뜻 가리기</button> <div class=\"expressions-content\"> 📊 변화/발전 표현 has varied throughout history <span class=\"meaning\">(역사를 통틀어 변화해왔다) - \"How art is defined has varied throughout history\"</span> leading to <span class=\"meaning\">(~로 이어지는) - \"leading to a wide range of meanings\"</span> came about <span class=\"meaning\">(생겨나다, 발생하다) - \"came about only in the Renaissance\"</span> came into fashion <span class=\"meaning\">(유행하게 되다) - \"a new meaning came into fashion\"</span> began to change <span class=\"meaning\">(변화하기 시작했다) - \"things began to change during the Renaissance\"</span> at the forefront of <span class=\"meaning\">(~의 최전선에) - \"At the forefront of this transformation\"</span> 🎯 대조/비교 표현 in contrast to <span class=\"meaning\">(~와 대조적으로) - \"This is in contrast to crafts\"</span> more similar to <span class=\"meaning\">(~에 더 가까운) - \"more similar to manual laborers than artists\"</span> on par with <span class=\"meaning\">(~와 동등한) - \"on par with that of poets\"</span> as opposed to <span class=\"meaning\">(~와는 반대로) - implied in contrasts</span> in a clear mockery of <span class=\"meaning\">(~에 대한 명백한 조롱으로) - \"In a clear mockery of the classical tradition\"</span> ⏰ 시간/순서 표현 subsequently <span class=\"meaning\">(그 후에) - \"Subsequently, in modern times\"</span> gradually <span class=\"meaning\">(점차적으로) - \"Artists gradually gained acceptance\"</span> increasingly <span class=\"meaning\">(점점 더) - \"increasingly employed scientific methods\"</span> eventually <span class=\"meaning\">(결국) - \"eventually erased the fragmentation\"</span> saw its peak <span class=\"meaning\">(절정에 달했다) - \"saw its peak in the eighteenth century\"</span> 🎨 예술 관련 표현 applied exclusively to <span class=\"meaning\">(~에만 독점적으로 적용되다) - \"applied exclusively to works\"</span> became synonymous with <span class=\"meaning\">(~와 동의어가 되다) - \"became synonymous with classical art\"</span> came to represent <span class=\"meaning\">(~를 대표하게 되다) - \"came to represent the fine arts\"</span> in favor of <span class=\"meaning\">(~을 선호하여) - \"in favor of stylistic expression\"</span> beyond the realm of <span class=\"meaning\">(~의 영역을 넘어서) - \"beyond the realm of productive creativity\"</span> 📝 학술적 표현 stood for <span class=\"meaning\">(~를 의미했다) - \"they stood for any activity\"</span> were viewed as <span class=\"meaning\">(~로 여겨졌다) - \"were viewed as craftsmen\"</span> necessitating <span class=\"meaning\">(~를 필요로 하는) - \"necessitating skills\"</span> occupied the bottom of <span class=\"meaning\">(~의 맨 아래를 차지했다) - \"occupied the bottom of the social hierarchy\"</span> inherited this view <span class=\"meaning\">(이 견해를 물려받았다) - \"The medieval world inherited this view\"</span> 🔍 강조/설명 표현 it was also... that <span class=\"meaning\">(~한 것도 바로 ...였다) - \"It was also in the Renaissance that...\"</span> accordingly <span class=\"meaning\">(따라서) - \"Accordingly, they believed\"</span> in addition <span class=\"meaning\">(또한) - \"In addition, artists experimented\"</span> therefore <span class=\"meaning\">(따라서) - \"Therefore, it effectively eliminated\"</span> certainly <span class=\"meaning\">(확실히) - \"Certainly, the question has become\"</span> 💭 의견/판단 표현 argued that <span class=\"meaning\">(~라고 주장했다) - \"artists argued that they were superior\"</span> believed to require <span class=\"meaning\">(~를 요구한다고 여겨졌다) - \"believed to require no unique inspiration\"</span> denied that <span class=\"meaning\">(~라는 것을 부인했다) - \"Modernists denied that purpose\"</span> sought to promote <span class=\"meaning\">(~를 촉진하려 했다) - \"sought to promote the idea\"</span> felt hindered by <span class=\"meaning\">(~에 의해 방해받는다고 느꼈다) - \"felt hindered by the conventions\"</span> </div> --- 문제풀이 TOEFL Reading Comprehension Questions Question 12 Which of the following is consistent with the ancient view of art as described in paragraph 2? (A) Visual art was not an undertaking worthy of esteem. (B) Art was an activity that required special inspiration. (C) Visual art occupied a special place in higher learning. (D) Art applied to any pursuit involving formal education. <button class=\"toggle-button answer\" data-type=\"answer12\">🔍 정답 및 해설 가리기</button> <div class=\"answer12-content\"> 정답: (A) Visual art was not an undertaking worthy of esteem. 해설: 2단락에서 고대인들의 예술관: 핵심 근거: \"The visual arts were viewed simply as manual crafts\" \"because the ancients looked down on manual labor, artisans occupied the bottom of the social hierarchy\" \"Those engaged in the production of art were viewed as craftsmen, more similar to manual laborers than artists\" 고대인들은 시각 예술을 단순한 수공 기술로 여겼고, 육체 노동을 업신여겼기 때문에 시각 예술은 존경받을 만한 일이 아니었습니다. 오답 분석: (B) \"believed to require no unique inspiration\" - 특별한 영감이 필요하지 않다고 여김 (C) \"Painting and sculpture had no special place in higher learning\" - 고등 학문에서 특별한 자리 없음 (D) 공식 교육이 아닌 \"rules and required expertise\"를 요구하는 모든 활동에 적용 </div> Question 13 Which of the following is NOT listed as an example of fine art in paragraph 3? (A) Sculpture (B) Architecture (C) Mathematics (D) Music <button class=\"toggle-button answer\" data-type=\"answer13\">🔍 정답 및 해설 가리기</button> <div class=\"answer13-content\"> 정답: (C) Mathematics 해설: 3단락에서 언급된 순수 예술(fine arts)들: 명시적으로 언급된 것들: \"Painting, sculpture, and architecture were given their own place in academies\" \"Collectively, the classical arts, along with music and poetry, came to represent the fine arts\" 수학의 역할: \"mathematics became central to their trade\"로 언급되지만, 이는 예술가들이 사용하는 도구로서의 수학이지, 수학 자체가 순수 예술로 분류된 것은 아닙니다. 순수 예술로 언급된 것들: (A) Sculpture ✓ - 명시적 언급 (B) Architecture ✓ - 명시적 언급 (D) Music ✓ - \"along with music and poetry\" 언급 </div> Question 14 Which of the following is a distinguishing feature of crafts according to paragraph 4? (A) They were made for decoration. (B) They were produced in large quantities. (C) They were produced by artisans in the academies. (D) They were made to closely follow aesthetic principles. <button class=\"toggle-button answer\" data-type=\"answer14\">🔍 정답 및 해설 가리기</button> <div class=\"answer14-content\"> 정답: (A) They were made for decoration. 해설: 4단락에서 공예의 특징: 핵심 근거: \"This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes\" \"Craftsmen whose production consisted of such practical, or merely decorative, items were called artisans\" 공예는 실용적(practical)이거나 단순히 장식적(decorative)인 목적으로 만들어졌다고 명시되어 있습니다. 예술과의 대조: 예술: \"create perfect and idealized representations of reality that followed established principles of beauty\" 공예: \"fashioned largely for everyday purposes\" + \"merely decorative\" 오답 분석: (B) 대량 생산에 대한 언급 없음 (C) \"they did not study art in the academies\" - 학원에서 공부하지 않음 (D) 미적 원칙을 따르는 것은 예술의 특징 </div> Question 15 The word \"convictions\" in the passage is closest in meaning to (A) intense oppositions (B) strong beliefs (C) wrong assumptions (D) feeble suggestions <button class=\"toggle-button answer\" data-type=\"answer15\">🔍 정답 및 해설 가리기</button> <div class=\"answer15-content\"> 정답: (B) strong beliefs 해설: 5단락의 문맥에서 \"convictions\" 사용: 원문: \"they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society\" 문맥 분석: 모더니스트들이 거부한 것은 동시대인들의 신념/확신이었습니다. 이들은 예술이 실용적이고 사회에 유익해야 한다고 강하게 믿었던 사람들이었습니다. 어휘 분석: conviction = 확신, 신념 (강한 믿음) (A) intense oppositions: 강한 반대 - 문맥에 맞지 않음 (B) strong beliefs: 강한 믿음 ✓ (C) wrong assumptions: 잘못된 가정 - 가치 판단적 (D) feeble suggestions: 약한 제안 - conviction의 강한 의미와 반대 </div> Question 16 What can be inferred from paragraph 5 about artists in the nineteenth century? (A) Some of them thought that the primary value of art was in its social purpose. (B) Most of them approached art as an academic discipline that required learning. (C) They placed a greater emphasis on aesthetics than did classical artists. (D) They viewed moral and political art as having no intrinsic value. <button class=\"toggle-button answer\" data-type=\"answer16\">🔍 정답 및 해설 가리기</button> <div class=\"answer16-content\"> 정답: (A) Some of them thought that the primary value of art was in its social purpose. 해설: 5단락에서 19세기 예술가들의 상황: 핵심 근거: \"At the same time, they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society\" 이 문장은 모더니스트들이 거부한 동시대인들의 견해를 설명합니다. 즉, 19세기에는 일부 예술가들이 예술의 주된 가치가 사회적 목적에 있다고 생각했다는 것을 추론할 수 있습니다. 논리 구조: 모더니스트들이 \"art for art's sake\"를 주장 그들이 거부한 것: 예술이 실용적이고 사회에 유익해야 한다는 견해 → 따라서 그런 견해를 가진 사람들이 존재했음을 의미 오답 분석: (B) 모더니스트들은 오히려 전통적 학문적 접근을 거부 (C) 형태보다 양식적 표현을 강조 - 순수 미학적 접근은 아님 (D) 모더니스트들이 거부한 것이지, 모든 예술가가 그런 것은 아님 </div> Question 17 What is the purpose of paragraph 6 in the passage? (A) To introduce the reasons for the changes outlined in the preceding paragraphs (B) To describe the resulting outcome of the ideas described in the previous paragraph (C) To offer supporting evidence for the ideas introduced in the previous paragraph (D) To provide a transition between the preceding paragraphs and the final paragraph <button class=\"toggle-button answer\" data-type=\"answer17\">🔍 정답 및 해설 가리기</button> <div class=\"answer17-content\"> 정답: (B) To describe the resulting outcome of the ideas described in the previous paragraph 해설: 단락들 간의 논리적 연결: 5단락 (원인): 모더니스트들의 사상 \"art for art's sake\" 예술의 내재적 가치 실험적 기법 추구 6단락 (결과): 이러한 사상의 결과 \"This new perception of art as an end in itself eventually erased the fragmentation between artist and artisan\" \"effectively eliminated the distinction between crafts and the so-called fine arts\" 예술 개념의 극단적 확장 논리적 구조: 5단락: 모더니스트 사상 소개 (원인) 6단락: \"This new perception\" → 그 결과 설명 연결어: \"This\" = 앞 단락의 내용을 받음 오답 분석: (A) 변화의 이유 소개 - 5단락에서 이미 다룸 (C) 지지 증거 제공 - 새로운 결과를 설명하는 것 (D) 전환 역할 - 6단락 자체가 실질적 내용을 담고 있음 </div> Question 18 The word \"prestigious\" in the passage is closest in meaning to (A) obscure (B) ubiquitous (C) reputable (D) credulous <button class=\"toggle-button answer\" data-type=\"answer18\">🔍 정답 및 해설 가리기</button> <div class=\"answer18-content\"> 정답: (C) reputable 해설: 6단락의 문맥에서 \"prestigious\" 사용: 원문: \"This meant that any creative work could be viewed as genuine and prestigious\" 문맥 분석: 모든 창조적 작업이 진정하고(genuine) 권위 있는(prestigious) 것으로 여겨질 수 있다는 의미입니다. 여기서 \"prestigious\"는 높은 지위나 명성을 가진이라는 뜻입니다. 어휘 분석: prestigious = 권위 있는, 명성이 높은 (A) obscure: 모호한, 잘 알려지지 않은 - 정반대 의미 (B) ubiquitous: 어디에나 있는 - 관련 없음 (C) reputable: 평판이 좋은, 명성 있는 ✓ (D) credulous: 잘 믿는, 순진한 - 관련 없음 동의어 관계: prestigious ≈ reputable ≈ respected ≈ esteemed </div> Question 19 Look at the four squares [■] that indicate where the following sentence could be added to the passage. Other examples of crafts included studio pottery and metalworking. Where would the sentence best fit? <button class=\"toggle-button answer\" data-type=\"answer19\">🔍 정답 및 해설 가리기</button> <div class=\"answer19-content\"> 적절한 위치: 4단락의 세 번째 [■] 뒤 해설: 삽입할 문장: \"Other examples of crafts included studio pottery and metalworking.\" (공예의 다른 예시들로는 스튜디오 도자기와 금속 공예가 포함되었다.) 4단락의 구조: \"artisan\" 용어의 등장 예술과 공예의 구별 [■ 여기] \"This is in contrast to crafts, like beadwork or basket making...\" 아티잔의 정의 아티잔의 교육 방식 예술과 공예 구분의 절정 가장 적절한 위치 분석: 세 번째 [■] 뒤가 가장 적절: 앞 문장: \"This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes\" 삽입 문장: \"Other examples of crafts included studio pottery and metalworking\" 뒤 문장: \"Craftsmen whose production consisted of such practical...\" 논리적 연결성: 공예의 첫 번째 예시 제시 (beadwork, basket making) → 공예의 추가 예시 제공 (pottery, metalworking) → 이러한 공예품을 만드는 사람들의 명칭 설명 \"Other examples\"라는 표현이 앞서 제시된 예시들을 받아서 추가 정보를 제공하는 역할 </div> Question 20 Directions: An introductory sentence for a brief summary of the passage is provided below. Complete the summary by selecting the THREE answer choices that express the most important ideas in the passage. Some sentences do not belong in the summary because they express ideas that are not presented in the passage or are minor ideas in the passage. This question is worth 2 points. The term \"art\" has changed so much that its meaning has become rather subjective. Answer Choices: (A) In the Latin and Greek worlds, art was looked down upon as a form of manual labor. (B) Artists of the modern period began to experiment by combining classical and abstract techniques in their art. (C) After the medieval period, the number of artists attending academies rose due to increased prestige in the arts. (D) The ancients applied the term in a broad manner to any rule-based activity that required skill. (E) Modernists, hoping to revolutionize the way art was conceived, completely redefined the term. (F) As artists began to gain status, the distinction between arts and crafts became more apparent. <button class=\"toggle-button answer\" data-type=\"answer20\">🔍 정답 및 해설 가리기</button> <div class=\"answer20-content\"> 정답: (D), (E), (F) 해설: 선택된 답안들: (D) The ancients applied the term in a broad manner to any rule-based activity that required skill. ✓ 2단락: \"they stood for any activity that had rules and required expertise\" \"from medicine and law to blacksmithing\" 고대의 광범위한 예술 개념이 주요 주제 (E) Modernists, hoping to revolutionize the way art was conceived, completely redefined the term. ✓ 5-6단락: 모더니스트들의 혁명적 변화 \"art for art's sake\", 예술과 공예의 구분 제거 현대적 예술 개념의 핵심 변화 (F) As artists began to gain status, the distinction between arts and crafts became more apparent. ✓ 3-4단락: 르네상스 시대 예술가 지위 향상 \"artisan\" 용어 등장, 예술과 공예의 명확한 구분 역사적 발전 과정의 중요한 단계 제외된 답안들: (A) 고대에서 예술이 업신여겨진 것은 맞지만, 핵심은 \"광범위한 적용\" (B) 고전과 추상 기법의 결합은 언급되지 않음 (C) 학원 출석자 수 증가는 세부 사항, 주요 아이디어 아님 요약의 논리적 흐름: 고대: 광범위한 적용 (D) 르네상스: 지위 향상과 구분 명확화 (F) 현대: 모더니스트의 재정의 (E) </div>",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-15-정신똑바로차리고있어",
    "title": "정신 똑바로 차리고 있어",
    "date": "2025-07-15",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "고민 및 조언하는 표햔"
    ],
    "content": "배운 것들 Be on the lookout : 정신 똑바로 차려 Be on the lookout for pickpockets Be on the lookout for voicefishing Be careful with Be careful with your bag on the subway. in + 자동차 (타서 앉으면 끝) on + 비행기, 배, 지하철 passport catch a cold Be careful not to catch a cold Be careful not to burn yourself Be careful not to hurt yourself During this outbreak? outbreak: 질병 발생기간 scammers : 사기꾼 Make sure not to go 간단한 작문 As usaul, I talk to myself. Hey MK, be on the lookout for 나태함. Be careful with your habit that do work later (미루는 습관) Nice work today, just keep this pace. But don`t forget happy is around you. Feel happy with 사소한 thing around you, and victories you made in today. 수정된 작문 (Improved Version) As usual, time for my daily self-talk. Hey MK, don't get lazy on me! Stop putting things off - let's break this procrastination habit once and for all. *procrastination: 꾸물거리다 예문: \"Stop procrastinating and start studying!\" (미루지 말고 공부 시작해!) \"Procrastination is the thief of time.\" (미루기는 시간 도둑이다) \"I need to overcome my procrastination habit.\" (미루는 습관을 고쳐야 해) You crushed it today, keep this energy going! And hey, don't forget to enjoy the little things around you. Take a moment to appreciate the small wins and celebrate what you accomplished today. 피드백 (Feedback) 개선된 점들: 문법적 수정 \"As usaul\" → \"As usual\" (철자 오류 수정) \"do work later\" → \"Stop putting things off\" (더 직접적이고 자연스러운 표현) \"happy is around you\" → \"enjoy the little things around you\" (자연스러운 문장 구조) 어휘 선택 개선 (더 캐주얼하고 현대적으로) \"나태함\" → \"don't get lazy on me!\" (친근하고 에너지 넘치는 표현) \"Nice work\" → \"You crushed it\" (현대적이고 임팩트 있는 칭찬) \"keep this pace\" → \"keep this energy going\" (더 자연스러운 표현) \"사소한 thing\" → \"little things\", \"small wins\" (일상적인 표현) 문장 구조 개선 너무 긴 문장을 짧게 나누어 읽기 쉽게 만듦 명령형을 사용해 더 직접적이고 동기부여적으로 표현 실제 자기 대화에서 쓸 법한 자연스러운 톤으로 변경 전체적인 톤 개선 Formal한 표현을 캐주얼하고 친근한 표현으로 변경 감탄사(\"hey\") 추가로 더 생동감 있는 대화체 구현 짧고 임팩트 있는 문장으로 동기부여 효과 극대화 핵심 학습 포인트: 자기 대화는 캐주얼하고 직접적일수록 자연스럽다 현대적인 슬랭 표현(\"crushed it\")도 적절히 활용하기 너무 긴 문장보다는 짧고 명확한 문장이 효과적 감정을 담은 표현(\"don't get lazy on me!\")이 더 동기부여가 된다 앞으로 연습할 점: 한국어와 영어가 섞인 부분을 피하고 완전한 영어 문장으로 작성하기 현대적이고 자연스러운 구어체 표현 더 학습하기 감정을 담은 생동감 있는 표현 연습하기",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/user-based-collaborative-filtering",
    "title": "User-Based Collaborative Filtering",
    "date": "2025-07-15",
    "excerpt": "User-Based Collaborative Filtering 알고리즘에 대한 설명",
    "category": "Recommendation",
    "tags": [
      "추천시스템"
    ],
    "content": "User-Based Collaborative Filtering (사용자 기반 협업 필터링) 개요 User-Based Collaborative Filtering은 추천 시스템에서 가장 오래되고 직관적인 이웃(neighborhood) 기반 방법입니다. 핵심 아이디어는 \"비슷한 취향을 가진 사용자들이 비슷한 아이템을 좋아할 것이다\" 라는 가정에 기반합니다. 기본 개념 사용자-아이템 평점 행렬 $R$: 행은 사용자, 열은 아이템 원소 $r_{u,i}$: 사용자 $u$가 아이템 $i$에 매긴 평점 (또는 암시적 행동) 목표: 사용자 $u$에게 아이템 $i$를 추천할지 결정 알고리즘 절차 유사도(Similarity) 계산 사용자 간 유사도를 계산하는 방법들: Cosine Similarity $$\\text{sim}(u,v) = \\frac{\\mathbf{r}_u \\cdot \\mathbf{r}_v}{\\ \\mathbf{r}_u\\ \\times \\ \\mathbf{r}_v\\ }$$ Pearson Correlation $$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)(r_{v,i} - \\bar{r}_v)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_v)^2}}$$ 여기서 $I_{uv}$는 두 사용자가 모두 평가한 아이템 집합 Adjusted Cosine Similarity $$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)(r_{v,i} - \\bar{r}_i)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_i)^2}}$$ 여기서 $\\bar{r}_i$는 아이템 $i$의 평균 평점 Top-K 이웃 선택 사용자 $u$와 유사도가 가장 높은 $K$명의 사용자 이웃 집합 $N_u$ 선택 평점 예측 / 추천 점수 계산 가중 평균 방식 $$\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N_u} \\text{sim}(u,v) \\times (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N_u} \\text{sim}(u,v) }$$ 단순 가중합 방식 $$\\hat{r}_{u,i} = \\frac{\\sum_{v \\in N_u} \\text{sim}(u,v) \\times r_{v,i}}{\\sum_{v \\in N_u} \\text{sim}(u,v) }$$ 추천 리스트 생성 아직 소비하지 않은 아이템 중 $\\hat{r}_{u,i}$가 높은 순으로 $N$개 반환 장점 직관성: 구현이 간단하고 이해하기 쉬움 New Item Cold-Start 대응: 새로운 아이템이 들어와도 비슷한 사용자가 평가했다면 곧바로 추천 가능 미묘한 패턴 포착: 사용자 선호의 세밀한 패턴을 이웃 기반으로 포착 가능 해석 가능성: 왜 이 아이템을 추천했는지 설명 가능 단점 희소성(Sparsity): 평점 행렬이 희소하면 공통 평가 아이템이 적어 유사도 계산이 불안정 확장성(Scalability): 사용자 수가 늘어날수록 전체 유사도 계산 비용 $O( U ^2)$ 사용자 Cold-Start: 새 사용자는 평점이 없어 추천 불가 Popularity Bias: 많은 평점을 남긴 헤비 유저에게 유사도가 집중될 수 있음 Gray Sheep 문제: 독특한 취향을 가진 사용자는 유사한 이웃을 찾기 어려움 개선 기법 Significance Weighting 공통 평가 아이템 수가 적으면 유사도에 패널티 부여 Neighborhood Size 최적화 $K$값 튜닝 유사도 threshold 설정 정규화 및 보정 Normalization (평균·표준편차 제거) Shrinkage Baseline 보정 (글로벌 평균 + user bias + item bias) Hybrid 접근법 사용자 기반 CF + 아이템 기반 CF 결합 콘텐츠 기반 특징과 결합 모델 기반(행렬 분해)과 스위칭 아이템 기반 CF와 비교 구분 User-Based CF Item-Based CF ------ --------------- --------------- 가정 비슷한 사용자가 비슷한 아이템을 좋아함 비슷한 아이템을 소비한 사용자는 향후 비슷한 아이템을 소비 적합한 상황 사용자 행동이 풍부하고 맞춤형 관계 파악 필요 대량 사용자, 상대적으로 적은 아이템 확장성 사용자 수에 따라 성능 저하 아이템 수에 따라 성능 저하 Cold-Start New Item에 강함, New User에 약함 New User에 강함, New Item에 약함 구현 시 고려사항 라이브러리 활용 Scikit-surprise: 다양한 CF 알고리즘 지원 Implicit: 암시적 피드백 기반 CF LightFM: 하이브리드 모델 성능 최적화 오프라인 계산: 유사도 행렬을 미리 계산하여 캐시 샘플링: 대규모 데이터에서 랜덤 샘플링으로 계산량 감소 Locality Sensitive Hashing: 근사 유사도 계산으로 속도 향상 실시간 처리 주기적 재계산 + 캐시로 대응 증분 업데이트 방식 고려 A/B 테스트 $K$값, 유사도 종류, 정규화 방식 튜닝 추천 품질과 성능 간 균형점 찾기 Python 구현 예시 ``python import numpy as np from scipy.spatial.distance import cosine from scipy.stats import pearsonr class UserBasedCF: def __init__(self, k=10): self.k = k self.user_similarities = None def fit(self, ratings_matrix): \"\"\"사용자 유사도 행렬 계산\"\"\" n_users = ratings_matrix.shape[0] self.user_similarities = np.zeros((n_users, n_users)) for i in range(n_users): for j in range(i+1, n_users): # 공통 평가 아이템 찾기 common_items = np.where((ratings_matrix[i] != 0) & (ratings_matrix[j] != 0))[0] if len(common_items) < 2: similarity = 0 else: # Pearson correlation 계산 user_i_ratings = ratings_matrix[i, common_items] user_j_ratings = ratings_matrix[j, common_items] try: similarity, _ = pearsonr(user_i_ratings, user_j_ratings) if np.isnan(similarity): similarity = 0 except: similarity = 0 self.user_similarities[i, j] = similarity self.user_similarities[j, i] = similarity def predict(self, user_id, item_id, ratings_matrix): \"\"\"특정 사용자의 특정 아이템 평점 예측\"\"\" if self.user_similarities is None: raise ValueError(\"Model must be fitted first\") # 사용자의 평균 평점 user_ratings = ratings_matrix[user_id] user_mean = np.mean(user_ratings[user_ratings != 0]) # 이웃 사용자들 찾기 similarities = self.user_similarities[user_id] neighbors = np.argsort(similarities)[::-1][1:self.k+1] numerator = 0 denominator = 0 for neighbor in neighbors: if similarities[neighbor] <= 0: continue neighbor_rating = ratings_matrix[neighbor, item_id] if neighbor_rating == 0: continue neighbor_mean = np.mean(ratings_matrix[neighbor][ratings_matrix[neighbor] != 0]) numerator += similarities[neighbor] * (neighbor_rating - neighbor_mean) denominator += abs(similarities[neighbor]) if denominator == 0: return user_mean predicted_rating = user_mean + (numerator / denominator) return max(1, min(5, predicted_rating)) # 평점 범위 제한 `` 결론 User-Based CF는 추천 시스템의 기본이 되는 직관적이고 효과적인 방법입니다. 하지만 데이터 희소성과 확장성 한계로 인해, 대규모 서비스에서는 다른 기법(아이템 기반, 행렬 분해, 딥러닝)과 결합하여 사용하는 것이 일반적입니다. 실제 적용 시에는 비즈니스 도메인, 데이터 특성, 성능 요구사항을 종합적으로 고려하여 적절한 하이브리드 접근법을 선택하는 것이 중요합니다. --- 참고 자료: Collaborative Filtering for Implicit Feedback Datasets Item-based collaborative filtering recommendation algorithms Matrix Factorization Techniques for Recommender Systems",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/tilde-tau-explained",
    "title": "Why Do We Introduce the Centered Function tilde_tau(x,t)?",
    "date": "2025-07-15",
    "excerpt": "The role of the intermediate, mean-zero CATE in the continuous-treatment R-learner.",
    "category": "Causal Inference",
    "tags": [
      "Concept",
      "R-learner",
      "Continuous Treatment"
    ],
    "content": "The problem in one sentence R-loss를 그대로 최소화하면 $$ h(x,t) = \\tau(x,t) + s(x) $$ 형태로 x 에만 의존하는 덧붙임 함수 $s(x)$ 가 있어도 손실값이 변하지 않는다. 그래서 최소값이 무한히 많아져 해가 유일하지 않다. --- Where does s(x) come from? R-loss에 $\\mathbb{E}[ h(X,T) \\mid X ]$ 항이 들어가므로 $h(x,t) + s(x)$ 를 넣어도 $s(x)$ 가 자연스럽게 상쇄된다. 그 결과 손실값이 동일하니 $s(x)$ 를 얼마든지 붙일 수 있다. 이것이 non-identification(식별 불능) 문제의 근원이다. --- Idea: “center” the function 중간 함수 $\\tilde{\\tau}(x,t)$ 를 다음처럼 정의한다. $$ \\tilde{\\tau}(x,t) = \\tau(x,t) - \\mathbb{E}[ \\tau(X,T) \\mid X = x ] $$ 즉, 각 x 마다 t 전역 평균이 0 이 되도록 중심화한다. mean-zero 성질 때문에 $h(x,t)$ 와 $h(x,t)+s(x)$ 의 차이점이 R-loss 안에서는 지워지지 않는다. 실제로 $h(X,T)+s(X) - \\mathbb{E}[h(X,T)+s(X)\\mid X] = h(X,T)-\\mathbb{E}[h(X,T)\\mid X]$ 이므로, R-loss 값은 그대로다. 따라서 중심화만으로는 식별 문제가 완전히 사라지지 않는다. 결정적인 열쇠는 다음 단계의 L2 정규화다. L2 패널티 $\\rho\\,\\ h\\ _2^2$ 가 추가되면, $s(x)$ 를 붙이는 순간 노름이 커져서 목적함수가 증가한다. 최적화는 $s(x)=0$ 을 선택해 유일해를 만든다. 즉 $\\tilde{\\tau}$ 는 \"mean-zero\" 조건을 만족하는 후보 중 하나이며, 식별을 완성하는 것은 정규화라는 점을 기억해야 한다. --- How regularisation fits in Step-1 (Tikhonov) 정규화된 최적화 문제를 푼다. $$ \\tau_{\\rho} = \\operatorname{arg\\,min}_{h} \\Bigl\\{ \\text{R-loss}(h) + \\rho \\, \\ h \\ _2^2 \\Bigr\\} $$ L2 패널티가 들어가면 문제가 엄밀히 convex 해져서 유일한 해가 보장된다. Step-2 (Scaling) 이론적으로 $\\tilde{\\tau} = (1 + \\rho)\\, \\tau_{\\rho}$ 임이 증명된다(Theorem 1). Step-3 (Zero-constraining operator) 마지막으로 $$ \\mathcal{C}(f)(x,t) = f(x,t) - f(x,0) $$ 를 적용하면 실제 관심 함수 $\\tau(x,t)$ 를 복원한다. --- Practical take-aways 모델링 – 신경망 같은 유연한 함수 근사기를 쓸 때도 $\\tilde{\\tau}$ 를 학습하면 식별 문제가 자동으로 해결된다. 수치 안정성 – L2 정규화(ρ>0)가 없으면 여전히 ill-posed; ρ 값은 크지 않아도 \"유일 최소\" 확보에 충분하다. 해석 – $\\tilde{\\tau}$ 는 \"x 조건부에서 t 변화만 담은 순수 효과\"라서 $s(x)$(x 전용 편향)와 깔끔히 분리된다. --- Mini-checklist for implementation 두 nuisance 함수 m(x) 와 varpi(t x) 추정 R-loss + L2 패널티로 $\\tau_{\\rho}$ 계산 $$\\tilde{\\tau} = (1 + \\rho)\\, \\tau_{\\rho}$$ $$\\hat{\\tau}(x,t) = \\tilde{\\tau}(x,t) - \\tilde{\\tau}(x,0)$$ --- Related reading Robinson (1988) – partially linear models Nie & Wager (2021) – Quasi-Oracle R-learner Kennedy et al. (2017) – continuous-treatment GPS Tikhonov (1963) – regularisation for ill-posed problems --- 3.5 Why do we still center? L2 정규화가 최종적으로 $s(x)$ 를 제거하더라도, 중심화가 선행돼야 하는 실용적 이유가 있습니다. Canonical representative – 같은 R-loss 값을 갖는 동치류 $\\{h+s(x)\\}$ 중에서 $\\mathbb{E}[h\\mid X]=0$ 인 함수가 가장 작은 $L_2$ 노름을 가지므로, 정규화가 자연스럽게 그 함수를 선택하게 된다. 닫힌 형태 해 – Theorem 1 의 $(1+\\rho)^{-1}$ 스케일 관계는 $\\mathbb{E}[h\\mid X]=0$ 조건이 있어야 성립한다. Zero-constraining operator – 마지막 단계에서 $\\mathcal{C}(f)(x,t)=f(x,t)-f(x,0)$ 를 적용할 때, 중심화돼 있어야 정확히 $\\tau(x,t)$ 로 복원된다. ---",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/준비전략서치",
    "title": "어떻게 공부해야할지 서치하고 정리",
    "date": "2025-07-14",
    "excerpt": "OMSCS 지원을 위한 토플 공부",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/Fact and Negative Fact",
    "title": "Fact와 Negative Fact 차이",
    "date": "2025-07-14",
    "excerpt": "Fact와 Negative Fact 유형 풀이법",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/BirdDialects",
    "title": "Bird Dialects",
    "date": "2025-07-14",
    "excerpt": "Diagnostic Test 문제 공부",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원"
    ],
    "content": "Bird Dialects 새 다이러렉츠? 새의 방언 Paragraph 1 <button class=\"toggle-button translations\" data-type=\"paragraph1\">🔍 AI 해석 가리기</button> <div class=\"paragraph1-content\"> > Birds learn acoustic information from their parents and environment from an early age. <span class=\"translation\">새들은 어린 나이부터 부모와 환경으로부터 음향 정보를 배운다.</span> > The most complex form of avian vocalization is the birdsong, an often melodic series of communicative sounds produced primarily during breeding. <span class=\"translation\">조류의 발성 중 가장 복잡한 형태는 새의 노래로, 번식기에 주로 발생하는 종종 선율적인 의사소통 소리의 연속이다.</span> > In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects. <span class=\"translation\">개체군이 지리적 경계를 가진 경우, 노래에서 차이가 나타날 수 있으며, 이러한 지리적 한계가 뚜렷하면, 한 개체군 내에서 공유되는 노래 유형을 방언이라고 부른다.</span> > Ornithologists are keenly interested in how these dialects form and to what extent genetic factors contribute to dialectical variation. <span class=\"translation\">조류학자들은 이러한 방언이 어떻게 형성되는지와 유전적 요인이 방언적 변화에 어느 정도 기여하는지에 깊은 관심을 가지고 있다.</span> > One of the best-studied songbirds in this regard is the white-crowned sparrow, Zonotrichia leucophrys, a species with several subspecies and many dialects. <span class=\"translation\">이와 관련하여 가장 잘 연구된 명금 중 하나는 흰관찰찌새(Zonotrichia leucophrys)로, 여러 아종과 많은 방언을 가진 종이다.</span> </div> Paragraph 2 <button class=\"toggle-button translations\" data-type=\"paragraph2\">🔍 AI 해석 가리기</button> <div class=\"paragraph2-content\"> > In Zonotrichia leucophrys, males demonstrate the greatest dialectical diversity. Therefore, studying male birds offers the best opportunity for understanding dialect formation. <span class=\"translation\">흰관찰찌새(Zonotrichia leucophrys)에서 수컷이 가장 큰 방언적 다양성을 보인다. 따라서 수컷 새를 연구하는 것이 방언 형성을 이해하는 데 가장 좋은 기회를 제공한다.</span> > In order to make sense of birdsongs, scientists break them into smaller units. <span class=\"translation\">새의 노래를 이해하기 위해 과학자들은 그것들을 더 작은 단위로 나눈다.</span> > A phrase, for example, is a large category below the level of song, and specific sound types like whistles, pulses, and buzzes reflect the individual speech parts, or syllables, of a phrase. <span class=\"translation\">예를 들어, 구문은 노래 수준 아래의 큰 범주이며, 휘파람, 펄스, 윙윙거리는 소리와 같은 특정 소리 유형은 구문의 개별 발화 부분, 즉 음절을 반영한다.</span> > The phrases are ordered according to their position in the song so that specific parts can be analyzed separately and compared across different individuals. <span class=\"translation\">구문들은 노래에서의 위치에 따라 순서가 정해져서, 특정 부분들을 개별적으로 분석하고 서로 다른 개체들 간에 비교할 수 있다.</span> </div> Paragraph 3 <button class=\"toggle-button translations\" data-type=\"paragraph3\">🔍 AI 해석 가리기</button> <div class=\"paragraph3-content\"> > Some phrases show more variability than others, and once scientists detect variability in a specific category, they can focus on that section of the songs in future studies. <span class=\"translation\">일부 구문들은 다른 구문들보다 더 많은 변이성을 보이며, 과학자들이 특정 범주에서 변이성을 발견하면, 향후 연구에서 노래의 그 부분에 집중할 수 있다.</span> > In other words, if the terminal phrase shows the greatest inconsistency, recordings can be edited to single it out for comparisons. <span class=\"translation\">다시 말해서, 만약 마지막 구문이 가장 큰 불일치성을 보인다면, 녹음을 편집하여 비교를 위해 그것만 따로 분리할 수 있다.</span> > Most white-crowned sparrow songs begin similarly with one or two whistles; consequently, the initial sequence is generally the most consistent component across subgroups. <span class=\"translation\">대부분의 흰관찰찌새 노래는 하나 또는 두 개의 휘파람으로 비슷하게 시작한다. 결과적으로, 초기 순서는 일반적으로 아종들 간에 가장 일관된 구성 요소이다.</span> > In addition, a convenient feature of male white-crowned sparrows is that individual adults generally sing a single song, making analysis across groups a relatively simple task in comparison to other bird species that routinely learn multiple songs, such as song sparrows (Melospiza melodia). <span class=\"translation\">또한, 수컷 흰관찰찌새의 편리한 특징 중 하나는 개별 성체가 일반적으로 하나의 노래만 부른다는 것으로, 이는 여러 노래를 정기적으로 배우는 노래찌새(Melospiza melodia)와 같은 다른 조류 종에 비해 집단 간 분석을 상대적으로 간단한 작업으로 만든다.</span> </div> Paragraph 4 <button class=\"toggle-button translations\" data-type=\"paragraph4\">🔍 AI 해석 가리기</button> <div class=\"paragraph4-content\"> > Variations present among groups led some scientists to wonder whether there might be a genetic basis for dialectical uniqueness. <span class=\"translation\">집단들 간에 존재하는 변이들은 일부 과학자들이 방언적 고유성에 유전자적인 기저가 있는지 궁금하게 했다.</span> > Perhaps through the evolutionary isolation of some populations (for example, female preference for males of their own dialect), dispersal of the gene pool has been slowed to the point that differences in song reflect expressions of varying genetic compositions. <span class=\"translation\">아마도 일부 개체군의 진화적 격리를 통해 (예를 들어, 암컷이 자신의 방언을 가진 수컷을 선호하는 것), 유전자 풀의 분산이 느려져서 노래의 차이가 다양한 유전적 구성의 표현을 반영하게 되었을 것이다.</span> > Though a genetic explanation seems plausible, laboratory experiments have found that white-crowned sparrow chicks of particular dialect groups can learn the songs of other groups in their own subspecies in addition to dialects of other subspecies. <span class=\"translation\">유전자적 해석이 말이 플러서블 하다고 볼 수도 있지만, 실험에서 하얀 군중 까마귀는 다른 아종의 노래나 방언을 배울 수 있다.</span> > Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects, although the maximum number is approximately seven. <span class=\"translation\">성체의 녹음을 사용하여, 과학자들은 어린 새들이 여러 방언을 재현하도록 훈련시킬 수 있었는데, 최대 개수는 약 7개였다.</span> > In addition, comparisons of genetic material taken from blood sample DNA have revealed unsubstantial differences. <span class=\"translation\">또한, 혈액 샘플 DNA에서 추출한 유전 물질의 비교는 실질적이지 않은 차이를 보여주었다.</span> > In some instances, genetic variation among individuals of a dialect group was found to be greater than genetic variation among larger groups, such as subspecies. <span class=\"translation\">어떤 경우에는, 방언 집단 내 개체들 간의 유전적 변이가 아종과 같은 더 큰 집단들 간의 유전적 변이보다 더 큰 것으로 밝혀졌다.</span> </div> Paragraph 5 <button class=\"toggle-button translations\" data-type=\"paragraph5\">🔍 AI 해석 가리기</button> <div class=\"paragraph5-content\"> > If genetic differences are insignificant in comparison to cultural influences, why do sparrows with overlapping territories sing only one song in the field? <span class=\"translation\">만약 유전적 차이가 문화적 영향에 비해 중요하지 않다면, 왜 겹치는 영역을 가진 참새들은 야생에서 단 하나의 노래만 부르는가?</span> > Some, such as those of the sedentary subspecies known as Nuttall's sparrow, do encounter other dialect groups. <span class=\"translation\">일부는, 너탈 참새로 알려진 정주성 아종 같은 경우, 다른 방언 그룹과 마주친다.</span> > Gambell's sparrow, a migratory subspecies, overwinters in the coastal California area that Nuttall's sparrow inhabits. <span class=\"translation\">감벨 참새는, 이동성 아종으로, 너탈 참새가 서식하는 캘리포니아 연안 지역에서 월동한다.</span> > Interestingly, despite contacts between October and March with Gambell's sparrow, male Nuttall's sparrows typically sing their own distinct songs. <span class=\"translation\">흥미롭게도, 10월과 3월 사이에 감벨 참새와 접촉함에도 불구하고, 수컷 너탈 참새들은 일반적으로 자신들만의 독특한 노래를 부른다.</span> > This is because the encounters between the two subspecies occur outside the primary song crystallization period, which occurs during the first two months of a bird's life. <span class=\"translation\">이것은 두 아종 간의 만남이 주요 노래 결정화 기간 밖에서 일어나기 때문인데, 이 기간은 새의 생후 첫 두 달 동안 발생한다.</span> </div> Paragraph 6 <button class=\"toggle-button translations\" data-type=\"paragraph6\">🔍 AI 해석 가리기</button> <div class=\"paragraph6-content\"> > Detailed field research led to the discovery that some exceptional members of the species do master multiple songs, but their knowledge acquisition is contingent on contact with other groups in one of two ways. <span class=\"translation\">상세한 현장 연구는 이 종의 일부 예외적인 구성원들이 여러 노래를 숙달한다는 것을 발견했지만, 그들의 지식 습득은 두 가지 방법 중 하나로 다른 집단과의 접촉에 달려있다.</span> > Either young birds come in contact with migratory subspecies during the formative period of language learning or they live on the margins of dialect borders. <span class=\"translation\">어린 새들이 언어 학습의 형성 기간 동안 이동성 아종과 접촉하거나, 방언 경계의 가장자리에 살고 있다.</span> > Nuttall's sparrows hatching early or late in the season are more likely to be exposed to Gambell's sparrows, but broods hatching after March or before October will mostly only interact with other members of their own kind. <span class=\"translation\">계절에 일찍 또는 늦게 부화하는 Nuttall's sparrow들은 Gambell's sparrow들에 노출될 가능성이 더 높지만, 3월 이후 또는 10월 이전에 부화하는 새끼들은 대부분 자신의 종류의 다른 구성원들과만 상호작용할 것이다.</span> > In like fashion, individuals living near the peripheries of their dialectic range may regularly hear the songs of an adjacent group and develop multilingual abilities. <span class=\"translation\">같은 방식으로, 방언 범위의 주변에 사는 개체들은 인접한 집단의 노래를 정기적으로 듣고 다언어 능력을 개발할 수 있다.</span> </div> Paragraph 7 <button class=\"toggle-button translations\" data-type=\"paragraph7\">🔍 AI 해석 가리기</button> <div class=\"paragraph7-content\"> > In light of these findings, most scientists have concluded that geographic differences may mildly restrict gene flow, but that genetic isolation has not been strong enough to create a rigid linguistic barrier. <span class=\"translation\">이러한 발견을 바탕으로, 대부분의 과학자들은 지리적 차이가 유전자 흐름을 약간 제한할 수 있지만, 유전적 격리가 엄격한 언어적 장벽을 만들 만큼 충분히 강하지 않았다고 결론지었다.</span> > Dialectical variety in birds can thus be viewed as primarily a cultural phenomenon. <span class=\"translation\">따라서 새들의 방언적 다양성은 주로 문화적 현상으로 볼 수 있다.</span> </div> --- 📚 핵심 단어 사전 <button class=\"toggle-button vocabulary\" data-type=\"vocabulary\">🔍 단어 뜻 가리기</button> <div class=\"vocabulary-content\"> 🐦 조류학 관련 용어 avian vocalization <span class=\"meaning\">(조류의 발성) - 새가 내는 소리</span> birdsong <span class=\"meaning\">(새의 노래) - 새가 부르는 복잡한 소리</span> dialect <span class=\"meaning\">(방언) - 지리적 경계에 따른 노래의 차이</span> phrase <span class=\"meaning\">(구문) - 노래의 구성 단위</span> syllable <span class=\"meaning\">(음절) - 구문의 개별 발화 부분</span> whistle <span class=\"meaning\">(휘파람) - 새가 내는 소리 유형</span> pulse <span class=\"meaning\">(펄스) - 새가 내는 소리 유형</span> buzz <span class=\"meaning\">(윙윙거리는 소리) - 새가 내는 소리 유형</span> 🧬 유전학 관련 용어 gene pool <span class=\"meaning\">(유전자 풀) - 집단의 전체 유전적 다양성</span> genetic isolation <span class=\"meaning\">(유전적 격리) - 유전자 교환의 차단</span> subspecies <span class=\"meaning\">(아종) - 종의 하위 분류</span> genetic variation <span class=\"meaning\">(유전적 변이) - 유전적 차이</span> gene flow <span class=\"meaning\">(유전자 흐름) - 집단 간 유전자 교환</span> 🕐 시간 관련 용어 formative period <span class=\"meaning\">(형성 기간) - 학습이 이루어지는 중요한 시기</span> song crystallization period <span class=\"meaning\">(노래 결정화 기간) - 생후 2개월의 핵심 학습 시기</span> overwinter <span class=\"meaning\">(월동하다) - 겨울을 나다</span> 🗺️ 지리 관련 용어 geographical boundaries <span class=\"meaning\">(지리적 경계) - 지역적 한계</span> overlapping territories <span class=\"meaning\">(겹치는 영역) - 서로 겹치는 서식지</span> sedentary <span class=\"meaning\">(정주성) - 한 곳에 머무르는</span> migratory <span class=\"meaning\">(이동성) - 계절에 따라 이동하는</span> peripheries <span class=\"meaning\">(주변부) - 가장자리 지역</span> </div> --- 💬 유용한 영어 표현들 <button class=\"toggle-button expressions\" data-type=\"expressions\">🔍 표현 뜻 가리기</button> <div class=\"expressions-content\"> 🔬 연구/실험 관련 표현 in this regard <span class=\"meaning\">(이와 관련하여) - \"One of the best-studied songbirds in this regard is...\"</span> led to the discovery <span class=\"meaning\">(발견으로 이어졌다) - \"Detailed field research led to the discovery that...\"</span> have been able to <span class=\"meaning\">(할 수 있었다) - \"Scientists have been able to train fledglings...\"</span> have found that <span class=\"meaning\">(발견했다) - \"Laboratory experiments have found that...\"</span> have revealed <span class=\"meaning\">(보여주었다) - \"Comparisons have revealed unsubstantial differences.\"</span> 📊 비교/대조 표현 in comparison to <span class=\"meaning\">(비교했을 때) - \"If genetic differences are insignificant in comparison to cultural influences...\"</span> in addition <span class=\"meaning\">(또한) - \"In addition, a convenient feature...\"</span> though <span class=\"meaning\">(비록...하지만) - \"Though a genetic explanation seems plausible...\"</span> despite <span class=\"meaning\">(에도 불구하고) - \"Despite contacts between October and March...\"</span> in light of <span class=\"meaning\">(을 바탕으로) - \"In light of these findings...\"</span> ⏰ 시간/순서 표현 from an early age <span class=\"meaning\">(어린 나이부터) - \"Birds learn from an early age.\"</span> during <span class=\"meaning\">(동안) - \"Produced primarily during breeding.\"</span> between <span class=\"meaning\">(사이에) - \"Between October and March.\"</span> after/before <span class=\"meaning\">(이후/이전) - \"After March or before October.\"</span> once <span class=\"meaning\">(일단...하면) - \"Once scientists detect variability...\"</span> 🎯 강조/설명 표현 in other words <span class=\"meaning\">(다시 말해서) - \"In other words, if the terminal phrase...\"</span> for example <span class=\"meaning\">(예를 들어) - \"For example, female preference for males...\"</span> such as <span class=\"meaning\">(같은) - \"Such as song sparrows.\"</span> in like fashion <span class=\"meaning\">(같은 방식으로) - \"In like fashion, individuals living near...\"</span> thus <span class=\"meaning\">(따라서) - \"Dialectical variety can thus be viewed as...\"</span> 🔍 조건/가정 표현 in cases where <span class=\"meaning\">(인 경우에) - \"In cases where populations have geographical boundaries...\"</span> if <span class=\"meaning\">(만약) - \"If these geographical limits are distinct...\"</span> whether <span class=\"meaning\">(인지 아닌지) - \"Whether there might be a genetic basis...\"</span> perhaps <span class=\"meaning\">(아마도) - \"Perhaps through the evolutionary isolation...\"</span> may <span class=\"meaning\">(할 수 있다) - \"Geographic differences may mildly restrict gene flow.\"</span> 📝 학술적 표현 referred to as <span class=\"meaning\">(라고 불린다) - \"Are referred to as dialects.\"</span> keenly interested in <span class=\"meaning\">(에 깊은 관심을 가지고 있다) - \"Ornithologists are keenly interested in...\"</span> make sense of <span class=\"meaning\">(이해하다) - \"In order to make sense of birdsongs...\"</span> contingent on <span class=\"meaning\">(에 달려있다) - \"Knowledge acquisition is contingent on contact...\"</span> concluded that <span class=\"meaning\">(라고 결론지었다) - \"Scientists have concluded that...\"</span> </div> 문제풀이 TOEFL Reading Comprehension Questions Question 1 The word \"primarily\" in the passage is closest in meaning to (A) abundantly (B) mainly (C) loudly (D) commonly <button class=\"toggle-button answer\" data-type=\"answer1\">🔍 정답 및 해설 가리기</button> <div class=\"answer1-content\"> 정답: (B) mainly 해설: \"primarily\"는 \"주로, 주된\"이라는 뜻으로 사용됩니다. 지문에서 \"produced primarily during breeding\"은 \"번식기에 주로 발생하는\"이라는 의미입니다. (A) abundantly: 풍부하게 - 양적 의미 (B) mainly: 주로 - 주된 목적이나 시기를 나타냄 (C) loudly: 크게 - 소리의 크기 (D) commonly: 흔히 - 빈도의 의미 </div> Question 2 Which of the sentences below best expresses the essential information in the highlighted sentence in the passage? Incorrect choices change the meaning in important ways or leave out essential information. 원문: \"In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects.\" (A) Dialects are those song types within a population that are shared by the group. (B) Shared song types within a population with clear geographical boundaries are called dialects. (C) Geographical boundaries may result in differences in song between distinct populations. (D) Populations having clear dialects result from the shared song types within the group. <button class=\"toggle-button answer\" data-type=\"answer2\">🔍 정답 및 해설 가리기</button> <div class=\"answer2-content\"> 정답: (B) Shared song types within a population with clear geographical boundaries are called dialects. 해설: 원문의 핵심 정보는 다음과 같습니다: 지리적 경계가 있는 개체군에서는 노래의 차이가 나타날 수 있다 이러한 지리적 한계가 뚜렷하면, 개체군 내에서 공유되는 노래 유형을 방언이라고 부른다 (A) 방언의 정의만 포함, 지리적 경계 조건 누락 (B) 지리적 경계와 공유된 노래 유형이 방언이라는 핵심 정보 모두 포함 (C) 지리적 경계가 차이를 만든다는 부분만, 방언의 정의 누락 (D) 인과관계가 잘못됨 (방언이 개체군을 만드는 것이 아님) </div> Question 3 According to paragraph 2, scientists attempt to understand birdsongs by (A) dividing them into smaller categories (B) analyzing them as a single unit (C) listening to phrases in reverse order (D) labeling them as male or female <button class=\"toggle-button answer\" data-type=\"answer3\">🔍 정답 및 해설 가리기</button> <div class=\"answer3-content\"> 정답: (A) dividing them into smaller categories 해설: 2단락에서 \"In order to make sense of birdsongs, scientists break them into smaller units\"라고 명시되어 있습니다. 과학자들은 새의 노래를 이해하기 위해 더 작은 단위로 나눈다고 설명하고 있습니다. 구체적으로: 노래(song) → 구문(phrase) → 음절(syllables)로 세분화 휘파람, 펄스, 윙윙거리는 소리 등 특정 소리 유형으로 분류 이는 \"더 작은 범주로 나누는 것\"과 정확히 일치합니다. </div> Question 4 What can be inferred from paragraph 4 about white-crowned sparrow chicks? (A) The simplest dialects are the ones they learn first. (B) The number of songs they can be trained to learn is limited. (C) They learn songs of their own subspecies faster than those of other subspecies. (D) They learn more easily from laboratory recordings than in the wild. <button class=\"toggle-button answer\" data-type=\"answer4\">🔍 정답 및 해설 가리기</button> <div class=\"answer4-content\"> 정답: (B) The number of songs they can be trained to learn is limited. 해설: 4단락에서 \"Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects, although the maximum number is approximately seven\"라고 명시되어 있습니다. 과학자들이 어린 새들을 훈련시켜 여러 방언을 재현하게 할 수 있었지만, 최대 개수는 약 7개라고 한정되어 있습니다. 이는 학습할 수 있는 노래의 수가 제한적임을 보여줍니다. (A) 가장 간단한 방언을 먼저 배운다는 언급 없음 (C) 자신의 아종 노래를 더 빨리 배운다는 언급 없음 (D) 실험실 녹음으로 더 쉽게 배운다는 비교 없음 </div> Question 5 According to paragraph 5, which of the following is true of Nuttall's sparrows? (A) They migrate to the same overwintering locations that Gambell's sparrows do. (B) They do not often encounter Gambell's sparrows during the initial period of song formation. (C) They only have contact with Gambell's sparrows during the breeding season. (D) They do not have the same inherent vocalization abilities that Gambell's sparrows do. <button class=\"toggle-button answer\" data-type=\"answer5\">🔍 정답 및 해설 가리기</button> <div class=\"answer5-content\"> 정답: (B) They do not often encounter Gambell's sparrows during the initial period of song formation. 해설: 5단락에서 핵심 정보: Nuttall's sparrow는 정주성(sedentary) 아종 Gambell's sparrow는 이동성(migratory) 아종으로 10월~3월에 Nuttall's 서식지에서 월동 \"This is because the encounters between the two subspecies occur outside the primary song crystallization period, which occurs during the first two months of a bird's life\" 즉, 두 아종의 만남은 주요 노래 결정화 기간(생후 첫 2개월) 밖에서 일어나기 때문에, Nuttall's sparrow는 노래 형성 초기에 Gambell's sparrow와 자주 마주치지 않습니다. (A) Nuttall's는 이동하지 않음 (sedentary) (C) 10월~3월에 접촉 (번식기가 아님) (D) 발성 능력의 차이에 대한 언급 없음 </div> Question 6 The word \"exceptional\" in the passage is closest in meaning to (A) extraordinary (B) perceptual (C) intelligent (D) capable <button class=\"toggle-button answer\" data-type=\"answer6\">🔍 정답 및 해설 가리기</button> <div class=\"answer6-content\"> 정답: (A) extraordinary 해설: 6단락에서 \"some exceptional members of the species do master multiple songs\"라고 사용되었습니다. 문맥상 대부분의 개체들은 하나의 노래만 부르는데, 일부 예외적인 구성원들은 여러 노래를 숙달한다는 의미입니다. (A) extraordinary: 특별한, 예외적인 (B) perceptual: 지각의, 인식의 (C) intelligent: 지능적인 (D) capable: 능력있는 \"exceptional\"은 \"일반적이지 않은, 특별한\"이라는 뜻으로 \"extraordinary\"와 가장 의미가 가깝습니다. </div> Question 7 According to paragraph 6, which of the following does NOT contribute to multilingualism in Nuttall's sparrows? (A) Living on the edges of dialectical boundaries (B) Interacting with other subspecies (C) Hatching early or late in the season (D) Mating between March and October ✓ <button class=\"toggle-button answer\" data-type=\"answer7\">🔍 정답 및 해설 가리기</button> <div class=\"answer7-content\"> 정답: (D) Mating between March and October 해설: 6단락에서 다언어 능력에 기여하는 요인들: 기여하는 것들: (A) 방언 경계의 가장자리에 거주: \"individuals living near the peripheries of their dialectic range may regularly hear the songs of an adjacent group\" (B) 다른 아종과의 상호작용: \"young birds come in contact with migratory subspecies during the formative period\" (C) 계절 초기나 말기에 부화: \"Nuttall's sparrows hatching early or late in the season are more likely to be exposed to Gambell's sparrows\" 기여하지 않는 것: (D) 3월~10월 사이의 짝짓기: 지문에서 언급되지 않았으며, 오히려 \"broods hatching after March or before October will mostly only interact with other members of their own kind\"라고 하여 이 시기에 부화하는 새끼들은 자신의 종류와만 상호작용한다고 명시 </div> Question 8 Why does the author include paragraph 7 in the passage? (A) To argue that genetic factors have nothing to do with linguistic ability in birds (B) To offer a concluding argument for why birds have a variety of dialects (C) To introduce a new issue that has emerged in research on bird dialects (D) To suggest that language acquisition in birds is still poorly understood <button class=\"toggle-button answer\" data-type=\"answer8\">🔍 정답 및 해설 가리기</button> <div class=\"answer8-content\"> 정답: (B) To offer a concluding argument for why birds have a variety of dialects 해설: 7단락은 전체 지문의 결론 단락입니다: \"In light of these findings, most scientists have concluded that geographic differences may mildly restrict gene flow, but that genetic isolation has not been strong enough to create a rigid linguistic barrier. Dialectical variety in birds can thus be viewed as primarily a cultural phenomenon.\" 이 단락에서 저자는: 앞서 제시된 연구 결과들을 종합 유전적 격리보다는 문화적 요인이 더 중요하다고 결론 새들의 방언적 다양성이 주로 문화적 현상이라는 최종 답을 제시 이는 새들이 왜 다양한 방언을 가지게 되는지에 대한 결론적 주장을 제공하는 것입니다. (A) 유전적 요인이 전혀 관련없다고 주장하지 않음 (mildly restrict라고 인정) (C) 새로운 이슈를 소개하는 것이 아님 (D) 언어 습득이 잘 이해되지 않는다고 제시하지 않음 </div> Question 9 Look at the four squares [■] that indicate where the following sentence could be added to the passage. Single adult males of this species have been recorded singing eleven different songs. Where would the sentence best fit? <button class=\"toggle-button answer\" data-type=\"answer9\">🔍 정답 및 해설 가리기</button> <div class=\"answer9-content\"> 적절한 위치: Paragraph 6의 첫 번째 문장 뒤 해설: 삽입할 문장: \"Single adult males of this species have been recorded singing eleven different songs.\" (이 종의 성체 수컷 개체들이 11개의 서로 다른 노래를 부르는 것이 기록되었다.) 가장 적절한 위치는 6단락 첫 문장 뒤입니다: \"Detailed field research led to the discovery that some exceptional members of the species do master multiple songs, but their knowledge acquisition is contingent on contact with other groups in one of two ways. [■ 여기] Either young birds come in contact with migratory subspecies...\" 이유: 논리적 연결: 앞 문장에서 \"일부 예외적인 구성원들이 여러 노래를 숙달한다\"고 하고, 삽입 문장은 그 구체적인 예시(11개 노래)를 제공 구체적 증거: \"some exceptional members\"의 구체적인 사례 문맥의 일관성: 다언어 능력에 대한 논의가 이어지는 자연스러운 위치 다른 위치들은 문맥상 맞지 않습니다. </div> Question 10 Directions: An introductory sentence for a brief summary of the passage is provided below. Complete the summary by selecting the THREE answer choices that express the most important ideas in the passage. Some sentences do not belong in the summary because they express ideas that are not presented in the passage or are minor ideas in the passage. This question is worth 2 points. In some bird populations, there are regional dialects in the birds' songs. Answer Choices: (A) White-crowned sparrows are useful subjects for studying bird dialects because they usually only learn a single song and have many dialects. (B) Using recordings in a laboratory setting, scientists have demonstrated that white-crowned sparrow chicks are capable of learning multiple dialects. (C) Research conducted on Nuttall's sparrows and Gambell's sparrows suggests that the two subspecies have developed different dialects because one is sedentary while the other is migratory. (D) Scientists thought that genetic factors may have contributed to dialectical diversity, but research has shown that genetic factors are limited. (E) Field studies of two Zonotrichia leucophrys subspecies revealed that multilingualism is dependent upon the timing and nature of interactions between dialectical group members. (F) One thing that is clear about the various subspecies of Zonotrichia leucophrys is that they all seem to begin learning songs for the first time after two months of age. <button class=\"toggle-button answer\" data-type=\"answer10\">🔍 정답 및 해설 가리기</button> <div class=\"answer10-content\"> 정답: (A), (B), (D) 해설: 선택된 답안들: (A) White-crowned sparrows are useful subjects for studying bird dialects because they usually only learn a single song and have many dialects. ✓ 1단락: \"One of the best-studied songbirds... is the white-crowned sparrow\" 3단락: \"individual adults generally sing a single song\" 연구 대상으로서의 유용성이 주요 주제 (B) Using recordings in a laboratory setting, scientists have demonstrated that white-crowned sparrow chicks are capable of learning multiple dialects. ✓ 4단락: \"Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects\" 실험실 연구의 중요한 발견 (D) Scientists thought that genetic factors may have contributed to dialectical diversity, but research has shown that genetic factors are limited. ✓ 4단락: 유전적 설명이 그럴듯해 보였지만 7단락: \"genetic isolation has not been strong enough... primarily a cultural phenomenon\" 지문의 핵심 결론 제외된 답안들: (C) 정주성/이동성의 차이가 방언 발달의 원인이라고 제시하지 않음 (E) 다언어 능력은 부차적 주제 (F) 2개월 후에 학습을 시작한다는 내용 없음 (오히려 첫 2개월이 중요한 시기) </div>",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-14-걱정하는표현",
    "title": "걱정하는 표현",
    "date": "2025-07-14",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "배운 것들 worried love life : 연애 사업 I'm worried about my love life. worry about : (항상) ~를 걱정해 I worry about my future. 'm worried about : (요즘) 걱정 돼 I'm worried about my love life. I'm worried about my job interview. I'm worried about the traffic. 차가 막힐까바ㅗ I'm worried about my presentation tommorow. 조금 걱정된다 I'm a little worried about 엄청 걱정된다 I'm really worried about 신경쓰인다 I'm concerned about the new flu virus. I'm concerned about the news lately. travel ban What are some things that are on your mind? as well , 나도 간단한 작문 Nowadays, I'm worried about my future of career. 내가 시장 가치가 있을지? 다른 회사에 갈 수 있을지?에 대해 고민을 하고 있어. I'm preparing the job transition, but if fails in the resume screening. So I can believe my self. 앞으로 더 나아질지. My 연봉 can be better than now? I have a some concerns, but what I have to do or I can to is just focus on the today, just step ahead. So 호흡을 가다듬고, 현재에 집중하려고 노력한다. 수정된 영어 글 Nowadays, I'm worried about my career path. I keep wondering if I'm competitive enough in the job market to move to another company. I'm preparing for a job transition, but I'm afraid of failing at the resume screening stage. It makes me doubt myself and wonder if I can actually improve from here. I also question if I can earn a better salary than I do now. Even with these concerns, I know that all I can really do is focus on today and take things one step at a time. So, I'm trying to take a deep breath and concentrate on the present. 피드백 및 해설 my future of career → my career path 피드백: career는 이미 미래의 길(path)이라는 뉘앙스를 포함하고 있어요. my career path (나의 커리어 경로) 또는 my future career (나의 미래 커리어)라고 표현하는 것이 훨씬 자연스러워요. 예시: \"I need some advice on my career path.\" (제 커리어 경로에 대해 조언이 필요해요.) if fails in the resume screening → I'm afraid of failing at the resume screening stage 피드백: 원래 문장은 \"만약 서류 심사에서 떨어지면\"이라는 조건절만 있어서 문법적으로 완전하지 않아요. \"서류 심사 단계에서 떨어지는 것이 두렵다\"는 의미로 I'm afraid of failing...과 같이 완전한 문장으로 만들어주었어요. 팁: afraid of + (동)명사는 \"~하는 것이 두렵다\"는 표현으로 자주 쓰여요. So I can believe my self → It makes me doubt myself 피드백: 문맥상 \"나 자신을 믿을 수 있을까?\"라며 스스로를 의심하는 의미로 해석돼요. doubt myself (나 자신을 의심하다)라는 표현이 더 명확하게 의미를 전달해요. 또한, myself는 항상 한 단어로 붙여 써야 합니다. 예시: \"Don't doubt yourself. You can do it!\" (스스로를 의심하지 마. 넌 할 수 있어!) My 연봉 can be better than now? → if I can earn a better salary than I do now 피드백: \"내 연봉이 지금보다 나아질 수 있을까?\"라는 질문을 if (~인지 아닌지)를 사용해 \"더 나은 연봉을 받을 수 있을지 궁금하다/의문이다\"는 의미로 자연스럽게 문장에 녹여냈어요. 예시: \"I wonder if I can finish this on time.\" (이걸 제시간에 끝낼 수 있을지 모르겠어.) I have a some concerns → Even with these concerns 피드백: a와 some은 함께 쓸 수 없어요. I have some concerns (몇 가지 걱정이 있다)가 맞습니다. 문맥의 흐름을 더 자연스럽게 하기 위해 \"이런 걱정에도 불구하고\"라는 의미의 Even with these concerns로 수정했어요. what I have to do or I can to is... → all I can really do is... 피드백: 원래 문장의 I can to는 문법적으로 틀린 표현이에요. I can do가 맞습니다. 의미를 살려서 \"내가 정말로 할 수 있는 모든 것은\"이라는 all I can really do is...로 다듬었어요. focus on the today, just step ahead → focus on today and take things one step at a time 피드백: today, yesterday, tomorrow 앞에는 보통 the를 붙이지 않아요. just step ahead도 좋은 표현이지만, \"차근차근 나아가다\"라는 의미를 더 잘 살리는 관용구인 take things one step at a time이 이런 상황에 아주 잘 어울려요.",
    "public": true
  },
  {
    "id": "Data Science/Clean Code Principles/dependency-injection-pattern",
    "title": "의존성 주입 패턴으로 코드 품질 향상하기",
    "date": "2025-07-14",
    "excerpt": "의존성 주입 패턴을 통해 코드의 결합도를 낮추고 테스트 용이성을 높이는 방법을 실제 예제와 함께 설명합니다.",
    "category": "Data Science",
    "tags": [
      "python",
      "design-pattern",
      "dependency-injection",
      "software-architecture",
      "clean-code",
      "testing"
    ],
    "content": "배경 최근에 대화형 AI 챗봇 프로젝트를 개발하면서 겪었던 코드 구조의 문제점들과, 이를 의존성 주입 패턴으로 개선한 경험을 정리해보았습니다. 프로젝트 상황 FastAPI 기반 웹 애플리케이션 OpenAI GPT 모델을 활용한 대화형 검색 시스템 ChromaDB를 사용한 벡터 검색 사용자별 대화 세션 관리 겪었던 문제점들 강한 결합 (Tight Coupling) 클래스들이 서로 너무 밀접하게 연결되어 있어서, 하나를 수정하면 다른 것들도 함께 수정해야 하는 상황이 발생했습니다. 하드코딩된 설정 모델명, 데이터베이스 경로, API 키 등이 코드에 직접 작성되어 있어서 환경별로 설정을 바꾸기 어려웠습니다. 테스트 어려움 각 컴포넌트를 독립적으로 테스트하기 어려워서, 전체 시스템을 실행해야만 테스트할 수 있었습니다. 관련 개념들 정리 강한 결합 (Tight Coupling) - 쉽게 이해하기 🏠 집 짓기 비유 강한 결합의 예시: ``python 문제가 있는 코드 - 강한 결합 class 집: def __init__(self): # 집이 직접 벽돌공장을 만들어서 벽돌을 가져옴 self.벽돌공장 = 벽돌공장() self.벽돌 = self.벽돌공장.벽돌만들기() # 집이 직접 목수공장을 만들어서 문을 가져옴 self.목수공장 = 목수공장() self.문 = self.목수공장.문만들기() ` 문제점: 집이 벽돌공장과 목수공장을 직접 알고 있어야 함 다른 재료(콘크리트, 알루미늄)로 바꾸려면 집 클래스를 수정해야 함 테스트할 때 진짜 공장 대신 가짜 공장을 사용하기 어려움 해결책 - 느슨한 결합: `python 개선된 코드 - 느슨한 결합 class 집: def __init__(self, 벽돌제공자, 문제공자): # 외부에서 벽돌과 문을 받아옴 self.벽돌 = 벽돌제공자.벽돌만들기() self.문 = 문제공자.문만들기() 사용할 때 집1 = 집(벽돌공장(), 목수공장()) 집2 = 집(콘크리트공장(), 알루미늄공장()) # 쉽게 바꿀 수 있음! ` 실제 코드 예시 `python 강한 결합 - 문제가 있는 코드 class ChatBot: def __init__(self): # 직접 의존성을 생성 self.llm = OpenAI(model=\"gpt-4\") # OpenAI에 강하게 결합 self.database = ChromaDB(path=\"./db\") # ChromaDB에 강하게 결합 self.scheduler = APScheduler() # APScheduler에 강하게 결합 느슨한 결합 - 개선된 코드 class ChatBot: def __init__(self, llm_provider, database, scheduler): # 외부에서 의존성을 주입받음 self.llm = llm_provider self.database = database self.scheduler = scheduler ` 하드코딩된 설정 - 왜 문제인가? 🍕 피자 주문 비유 하드코딩된 설정의 문제: `python 문제가 있는 코드 class 피자집: def __init__(self): self.토핑 = \"페퍼로니\" # 하드코딩! self.크기 = \"라지\" # 하드코딩! self.가격 = 20000 # 하드코딩! 모든 피자가 똑같이 나옴 😞 피자1 = 피자집() # 항상 페퍼로니 라지 20000원 피자2 = 피자집() # 항상 페퍼로니 라지 20000원 ` 개선된 코드: `python 설정 파일 사용 class 피자설정: def __init__(self): self.기본토핑 = \"페퍼로니\" self.기본크기 = \"라지\" self.기본가격 = 20000 class 피자집: def __init__(self, 설정): self.토핑 = 설정.기본토핑 self.크기 = 설정.기본크기 self.가격 = 설정.기본가격 환경별로 다른 설정 사용 가능 개발설정 = 피자설정() 개발설정.기본토핑 = \"치즈\" # 개발환경은 치즈 운영설정 = 피자설정() 운영설정.기본토핑 = \"페퍼로니\" # 운영환경은 페퍼로니 ` 실제 코드 예시 `python 하드코딩된 설정 - 문제가 있는 코드 class AI챗봇: def __init__(self): self.model = \"gpt-4o-mini-2024-07-18\" # 하드코딩! self.api_key = \"sk-1234567890abcdef\" # 하드코딩! self.db_path = \"./db/chroma_db\" # 하드코딩! 개선된 코드 - 설정 파일 사용 import os from pydantic import BaseSettings class Settings(BaseSettings): model: str = \"gpt-4o-mini-2024-07-18\" api_key: str = \"\" db_path: str = \"./db/chroma_db\" class Config: env_file = \".env\" # 환경변수 파일에서 읽어옴 class AI챗봇: def __init__(self, settings: Settings): self.model = settings.model self.api_key = settings.api_key self.db_path = settings.db_path 사용 settings = Settings() 챗봇 = AI챗봇(settings) ` 의존성 주입 (Dependency Injection) - 핵심 개념 🎯 의존성 주입이란? 의존성(Dependency): 객체가 동작하기 위해 필요한 다른 객체들 주입(Injection): 외부에서 그 객체들을 제공해주는 것 🚗 자동차 비유 `python 의존성 주입 없이 - 문제가 있는 코드 class 자동차: def __init__(self): # 자동차가 직접 엔진을 만듦 self.엔진 = 가솔린엔진() # 가솔린엔진에 강하게 결합 self.타이어 = 미쉐린타이어() # 미쉐린타이어에 강하게 결합 문제점: 전기차로 바꾸려면 자동차 클래스를 수정해야 함 테스트할 때 진짜 엔진 대신 가짜 엔진을 사용할 수 없음 다른 브랜드 타이어로 바꾸기 어려움 의존성 주입 사용 - 개선된 코드 class 자동차: def __init__(self, 엔진, 타이어): # 외부에서 엔진과 타이어를 받아옴 self.엔진 = 엔진 self.타이어 = 타이어 사용할 때 가솔린차 = 자동차(가솔린엔진(), 미쉐린타이어()) 전기차 = 자동차(전기엔진(), 한국타이어()) # 쉽게 바꿀 수 있음! 테스트할 때 테스트차 = 자동차(가짜엔진(), 가짜타이어()) # 가짜 부품으로 테스트 가능! ` 실제 코드 예시 `python 의존성 주입 없이 class ChatBot: def __init__(self): self.llm = OpenAI(model=\"gpt-4\") # 직접 생성 self.db = ChromaDB(path=\"./db\") # 직접 생성 의존성 주입 사용 class ChatBot: def __init__(self, llm_provider, database): self.llm = llm_provider # 외부에서 주입 self.database = database # 외부에서 주입 사용할 때 챗봇1 = ChatBot(OpenAI(model=\"gpt-4\"), ChromaDB(path=\"./db\")) 챗봇2 = ChatBot(Anthropic(model=\"claude\"), Pinecone(index=\"my-index\")) 테스트할 때 테스트챗봇 = ChatBot(MockLLM(), MockDatabase()) ` 팩토리 패턴 (Factory Pattern) - 객체 생산 공장 🏭 공장 비유 팩토리 패턴이란? 객체를 만드는 전용 공장을 따로 두는 패턴 `python 팩토리 없이 - 문제가 있는 코드 class 피자집: def __init__(self): # 피자집이 직접 재료를 준비 self.도우 = 밀가루도우() self.소스 = 토마토소스() self.치즈 = 모짜렐라치즈() 문제점: 피자집이 재료 준비 방법을 모두 알아야 함 팩토리 패턴 사용 - 개선된 코드 class 재료공장: def 도우만들기(self): return 밀가루도우() def 소스만들기(self): return 토마토소스() def 치즈만들기(self): return 모짜렐라치즈() class 피자집: def __init__(self, 재료공장): self.공장 = 재료공장 self.도우 = self.공장.도우만들기() self.소스 = self.공장.소스만들기() self.치즈 = self.공장.치즈만들기() 사용 공장 = 재료공장() 피자집 = 피자집(공장) ` 실제 코드 예시 `python LLM Factory class LLMFactory: def __init__(self, config): self.config = config def create_refine_llm(self): return OpenAI( model=self.config.get(\"refine_model\", \"gpt-4o-mini\"), api_key=self.config.get(\"api_key\") ) def create_answer_llm(self): return OpenAI( model=self.config.get(\"answer_model\", \"gpt-4o\"), api_key=self.config.get(\"api_key\") ) 사용 config = {\"refine_model\": \"gpt-4o-mini\", \"answer_model\": \"gpt-4o\"} factory = LLMFactory(config) refine_llm = factory.create_refine_llm() answer_llm = factory.create_answer_llm() ` 싱글톤 패턴 (Singleton Pattern) - 하나만 존재하는 객체 👑 왕 비유 싱글톤 패턴이란? 클래스의 인스턴스가 하나만 존재하도록 보장하는 패턴 `python 일반적인 클래스 - 여러 인스턴스 생성 가능 class 일반사람: def __init__(self, 이름): self.이름 = 이름 사람1 = 일반사람(\"김철수\") 사람2 = 일반사람(\"이영희\") print(사람1 is 사람2) # False - 다른 객체 싱글톤 패턴 - 하나만 존재 class 왕: _instance = None # 클래스 변수로 인스턴스 저장 def __new__(cls): if cls._instance is None: cls._instance = super().__new__(cls) cls._instance.이름 = \"현재왕\" return cls._instance 왕1 = 왕() 왕2 = 왕() print(왕1 is 왕2) # True - 같은 객체! ` 실제 코드 예시 `python 데이터베이스 연결 - 싱글톤 패턴 class DatabaseConnection: _instance = None def __new__(cls): if cls._instance is None: cls._instance = super().__new__(cls) cls._instance.connection = create_database_connection() return cls._instance def get_connection(self): return self.connection 사용 db1 = DatabaseConnection() db2 = DatabaseConnection() print(db1 is db2) # True - 같은 연결 객체 ` 프로토콜 (Protocol) - 인터페이스 정의 📋 계약서 비유 프로토콜이란? 객체가 어떤 메서드를 가져야 하는지 정의하는 \"계약서\" `python 프로토콜 없이 - 문제가 있는 코드 class ChatBot: def __init__(self, llm): self.llm = llm def 대화하기(self, 메시지): # llm이 generate 메서드를 가지고 있다고 가정 return self.llm.generate(메시지) # 런타임에 에러 가능! 문제점: llm 객체가 generate 메서드가 없으면 에러 발생 프로토콜 사용 - 개선된 코드 from typing import Protocol class LLMProvider(Protocol): \"\"\"LLM 제공자는 반드시 generate 메서드를 가져야 함\"\"\" def generate(self, message: str) -> str: ... class ChatBot: def __init__(self, llm: LLMProvider): # 타입 힌트로 검증 self.llm = llm def 대화하기(self, 메시지): return self.llm.generate(메시지) # 안전! 사용 class OpenAI: def generate(self, message: str) -> str: return f\"OpenAI: {message}\" class Anthropic: def generate(self, message: str) -> str: return f\"Anthropic: {message}\" 둘 다 LLMProvider 프로토콜을 만족하므로 사용 가능 챗봇1 = ChatBot(OpenAI()) 챗봇2 = ChatBot(Anthropic()) ` 실제 코드 예시 `python from typing import Protocol, List, Dict, Any class Retriever(Protocol): \"\"\"검색기 인터페이스\"\"\" def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: \"\"\"쿼리로 문서를 검색\"\"\" ... class ChromaRetriever: def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: # ChromaDB로 검색하는 실제 구현 return [{\"content\": \"검색된 문서\", \"score\": 0.9}] class PineconeRetriever: def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: # Pinecone으로 검색하는 실제 구현 return [{\"content\": \"검색된 문서\", \"score\": 0.8}] 둘 다 Retriever 프로토콜을 만족하므로 같은 방식으로 사용 가능 def search_documents(retriever: Retriever, query: str): return retriever.search(query, max_results=5) 사용 chroma_results = search_documents(ChromaRetriever(), \"검색어\") pinecone_results = search_documents(PineconeRetriever(), \"검색어\") ` 의존성 주입 패턴으로 코드 품질 향상하기 개요 의존성 주입(Dependency Injection, DI)은 객체가 필요로 하는 의존성을 외부에서 제공받는 디자인 패턴입니다. 이 패턴을 통해 코드의 결합도를 낮추고, 테스트 용이성을 높이며, 유지보수성을 개선할 수 있습니다. 현재 코드의 문제점 강한 결합 (Tight Coupling) `python 문제가 있는 코드 예시 class MyApp: def __init__(self): self.app = FastAPI() # 직접 의존성 생성 - 강한 결합 self.retriever = ChromadbRetriever(\"./db/chroma_db\") self.scheduler = BackgroundScheduler(timezone=\"Asia/Seoul\") self.llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\") ` 문제점: 모든 의존성을 클래스 내부에서 직접 생성 다른 구현체로 교체하기 어려움 테스트 시 Mock 객체 주입 불가능 하드코딩된 설정 `python class ConversationalRetrievalChain: def __init__(self, retriever: ChromadbRetriever): # 하드코딩된 모델명들 self.refine_llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\", system_message=\"\") self.answer_llm = LLM_openai(model=\"gpt-4o-2024-05-13\", system_message=\"\") self.lang_detect_llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\", system_message=\"\") ` 문제점: 모델명이 코드에 하드코딩됨 환경별 설정 변경 어려움 설정 관리의 중앙화 부족 의존성 주입 패턴 적용 인터페이스 정의 (Protocol 사용) 먼저 의존성들의 인터페이스를 정의합니다: `python interfaces/llm_provider.py from typing import Protocol, List, Dict, Any class LLMProvider(Protocol): \"\"\"LLM 제공자 인터페이스\"\"\" def generate(self, user_message: str) -> str: \"\"\"단일 메시지 생성\"\"\" ... def generate_with_chat_history(self, history: List, user_message: str) -> str: \"\"\"채팅 히스토리와 함께 메시지 생성\"\"\" ... class Retriever(Protocol): \"\"\"검색기 인터페이스\"\"\" def get_docs_with_decomposition(self, query: str, max_k: int, threshold: float, lang: str) -> Dict[str, Any]: \"\"\"쿼리 분해를 통한 문서 검색\"\"\" ... class Scheduler(Protocol): \"\"\"스케줄러 인터페이스\"\"\" def add_job(self, func, trigger, kwargs): \"\"\"작업 추가\"\"\" ... def get_job(self, job_id: str): \"\"\"작업 조회\"\"\" ... def start(self): \"\"\"스케줄러 시작\"\"\" ... ` Factory 클래스들 생성 의존성 생성을 담당하는 Factory 클래스들을 만듭니다: `python factories/llm_factory.py from typing import Dict from utils.llm_providers import LLM_openai from interfaces.llm_provider import LLMProvider class LLMFactory: \"\"\"LLM 인스턴스 생성을 담당하는 Factory\"\"\" def __init__(self, config: Dict[str, str]): self.config = config def create_refine_llm(self) -> LLMProvider: \"\"\"정제용 LLM 생성\"\"\" return LLM_openai( model=self.config.get(\"refine_model\", \"gpt-4o-mini-2024-07-18\"), system_message=\"\" ) def create_answer_llm(self) -> LLMProvider: \"\"\"답변용 LLM 생성\"\"\" return LLM_openai( model=self.config.get(\"answer_model\", \"gpt-4o-2024-05-13\"), system_message=\"\" ) def create_lang_detect_llm(self) -> LLMProvider: \"\"\"언어 감지용 LLM 생성\"\"\" return LLM_openai( model=self.config.get(\"lang_detect_model\", \"gpt-4o-mini-2024-07-18\"), system_message=get_lang_detect_message(type=\"system\") ) factories/retriever_factory.py from chains.custom_retriever import ChromadbRetriever from interfaces.retriever import Retriever class RetrieverFactory: \"\"\"검색기 인스턴스 생성을 담당하는 Factory\"\"\" def __init__(self, db_path: str): self.db_path = db_path def create_retriever(self) -> Retriever: \"\"\"검색기 생성\"\"\" return ChromadbRetriever(self.db_path) ` 설정 관리 개선 중앙화된 설정 관리를 위해 Pydantic을 사용합니다: `python config/settings.py from pydantic import BaseSettings from typing import Dict class Settings(BaseSettings): \"\"\"애플리케이션 설정\"\"\" # LLM 설정 refine_model: str = \"gpt-4o-mini-2024-07-18\" answer_model: str = \"gpt-4o-2024-05-13\" lang_detect_model: str = \"gpt-4o-mini-2024-07-18\" # 데이터베이스 설정 chroma_db_path: str = \"./db/chroma_db\" # 스케줄러 설정 timezone: str = \"Asia/Seoul\" # API 설정 api_host: str = \"0.0.0.0\" api_port: int = 8000 class Config: env_file = \".env\" case_sensitive = False 전역 설정 인스턴스 settings = Settings() ` 의존성 컨테이너 생성 의존성들을 관리하는 컨테이너를 만듭니다: `python di/container.py from typing import Dict from fastapi import FastAPI from apscheduler.schedulers.background import BackgroundScheduler from factories.llm_factory import LLMFactory from factories.retriever_factory import RetrieverFactory from config.settings import settings class DependencyContainer: \"\"\"의존성 주입 컨테이너\"\"\" def __init__(self): self._services: Dict[str, object] = {} self._factories: Dict[str, object] = {} self._setup_factories() def _setup_factories(self): \"\"\"Factory 초기화\"\"\" self._factories[\"llm\"] = LLMFactory({ \"refine_model\": settings.refine_model, \"answer_model\": settings.answer_model, \"lang_detect_model\": settings.lang_detect_model }) self._factories[\"retriever\"] = RetrieverFactory(settings.chroma_db_path) def get_retriever(self): \"\"\"검색기 인스턴스 반환 (싱글톤 패턴)\"\"\" if \"retriever\" not in self._services: self._services[\"retriever\"] = self._factories[\"retriever\"].create_retriever() return self._services[\"retriever\"] def get_scheduler(self): \"\"\"스케줄러 인스턴스 반환 (싱글톤 패턴)\"\"\" if \"scheduler\" not in self._services: scheduler = BackgroundScheduler(timezone=settings.timezone) scheduler.start() self._services[\"scheduler\"] = scheduler return self._services[\"scheduler\"] def get_llm_factory(self): \"\"\"LLM Factory 반환\"\"\" return self._factories[\"llm\"] def reset(self): \"\"\"테스트용 리셋 메서드\"\"\" self._services.clear() 전역 컨테이너 인스턴스 container = DependencyContainer() ` 리팩토링된 MyApp 클래스 의존성 주입을 적용한 MyApp 클래스: `python app.py (리팩토링 후) from fastapi import FastAPI from di.container import container from interfaces.retriever import Retriever from interfaces.scheduler import Scheduler class MyApp: \"\"\"의존성 주입이 적용된 메인 애플리케이션 클래스\"\"\" def __init__(self, retriever: Retriever = None, scheduler: Scheduler = None): self.app = FastAPI() # 의존성 주입 self.retriever = retriever or container.get_retriever() self.scheduler = scheduler or container.get_scheduler() # 사용자별 세션 관리 self.user_conversations: Dict[str, ConversationalRetrievalChain] = {} # 구글 스프레드시트 연동 self.sheet = None self.setup_events() def setup_events(self): \"\"\"이벤트 설정\"\"\" select_sheet(self) self.scheduler.add_job( select_sheet, \"cron\", hour=0, minute=0, args=[self], id=\"create new sheet\" ) def get_conversation_chain(self, user_id: str) -> ConversationalRetrievalChain: \"\"\"사용자별 대화 체인 반환\"\"\" if user_id not in self.user_conversations: self.user_conversations[user_id] = ConversationalRetrievalChain( retriever=self.retriever ) return self.user_conversations[user_id] 테스트용 팩토리 함수 def create_app(retriever=None, scheduler=None) -> MyApp: \"\"\"애플리케이션 인스턴스 생성 (테스트용)\"\"\" return MyApp(retriever=retriever, scheduler=scheduler) 애플리케이션 인스턴스 생성 myapp = create_app() ` 리팩토링된 ConversationalRetrievalChain 클래스 의존성 주입을 적용한 체인 클래스: `python chains/main_chain.py (리팩토링 후) from interfaces.retriever import Retriever from factories.llm_factory import LLMFactory from di.container import container class ConversationalRetrievalChain: \"\"\"의존성 주입이 적용된 대화형 검색 체인\"\"\" def __init__(self, retriever: Retriever, llm_factory: LLMFactory = None): self.retriever = retriever # Factory를 통한 의존성 생성 factory = llm_factory or container.get_llm_factory() self.refine_llm = factory.create_refine_llm() self.answer_llm = factory.create_answer_llm() self.lang_detect_llm = factory.create_lang_detect_llm() # 체커 체인들 self.checker_runnable_executor = get_combined_checker_chain( model_name=settings.lang_detect_model ) self.checker_reservation_runnable_executor = get_combined_checker_chain_reservation( model_name=settings.lang_detect_model ) self.chat_history = [] def process_query(self, user_message: str, user_id: str) -> str: \"\"\"사용자 쿼리 처리\"\"\" # 언어 감지 detected_lang = self.detect_language(user_message) # 문서 검색 docs = self.retriever.get_docs_with_decomposition( query=user_message, max_k=5, threshold=0.7, lang=detected_lang ) # 답변 생성 response = self.generate_response(user_message, docs) # 히스토리 업데이트 self.update_chat_history(user_message, response) return response ` 테스트 용이성 향상 Mock 객체를 사용한 단위 테스트 `python tests/test_myapp.py import pytest from unittest.mock import Mock, MagicMock from app import create_app class TestMyApp: \"\"\"MyApp 클래스 테스트\"\"\" def test_app_initialization_with_mocks(self): \"\"\"Mock 객체를 사용한 초기화 테스트\"\"\" # Mock 객체 생성 mock_retriever = Mock() mock_scheduler = Mock() # 의존성 주입으로 앱 생성 app = create_app(retriever=mock_retriever, scheduler=mock_scheduler) # 검증 assert app.retriever == mock_retriever assert app.scheduler == mock_scheduler assert isinstance(app.app, FastAPI) def test_conversation_chain_creation(self): \"\"\"대화 체인 생성 테스트\"\"\" mock_retriever = Mock() app = create_app(retriever=mock_retriever) # 사용자별 체인 생성 chain = app.get_conversation_chain(\"user123\") assert chain is not None assert chain.retriever == mock_retriever tests/test_conversational_chain.py class TestConversationalRetrievalChain: \"\"\"ConversationalRetrievalChain 클래스 테스트\"\"\" def test_chain_initialization_with_mock_factory(self): \"\"\"Mock Factory를 사용한 초기화 테스트\"\"\" mock_retriever = Mock() mock_factory = Mock() # Mock LLM 인스턴스들 mock_refine_llm = Mock() mock_answer_llm = Mock() mock_lang_detect_llm = Mock() mock_factory.create_refine_llm.return_value = mock_refine_llm mock_factory.create_answer_llm.return_value = mock_answer_llm mock_factory.create_lang_detect_llm.return_value = mock_lang_detect_llm # 체인 생성 chain = ConversationalRetrievalChain( retriever=mock_retriever, llm_factory=mock_factory ) # 검증 assert chain.retriever == mock_retriever assert chain.refine_llm == mock_refine_llm assert chain.answer_llm == mock_answer_llm assert chain.lang_detect_llm == mock_lang_detect_llm ` 통합 테스트 `python tests/test_integration.py import pytest from di.container import container class TestIntegration: \"\"\"통합 테스트\"\"\" def setup_method(self): \"\"\"테스트 전 컨테이너 리셋\"\"\" container.reset() def test_container_dependency_resolution(self): \"\"\"컨테이너 의존성 해결 테스트\"\"\" retriever = container.get_retriever() scheduler = container.get_scheduler() llm_factory = container.get_llm_factory() assert retriever is not None assert scheduler is not None assert llm_factory is not None # 싱글톤 패턴 검증 retriever2 = container.get_retriever() assert retriever is retriever2 ` 환경별 설정 관리 환경 변수 설정 `bash .env.development REFINE_MODEL=gpt-4o-mini-2024-07-18 ANSWER_MODEL=gpt-4o-2024-05-13 LANG_DETECT_MODEL=gpt-4o-mini-2024-07-18 CHROMA_DB_PATH=./db/chroma_db_dev TIMEZONE=Asia/Seoul API_HOST=0.0.0.0 API_PORT=8000 ` `bash .env.production REFINE_MODEL=gpt-4o-2024-05-13 ANSWER_MODEL=gpt-4o-2024-05-13 LANG_DETECT_MODEL=gpt-4o-mini-2024-07-18 CHROMA_DB_PATH=/data/chroma_db_prod TIMEZONE=Asia/Seoul API_HOST=0.0.0.0 API_PORT=80 ` 설정 로딩 개선 `python config/settings.py (개선된 버전) import os from pydantic import BaseSettings from typing import Dict class Settings(BaseSettings): \"\"\"환경별 설정 관리\"\"\" # 환경 설정 environment: str = \"development\" # LLM 설정 refine_model: str = \"gpt-4o-mini-2024-07-18\" answer_model: str = \"gpt-4o-2024-05-13\" lang_detect_model: str = \"gpt-4o-mini-2024-07-18\" # 데이터베이스 설정 chroma_db_path: str = \"./db/chroma_db\" # 스케줄러 설정 timezone: str = \"Asia/Seoul\" # API 설정 api_host: str = \"0.0.0.0\" api_port: int = 8000 class Config: env_file = f\".env.{os.getenv('ENVIRONMENT', 'development')}\" case_sensitive = False settings = Settings() `` 장점과 효과 테스트 용이성 Mock 객체를 쉽게 주입할 수 있음 단위 테스트와 통합 테스트 분리 가능 테스트 격리 보장 유연성 런타임에 다른 구현체로 교체 가능 새로운 LLM 제공자 추가 용이 설정 변경 시 코드 수정 불필요 유지보수성 중앙화된 설정 관리 명확한 의존성 관계 단일 책임 원칙 준수 확장성 새로운 기능 추가 시 기존 코드 영향 최소화 플러그인 아키텍처 구현 가능 마이크로서비스 전환 용이 결론 의존성 주입 패턴을 적용함으로써 코드의 품질을 크게 향상시킬 수 있습니다. 특히: 결합도 감소: 클래스 간의 의존성이 명시적으로 관리됨 테스트 용이성: Mock 객체 주입으로 격리된 테스트 가능 설정 관리: 중앙화된 설정으로 환경별 관리 용이 확장성**: 새로운 기능 추가 시 기존 코드 영향 최소화 이러한 패턴은 대규모 프로젝트에서 특히 유용하며, 코드의 장기적인 유지보수성을 보장합니다. 참고 자료 Python Protocol Classes Pydantic Settings Management FastAPI Dependency Injection Clean Architecture by Robert C. Martin",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/why-we-need-causal-inference",
    "title": "인과 추론 도입배경",
    "date": "2025-07-14",
    "excerpt": "인과추론을 왜 사용해야하는지 설명하고 이를 도입하기 위해 설득하는 글",
    "category": "Causal Inference",
    "tags": [
      "ICL-Lenze-sizing"
    ],
    "content": "Abstract - 배경, 요약 --- 환자에게 렌즈 삽입술을 위한 최적의 렌즈 사이즈를 추천하는, 렌즈 사이징 문제는 예측 모델을 고도화하는 방향으로 연구가 진행되어왔다. 환자의 눈의 조건과 삽입한 렌즈 사이즈를 입력 받아 수술결과(vaulting, 렌즈 후면과 수정체 전면까지의 거리)를 예측하는 모델을 개발하고, 이 모델의 예측값을 통해 최적의 렌즈 사이즈를 제안하는 방식이다. 하지만 기존 예측 모델은 한계점이 존재한다. 이번 보고서를 통해 기존 예측 모델의 한계점을 분석하고 이를 해결하기 위한 방법을 제안한다. 기존 예측 모델의 한계점을 요약하면 렌즈 사이즈가 결과에 영향을 주는 인과적인 영향도를 추정하지 못하며, 단순히 예측 평가지표만을 고려하고 있어 추천의 신뢰도를 보장하지 못한다는 것이다. 이러한 한계점을 극복하기 위해 인과추론 방법론을 도입할 것을 제안한다. 이때 기대되는 효과는 다음과 같다. 개인화된 처치 효과를 추정할 수 있고 이를 통해 개인화된 최적의 렌즈 사이즈를 제안할 수 있다. 그리고 측정되지 않은 데이터에 대한 평가를 고려하여 추천의 신뢰도를 보장할 수 있다. Introduction - 기존 연구의 한계점 분석 --- 렌즈 삽입술을 위한 최적의 렌즈 사이즈를 찾는 일은 의사의 높은 수준의 경험과 노하우에 의존하는 일이다. 이를 도와주기 위해 최적의 렌즈 사이즈를 추천하는 모델이 개발되어왔다. 기존의 최적의 사이즈를 찾기 위한 렌즈 사이징 문제는 예측 모델을 고도화하는 방향으로 연구가 진행되어왔다. 환자의 눈의 조건과 삽입한 렌즈 사이즈를 입력 받아 수술결과(vaulting, 렌즈 후면과 수정체 전면까지의 거리)를 예측하는 모델의 성능을 향상시키는 방향을 진행되었다. 대표적으로 MAE와 같이 실제 값과 예측값의 차이를 평가하는 예측평가지표를 통해 모델의 성능을 평가하고 이를 개선하는 방향으로 연구가 진행되었다. MAE: Mean Absolute Error, 실제 값과 예측값의 차이의 절대값의 평균 하지만 예측 모델 기반의 렌즈 사이징 모델인 기존 연구는 분명한 한계점이 존재한다. 먼저 이를 이해하기 위해서는 예측 문제와 선택의 문제를 이해하고 구분해야 한다. 1.1 예측 문제와 선택의 문제 차이 --- 예측 문제와 선택의 문제는 근본적으로 다른 성격을 가진다. 먼저 예측 문제는 주어진 입력에 대해 결과를 예측하는 문제를 말한다. 예를 들어 \"환자 A가 12.6mm 렌즈로 수술을 받으면 vaulting이 얼마가 될까?\"와 같은 문제가 예측 문제에 해당한다. 예측 문제의 목표는 정확한 예측이다. 예측 문제의 성능 지표는 MAE, RMSE, R² 등이 있다. 선택의 문제는 여러 옵션 중 최적의 선택을 하는 문제를 말한다. 예를 들어 \"환자 A에게 12.1mm, 12.6mm, 13.2mm 중 어떤 렌즈를 추천할까?\"와 같은 문제가 선택의 문제에 해당한다. 선택 문제의 목표는 최적의 선택이다. 선택 문제의 성능 지표는 처치 효과, 개인화된 추천 정확도 등이 있다. 예측 문제와 선택 문제의 가장 큰 차이점은 통제 변수(Control Variable)의 존재 여부이다. 통제 변수는 연구자가 직접 조작할 수 있는 변수로, 실험 설계에서 핵심적인 역할을 한다. 이는 인과추론에서 처치(Treatment)와 동일한 개념으로, 연구자가 의도적으로 변화시킬 수 있는 독립변수를 의미한다. 렌즈 사이징 문제에서는 렌즈의 사이즈가 처치에 해당하며 처치가 결과에 미치는 영향을 처치효과라고 한다. 예측 문제에서는 모든 입력 변수가 주어진 값으로 고정되어 있다. 즉, 모델의 목표는 주어진 조건 하에서 결과를 정확히 예측하는 것이다. 예를 들어, \"환자 A가 12.6mm 렌즈로 수술을 받으면 vaulting이 얼마가 될까?\"라는 질문에서 렌즈 사이즈(12.6mm)는 이미 결정된 고정값이며, 이를 변경할 수 없다. 이 경우 모델은 환자의 눈 조건과 고정된 렌즈 사이즈를 입력으로 받아 vaulting 값을 예측하는 함수를 학습한다. 선택 문제에서는 통제 변수가 핵심 역할을 한다. 연구자는 여러 옵션 중에서 최적의 선택을 하기 위해 통제 변수의 값을 변화시킬 수 있다. \"환자 A에게 12.1mm, 12.6mm, 13.2mm 중 어떤 렌즈를 추천할까?\"라는 질문에서 렌즈 사이즈는 통제 변수로, 여러 옵션 중에서 선택할 수 있는 변수이다. 이 경우 모델은 각 렌즈 사이즈 옵션에 대한 결과를 예측하고, 최적의 선택을 제안하는 함수를 학습해야 한다. 예측 문제는 조건부 확률 $P(Y X)$를 추정하는 문제이고, 선택 문제는 조건부 평균 처치 효과 $E[Y(T=t) X]$를 추정하는 문제이다. 여기서 $T$는 통제 변수(처치)를, $X$는 환자 특성을, $Y$는 결과를 나타낸다. 조건부 평균 처치 효과란 통제 변수가 주어졌을 때 처치 효과의 평균을 의미한다. 렌즈 사이징은 본질적으로 선택의 문제이다. 환자에게 최적의 렌즈를 추천하는 것이 목표이기 때문이다. 하지만 기존 연구는 예측 모델의 성능 향상에만 집중하여 선택의 문제를 제대로 해결하지 못한다. 1.2 예측 모델이 선택의 문제에 활용되기 어려운 이유 --- 렌즈 사이징이 선택의 문제임을 확인했으므로, 이제 예측 모델이 선택의 문제에 활용되기 어려운 이유를 분석한다. 예측 모델이 가지는 한계점은 무엇인지, 이를 기반으로 선택을 할 때의 위험성에 대해 이야기한다. 1.2.1 상관관계에 기반한 예측 모델의 한계 예측 모델은 주어진 데이터에서 변수 간의 동시 변동 패턴, 즉 상관관계를 학습하여 결과를 예측한다. 상관관계는 두 변수 사이에 일정한 패턴이나 동반 변동이 존재함을 의미하며, 한 변수가 커질 때 다른 변수가 일정하게 커지거나 작아지는 경향을 보이는 통계적 관계를 말한다. 예측 모델은 상관관계만을 학습하므로 숨겨진 교란변수(Confounding Variable)의 존재를 인식하지 못한다. 교란변수(Confounding Variable) : 처치(Treatment)와 결과(Outcome) 모두에 영향을 미치는 제3의 변수로, 처치와 결과 사이의 인과관계를 왜곡시키는 변수를 의미한다. 교란변수는 처치 선택과 결과에 동시에 영향을 주어 가짜 인과관계를 만들어내거나 실제 인과관계를 숨길 수 있다. 교란편향(Confounding Bias) : 교란변수로 인해 발생하는 통계적 편향으로, 처치와 결과 사이의 실제 인과관계가 왜곡되어 나타나는 현상을 의미한다. 이는 처치 그룹 간에 교란변수의 분포가 다를 때 발생하며, 처치 효과의 정확한 추정을 방해한다. 예를 들어, 숙박 가격과 매출 데이터에서 가격이 높을수록 매출이 높아 보이는 패턴이 관찰되면, 예측 모델은 \"가격을 올리면 매출이 오른다\"고 결론을 내린다. 하지만 이 패턴 뒤에는 성수기/비수기 여부라는 숨겨진 교란변수가 존재할 수 있다. 실제로는 성수기에 가격과 매출이 동시에 상승하고, 비수기에 가격과 매출이 동시에 하락하는 것이며, 가격과 매출 사이에는 직접적인 인과관계가 없다. 이러한 상관관계를 인과관계로 오해하면 \"매출을 올리려면 가격을 올려야 한다\"는 잘못된 결론에 이를 수 있고, 실제 효과가 있는 개입(예: 비수기 프로모션) 대신 부적절한 가격 정책을 선택하게 만든다. 인과관계: 한 변수가 변할 때 다른 변수가 그 변화에 의해 직접적·체계적으로 영향을 받아 변화하는 관계를 의미 예측 모델의 예측 성능이 좋다고 해서 인과관계를 잘 찾았다는 보장이 없으며, 렌즈 사이징과 같은 선택 문제에서는 인과관계가 필요하지만 예측 모델은 이를 제공하지 못한다. 따라서 예측 모델에 기반해 선택을 하는 것은 위험하며, 인과관계를 직접적으로 추론할 수 있는 방법론이 필요하다. 인과관계를 직접적으로 추정하는 모델을 통해 처치 (통제변수, 렌즈사이즈)가 결과에 미치는 영향을 직접적으로 추정할 수 있고 이를 통해 올바른 선택을 할 수 있다. 1.2.2 데이터 안에 존재하는 통계적 편향 앞서 언급한 것처럼 데이터에 통계적 편향(교란편향)이 존재하면 예측 모델은 잘못된 패턴, 즉 인과관계가 아닌 패턴을 학습할 수 있다. 렌즈 사이징 문제에서 데이터는 통계적 편향을 가지고 있다. 이는 렌즈 크기별(처치별)로 환자의 분포가 다르다는 의미이며, 데이터에서 확인된다. <figure> <img src=\"/post/what-is-statistical-bias/처치별_Feature_DensityPlot.png\" alt=\"처치별 Feature Density Plot\"> <figcaption>그림 1: 처치별 환자 눈의 크기 분포 비교</figcaption> </figure> 그림 1은 처치(렌즈 사이즈)별로 환자의 눈 크기에 대한 분포를 시각화한 것이다. 그림 1에서 볼 수 있듯이, 처치별로 환자의 눈 크기 분포(ATA, ACD)가 명확히 다르다. 이는 의사가 환자의 개별 특성을 고려하여 적절한 렌즈를 선택하는 임상적 판단에 따른 결과이다. 예를 들어, 눈이 작은 환자에게는 작은 렌즈를, 눈이 큰 환자에게는 큰 렌즈를 선택하는 경향이 있다. 이러한 선택 편향은 교란편향을 유발한다. 환자의 눈 크기, 나이, 각막 두께 등은 렌즈 선택과 수술 결과 모두에 영향을 미치는 교란변수이기 때문이다. 앞서 1.2.1에서 언급한 바와 같이, 데이터에 통계적 편향이 존재할 경우 예측 모델은 인과관계가 아닌 잘못된 패턴을 학습할 수 있으며, 이러한 패턴에 기반한 선택은 위험하다. 또한 통계적 편향은 렌즈의 크기가 결과에 미치는 영향인 처치효과를 왜곡한다. 렌즈 사이즈별로 환자 눈의 분포가 다르기 때문에 단순히 결과만 비교해서는 렌즈 사이즈의 효과를 알 수 없다. 환자의 개별 특성을 동일하게 맞춘 상태에서 렌즈 효과를 비교해야 한다. 이는 실험을 설계할 때 대조군과 처치군을 동일하게 맞춰야 하는 것과 같다. 대조군과 실험군을 동일하게 맞춰야만 실험 조건만의 영향을 추정할 수 있기 때문이다. 하지만 처치효과를 추정하기 위해 무작위 대조 실험(RCT, Randomized Controlled Trial)을 진행할 수 없다. 무작위 대조 실험 : 대조군과 실험군을 무작위로 선택하여 실험을 진행하는 방법 따라서 처치 효과를 추정하기 위해서는 통계적 편향을 제거하고 인과관계를 직접적으로 추론해야 한다. 인과추론 방법론은 이러한 교란편향을 통계적으로 제거하여 실제 처치 효과를 추정할 수 있는 방법을 제공한다. 구체적인 방법은 다음 섹션에서 다룬다. 1.3 예측 평가지표의 한계 --- 예측 모델의 또 다른 한계는 측정되지 않은 데이터(반사실, Counterfactual)에 대한 평가를 고려하지 않는다는 점이다. 기존 연구들은 예측 모델을 고도화하는 방향으로 진행되었으며, 예측 성능지표를 통해 모델의 신뢰도를 보장하고 있다. 하지만 최적의 렌즈를 추천하기 위해서는 수술받은 렌즈 외에 다른 렌즈 크기에 대한 결과도 예측해야 한다. 예측 모델의 한계는 실제로 일어나지 않은 상황에 대한 평가가 불가능하다는 점이다. 예를 들어, 환자 A가 12.6mm 렌즈로 수술을 받고 이에 결과를 얻었다고 가정한다. 이때 추천 모델이 증명해야 하는 것은 \"만약 이 환자가 13.2mm 렌즈로 수술을 받았다면 어떤 결과가 나왔을까?\"이다. 하지만 이 환자는 이미 12.6mm 렌즈로 수술을 받았기 때문에, 13.2mm 렌즈의 결과는 알 수 없다. 이렇게 실제로는 관찰되지 않은 가상의 상황을 \"반사실(Counterfactual)\"이라고 한다. 기존 예측 모델들은 관찰된 데이터만으로 학습하기 때문에, 이런 반사실적 상황에 대한 추정이 불가능하다. 그리고 이에 대한 평가지표를 고려하지 않는다. 인과추론은 이런 반사실적 상황을 통계적으로 추정할 수 있는 방법론을 제공하며, 이에 대한 평가지표도 함께 고려해 신뢰도를 확보한다. Method & Result - 인과추론에 대한 소개와 인과추론을 통해 이를 해결한 방법 소개 --- 앞서 언급한 것처럼 선택의 문제에서 예측 모델은 한계점을 가진다. 이를 극복하기 위해 인과추론 방법론을 도입하고자 한다. 인과추론이란 의학, 마케팅, 교육 등에서 자주 사용되는 방법론으로 정책 및 비즈니스에 수반된 수많은 의사결정들의 원인과 결과를 분석하는 방법이다. 인과추론은 인과관계를 분석하고 처치 효과를 추정하여 의사결정을 돕는 방법론이다. 예를 들어 정밀의학에서는 환자별 처치 효과를 추정하여 수술방법을 선택을 결정하고, 교육에서는 학생별 처치 효과를 추정하여 교육 방법을 결정하고, 온라인 마케팅에서는 사용자별 처치 효과를 추정하여 맞춤형 광고를 제공하고, 오프라인 정책 평가에서는 지역별 처치 효과를 추정하여 정책을 결정할 수 있다. 여기서 처치란 인과추론에서 의사결정의 대상(예: 수술 방법, 약물 투여, 교육정책 등)을 의미한다. 렌즈 사이징 문제에서는 렌즈 사이즈가 처치에 해당한다. 인과추론을 통해 렌즈의 사이즈와 결과의 인과관계를 분석하고 처치효과를 추정할 수 있다. 그리고 이를 통해 최적의 렌즈 사이즈를 결정하는데 도움을 줄 수 있다. 인과추론을 도입함으로써 다음과 같은 효과를 기대할 수 있다. 첫째, 통계적 편향을 제거하고 인과관계를 직접적으로 추론할 수 있어 렌즈 크기가 결과에 미치는 실제 영향을 정확히 추정할 수 있다. 둘째, 측정되지 않은 데이터(반사실적 상황)에 대한 평가를 고려하여 모델의 신뢰도를 확보할 수 있다. 셋째, 개인화된 처치 효과(Heterogeneous Treatment Effects)를 추정하여 환자별로 최적화된 렌즈 사이즈를 제안할 수 있다. 이번 보고서에서는 통계적 편향을 제거한 결과에 대해서만 소개하며, 개인화된 처치 효과를 추정하여 개인화된 추천을 하는 방법에 대해서는 추후에 다루도록 한다. 2.1 인과추론 방법을 통해 통계적 편향을 제거 렌즈 사이즈가 결과에 미치는 영향인 처치효과를 추정하기 위해서는 대조군과 실험군처럼 렌즈 사이즈를 제외한 나머지 변수는 통제해야 한다. 즉 렌즈 사이즈별로 환자의 눈의 크기에 대한 분포가 같아야 한다. 하지만 앞서 확인한 것처럼 데이터에는 편향이 존재하며, 처치효과를 추정하기 위해 RCT를 수행할 수 없다. 이때 인과추론 방법론을 통해 통계적 편향을 제거할 수 있다. <figure> <img src=\"/post/what-is-statistical-bias/처치별결과비교.png\" alt=\"처치별 결과 비교\"> <figcaption>그림 2: 처치별 결과 분포 비교</figcaption> </figure> 그림 2는 처치별 결과의 분포를 시각화한 것이다. 앞서 설명한 것처럼 처치별로 환자의 눈의 분포가 다르기 때문에 처치별 분포를 비교해 처치효과를 추정할 수 없다. 처치효과를 추정하기 위해서는 통계적 편향을 제거해야 한다. <figure> <img src=\"/post/what-is-statistical-bias/사이즈별잔차분포.png\" alt=\"결과에서 교란변수에 대한 영향 제거후 분포\"> <figcaption>그림 3: 잔차화를 통해 통계적 편향을 제거한 결과 분포</figcaption> </figure> 그림 3은 그림 2의 결과에서 잔차화를 통해 통계적 편향을 제거한 것이다. 잔차화란 결과(vaulting)에 렌즈 사이즈를 제외한 다른 변수(교란변수)가 미치는 영향을 제거해 교란변수를 통제한 것과 같은 효과를 만드는 방법론이다. 잔차화를 통해 통계적 편향을 제거하면 렌즈 사이즈별로 환자의 눈의 분포가 같아지므로 렌즈 사이즈가 결과에 미치는 영향을 추정할 수 있다. 이처럼 인과추론 방법론은 데이터가 가지고 있는 통계적 편향을 확인하고 이를 제거해 처치의 효과를 추정하고 인과적인 관계를 추론하는 방법론이다. Conclusion - 연구 요약 및 향후 계획 --- 본 연구에서는 렌즈 사이징 문제에서 기존 예측 모델의 한계점을 분석하고, 이를 해결하기 위한 인과추론 방법론의 도입을 제안하였다. 기존 예측 모델은 선택의 문제에서 통계적 편향을 고려하지 못하고 인과관계를 직접적으로 추론하지 못하는 한계를 가지고 있다. 이러한 한계를 극복하기 위해 인과추론 방법론을 도입하여 통계적 편향을 제거하고 처치 효과를 추정하는 방법을 제시하였다. 인과추론 방법론의 도입을 통해 다음과 같은 효과를 기대할 수 있다. 첫째, 통계적 편향을 제거하여 렌즈 크기가 결과에 미치는 실제 인과적 영향을 정확히 추정할 수 있다. 둘째, 측정되지 않은 데이터(반사실적 상황)에 대한 평가를 고려하여 모델의 신뢰도를 확보할 수 있다. 셋째, 개인화된 처치 효과(Heterogeneous Treatment Effects)를 추정하여 환자별로 최적화된 렌즈 사이즈를 제안할 수 있다. 향후 연구에서는 개인화된 처치 효과 추정을 통한 개인화된 추천 시스템 개발에 집중할 예정이다. 이를 통해 환자별 특성을 고려한 맞춤형 렌즈 사이즈 추천이 가능한 모델을 구축하고자 한다.",
    "public": true
  },
  {
    "id": "Self_Development/Fitness/Workout Log/250713",
    "title": "간단한 달리기 + 산책",
    "date": "2025-07-13",
    "excerpt": "운동일지",
    "category": "Workout Log",
    "tags": [],
    "content": "확실히 런닝 자주 안하니까 진짜 안뛰어진다. 꾸준히 뛰어보자.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/TOEFL/토플준비",
    "title": "TOEFL 100점 이상 목표로 공부",
    "date": "2025-07-13",
    "excerpt": "OMSCS 지원을 위한 토플 공부",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "영어",
      "대학원"
    ],
    "content": "배경 OMSCS를 지원하기 위해서는 토플 100점 이상이 필요한데요, 이를 위해서 토플 공부를 시작했습니다. 공부하면서 어떻게 공부해야할지 정리해나가고 있습니다. 목표 기간은 3~4개월 정도입니다. 올해 11월부터는 OSMC에 서류를 넣으려고 합니다. 학습 계획 세워보기 공부 전략 유튜브 링크 - 🍀토플 독학을 마음먹으신 분들을 위한 첫 번째 가이드 (시험 이해, 공부 방법, 두려움 극복, 참고 링크) 명확한 목표 기간을 잡고 집중해서 공부하기 시험구성에 대해 정확히 이해하고 시작하기 시험 구성 Reading 2개 지문 x 10문제. 지문하나당 약 700단어. 총 35분정도. Listening 2개 대화 (각 3분) x 5문제 3개 강의 (각 3-5분) x 6문제 총 36분 정도. Speaking (1) 독립형 (15초 준비, 45초 답변) (2), (3) 통합형 - 읽+듣+말 (30초 준비, 60초 답변) (4) 통합형 - 듣+말 (20초 준비, 60초 답변) 총 16분 정도. Writing (1) 통합형 - 읽+듣+쓰 (20분) (2) 토론형 - 읽+쓰 (10분) 총 30분정도. 참고 사이트 🔗참고 사이트 링크 ETS TOEFL 공식 웹사이트 시험 구성 및 문제 유형 안내 https://www.ets.org/toefl/test-takers... ETS TOEFL TestReady 웹사이트 (모의고사, 액티비티) https://testready.ets.org/test-prep-l... 테스트글라이더 웹사이트 https://www.testglider.com/ko/toefl 참고자료 [[토플공부/토플 공부법] 초보자를 위한 토플 독학 가이드 : A to Z](https://beyondourlimit.tistory.com/68) : 블로그, 기본적인 개념 알기 좋다",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-13-조언하는 표현",
    "title": "조언하는 표현",
    "date": "2025-07-13",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "배운 것들 Try to drink more water Drink more water (X, 명령하는 느낌이 남) 발음 팁! t -> d Try to get more sleep Try to cut down on smoking cut down on : ~을 줄이다 Try not to skip breakfast. Make sure to + Make sure to take probiotics every day. Make sure to take Vitamin C. Make sure not to eat too much junk food. wipe down : 말끔히 닦다 come to work stock up : 대량으로 사놓다. 간단한 작문 I read a book , atomic habits that tell me how to make habit. The book says that try to split the things to very small things. Because it make me to do more. So the lesson in the book is that make sure to make things, clear, attractive, easy. 피드백. AI 작문 피드백: \"I read a book , atomic habits\" → 쉼표 앞 공백 제거, 책 제목은 따옴표 사용 \"that tell me\" → 단수 주어이므로 \"tells me\" \"how to make habit\" → \"how to build habits\" (복수형, 더 자연스러운 표현) \"says that try to\" → \"says to try to\" 또는 \"says that I should try to\" \"split the things to very small things\" → \"break things down into smaller pieces\" \"Because it make me to do more\" → \"makes\" (단수), \"make me do\" (to 불필요) \"make sure to make things, clear, attractive, easy\" → 문장 구조 개선 필요 수정된 작문: I read a book called \"Atomic Habits\" that tells me how to build habits. The book says to try to break things down into smaller pieces. This makes me do more because it reduces the psychological burden. So the lesson in the book is to make sure to make things clear, attractive, and easy. 영어 작문 연습 (한글 내용 기반) I think habits are the most important thing in life. According to research by Professor Wendy Wood at Duke University, about 45% of our behavior comes from habits. The book \"Atomic Habits\" suggests the 2-minute rule. Try to break down what you need to do into very small tasks that can be finished in 2 minutes. I applied this to my life for about 1-2 months and it was really effective. The biggest reason is that it reduces psychological burden. Usually, we procrastinate because of psychological pressure. When I break down tasks into small 2-minute pieces, this burden almost disappears. By achieving small things, I can gradually do more things with that sense of accomplishment. For breaking bad habits, you should make them difficult to do. Human willpower has limits, so environment is the most important thing. I had a bad habit of looking at my phone late at night and sleeping late. To solve this, I bought an alarm clock. Now I don't bring my phone to bed and turn it off as soon as I get home. The key lessons are: Try to break tasks into very small pieces to reduce psychological burden Make sure to create an environment where you can't do bad habits Remember that willpower is limited, so focus on changing your environment",
    "public": true
  },
  {
    "id": "Self_Development/Book/아주 작은 습관의 힘 (Atomic Habits)/골디락스의 법칙",
    "title": "골디락스의 법칙",
    "date": "2025-07-13",
    "excerpt": "지금보다 조금 더 어려운 일에 도전하자",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "'아주 작은 습관의 힘'에서는 꾸준함을 어떻게 이어갈 것인지, 그리고 이에 도움을 주는 도구들을 소개하고 있습니다. 이 중에서 '골디 락스의 법칙'에 대해 이야기한 부분을 정리해봅니다. <figure> <img src=\"/post/골디락스의법칙/골디락스의법칙_그래프_이미지.jpg\" alt=\"골디락스의 법칙 그래프\" /> <figcaption>그림1.골디락스의 법칙: 너무 쉬우면 지루하고, 너무 어려우면 포기하게 된다. 적당히 도전적인 수준이 가장 효과적이다.</figcaption> </figure> 골디락스의 법칙(Goldilocks Rule)의 법칙이란 일의 난이도가 너무 쉬우면 지루하고, 너무 어려우면 포기하게 되기 때문에, 조금 어려운 정도의 난이도가 일에 몰입하는데 효과적이라는 이야기입니다. > 골디락스의 법칙이란 인간은 자신이 할 수 있는 적합한 일을 할 때 동기가 극대화되는 경험을 한다는 것이다. > 지나치게 어려워서도 안되며 지나치게 쉬워서도 안된다. 딱 들어맞아야 한다. > > — 아주 작은 습관의 힘 어떤 일을 꾸준히 하기 위해서는 이 법칙을 이해하고 활용할 수 있습니다. 특히 어떤 일을 처음 시작하는거라면, 그 자체로 심리적 부담감이 크고 난이도가 어렵다고 느껴지기 때문에 정말 쉬운 일로 작게 쪼개는게 효과적일 것입니다. 그 후에 어느정도 반복하면서 습관이 되었다면 조금씩 난이도를 올려가야 지루함을 느끼지 않고 꾸준히 할 수 있을 것입니다. 이 골디락스의 법칙은 뇌의 긴장도를 어떻게 유지할 것인지와도 연결됩니다. 예르크스-도드슨 법칙과의 연결 골디락스의 법칙은 심리학의 \"예르크스-도드슨 법칙(Yerkes-Dodson Law)\"과 밀접한 관련이 있습니다. <figure> <img src=\"/post/골디락스의법칙/여키스도슨법칙_그래프.png\" alt=\"예르크스-도드슨 법칙 그래프\" /> <figcaption>그림2. 예르크스-도드슨 법칙: 긴장도(각성 수준)와 성과의 관계. 적절한 긴장도에서 최고의 성과를 보이지만, 너무 낮거나 높은 긴장도에서는 성과가 떨어진다.</figcaption> </figure> > 예르크스-도드슨 법칙이란 긴장도(각성 수준)와 성과 간의 관계를 설명하는 법칙으로, 적절한 긴장도에서 최고의 성과를 보이지만, 너무 낮거나 높은 긴장도에서는 성과가 떨어진다는 것을 말합니다. 너무 쉬운 일: 긴장도가 낮아서 지루함을 느끼고 성과가 떨어짐 너무 어려운 일: 긴장도가 너무 높아서 스트레스와 불안으로 성과가 떨어짐 적당히 도전적인 일: 적절한 긴장도를 유지하여 최고의 성과와 몰입을 경험 사람마다 최적의 긴장도 수준이 다르기 때문에, 자신에게 맞는 난이도를 찾아가는 것이 중요합니다. 처음에는 쉬운 것부터 시작해서 점진적으로 난이도를 높여가며 자신만의 \"골디락스 존\"을 찾아가는 것이 효과적입니다. 정리 골디락스의 법칙을 실생활에 적용하는 방법: 난이도 조절 전략 지루함을 느낄 때: 난이도를 10-15% 높여서 도전감을 만들어라 압도당할 때: 난이도를 낮추고 더 작은 단위로 쪼개라 적정 수준: 약간의 긴장감이 있지만 포기하지 않을 수 있는 수준 실천 방법 습관 형성 초기: 정말 쉬운 것부터 시작 (예: 매일 1분 명상) 습관 안정화 후: 점진적으로 난이도 상승 (예: 5분 → 10분 → 15분) 지속적 모니터링: 자신의 몰입도와 동기를 체크하며 조절 핵심 원칙 > \"너무 쉬우면 지루하고, 너무 어려우면 포기한다. 적당히 도전적인 수준이 가장 효과적이다.\"",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/250713",
    "title": "델리스파이스 고백",
    "date": "2025-07-12",
    "excerpt": "기타연습일지",
    "category": "Practice Log",
    "tags": [],
    "content": "크로매틱 80bpm 16bit. 처음엔 잘 안된다가 나중에 손풀리니까 잘 됨 스케일 0~4. 이제 다 외울 때가 됐는데.. ㅎㅎ 곡 연습 델리스파이스 고백. 막 빡세게는 안하고 그냥 한번씩 쳐보는 정도?",
    "public": true
  },
  {
    "id": "Self_Development/Guitar/Practice Log/250712",
    "title": "어떻게",
    "date": "2025-07-12",
    "excerpt": "기타연습일지",
    "category": "Practice Log",
    "tags": [],
    "content": "연습 체크 7월 12. 토 [ ] 크로메틱",
    "public": true
  },
  {
    "id": "Self_Development/Book/아주 작은 습관의 힘 (Atomic Habits)/습관추적",
    "title": "습관추적에 대하여",
    "date": "2025-07-12",
    "excerpt": "습관추적은 어떤 습관을 만들기 위한 훌륭한 도구다",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "'아주 작은 습관의 힘'에서 이야기하는 컨셉은 아주 명확합니다. 어떤 일을 꾸준히 하기 위해서는 '명확해야 하며', '하기 쉬워야 하고', '만족감을 줘야 한다'. 그리고 이렇게 만들기 위한 좋은 방법 중 하나로 습관 추적을 설명하고 있습니다. 습관을 추적한다는 건 내가 어떤 일을 꾸준히 하고 있는지와 어떤 일을 해야하는지를 적고 확인해나가는 것입니다. 내가 어떤 일을 해야하는지 적어서 해야할 일을 명확히 만들고, 했을 때는 이를 기록하면서 성취감을 느낄 수 있습니다. > 꾸준히 하기 위해서는 성취감을 받아야 한다. 이를 위해 습관추적은 좋은 도구다. 특히 저에게는 매일 해나가면서 꾸준히 쌓이는 느낌이 강력한 동기부여로 작용합니다. 개발을 하는 입장에서 가장 대표적인게 깃헙의 잔디밭이라고 생각합니다. <figure> <img src=\"/post/습관추적/깃헙잔디밭.png\" alt=\"깃헙 잔디밭\"> <figcaption>깃헙 잔디밭</figcaption> </figure> 내가 꾸준히 커밋(코드 작업)을 하고 있는게 한눈에 보이고, 이게 이 일을 더 꾸준히 하는데 동기부여를 해줍니다. > 꾸준히 쌓여가는 느낌은 굉장한 동기부여를 제공한다. 또한 이렇게 쌓여가는 느낌을 받기 위해 블로그도 작성하기도 합니다. 공부한 것들을 글로 작성하는 연습을 하기도 하고, 또 이렇게 공부한 것들이 하나씩 쌓여가는게 눈에 보이기 때문에 성취감을 줍니다. 그래서 그냥 공부하는 것보다 저는 블로그에 글로 남기면서 하나씩 쌓여가는 걸 눈으로 볼 때 더 큰 성취감과 동기부여를 받습니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-11-긍정적인피드백주기",
    "title": "긍정적인 피드백 주는 표현들 복습",
    "date": "2025-07-11",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "리뷰 데이 I like the way I appreciate it I know that its hard I want you to know that you call me any time. This too shall pass. I was really impressed with. It couldnt be any better I couldnt have done it without you. You did a great job on work Im happy to see that I can tell that I believe in you Youve made it this far. You`re almost there",
    "public": true
  },
  {
    "id": "Self_Development/Book/아주 작은 습관의 힘 (Atomic Habits)/만족스럽게만들어라",
    "title": "만족스럽게 만들어라",
    "date": "2025-07-11",
    "excerpt": "지속하기 위해서는 즉각적인 보상이, 그만두기 위해서는 즉각적인 페널티가 필요하다",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "어떤 일을 지속하기 위해 중요한 건 뭘까? 많은 사람들이 원하는 결과를 얻기 위해서는 '꾸준함'이 가장 중요하다고 말합니다. 이 꾸준함은 어떻게 만들 수 있을까요? 강력한 의지와 다짐이 필요한 걸까요? '아주 작은 습관의 힘'에서는 원하는 습관을 만들기 위한 구체적인 방법들을 제시하고 있습니다. 그 중 하나는 해야하는 일을 아주 작게 쪼개라는 것입니다. 쉬워야 달라진다/쉬워야달라진다) 이에 이어서 저자는 어떤 일을 꾸준히 하기 위해서는 만족스럽게 만들어라 라고 말하고 있습니다. 우리의 뇌를 이해해보자 우리의 뇌는 과거에 맞춰져 있습니다. 뇌과학에서 밝혀진 바에 따르면, 인간의 뇌는 수십만 년 동안 수렵채집 시대의 환경에 최적화되어 진화했습니다. 우리가 현대 문명에 살기 시작한 것은 불과 몇 천 년에 불과하기 때문입니다. 이러한 진화적 배경 때문에 우리의 뇌는 장기적인 목표보다는 즉각적인 보상에 훨씬 더 강하게 반응하도록 설계되어 있습니다. 원시 시대에는 지금 당장 얻을 수 있는 음식, 안전, 번식 기회가 생존에 직결되었기 때문입니다. 따라서 우리가 장기적인 목표를 달성하려면 이러한 뇌의 특성을 이해하고 활용해야 합니다. > 우리의 뇌는 당장 즐거운면 하고 당장 고통스러우면 안한다. 우리의 의지력은 한계가 있기 때문에, 이를 이해하고 활용해서 꾸준히 할 수 있는 환경 혹은 시스템을 만들어야 합니다. > 우리는 본능의 씨앗들에 반기를 들지 못하며 그것들과 함께 나아가야 한다. 가장 최선의 방법은 장기적으로 보상을 주는 습관에는 즉시적인 기쁨의 조각들을 덧붙이고, 그렇지 않은 습관에는 고통의 조각들을 덧붙이는 것이다. > > - 《아주 작은 습관의 힘》 원하는 습관을 만들려면 즐겁게 만들자 > 습관을 계속 유지하기 위해서는 성공했다는 느낌을 필수적으로 받아야 한다. 비록 아주 사소한 방식일지라도 말이다. 성공했다고 느끼는 것은 습관이 성과를 냈고, 그 일이 노력할 만한 가치가 있다는 신호이기 때문이다. > > - 《아주 작은 습관의 힘》 우리는 즉각적으로 즐거움을 느낄 수 있는 일을 하게 됩니다. 하지만 원하는 목표들은 지연적인 보상을 제공하는 일들이 많습니다. 이를 위해 원하는 목표로 가는 길을 아주 잘게 쪼개고, 각 단계에서 즉각적인 보상을 받도록 설계해야합니다. 가장 좋은 건 \"성공했다는 느낌\"을 받도록 설계하는 것입니다. > 실제로 저는 이 전략으로 운동하는 습관을 만들었고, 6년정도 꾸준히 이어서 하고 있습니다. 많은 사람들이 운동을 시작할 때 \"진짜 운동 열심히 할거야, 진짜 빡세게 해야지\" 등의 다짐을 하고 시작하게 됩니다. 하지만 막상 운동을 하게 되면 힘들고 피곤하기 때문에 부정적인 감정을 경험하게 되고 행동을 이어가지 못합니다. 제가 운동을 시작할 때의 목표는 \"헬스장에 들어가서 푸쉬업 1개만 하고 칭찬하기\" 였습니다. 그래서 아무 생각없이 헬스장에 가서 진짜 푸쉬업 1개만 하고 \"성공했다는 느낌\"을 받으며 스스로를 칭찬했습니다. 이러다보니 헬스장에 가는 건 저에게 성취감을 주는 재밌는 일이 되었고, 점점 욕심을 느껴 강도를 늘려나갔습니댜. 푸쉬업을 기준으로 보면 처음에는 한번에 10개정도 밖에 못했지만 지금은 100개정도는 할 수 있는, 10배 이상의 근력을 얻을 수 있었습니다. 정리하면 원하는 목표를 달성하기 위해서는 즉각적인 보상을 받도록 설계해야 합니다. 그런 환경, 시스템을 만들어야합니다. 매일할 수 있는 작은 목표들로 쪼개고, 이를 했을 때 \"성공했다는 느낌\"을 받도록 설계 해야하고, 이때 이를 이어갈 수 있습니다. 나쁜 습관을 만들려면 페널티를 주자 반대로 나쁜 습관을 없애려면 보상을 지연시키거나 패널티를 주면 됩니다. 하지만 제 생각에 이 부분은 적용하기 어려웠습니다. 휴대폰이나 노트북으로 너무나도 쉽게 즉각적인 보상을 얻을 수 있는 세상이고, 이를 지연시킬 수 있는 방법은 아직 찾지 못했습니다. 그래서 제가 나름대로 적용하고 있는 부분은 \"회의적인 생각\"을 적용하는 것입니다. \"유튜브 봐서 뭐해? 재미도 없는거\" 이런식으로 피해야하는 행동들에 대해서는 하고 싶지만 참는게 아닌 하는게 의미가 없다고 생각하려고 하고 있습니다. 실제로 이런 생각을 가지고 나니, 해봤자 의미없는 행동들은 하지 않게 되었고, 실질적으로 나에게 성취감을 주는 행동들에 집중할 수 있게 되었습니다. 정리 우리의 뇌는 즉각적인 보상에 집중한다. 미래의 보상에는 집중하지 못한다. 이를 활용해 전략을 세우고 환경을 만들자. 의지력의 문제가 아니다! 작은 목표들로 쪼개고 이를 해나가며 \"성공했다는 느낌\"에 집중하자 피해야 하는 일들에는 회의적인 생각을 적용해 불편한 느낌을 가지도록 하자",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/ill-conditioned-matrix-theory",
    "title": "Ill-conditioned 행렬과 고유값: 연속형 처치 R-learner의 이론적 배경",
    "date": "2025-07-11",
    "excerpt": "연속형 처치에서 발생하는 non-identification 문제와 ill-conditioned 행렬, 고유값의 관계를 자세히 설명",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Linear Algebra",
      "Matrix Analysis"
    ],
    "content": "Ill-conditioned 행렬과 고유값: 연속형 처치 R-learner의 이론적 배경 개요 연속형 처치 R-learner에서 발생하는 non-identification 문제는 수학적으로 ill-conditioned 행렬 문제로 나타납니다. 이 글에서는 이 개념들을 단계별로 자세히 설명합니다. 고유값(Eigenvalue)이란? 1.1 기본 개념 고유값은 행렬의 중요한 특성을 나타내는 스칼라 값입니다. 수학적 정의: 행렬 A에 대해, 0이 아닌 벡터 v와 스칼라 λ가 다음을 만족할 때: $$Av = \\lambda v$$ λ를 A의 고유값, v를 고유벡터라고 합니다. 1.2 직관적 이해 고유값은 행렬이 벡터를 어떤 방향으로 얼마나 늘리거나 줄이는지를 나타냅니다. 예시: $$ A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix} $$ 고유값: $\\lambda_1 = 2$, $\\lambda_2 = 3$ 고유벡터: $v_1 = [1, 0]$, $v_2 = [0, 1]$ λ₁ = 2: x축 방향으로 2배 늘림 λ₂ = 3: y축 방향으로 3배 늘림 1.3 고유값의 의미 λ > 1: 해당 방향으로 확대 0 < λ < 1: 해당 방향으로 축소 λ = 0: 해당 방향으로 완전히 압축 (정보 손실) λ < 0: 해당 방향으로 반전 Ill-conditioned 행렬이란? 2.1 조건수(Condition Number) 조건수는 행렬이 얼마나 \"불안정한지\"를 측정하는 지표입니다. 정의: $$ \\kappa(A) = \\ A\\ \\times \\ A^{-1}\\ $$ 여기서 $\\ A\\ $는 행렬 A의 노름(norm)입니다. 2.2 Ill-conditioned의 의미 Well-conditioned: 조건수가 작음 (≈ 1) Ill-conditioned: 조건수가 큼 (≫ 1) 2.3 왜 문제가 되는가? Ill-conditioned 행렬에서는 작은 입력 변화가 큰 출력 변화를 야기합니다. 예시: $$ A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{bmatrix} $$ $$ A^{-1} \\approx \\begin{bmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{bmatrix} $$ 조건수 $\\approx 4000$ (매우 큼!) 입력에 작은 노이즈가 있으면 해가 크게 달라집니다. 연속형 처치에서 Ill-conditioned가 발생하는 이유 3.1 Non-identification 문제 연속형 처치에서는 무한히 많은 처치 수준이 존재합니다: 이진 처치: $T \\in \\{0, 1\\}$ (2개 값) 연속형 처치: $T \\in [0, 1]$ (무한 개 값) 3.2 Generalized R-loss의 특성 이진 처치 R-loss: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X)(T - \\pi(X)))^2]$$ 연속형 처치 R-loss: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X,T)(T - \\pi(X)))^2]$$ 3.3 핵심 차이점 이진 처치: τ(X)는 X에만 의존 (유한차원) 연속형 처치: τ(X,T)는 X와 T 모두에 의존 (무한차원) 3.4 행렬로 표현하면 연속형 처치를 행렬로 표현하면: $$ \\begin{bmatrix} \\tau(x_1,t_1) & \\tau(x_1,t_2) & \\cdots & \\tau(x_1,t_n) \\\\ \\tau(x_2,t_1) & \\tau(x_2,t_2) & \\cdots & \\tau(x_2,t_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\tau(x_m,t_1) & \\tau(x_m,t_2) & \\cdots & \\tau(x_m,t_n) \\end{bmatrix} $$ 이 행렬은 매우 큰 크기를 가지며, 많은 행들이 거의 선형 종속입니다. 고유값이 0에 가까워지는 이유 4.1 선형 종속성 연속형 처치에서 τ(x,t) 함수들은 서로 매우 유사합니다: $$\\tau(x, 0.1) \\approx \\tau(x, 0.11) \\approx \\tau(x, 0.12) \\cdots$$ 이는 행렬의 행들이 거의 선형 종속임을 의미합니다. 4.2 고유값의 의미 고유값이 0에 가까움 = 해당 방향의 정보가 거의 없음 예시: 행렬 A의 고유값: $\\lambda_1 = 10$, $\\lambda_2 = 0.001$, $\\lambda_3 = 0.0001$ $\\lambda_1 = 10$: 이 방향은 정보가 풍부 $\\lambda_2 = 0.001$: 이 방향은 정보가 거의 없음 $\\lambda_3 = 0.0001$: 이 방향은 거의 정보 없음 4.3 연속형 처치에서의 상황 연속형 처치에서는: 대부분의 고유값이 0에 가까움 몇 개의 고유값만 의미 있음 행렬이 거의 특이(singular)에 가까움 왜 문제가 되는가? 5.1 수치적 불안정성 작은 노이즈 → 큰 오차 원래 문제: $Ax = b$ 노이즈가 있는 문제: $(A + \\epsilon)\\tilde{x} = b + \\delta$ 결과: $\\ x - \\tilde{x}\\ $가 매우 클 수 있음 5.2 해의 유일성 부족 무수히 많은 해가 존재: $$\\tau(x,t) = \\tau_0(x,t) + \\epsilon(x,t)$$ 여기서 $\\epsilon(x,t)$는 R-loss를 거의 변화시키지 않는 함수 5.3 추정의 어려움 어떤 해가 \"정답\"인지 알 수 없음 해결 방법: Tikhonov 정규화 6.1 기본 아이디어 정규화 항 추가: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X,T)(T - \\pi(X)))^2] + \\lambda\\ \\tau\\ ^2$$ 6.2 수학적 효과 행렬 표현: $$(A^TA + \\lambda I)x = A^Tb$$ 고유값 변화: 원래 고유값: $\\lambda_i$ 정규화 후: $\\lambda_i + \\lambda$ 6.3 왜 도움이 되는가? 0에 가까운 고유값들이 λ만큼 증가 행렬이 더 안정적(well-conditioned)이 됨 유일한 해 보장 실제 예시 7.1 간단한 예시 연속형 처치 함수: $$\\tau(x,t) = 2t + 0.1\\sin(10t)$$ 문제: sin(10t) 항이 R-loss에 거의 영향을 주지 않음 해결: 정규화를 통해 \"부드러운\" 함수 선호 7.2 수치적 예시 원래 행렬 A의 고유값: $[10, 0.1, 0.01, 0.001, \\ldots]$ 정규화 후 ($\\lambda=1$): $[11, 1.1, 1.01, 1.001, \\ldots]$ 조건수가 크게 개선됩니다. 결론 연속형 처치 R-learner에서 발생하는 non-identification 문제는: 무한차원 함수 공간에서 발생 Ill-conditioned 행렬로 수학적 표현 고유값들이 0에 가까움으로 특징 Tikhonov 정규화로 해결 이러한 이해는 연속형 처치 효과 추정의 이론적 기반을 제공합니다. 참고 자료 추천 도서: Golub, G. H., & Van Loan, C. F. (2013). Matrix computations. JHU press. Trefethen, L. N., & Bau, D. (1997). Numerical linear algebra. SIAM. 핵심 논문: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. Hansen, P. C. (1998). Rank-deficient and discrete ill-posed problems: numerical aspects of linear inversion. 온라인 자료: MIT OpenCourseWare: Linear Algebra Khan Academy: Eigenvalues and Eigenvectors",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Towards-R-learner-with-Continuous-Treatments",
    "title": "[Paper Review] Towards R-learner with Continuous Treatments",
    "date": "2025-07-11",
    "excerpt": "연속형 처치를 위한 R-learner를 어떻게 구현할 수 있는지에 대한 논의",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link 논문의 배경 --- Quasi-Oracle Estimation of Heterogeneous Treatment Effects 에서 개인화 처치 효과를 추정하는 방법을 제안했습니다. <small> * 개인화 처치효과 : 어떤 처치를 했을 때 개인별로 어떤 효과가 있을지 추정한 것</small> 개인화 처치효과를 추정하는 건 인과추론의 가장 핵심적인 문제이며, 이는 다양한 분야에서 통찰을 제공합니다. 예를 들어 정밀의학에서는 환자별 처치 효과를 추정하여 처치 선택을 결정하고, 교육에서는 학생별 처치 효과를 추정하여 교육 방법을 결정하고, 온라인 마케팅에서는 사용자별 처치 효과를 추정하여 맞춤형 광고를 제공하고, 오프라인 정책 평가에서는 지역별 처치 효과를 추정하여 정책을 결정할 수 있습니다. > 즉, 개인화 처치효과를 알게 되면 어떤 선택에 대한 근거를 제공할 수 있습니다. 기존 논문에서는 이진 처치의 개인화 처치효과를 추정하는 방법을 이야기했고, 이 논문에서는 이를 확장해서 연속형 처치에 대해서도 이를 적용하기 위한 방법론을 이 논문에서 이야기하고 있습니다. [[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects](/posts/Data%20Science/Causal%20Inference/Paper%20Review/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects) <- 이 논문에 대한 리뷰는 여기서 확인할 수 있습니다. > 기존의 방법을 확장할 때 발생하는 문제와 이를 해결한 방법론에 대한 이야기에 집중해서 이 논문을 이해했습니다. 간단히 요약하면 다음과 같습니다 이진 처치에서는 처치효과를 추정하는 방법이 있었는데, 이를 연속형 처치로 확장하면 문제가 발생한다. (유일해를 가지지 않는 문제) 이를 해결하기 위해서 2가지 방법을 제안한다. Tikhonov regularization zero-constraining operator를 통해서 해결한다. 논문 내용 정리 --- Abstract --- > However, extending the R-learner framework from binary to continuous treatments introduces a non-identifiability issue, as the functional zero constraint inherent to the conditional average treatment effect cannot be directly imposed in the R-loss under continuous treatments 이 논문의 핵심 주장입니다. > binary 를 continuous로 확장하면 non-identifiability issue가 발생한다. 이걸 해결하기 위한 과정을 identification strategy라고 칭하고 있습니다. 2가지 과정을 통해 이를 구현한다고 합니다. Tikhonov regularization zero-constraining operator 왜 이슈가 발생하고 어떻게 해결했는지를 이해하는게 이 논문의 핵심이라고 이해됩니다. 그리고 이 논문에서 generalized R-loss를 정의했는데, 이 수식이 어떻게 유도되는지를 이해하는 부분도 큰 도움이 됩니다. Introduction --- > Estimating heterogeneous treatment effects is fundamental in causal inference and provides insights into various fields, including precision medicine, education, online marketing, and offline policy evaluation 개인별 처치효과를 추정하는 건 다양한 분야에서 중요한 정보를 제공해줄 수 있기에 중요한 문제입니다. The treatment effect heterogeneity can be quantified by: $$ \\begin{equation} \\tau(x, t) = E[Y^{(t)} - Y^{(0)} X = x] \\end{equation} $$ $t = 0$ : reference treatment level heterogeneous treatment effects는 conditional average treatment effect (CATE)로 정의됩니다. 이는 조건부로 처치효과를 계산한 값을 의미합니다. (환자의 조건이 x일 때의 ATE) > On the contrary, the R-learner and its variants (Kennedy, 2023) target the treatment effect estimatio Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects 기존의 다른 연구들은 이 CATE를 직접적으로 추정하지 않았지만 R-learner 모델은 이를 직접적으로 추정하고 있습니다. > The R-learner capitalizes on the decomposition of the outcome model initially proposed by Robinson (1988) in partially linear models and extends for machine learning-based treatment effect estimation (Nie and Wager, 2021) R-learner는 partially linear model에서 제안한 수학적 분해 방법을 기반으로 삼아서 이를 머신러닝 기반 처치효과 추정으로 확장된 개념입니다. 이에 대한 간단한 정리는 What is FWL?에서 확인할 수 있습니다. 참고 논문: Robinson, P. M. (1988). Root-N-consistent semiparametric regression. Econometrica, 56(4), 931-954. [논문 링크] Nie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319. [논문 링크] > Notably, when using the two nuisance functions estimated under flexible models, the R-learner preserves the oracle property of treatment effect estimation as though the nuisance functions were known. Despite these advantages, the current R-learner framework applies only to binary or categorical treatments. R-learner의 중요한 특징은 유연한 모델을 통해 두 개의 nuisance function을 추정하더라도, 마치 이 함수들이 정확히 알려진 것처럼 처치효과 추정의 oracle 성질을 유지한다는 점입니다. 하지만 이러한 장점에도 불구하고, 현재 R-learner 프레임워크는 이진형 또는 범주형 처치에만 적용할 수 있다는 한계가 있습니다. > In this article, we extend the R-learner framework to estimate the conditional average treatment effect flexibly with continuous treatments. This extension is nontrivial in both identification and estimation. Echoing the approach of Nie and Wager (2021), we focus on adapting the generalized R-learner loss function with continuous treatments. 이 논문에서는 R-learner 프레임워크를 연속형 처치에 대해서 확장해서 CATE를 추정하는 방법을 제안합니다. 이 확장은 식별과 추정 모두에서 어려운 문제입니다. Nie와 Wager (2021)의 접근 방식을 따라서 연속형 처치에 대한 일반화된 R-learner 손실 함수를 적용하는 방법을 중점적으로 다룹니다. 이진처치 ($T = 1$ or $T = 0$ 이 존재하는 경우)와 다르게 연속형일 경우에 발생하는 어려움들에 대해 이야기하며 이를 해결하기 위한 방법을 제안합니다. > Unlike the binary-treatment case, we demonstrate that directly minimizing the generalized R-loss does not uniquely identify $\\tau (x, t)$ but instead identifies a broad class of functions. 이진처치와 달리, 연속형 처치에서는 R-loss를 최소화하는 것만으로는 유일한 CATE 함수를 식별할 수 없고, 대신 함수들의 넓은 집합만을 식별할 수 있다고 설명합니다. 이것이 바로 non-identifiability issue의 핵심입니다. 이진처치의 경우 zero condition ($\\tau (x, t): \\tau (x, 0) \\equiv 0$)을 만족하는 것이 쉽지만, 연속형 처치의 경우에는 이를 만족하는 것이 쉽지 않기 때문입니다. 간단하게 설명해보면, 이진처치의 경우 zero condition을 만족하고 이게 cate function을 유일하게 추정하도록 해주지만, 연속형 처치의 경우에는 zero condition을 만족하는 것이 쉽지 않기 때문에 이를 만족하는 함수들의 넓은 집합만을 추정하게 된다고 이해할 수 있습니다. 논문에서는 이 non-identifiability 문제를 해결하기 위해 ℓ2-정규화된 R-learner를 제안합니다. 이 방법은 Tikhonov 정규화 원리를 기반으로 합니다. Tikhonov 정규화란? Tikhonov 정규화는 ill-posed problem(잘 정의되지 않은 문제)을 해결하기 위한 방법입니다. 연속형 처치에서 발생하는 non-identifiability 문제가 바로 이런 ill-posed problem의 대표적인 예입니다. 핵심 아이디어: 문제가 너무 유연해서 해가 무수히 많을 때 추가적인 제약조건(정규화)을 도입하여 해를 유일하게 만듦 수학적으로는 f ² 같은 정규화 항을 손실 함수에 추가 2단계 추정 과정 논문의 핵심은 2단계 추정 과정을 통해 문제를 해결하는 것입니다: 1단계: 중간 함수 추정 $$\\tilde{\\tau}(x, t) = \\tau(x, t) - \\mathbb{E}\\{\\tau(X, T) \\mid X = x\\}$$ 2단계: Zero-constraining operator를 통한 변환 $$\\tau(x, t) = \\text{변환}(\\tilde{\\tau}(x, t))$$ Zero-constraining operator의 역할 이 operator는 추정된 함수가 항상 zero condition을 만족하도록 보장합니다: τ(x, 0) = 0 (기준 처치 수준에서의 효과는 0) 이를 통해 유일한 해를 찾을 수 있음 Sieve 방법론 Sieve 방법은 무한차원 함수 공간을 유한차원으로 근사하는 방법입니다: 기본 아이디어: 무한차원 함수를 유한개의 기저 함수들의 선형결합으로 근사 예: 다항식, 스플라인, 푸리에 급수 등 수학적 표현: $$\\tau(x, t) \\approx \\sum_i \\beta_i \\phi_i(x, t)$$ 여기서 $\\phi_i(x, t)$는 기저 함수들입니다. 저차원 행렬과 Non-identification 일반적인 sieve 회귀와 달리, 이 논문에서는 저차원 행렬이 등장합니다. 이는 연속형 처치에서 발생하는 non-identification 문제의 수학적 표현입니다. 핵심 개념: Generalized R-loss의 non-identification 특성 때문에 행렬이 ill-conditioned가 됨 행렬의 고유값들이 0에 가까워져서 수치적 불안정성 발생 이는 행렬 교란 이론(matrix perturbation theory)과 스펙트럴 분석이 필요한 이유 자세한 이론적 배경: Ill-conditioned 행렬과 고유값: 연속형 처치 R-learner의 이론적 배경 수렴 속도의 특징 핵심 결과: > \"nuisance 함수들이 oP(n^(-1/4)) 수렴 속도로 근사될 수 있다면, 추정량의 수렴 속도는 결과 모델의 매끄러움에 의존하지 않고, 오직 CATE와 propensity score 함수의 매끄러움에만 의존한다\" 의미: 결과 모델의 복잡성에 관계없이 좋은 성능 보장 CATE와 propensity score만 잘 추정되면 됨 이는 double robustness의 연속형 버전 점근적 정규성과 추론 점근적 정규성: 추정량이 정규분포로 수렴 이를 통해 신뢰구간과 가설검정 가능 닫힌 형태 분산 추정량: 복잡한 부트스트랩 없이도 분산 계산 가능 계산 효율성 향상 참고 논문: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. Doklady Akademii Nauk SSSR, 151(3), 501-504. [논문 링크] Bhatia, R. (2013). Matrix analysis. Springer Science & Business Media. [책 링크] Chen, X. (2007). Large sample sieve estimation of semi-nonparametric models. Handbook of econometrics, 6, 5549-5632. [논문 링크] 핵심 개념들: Ill-posed problem: 해가 유일하지 않거나 불안정한 문제 Tikhonov regularization: 정규화를 통한 ill-posed problem 해결 Sieve method: 무한차원을 유한차원으로 근사하는 방법 Matrix perturbation theory: 행렬의 작은 변화가 고유값에 미치는 영향 연구 Spectral analysis: 행렬의 고유값과 고유벡터 분석 1.1 Setup and notation 이론적 배경과 관련한 수식을 정리합니다. $\\{Z_i = (X_i, T_i, Y_i)\\}_{i=1}^n$ : independent and identically distributed samples from the distribution of $(X, T, Y)$ $X = (X^{(1)}, \\ldots, X^{(d)})$ : $d$-dimensional vector of covariates. $Y^{(t)}$ : potential outcome had the unit received treatment level $T = t \\in \\mathbb{R}$ $\\tau(x, t)$ : causal estimand defined in (1) Under Rubin's causal model framework (Rubin, 1974), $$ \\begin{equation} \\tau(x, t) = E[Y^{(t)} - Y^{(0)} X = x] \\end{equation} $$ > Due to the fundamental problem in causal inference that not all potential outcomes can be observed for a particular unit, $\\tau(x, t)$ is not identifiable without further assumptions. We employ common assumptions for continuous treatments (Kennedy et al., 2017). 인과추론에서는 모든 관측값을 확인할 수 없는 상태에서 처치효과를 추정하기 위해서 3가지의 가정이 존재합니다. Assumption 1 (No unmeasured confounding). We have $\\{Y^{(t)}\\}_{t \\in \\mathcal{T}} \\perp\\!\\!\\!\\perp T \\mid X$ Assumption 2 (Stable unit and treatment value). When $T = t \\in \\mathcal{T}$, we have $Y = Y^{(t)}$ Assumption 3 (Positivity). There exists an $\\epsilon > 0$ such that the generalized propensity score $f(T = t \\mid X = x) \\in (\\epsilon, 1/\\epsilon)$ for any $(x, t) \\in \\mathcal{X} \\times \\mathcal{T}$. Notation: For any vector $v$, $\\ v\\ $ denotes its $\\ell_2$ norm For any random variable $W \\in \\mathcal{W}$, $f(w)$ and $P(w)$ denote its probability density function and probability measure For any function $g(w)$: $P_n\\{g(W)\\} = \\frac{1}{n}\\sum_{i=1}^n g(W_i)$ denotes its empirical expectation $\\ g\\ _{L_2} = \\left\\{\\int_{w \\in \\mathcal{W}} g^2(w) dw\\right\\}^{1/2}$ denotes its $L_2$ norm $\\ g\\ _{L_2^P} = \\left\\{\\int_{w \\in \\mathcal{W}} g^2(w) dP(w)\\right\\}^{1/2}$ denotes its $L_2^P$ norm $\\ g\\ _{\\mathcal{W}} = \\sup_{w \\in \\mathcal{W}} g(w) $ denotes its $L_\\infty$ norm $L_2^P(\\mathcal{W})$ represents the function space of all $g(w)$ with a bounded $L_2^P$ norm When $g(w)$ is a multivariate function, denote $\\ g\\ _{\\mathcal{W}} = \\sup_{w \\in \\mathcal{W}} \\ g(w)\\ $ Nuisance Functions: Conditional outcome mean: $m(x) = E(Y \\mid X = x)$ Generalized propensity score: $\\varpi(t \\mid x) = f(T = t \\mid X = x)$ Full conditional outcome mean model: $$\\mu(x, t) = E(Y \\mid X = x, T = t)$$ Observation noises: $$\\varepsilon_i = Y_i - \\mu(X_i, T_i), \\quad i = 1, \\ldots, n$$ where $E(\\varepsilon_i \\mid X_i, T_i) = 0$, following the definition of $\\mu(x, t)$. Generalized R-learner --- 2.1 The generalized R-loss --- > We first generalize the idea of the Robinson's residual (Robinson, 1988; Nie and Wager, 2021) to the continuous-treatment scenario. 이번 절에서는 일반화된 R-loss를 어떻게 유도하는지에 대해 설명합니다. The unconfoundedness and stable unit and treatment value imply: $$ \\begin{equation} Y_i^{(T_i)} = \\mu(X_i, T_i) + \\varepsilon_i = \\mu(X_i, 0) + \\tau(X_i, T_i) + \\varepsilon_i \\tag{3} \\end{equation} $$ > 해석: > - 첫 번째 등식: Assumption 2와 equation (2)에서 유도 > - 두 번째 등식: Assumption 1과 $\\tau(x, t)$의 정의에서 유도 > - 특징: 비모수적 모델이며 추가적인 구조적 가정이 없음 Step 2: 조건부 기댓값 계산: Given $X_i$, taking the conditional expectation on (3) leads to: $$ \\begin{equation} m(X_i) = E\\left[Y_i^{(T_i)} \\mid X = X_i\\right] = \\mu(X_i, 0) + E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} \\tag{4} \\end{equation} $$ > 전체 기댓값 법칙 적용: > $$E(\\varepsilon_i \\mid X_i) = E\\left[E(\\varepsilon_i \\mid X_i, T_i) \\mid X_i\\right] = E[0 \\mid X_i] = 0$$ 일반화된 Propensity Score 기댓값: The notation $E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\}$ in (4) highlights the dependency of the conditional expectation on the generalized propensity score as: $$E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} = \\int_{t \\in \\mathcal{T}} \\tau(X_i, t) \\varpi(t \\mid X_i) dt$$ > 핵심: 연속형 처치에서는 적분을 통해 모든 처치 수준에 대한 가중 평균을 계산 Step 3: 잔차 도출: By subtracting (4) from (3) on both left- and right-hand sides, we have: $$ \\begin{equation} Y_i^{(T_i)} - m(X_i) = \\tau(X_i, T_i) - E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} + \\varepsilon_i \\tag{5} \\end{equation} $$ > 해석: > - 좌변: 관찰된 결과에서 조건부 평균을 뺀 값 (Robinson's residual) > - 우변: 처치효과에서 평균 처치효과를 뺀 값 + 노이즈 Step 4: 손실 함수 도출: By treating the left-hand side of (5) as the response and the right-hand side except $\\varepsilon_i$ as the mean function, we derive the following population loss function: $$ \\begin{equation} L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right] \\tag{6} \\end{equation} $$ > 핵심 특징: > - 최적해: $h = \\tau$에서 최소화됨 > - 일반화: 이진 처치 R-learner의 자연스러운 확장 > - 참고: Nie and Wager (2021, §7)의 다중 처치 설정에서 유사한 손실함수 등장 이진 처치와의 연결: > In particular, under the binary-treatment case, $\\tau(x, t)$ reduces to $\\{\\tau(x, 0), \\tau(x, 1)\\}$, where $\\tau(x, 0) = E(Y^{(0)} - Y^{(0)} \\mid X = x) = 0$ for any $x \\in \\mathcal{X}$, and $\\tau(x, 1)$ is the conditional average treatment effect of interest. It suffices to estimate $\\tau(x, 1)$ by solving the $h(\\cdot, 1)$ that minimizes (6), after imposing a zero condition of $h(\\cdot, 0)$: $$ \\begin{equation} h(x, 0) = 0, \\quad \\text{for any } x \\in \\mathcal{X} \\tag{7} \\end{equation} $$ > Zero condition: 기준 처치 수준에서의 효과는 0으로 설정 이진 처치 R-loss로의 환원: 유도된 일반화된 R-loss가 기존의 이진 처치에서의 loss function으로 유도될 수 있음을 설명합니다. > More specifically, observing that under (7) one has $h(X, T) - E_e\\{h(X, T) \\mid X\\} = \\{T - e(X)\\}h(X, 1)$ a.s., where $e(x) = \\text{pr}(T = 1 \\mid X = x)$ is the propensity score, the R-loss function (6) reduces to: $$ \\begin{equation} L_b(h) = E\\left[\\left\\{Y - m(X) - \\{T - e(X)\\}h(X, 1)\\right\\}^2\\right] \\tag{8} \\end{equation} $$ > 핵심: > - 조건: $h(x, 0) = 0$ (zero condition) > - 결과: $h(X, T) - E_e\\{h(X, T) \\mid X\\} = \\{T - e(X)\\}h(X, 1)$ > - 의미: 일반화된 R-loss가 이진 처치의 고전적 R-loss로 환원됨 > 핵심: T가 1 or 0인 상황에서 zero condition을 고려해 (6)번 수식을 전개하면 기존의 이진 처치에서의 손실함수 수식과 같은 (8)번 수식을 얻을 수 있습니다. 2.2 Non-identification of the generalized R-loss --- > The R-learner for continuous treatment will have poor estimation performance, due to the non-unique identifiability of the generalized R-loss 핵심 문제: 이 절에서는 non-identification 문제에 대해 설명합니다. 연속형 처치에서 일반화된 R-loss를 직접 최소화하면 유일한 해를 찾을 수 없다는 것이 핵심 문제입니다. 해집합 정의: $$ \\begin{equation} S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\} \\tag{9} \\end{equation} $$ > 해석: $S$는 목표 함수 $\\tau(X, T)$에 공변량 $X$의 함수 $s(X)$를 더한 모든 함수들의 집합 검증 과정: > It is easy to check that for any $h \\in S$: $$ Y - m(X) - [h(X, T) - E_{\\varpi}\\{h(X, T) \\mid X\\}] = Y - m(X) - [\\tau(X, T) - E_{\\varpi}\\{\\tau(X, T) \\mid X\\}] \\text{ a.s.} $$ > 의미: $S$에 속한 모든 함수 $h$가 동일한 손실값을 가짐 Non-identification 문제: > From (6), any function $h \\in S$ minimizes the generalized R-loss $L_c(\\cdot)$. Therefore, when $T$ is continuous, directly minimizing the generalized R-loss fails to uniquely identify the target estimand $\\tau(x, t)$, as there are infinitely many solutions in $S$. > 핵심 문제: > - 연속형 처치에서는 무한히 많은 해가 존재 > - 유일한 CATE 함수를 식별할 수 없음 > - 이는 ill-posed problem의 전형적인 예 이론적 근거: > This result theoretically substantiates the ill-posedness of estimating $\\tau(x, t)$ by minimizing the empirical counterpart of $L_c(\\cdot)$ using nonparametric estimators, and also explains the failure-to-estimate issue illustrated in Fig. 1. > Part (i) of Proposition 1 below rigorously proves that $S$ in fact contains all minima of $L_c(\\cdot)$ in $L_2^P(X, T)$. 이진 처치와의 대비: > In contrast, minimizing the binary-treatment R-loss (8) which incorporates the zero condition (7), can successfully identify $\\tau$, because (7) narrows the general solution set $S$ into: $$ \\begin{equation} S^\\natural = \\{h \\mid h(X, T) = \\tau(X, T) \\text{ a.s.}\\} \\tag{10} \\end{equation} $$ > 핵심 차이: > - 연속형: $S$ (무한히 많은 해) > - 이진형: $S^\\natural$ (유일한 해) > - 원인: Zero condition (7)이 해집합을 좁혀줌 Proposition 1 - Suppose Assumptions 1–2 hold. Then, the following statements hold: 이 부분은 generalized loss function (6) 을 최소화하면 h는 다양한 해를 가지지만, zero-condition을 녹여내 정리한 binary loss function (8) 을 최소화하면 h는 유일한 해를 가진다는 것을 보여줍니다. > Part (i): 처치 T가 연속형인 경우, S는 다음 최적화 문제의 해답해다. > - 최적화 문제: $$ \\begin{equation} \\arg\\min_{h \\in L_2^P(X,T)} L_c(h) \\tag{11} \\end{equation} $$ > - 해집합: $S$ > Part (ii): 처치 T가 이진형인 경우, S는 다음 최적화 문제의 해답해다. > - 최적화 문제: $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ > - 해집합: $S^\\natural$ > 결과: 관심 함수들의 집합 $\\mathcal{L}_b = \\{h \\mid h(\\cdot, 1) \\in L_2^P(X) \\text{ and } h(X, 0) = 0 \\text{ a.s.}\\}$ 중에서, (10)의 $S^\\natural$가 다음 최적화 문제의 해집합이다: > - 최적화 문제: $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ > - 해집합: $S^\\natural$ > 의미: 이진형 처치에서는 zero condition이 적용된 R-loss $L_b(h)$의 최소화가 유일한 해 $S^\\natural$를 제공 핵심 차이점: 구분 연속형 처치 이진형 처치 ------ ------------- ------------- 최적화 문제 $\\arg\\min_{h \\in L_2^P(X,T)} L_c(h)$ $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ 해집합 $S$ (무한히 많은 해) $S^\\natural$ (유일한 해) 제약조건 없음 $h(X, 0) = 0$ (zero condition) 식별 가능성 ❌ 불가능 ✅ 가능 2.3 One-step nonparametric identification with a functional zero constraint > 이진 처치의 경우 zero-condition를 loss function에 녹여낼 수 있어 유일해를 얻을 수 있었지만, 연속 처치의 경우 유일해를 얻지 못한다. 그럼 optimization problem을 풀 때 zero condition을 적용하면 되지 않을까? 라고 생각해볼 수 있습니다. $$ \\begin{equation} \\arg \\min_{h \\in L_2^P(X,T) \\cap \\{h \\mid h(x,0) = 0 \\text{ for any } x \\in \\mathcal{X}\\}} L_c(h) \\tag{12} \\end{equation} $$ 그러나 Proposition 2는 이 전략도 $\\tau(x, t)$의 비모수적 식별을 달성하는 데 계속 실패한다는 것을 보여줍니다. Proposition 2. Assumptions 1–2가 성립하고, $(X, T)$가 유계 밀도함수를 가진다고 가정하자. 즉, $$\\sup_{(x,t) \\in \\mathcal{X} \\times \\mathcal{T}} f(x, t) < \\infty$$ $\\tau^{\\vee}(x, t \\mid s)$를 다음과 같은 형태를 취하는 함수라고 하자: $$ \\begin{equation} \\tau^{\\vee}(x, t \\mid s) = \\begin{cases} \\tau(x, t) + s(x) & \\text{when } t \\neq 0 \\\\ 0 & \\text{when } t = 0 \\end{cases} \\tag{13} \\end{equation} $$ 여기서 $s$는 $L_2^P(X)$의 임의의 함수이다. 그러면 임의의 $s \\in L_2^P(X)$에 대해 $\\tau^{\\vee}(x, t \\mid s)$는 (12)를 해결한다. > 핵심: 임의의 $\\tau^{\\vee}(x, t \\mid s)$는 zero condition (7)을 만족하면서도 여전히 집합 $S$에 속할 수 있다. 즉, zero condition을 최적화문제에 강제로 적용해도 $t \\neq 0$인 부분에서는 여전히 같은 non-identification 문제를 보이고 있습니다. 2.4 Two-step Tikhonov identification and ℓ₂ regularization R-learner --- 2.3에서 강제로 zero condition을 적용해서 문제를 해결하는데 실패했고, 이제 정규화항을 도입해 문제를 해결하는 방법을 제안합니다. Step I: 모집단 수준에서 주어진 $\\rho > 0$에 대해 (11)의 $\\ell_2$-정규화 변형을 해결합니다: $$ \\begin{equation} \\tau_\\rho = \\arg \\min_{h \\in L_2^P(X,T)} L_{c,\\ell_2}(h \\mid \\rho) = \\arg \\min_{h \\in L_2^P(X,T)} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right] \\tag{14} \\end{equation} $$ > The new loss $L_{c,\\ell_2}(h \\mid \\rho)$ is strictly convex over $L_2^P(X, T)$ due to the addition of a strictly convex functional $\\rho \\ h\\ _{L_2^P}^2 = \\rho E\\{h^2(X, T)\\}$. > Thus minimizing $L_{c,\\ell_2}(h \\mid \\rho)$ becomes well-posed and yields a unique functional minimum $\\tau_\\rho$. Theorem 1 explicitly characterizes this unique minimum. 강한 (엄격한) convex function을 loss function에 추가하여, loss function을 convex하게 만들어 문제를 해결할 수 있다는 이야기입니다. 이론적 배경: Tikhonov regularization과 convex optimization의 자세한 원리는 Tikhonov Regularization과 Convex Optimization에서 확인할 수 있습니다. Theorem 1. 집합 $S$ 내에서 다음과 같은 중간 함수를 정의하자: $$ \\begin{equation} \\tilde{\\tau}(x, t) = \\tau(x, t) - E\\{\\tau(X, T) \\mid X = x\\} \\tag{15} \\end{equation} $$ Assumptions 1–2가 성립하고 $\\tau \\in L_2^P(X, T)$일 때, 주어진 $\\rho > 0$에 대해 (14)의 해집합은 다음과 같다: $$S_\\rho = \\{h \\mid h(X, T) = \\tau_\\rho(X, T) \\text{ a.s.}\\}$$ 여기서 $$\\tau_\\rho(x, t) = (1 + \\rho)^{-1} \\tilde{\\tau}(x, t)$$ Theorem 1의 의미: Theorem 1은 $\\tau_\\rho$에 $(1 + \\rho)$ 인수를 곱하여 집합 $S$ 내의 중간 함수 $\\tilde{\\tau}$를 식별할 수 있음을 의미합니다: $$\\tilde{\\tau} = (1 + \\rho)\\tau_\\rho$$ 이 $\\tilde{\\tau}$는 원래 R-loss $L_c(h)$의 해가 됩니다. Step II: Zero-constraining Operator Step II에서는 $\\tilde{\\tau}$를 zero-constraining operator $\\mathcal{C}(\\cdot): L_2^P(X, T) \\to L_2^P(X, T)$를 통해 변환합니다: $$ \\begin{equation} \\mathcal{C}(h)(x, t) = h(x, t) - h(x, 0) \\quad \\text{for any } h \\in L_2^P(X, T) \\end{equation} $$ 따라서: $$\\mathcal{C}(\\tilde{\\tau})(x, t) = \\mathcal{C}((1 + \\rho)\\tau_\\rho)(x, t) = (1 + \\rho)\\{\\tau_\\rho(x, t) - \\tau_\\rho(x, 0)\\}$$ Zero-constraining Operator의 역할: Zero-constraining Operator의 핵심 원리: 연산자 $\\mathcal{C}(\\cdot)$는 변환을 거치는 모든 함수가 zero condition (7)을 만족하도록 보장 $\\tau$는 집합 $S$에서 zero condition (7)을 만족하는 유일한 함수 따라서 해집합 $S$의 임의 함수를 변환하면 $\\tau$를 식별할 수 있음 $\\tilde{\\tau} \\in S$이므로, Step II의 $\\mathcal{C}(\\tilde{\\tau})$는 궁극적으로 $\\tau$를 식별함 > 핵심: 2단계 과정을 통해 non-identification 문제를 해결하고 유일한 CATE 함수 $\\tau(x, t)$를 추정할 수 있습니다. Supplementary material for \"Towards R-learner with Continuous Treatments\" S1 : $l_2$ regularization R-learner : Formal algorithm and more discussions 논문에서 의문이 들었던 부분들 정리 --- 왜 이진 처치에서는 Zero Condition이 적용되고, 연속 처치에서는 적용되지 않을까? 이진 처치에서의 Zero Condition 이진 처치의 특성: 처치 수준: $T \\in \\{0, 1\\}$ (이산적) 목표: $\\tau(x, 1)$ (처치 효과)만 추정 기준점: $T = 0$ (통제 그룹) Zero Condition의 자연스러운 적용: $$\\tau(x, 0) = E[Y^{(0)} - Y^{(0)} \\mid X = x] = 0$$ > 이유: > - $T = 0$은 \"처치를 받지 않은 상태\"를 의미 > - 같은 상태에서의 차이는 당연히 0 > - 이는 자연스러운 제약조건이 됨 수학적 효과: 해집합 $S$에서 $h(x, 0) = 0$ 조건을 추가 $S^\\natural = \\{h \\mid h(X, T) = \\tau(X, T) \\text{ a.s.}\\}$로 축소 유일한 해 보장 연속 처치에서의 문제 연속 처치의 특성: 처치 수준: $T \\in \\mathbb{R}$ (연속적) 목표: $\\tau(x, t)$ for all $t \\in \\mathcal{T}$ (모든 처치 수준에서의 효과) 기준점: $T = 0$이 임의적일 수 있음 Zero Condition 적용의 한계: > $L_b$를 통해 이진처치에서는 zero-condtion을 loss 함수에 녹여낼 수 있었는데, 연속형에서는 이를 loss function에 녹여낼 수 없다. 문제는 “zero condition을 R-loss 안에서 직접 구현하기 어렵다”는 점입니다. Binary T에서는 T∈{0, 1}이라 (T-e(X))h(X,1) 형태로 식을 재정리하면 h(x,0)=0이 자동으로 내재됩니다. 핵심 차이점 요약 구분 이진 처치 연속 처치 ------ ----------- ----------- 처치 특성 이산적 $\\{0, 1\\}$ 연속적 $\\mathbb{R}$ 기준점 $T = 0$ (자연스러움) $T = 0$ (임의적) Zero Condition 수식에서 쉽게 강제 가능 무한차원 제약 → 구현 난이도 높음 해집합(이론) $S^\\natural$ (유일) $S$ (무한) 식별 가능성(실전) ✅ 가능 ❌ 추가 장치 필요 \"h(x, 0)=0만 강제하면 되지 않나?\" – 왜 실전에서는 안 통할까? --- 수학적으로는 $h(x,0)=0$을 정확히 부과하면 $s(x)=−\\tau(x,0)=0$이 되어 해가 유일합니다. 문제는 모델 학습 단계에서 이 무한차원 제약을 그대로 구현하기 어렵다는 점에 있습니다. 실무에서는 $h$를 신경망·트리·커널 등 유연한 함수 근사기로 파라미터화하고, 손실을 확률적 경사하강법으로 최소화합니다. 이때 “$t=0$에서 반드시 0”이라는 제약을 네트워크 출력 전체에 강제하기가 쉽지 않습니다. 그래서 학습 과정에서는 $\\hat{h}(x,t) = \\tau(x,t) + s(x)$ 꼴의 함수도 손실을 동일하게 만들 수 있습니다. 이유: R-loss 안의 $E[h(X,T)\\mid X]$ 항에서 $s(x)$가 상쇄되기 때문입니다. 즉, zero condition을 코드에 명시하지 않으면 여전히 식별 실패(ill-posed)가 발생합니다. 따라서 저자들은 다음과 같은 두 단계 절차를 제안합니다. 1단계: 제약 없이 $\\tilde{h}$를 학습 2단계: 특수한 사상(\"zero-constraining operator\")으로 $\\tilde{h}$를 $t=0$에서 0이 되도록 변환 추가로 Tikhonov 정규화를 넣어 수치적 안정성을 확보합니다. 왜 theorem 1에서 중간함수를 정의하는거지? 왜 굳이 ‘중간 함수(tilde tau)’를 한 번 더 정의하느냐를 한마디로 요약하면 다음과 같아요. 비식별(non-identification)의 원인 R-loss를 그대로 최소화하면 $$h(x,t) = τ(x,t) + s(x)$$ 처럼 “x 에만 의존하는 함수”가 얼마든지 덧붙어도 손실값이 똑같습니다. 이 s(x) 때문에 최소값이 무한히 많아져서 해가 유일하지 않아요. 아이디어: ‘x 평균을 0으로 만드는’ 함수로 중심화(center)하기 s(x)는 t 와 무관합니다. 그렇다면 τ 에서 “x 에 따른 평균치”만 쏙 빼버리면 s(x)가 더 이상 붙을 수 없겠죠? 그게 바로 $$\\tilde{\\tau}(x,t) = \\tau(x,t) − E[ \\tau(X,T) X=x ]$$ 입니다. 이렇게 하면 tilde tau 는 어떤 x 에 대해서도 t 값 전체를 평균 내면 0 이 됩니다. 즉 “덧붙일 수 있는 s(x)” 여지가 사라집니다. Why Do We Introduce the Centered Function tilde_tau(x,t)? 정규화와의 연결 L2 정규화(Tikhonov)로 $$argmin_h { R-loss(h) + \\rho‖h‖_2^2 }$$ 를 풀면 유일한 해 $\\tau_\\rho$ 가 나옵니다. Theorem 1 은 “그 유일한 해가 바로 tilde tau 를 (1+ρ) 로 나눈 것”임을 보여줍니다. 즉 $\\tilde{\\tau} = (1+\\rho)\\tau_\\rho$. 결국 1단계에서 정규화로 유일하게 찾은 τ_ρ 로부터 2단계에서 다시 (1+ρ)을 곱해 tilde tau 를 복원하고, 마지막으로 zero-constraining operator 로 원래 τ 를 얻습니다. 한 줄 결론 중간 함수 tilde tau 를 쓰는 이유는 “x 에만 의존하는 불필요한 s(x)를 제거해 식별 문제를 깨끗하게 만들어 주기”입니다. 그렇게 중심화한 뒤 정규화를 걸어야 유일한 해를 안전하게 찾을 수 있어요. 추정값이 작아지는 걸 보완하기 위해 (1+ρ)을 곱하는 이유 아래 순서대로 ‘왜 (1 + ρ)를 곱해야 하나?’를 처음부터 다시 풀어볼게요. (여기서 ρ는 정규화 세기, p라고 쓰였던 기호입니다.) ────────────────── 아주 단순한 1-차원 예로 시작 ────────────────── • 목표 : 어떤 “진짜 값” θ 를 알아내고 싶다. • 하지만 문제(데이터)가 불안정해서 정규화를 넣어 풀기로 했다. 정규화된 최소화 문제 (평균제곱오차 + ρ × 값^2) ↓ F(h) = (h − θ)² / 2 + ρ h² → 여기서 h가 우리가 구할 추정치. ────────────────── 이 문제를 직접 풀어보자 ────────────────── F(h)를 h에 대해 미분해서 0으로 두면 (h − θ) + 2ρh = 0 ⇒ h (1 + 2ρ) = θ ⇒ h = θ / (1 + 2ρ) ※ 논문·포스트에서는 2 대신 1이 붙도록 (1/2)계수 등으로 맞춰 놓았기 때문에 최종계수는 “1 + ρ”가 됩니다. 이름만 다르고 본질은 같아요. 결국 정규화를 넣고 최소화하면 h = θ / (1 + ρ) 즉, 진짜 값이 (1 + ρ)배만큼 작아져 버린 결과를 얻는다. ────────────────── 왜 작아지나? 직관 ────────────────── • 정규화 항 ρ h² 는 “h 값을 0에 가깝게 끌어당기는 스프링” 같은 역할. • 손실식은 ① (h−θ)² → “진짜 값 θ와 가까워져라” ② ρ h² → “값이 클수록 페널티, 0이 좋다” 두 힘의 타협점이 θ의 축소판 h = θ/(1+ρ) 로 나타난 것. ────────────────── 축소(shrinkage) 편향을 어떻게 없앨까? ────────────────── 방법은 간단하다. 정규화를 넣어서 얻은 결과 h를 다시 (1 + ρ)배 키워 주면 된다. θ (원래 값) = (1 + ρ) × h (정규화 해) 그래서 정규화 뒤 보정 단계에서 (1 + ρ)를 곱한다. ────────────────── 함수(tilde τ, τρ) 로 돌아와서 ────────────────── • 함수 전체가 스칼라 θ 대신 “tilde_tau(x,t)” 라고 생각하면 위 계산이 점마다 동시에 일어난다. • 정규화가 끝나고 얻은 함수 τρ(x,t)는 tilde_tau(x,t) / (1 + ρ). • 그러므로 원하는 tilde_tau를 되찾으려면 tilde_tau = (1 + ρ) · τρ. ← 바로 이 식이었죠. ────────────────── ‘꼭 곱해야 하나?’에 대한 결론 ────────────────── • 절대 크기가 중요한 문제(예: 의약 용량, 가격 책정)라면 축소된 값을 그대로 쓰면 효과를 과소평가 → 잘못된 의사결정. • ρ가 아주 작아서 (1+ρ) ≈ 1 라면 생략해도 큰 영향은 없음. • 그러나 일반적으로는 정규화 → 축소 → (1+ρ) 배 되돌리기 순서를 지켜 주는 것이 편향 없는 추정을 보장한다. 즉, (1 + ρ)를 곱해 주는 이유는 “정규화가 만든 ‘축소 편향’을 정확히 반대로 상쇄해 원래 스케일(tilde_tau), 더 나아가 최종 τ(x,t)을 되찾기 위해서”입니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-10-격려와신뢰한다는표현",
    "title": "격려와 신뢰한다는 표현",
    "date": "2025-07-10",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "오늘 배운 것들 정리 I believe in you. I believe in your ability to come out ahead. I believe in your ability to stay focused. I believe in your ability to persevere. You've made it this far. You're almost there. 간단한 작문 As I did yesterday (어제처럼), I wanna talk to myself. Because I need some 격려. I believe in you MK. I believe in your ability to come out ahead. 지금은 힘들더라도, you will made it. Youve made it this far. Just focus on today, one step ahead. Youre almost there. 피드백 잘한 점들 ✨ 학습한 표현 활용: 오늘 배운 \"I believe in you\" 시리즈를 실제 작문에 바로 적용한 점이 훌륭합니다 감정 표현: 자신에게 격려가 필요하다는 솔직한 마음을 영어로 표현한 것이 자연스럽습니다 개인적 메시지: MK라는 이름을 넣어서 더 개인적이고 진심어린 느낌을 만들어냈습니다 개선할 부분 🔧 문법 오류: you will made it → you will make it (동사원형 사용) Youve → You've (올바른 아포스트로피 사용) 어조 통일: wanna → want to (좀 더 격식있는 표현) 한영 혼용을 좀 더 자연스럽게 표현 개선: one step ahead → one step at a time` (더 일반적인 표현) 수정 제안 📝 원문 유지하되 이렇게 쓸 수도 있어요: As I did yesterday, I want to talk to myself in English. Because I need some encouragement right now. I believe in you, MK. I believe in your ability to come out ahead. Even though it's tough right now, you will make it through. You've made it this far, and that's already incredible. Just focus on today, one step at a time. You're almost there. 총평 💭 영어로 자신에게 격려하는 습관이 정말 좋습니다! 배운 표현을 바로 사용하려는 적극성이 인상적이에요. 문법적인 부분만 조금씩 다듬어가면 더욱 자연스러운 영어 표현력을 기를 수 있을 것 같습니다. 계속 이런 식으로 연습하세요! 🌟",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-10-lunit-cancer-screening",
    "title": "(Seoul) Research Engineer · AutoML ",
    "date": "2025-07-10",
    "excerpt": "루닛 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "루닛 채용공고"
    ],
    "content": "채용공고 <details> <summary>📋 <strong>채용공고 상세정보</strong></summary> 링크 Lunit, a portmanteau of ‘Learning unit,' is a medical AI software company devoted to providing AI-powered total cancer care. Our AI solutions help discover cancer and predict cancer treatment outcomes, achieving timely and individually tailored cancer treatment. 🗨️ About The Team AutoML team at Lunit automates AI product development processes to streamline time-consuming tasks and advance cutting-edge AutoML research. By combining engineering expertise with state-of-the-art deep learning techniques, our team plays an important role in accelerating product development for cancer detection and treatment. Join us in our mission to Conquer Cancer Through AI where your contributions will directly influence AI innovations that improve patient outcomes worldwide In short, we're a team that maximizes AI tech to create AI models with extreme efficiency 🗨️ About The Position As a research engineer, you will work closely with the team to develop and optimize platforms for automating medical AI model research workflows Your responsibilities will involve implementing an automated product development framework for the medical domain, covering data ingestion, model training, packaging and inference You will collaborate with both research and engineering teams to bring cutting-edge AI research into real-world applications This position is ideal for someone with a strong research engineering background and deep understanding of the AI model development process 🚩 Roles & Responsibilities Develop and implement tailored AutoML platforms to automate medical AI model research workflows Integrate AutoML pipelines into AI agent which includes data ingestion, model training, hyperparameter optimization and inference Enhance the maintainability, reliability, and efficiency of both new and existing frameworks Build robust AI infrastructure using Kubernetes, Docker, and cloud computing services (GCP, AWS, Azure) Keep up with the latest advancements in AI agents, LLMs, and AutoML research to drive innovation 🚩 Tools Used Development Related: Django, Django REST Framework, React, Next.js, MySQL, PostgreSQL, Redis, Celery, Nginx, Go ML framework: PyTorch, Optuna Infrastructure: Google Cloud Platform, Kubernetes, Git, Docker, Helm, ArgoCD, Terraform General: Slack, Confluence, Jira Requirements 🎯Qualifications 3+ years of experience in research engineering in the AI industry Master's, or Ph.D. in Computer Science or a related field Proficiency in Python, unit/integration testing, documentation, Git, collaborative code development, and Docker Solid knowledge of software design and system architecture, with a strong understanding of RESTful API design Hands-on experience with developing and operating scalable AI platforms and infrastructure, including DevOps Effective communication skills: Ability to clearly and efficiently convey information, thoughts, and ideas to other developers Proven ability to take ownership and drive projects from concept to deployment Highly responsible and detail-oriented, with a strong motivation to build high-quality, reliable solutions in line with current best practices 🏅 Preferred Experiences Familiarity with web development, including both backend and frontend Deep insight into automating and orchestrating ML workflows Experience in designing or developing automated AI platforms Experience in designing or developing AI agents or LLM-based automation (e.g., fine-tuning, prompt engineering) Knowledge of cloud computing services (GCP, AWS, or Azure) Publications in AI or computer vision conferences, or active participation in the research community Ability to collaborate effectively as a team player in a cross-functional research-engineering environment Proactive in sharing knowledge, initiating collaborations, and promoting a positive research environment Passion for high-quality programming and software engineering to produce and maintain reliable code for the training and evaluation of models 📝 How To Apply CV (resume, free format) 🏃‍♀️ Hiring Process Document Screening → Technical Interview(Teams) → Assignment → PT Interview → Culture-fit Interview → Onboarding After the final interview, we may proceed with reference checks if needed. 🤝 Work Conditions and Environment Work type: Full Time Work location : Lunit HQ (5F, 374, Gangnam-daero, Gangnam-gu, Seoul) Salary: After Negotiation 🎸 ETC If you misrepresent your experience or education or provide false or fraudulent information in or with your application, it may be grounds for cancellation of the employment Lunit is committed in providing the preferential processing to those eligible for employment protection (national merits and people with disabilities) relevant to related laws and regulations Benefits 🌻 Benefits & Perks The office is at a very convenient location, just a minute away from Gangnam Station Exit 3 Meal Allowance is provided (up to 12,000 KRW per meal) when working at the office Up to 300,000 KRW is covered upon joining to decorate your personal workspace Latest computer models, such as Macs and 4K monitors are provided and can be renewed every three years Seminar registration fees and book purchases are covered Regular in-house AI and medical seminars are held Korean lessons are provided for Lunitians who do not speak Korean as their first language In-house English lessons (aka Luniversal) is provided for English development Access to high-quality AI learning resources & deep learning DevOps system Up to 1.2 million KRW worth of benefits points can be claimed annually Holiday Allowances are provided in the form of gifts or vouchers for Korean National holidays, Seollal and Chuseok Congratulatory and Condolence allowances, along with paid time off are provided Annual medical checkups and employee accident insurance are provided Expenses for monthly employees gatherings are partially covered </details> 준비해야하는 것들 일단 내 분야와 겹치는 의료쪽 채용공고고, 여기서 했던 ML개발들에 대한 경험을 살릴 수 있겠다. 이것도 이번주 주말에 이거에 맞게 이력서 쓰기. 여기에 너가 생각하는 준비하면 좋은 것들 추가해줘 🎯 핵심 준비사항 AutoML 전문성 강화 Optuna 실습: 하이퍼파라미터 최적화 프레임워크 경험 쌓기 AutoML 파이프라인: MLflow, DVC 등을 활용한 ML 워크플로우 자동화 경험 Neural Architecture Search (NAS) 관련 논문 및 구현 경험 Meta-learning 기초 개념 학습 의료 AI 도메인 지식 의료 영상 분석: DICOM 포맷, 의료 영상 전처리 기법 학습 Cancer detection papers: 루닛의 주요 논문들 읽어보기 (arXiv, 의료 AI 컨퍼런스) FDA 승인 AI 의료기기: 규제 요구사항 및 품질 관리 기준 이해 Medical imaging datasets: ChestX-ray, MIMIC 등 공개 데이터셋 경험 인프라/DevOps 스킬 업그레이드 Kubernetes 실습: 로컬 클러스터 구축 및 ML 워크로드 배포 Docker 고급 활용: Multi-stage builds, 최적화 기법 Cloud ML 서비스: GCP AI Platform, Vertex AI 실습 CI/CD for ML: GitHub Actions로 모델 학습/배포 파이프라인 구축 웹 개발 스킬 (Preferred 요구사항) Django REST Framework: API 개발 실습 React/Next.js 기초: 간단한 ML 모델 데모 페이지 구축 데이터베이스: PostgreSQL, Redis 활용 경험 포트폴리오 프로젝트 제안 의료 영상 AutoML 플랫폼: 간단한 AutoML 시스템 구축 MLOps 파이프라인: 모델 학습부터 배포까지 자동화 API 서버: ML 모델 서빙 REST API 개발 Kubernetes 배포: 위 프로젝트들을 K8s로 배포 이력서 작성 포인트 정량적 성과: 모델 성능 개선, 처리 시간 단축, 인프라 비용 절감 등 수치로 표현 의료 AI 관련 경험: 직접적이지 않더라도 헬스케어 데이터 다룬 경험 강조 AutoML/MLOps 키워드: 파이프라인 자동화, 모델 최적화 경험 부각 협업 경험: 연구팀과 개발팀 간 브릿지 역할 경험 면접 준비 기술 면접: AutoML 개념, PyTorch 심화, 시스템 설계 문제 과제: 간단한 AutoML 파이프라인 구현 또는 의료 영상 분석 태스크 예상 PT 면접: 본인의 프로젝트를 AutoML 관점에서 설명할 수 있도록 준비 컬처핏: 의료 AI로 환자에게 도움이 되고 싶다는 동기, 연구와 개발 사이의 균형감 추가 학습 자료 논문: \"Efficient Neural Architecture Search via Parameter Sharing\" (ENAS) 책: \"Hands-On Machine Learning\" (AutoML 챕터) 강의: CS330 (Stanford Meta-Learning), MLOps 관련 온라인 강의 블로그: Google AI Blog의 AutoML 관련 포스트들 네트워킹 의료 AI 커뮤니티: 한국 의료 AI 학회, 관련 밋업 참여 루닛 직원: LinkedIn으로 현직자와 커피챗 요청 컨퍼런스: MICCAI, SPIE Medical Imaging 등 의료 AI 컨퍼런스 논문 읽기 📅 주차별 준비 계획 (4주 가정) 1주차: AutoML 기초 학습 + Optuna 실습 2주차: 의료 AI 논문 리뷰 + Docker/K8s 실습 3주차: 포트폴리오 프로젝트 구현 4주차: 이력서 완성 + 면접 준비 나의 관련 직무경험과 필살기 준비하기",
    "public": true
  },
  {
    "id": "Self_Development/Career/Job Transition/25-07-10-hyundai-autoever",
    "title": "현대 오토에버 MLOps / AI 검색 엔지니어",
    "date": "2025-07-10",
    "excerpt": "현대 오토에버 채용공고 분석과 준비과정",
    "category": "Career",
    "tags": [
      "현대 오토에버 채용공고"
    ],
    "content": "채용공고 <details> <summary>📋 <strong>채용공고 상세정보</strong></summary> [Tech] Machine Learning Engineer - MLOps / AI 검색 엔지니어 📃 누구나 ​마음 ​속에 ​이력서 한 ​장은 있으니까, ⚡ 1분 ​컷 ​지원으로 현대오토에버로의 ​여정을 시작하세요. (이력서 ​자율양식) ✅ MLOps ​/ ​AI 검색 엔지니어 🚀 ​합류하실 ​팀을 ​소개해요 언어AI기술팀 💻 합류하시면 함께 ​할 ​업무예요 AI 대화형 서비스를 ​위한 ​데이터베이스 ​설계 및 개발 AI ​검색 엔진 ​개발 ​및 운영 MLOps ​구축 및 ​운영 🔍 ​이런 분과 함께 ​하고 싶어요 검색 ​/ 챗봇 관련 모듈 및 서비스 개발 경험 Docker / Kubernetes 활용 개발 및 배포 경험 Python / JAVA 개발 경력 3년 이상 혹은 그에 준하는 실력 🔍 이런 분이라면 더욱 좋아요 정보검색/SW공학 관련 석사 이상 ELK(Elasticsearch, Kibana, Kubernetes, Kafka) 구성 설계 및 운영 경험자 Neo4j / Redis 기반 DB 설계 및 구축 경험자 벡터 DB (Milvus / qdrant / faiss 등) 경험자 ES 플러그인 개발 경험자 ⌛ 이렇게 합류해요 서류 접수 → 서류 검토 → 직무역량테스트(코딩 또는 과제테스트) 및 인성검사 → 1차면접 → 2차면접 → 처우협의 및 채용검진 → 최종 합격🎉 📍 만나게 될 근무지는 여기예요 서울 강남 📌 참고해 주세요 채용 시 마감되는 상시 채용 공고로 운영되며, 채용 절차와 일정은 변동될 수 있어요. 사회적 배려 대상자(보훈 취업지원대상자, 장애인)는 관계 법령과 내규에 따라 우대해요. 모집 분야 및 담당 업무에 따라 영어 구술평가, 레퍼런스 체크, 또는 기타 전형이 실시될 수 있어요. 지원자의 경험과 역량을 고려하여 다른 포지션이 더 적합하다고 판단되는 경우 지원 분야가 변경될 수 있어요. 배치 부서 및 근무지는 회사 사정에 따라 변경될 수 있어요. 아래의 경우, 합격이 취소되거나 전형 진행에서 불이익을 받으실 수 있어요. 지원서가 사실과 다르거나 증빙이 불가할 경우 해외여행 결격 사유가 있는 경우 (남성의 경우, 회사가 지정한 입사일까지 병역 필 또는 면제 필요) 최종 합격 후 회사가 지정하는 입사일에 입사 불가한 경우 </details> 준비해야하는 것들 기본 계획 ✅ 이력서 & 포트폴리오 준비 내가 한 경험들 중에서 채용공고에 필요한 경험 뽑아서 정리 이력서랑 포트폴리오 만들기 코딩테스트 준비 기출문제 뽑아서 정리하고 모르는 개념 정리하기 보완 계획 💡 기술 스택 실무 준비 MLOps 도구 실습 Docker/Kubernetes 실습 프로젝트 CI/CD 파이프라인 구축 경험 검색 엔진 학습 Elasticsearch 학습 및 실습 검색 엔진 토이 프로젝트 구축 벡터 DB 경험 Milvus, qdrant 등 벡터 데이터베이스 학습 구체적 실행 계획 포트폴리오 프로젝트 목록 작성 MLOps 파이프라인 구축 프로젝트 AI 검색 시스템 구현 프로젝트 데이터베이스 설계 경험 정리 코딩테스트 영역 세분화 검색 알고리즘 (이진탐색, 문자열 처리) ML 관련 알고리즘 시스템 설계 문제 타임라인 설정 1주차: 이력서/포트폴리오 초안 작성 2-3주차: 기술 스택 실습 프로젝트 4주차: 코딩테스트 집중 준비 실무 프로젝트 아이디어 미니 MLOps 파이프라인: 간단한 ML 모델 배포 자동화 검색 엔진 프로젝트: 문서 검색 시스템 구현 벡터 DB 활용: 임베딩 기반 유사도 검색 시스템",
    "public": true
  },
  {
    "id": "Self_Development/Career/Graduate School/대학원에대한고민",
    "title": "대학원을 진학해야할까? 하면 어디로?",
    "date": "2025-07-10",
    "excerpt": "나의 다음 선택지는 어디로?",
    "category": "Career",
    "tags": [
      "대학원"
    ],
    "content": "커리어 선배형에게 물어본 내용들 ㅇㅇ 내가 자주보는 괜찮은 ds관련 공고사이트줄테니까, 일단 있는 공고들 쫘-악 훑어봐봐. 그리고 공통적인 키워드들이 있을테니까 그거위주로 생각해보든가. (ex. LLM, RAG, agent, ML serving, triton 등) zighang offercent bzpp",
    "public": true
  },
  {
    "id": "Self_Development/Career/Graduate School/고려대야간대학원",
    "title": "고려대 야간대학원 관련 정리",
    "date": "2025-07-10",
    "excerpt": "기회비용과 기대되는 가치",
    "category": "Career",
    "tags": [
      "대학원"
    ],
    "content": "배경 제가 원하는 커리어의 방향을 생각해볼 때 대학원에 대한 선택을 늘 고민하게 됩니다. AI/ML 리서처나 관련 연구를 하는 일들을 하기 위해서는 최소 석사 이상의 학력을 요구하는 경우가 많기 때문입니다. 늦었나? 지금 나이가 만으로 26정도, 내년 전기에 시작해도 29~30에 끝나니까 그렇게 늦은 시기도 아니라고 생각이 듭니다. 그리고 나이가 더 많았더라도 이걸 통해 얻을 수 있는게 분명하다면, 기회비용보다 더 크다고 판단된다면 늦은 시기란 없지 않을까 싶습니다. 기회비용 (시간과 가격) 학비가 대력 4천정도. 일단 돈이 정말 많이 듭니다. 아직 3천만원도 모아본 적 없는 제가 이렇게 큰 비용을 감당할 수 있을까? 라는 생각도 듭니다. (학자금 대출 받고 조금씩 갚으면 언젠간 갚겠죠...?) 그리고 시간도 많이 필요합니다. 퇴근 후에는 온전히 시간을 다 쏟아야 하고 좋은 결과를 위해서는 주말에도 이 부분만 보고 있지 않을까 싶습니다. 2년 반정도의 시간도 고려해야 합니다. 기대되는 가치 제가 원하는 커리어의 방향에 도움이 됩니다. 학력 석사라는 학력은 사실상 필수적이지 않을까 싶습니다. 사이드 프로젝트를 하면서 만나는 다른 분들을 봐도 다 석사 이상의 학력을 가지고 있습니다. 논문 그리고 좋은 논문을 작성할 수 있어야만 단순 학위에 그치지 않고 진짜 도움이 될 것 같습니다. 지원시기와 준비할 것들 올해 후기 지원은 놓쳤고 내년 전기에 지원하지 않을까 싶습니다. 다른 블로그들을 살펴보니 준비해야하는 건 기본지식(통계, 선형대수 등등)과 연구계획서 정도입니다. 기본지식 통계, 선형대수와 같은 질문들을 면접때 받는다고 합니다. 경쟁률이 약 6:1정도 된다고 하니, 이런 질문에 대한 대답을 미리 철저하게 준비할 필요가 있어보입니다. 연구계획서 이 부분에 대한 고민이 많이 필요합니다. > 그래서 어떤 연구를 하고 싶은거지? 이 부분이 많이 비어있어서 꾸준히 채워나가보려고 합니다. 참고 자료들 Hello New World - 고려대학교 SW AI 대학원 면접 후기 귀퉁이 서재 - 고려대학교 AI 야간대학원 면접 후기 From data - 재직자 빅데이터/인공지능 대학원 정리",
    "public": true
  },
  {
    "id": "Self_Development/Career/25-07-10-visuworks-thoughts-of-my-career",
    "title": "나의 현재 커리어 상황에 대한 생각",
    "date": "2025-07-10",
    "excerpt": "나의 현재 커리어에 대한 고찰",
    "category": "Career",
    "tags": [
      "내 커리어는 어디로 가야하나"
    ],
    "content": "현재 나의 상황 요즘 정말 커리어에 대한 걱정과 고민이 많습니다... ㅎㅎ 지금 다니고 있는 회사의 도메인에 묶여있진 않을지. 여기서 이룬 성과들이 나의 커리어에 도움이 될지. 사실 생각만하면 정리가 안되는 부분들이 많기 때문에 글로 적어보면서 고민들에 대한 나름의 대답들을 적어보려고 합니다. 저는 산업공학을 전공하고, 6개월정도 부트캠프에서 컴퓨터 비전 부분을 공부하고, 의료 도메인에서 일하고 있습니다. 지금 다니고 있는 회사(visuworks)는 시력교정병원 (비앤빛 안과)를 주된 고객사로 삼고 있고 이 병원에서 만들어졌다고도 볼 수 있습니다. 여기서 지금까지 한 프로젝트는 OCR pipeline 개발 고객 상담용 챗봇 개발 렌즈 사이징 추천 서비스 개발 정도가 있습니다. 문제 상황들 제가 생각하는 문제들을 정리해보고 어떻게 해결해볼 수 있을지에 대한 생각들을 정리해보려고 합니다. 연봉이 이렇게 적나? 되게 단순하게 일을 일단 시작하는 마음으로 이 회사에 고민없이 입사했는데, 연봉이 생각보다 현저히 적어서 조금 놀랐습니다. 더 정확히는 연봉이 낮지만 일단 경력을 쌓자는 마음으로 입사했습니다. 8개월정도 일하고 나름 2개의 프로젝트를 성공적으로 끝낸 시점에서, 연봉협상을 했지만 큰 폭으로 오르지는 못했습니다. > 연봉 테이블이 좋은 회사로 빠르게 옮겨야겠다. 하지만 쉽지 않네요... ㅠ 지금 4~5개월정도 이직을 준비하고 있는데 대부분의 기업에서 떨어지고 있습니다. 그냥 이정도 가치를 가진 능력인가?라는 의구심이 들지만 보완해야할 점들을 보완하면서 이직을 준비 중입니다.",
    "public": true
  },
  {
    "id": "Self_Development/Book/아주 작은 습관의 힘 (Atomic Habits)/쉬워야달라진다",
    "title": "쉬워야 달라진다",
    "date": "2025-07-10",
    "excerpt": "어떻게 좋은 습관을 만들고, 나쁜 습관은 버릴 것인가",
    "category": "Book",
    "tags": [
      "atomic habits",
      "아주 작은 습관의 힘"
    ],
    "content": "습관의 중요성 인생을 살아가면서 여러 중요한 요소들이 있겠지만, 가장 중요한 건 습관인 것 같습니다. 듀크 대학의 Wendy Wood 교수가 수행한 연구에 따르면, 우리 행동의 약 45%가 습관에 의한 것이라고 합니다. 제가 가장 좋아하는 문구도 다음과 같습니다. > Habit is second nature 이렇게 중요한 습관을 어떻게 관리할 수 있을까요? 좋은 습관을 많이 만들고, 나쁜 습관을 버려 좋은 시스템을 만들어 나간다면 제가 원하는 삶에 가까워질 것이라 생각합니다. 쉬워야 달라진다 '아주 작은 습관의 힘'이라는 책에서는 2분 법칙을 제시합니다. 해야하는 일을 아주 작게 쪼개라며 2분안에 끝낼 수 있는 일로 나누라고 말합니다. 1~2달정도 이 부분을 삶에 적용해봤는데 정말 효과가 많았습니다. 가장 큰 이유는 심리적 부담감을 줄여주기 때문이라고 생각합니다. 보통 일을 미루는 이유를 생각해보면 심리적 부담감 때문입니다. 특히 저는 일을 시작하기에 부담스러워서 시작조차 못하고 미루기만 했던 경험이 많습니다. 그런데 해야할 일을 2분안에 끝낼 수 있는 작은 일들로 쪼개니까 이런 부담감이 거의 없어졌고, 작은 일들을 성취하면서 그 성취감에 점점 더 많은 일들을 할 수 있게 됐습니다. > 아주 작은 일들로 쪼개자. 2분안에 끝낼 수 있는 일들로. 이건 제 삶의 하나의 원칙처럼 되었습니다. 아 미루지 말자 라고 생각하는게 아니라, 왜 미루는지 이해하고, 작은 일들로 쪼개기 시작했습니다. 그리고 이렇게 작은 성취들이 모여 노력하지 않아도 자동으로 실행이 되는 습관이 되어갑니다. 어떻게 나쁜 습관을 없앨까 좋은 습관을 만드는 것과 반대로 하기 어렵게 만들면 됩니다. 사람의 의지력은 한계가 있기 때문에 환경이 가장 중요하다고 생각합니다. Roy Baumeister의 연구에 의하면 의지력은 제한된 자원이라고 합니다. 의지력은 근육처럼 작동하며, 사용하면 피로해지고 휴식이 필요해지게 됩니다. > 의지력은 한계가 있다. 환경이 중요함! Massachusetts General Hospital 연구에서는 병원 카페테리아의 물의 위치만 바꿨는데도 탄산음료 판매 11.4% 감소, 생수 판매 25.8% 증가했다고 합니다. 따라서 나쁜 습관이 발견됐다면, 이를 할 수 없는 환경을 어떻게 만들지 고민해야 합니다. 저는 밤에 휴대폰을 보면서 늦게 자는 습관이 있어서 이를 해결하기 위해 자명종을 샀습니다. 그래서 알람 때문에 휴대폰을 침대에 들고 가지 않고 집에 들어오면 바로 꺼버립니다. > 나쁜 습관을 할 수 없는 환경을 만들자 집에서 아에 휴대폰을 보지 않게 되었고, 늦게 자는 습관도 해결할 수 있었습니다. 정리 해야할 일을 아주 작게 쪼개 심리적 부담감을 없애자 -> 작은 성취들을 맛보며 꾸준히 이어가자 실제로 해보니 그동안 미루던 많은 일들을 이어가고 있다. 의지력은 한계가 있으니, 나쁜 습관을 할 수 없는 환경을 만들자 알람은 자명종으로 대체하고, 휴대폰은 집에 오면 바로 꺼버리기! -> 늦게까지 휴대폰 보는 습관 제거됨",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-HETEROGENEOUS-TREATMENT-EFFECTS-ESTIMATION-WHEN-MACHINE-LEARNING-MEETS-MULTIPLE-TREATMENT-REGIME",
    "title": "[Paper Review] Heterogeneous Treatment Effects Estimation: When Machine Learning Meets Multiple Treatment Regime",
    "date": "2025-07-10",
    "excerpt": "HETEROGENEOUS TREATMENT EFFECTS ESTIMATION: WHEN MACHINE LEARNING MEETS MULTIPLE TREATMENT REGIME",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link",
    "public": false
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Data-Fusion-for-Partial-Identification-of-Causal-Effects",
    "title": "[Paper Review] Data Fusion for Partial Identification of Causal Effects",
    "date": "2025-07-10",
    "excerpt": "Data Fusion for Partial Identification of Causal Effects Paper review",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "Data Fusion for Partial Identification of Causal Effects > Data fusion techniques = 여러 데이터 소스를 효과적으로 통합하여 신뢰성 있고 정확한 결론을 이끌어내는 방법론 Abstact > However, when both assumptions simultaneously fail—a common scenario in practice—current methods cannot identify or estimate causal effects.",
    "public": false
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Causal-Effect-Inference-for-Structured-Treatments",
    "title": "[Paper Review] Causal Effect Inference for Structured Treatments",
    "date": "2025-07-10",
    "excerpt": "Causal Effect Inference for Structured Treatments",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link",
    "public": false
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-09-격려하는표현",
    "title": "격려하는 표현",
    "date": "2025-07-09",
    "excerpt": "You did a great job on, I'm happy to see that",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK"
    ],
    "content": "오늘 배운거 You did a great job on the speaking practice. I'm happy to see that you're improving. I'm happy to see that you've overcome your fear of English. I can tell that she's sad. I could tell that she broke up the her boyfriend. I can tell that you put a lof of thought into this. 간단한 작문 Today I wanna complement myself. You did a great job on the solving problem. Working on company 1년 조금넘게, I solved a lot of problems. On the OCR project, I solve communicate problem and make greak OCR pipeline. On the chatbot project, I made chatbot which 만족하다 고객사s needs. And then, I'm happy to see that youve overcome of 비교하면서 뒤쳐진다고 느끼는 fear. When I was 21-25, I was just frozen because of fear that 난 뒤쳐졌고 인생은 망했다. I can tell that your life is great. I 일인분의 삶은 살아간다. And normal life is always wonderful. claud-4-sonnet 피드백 > gemini-2-pro보다 나은 듯 수정된 버전: Today I want to compliment myself. You did a great job at problem-solving. Working at a company for a little over a year, I have solved many problems. On the OCR project, I solved communication issues and built a great OCR pipeline. On the chatbot project, I created a chatbot that satisfied the client's needs. I'm happy to see that you've overcome the fear of comparing yourself to others and feeling behind. When I was 21-25, I was frozen because of the fear that I was falling behind and my life was ruined. I can tell that your life is great. You're living a fulfilling life. And a normal life is always wonderful. 피드백: 잘한 점: 배운 표현들 (You did a great job on, I'm happy to see that, I can tell that)을 실제 상황에 적용하려고 노력했어요 자신의 경험을 바탕으로 진정성 있는 내용을 썼어요 자기 격려라는 주제를 잘 선택했어요 개선점: 단어 선택: \"complement\" → \"compliment\" (칭찬하다) 문법: \"on the solving problem\" → \"at problem-solving\" 시제 일관성: 과거와 현재완료 시제를 상황에 맞게 사용 한영 혼용: 한국어 부분을 영어로 완전히 번역하여 자연스럽게 표현 전치사: \"overcome of\" → \"overcome\" (overcome은 전치사 없이 사용) 다음에 시도해볼 것: 한 문장 안에서는 한 언어로 통일하기 문장을 쓰기 전에 전체 구조를 영어로 먼저 생각해보기 배운 표현을 더 다양한 상황에 적용해보기",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Industry Application/what-is-statistical-bias",
    "title": "통계적인 편향과 이를 제거하기 위한 방법론",
    "date": "2025-07-09",
    "excerpt": "실제 데이터에서 통계적 편향을 확인하고 이를 잔차화를 통해 제거하는 과정",
    "category": "Causal Inference",
    "tags": [
      "편향",
      "통계적",
      "bias"
    ],
    "content": "배경 --- 인과추론의 기본적인 개념은 통계적 편향 (Statistical Bias)을 제거하는 것입니다. 인과추론을 더 잘 이해하고 효과를 확인하기 위해서는, 이 통계적 편향을 이해하고 확인하는 과정이 필요하다고 생각합니다. 정의 --- > 통계적 편향(Statistical Bias)은 통계 분석 과정에서 발생하는 체계적인 오류로, 측정이나 추정 과정에서 실제 모집단의 모수(parameter)와 표본 통계량 사이에 일관된 차이가 발생하는 현상을 말합니다. 체계적(systematic) 오류란? 측정 방법이나 분석 방법에 문제가 있어 발생하는 오류 무작위가 아닌, 일정한 패턴이나 방향성을 가짐 체계적 오류 (Systematic Error) vs 무작위 오류 (Random Error) 체계적 오류 (Systematic Error): 항상 같은 방향으로 발생 (예: 항상 실제값보다 높게 측정) 측정 횟수를 늘려도 줄어들지 않음 측정 도구나 방법의 결함에서 비롯됨 무작위 오류 (Random Error): 방향이 불규칙함 (때로는 높게, 때로는 낮게) 측정 횟수를 늘리면 평균적으로 상쇄됨 우연한 변동에서 비롯됨 제가 이해한 개념은, 랜덤 노이즈가 아닌 구조상의 문제가 있어 결과를 확인하는데 오해를 만드는 것이라고 생각합니다. 예를 들어 보면: 구조상 문제: 온라인 설문조사로만 의견을 수집 → 디지털 기기 사용이 어려운 고령층 의견 누락되고 젊은 층의 의견만 반영됨 결과의 오해: \"젊은 층의 의견이 전체 의견\"이라고 잘못 해석할 수 있습니다. 통계적 편향의 종류와 설명 선택 편향 (Selection Bias) > 연구 대상을 선택하는 과정에서 발생하는 편향으로, 표본이 모집단을 제대로 대표하지 못할 때 발생합니다. 예시: 병원 데이터만으로 질병 연구를 할 때 → 중증 환자만 포함되어 질병의 심각성이 과대평가됨 자발적 참여자만으로 설문조사 → 특정 관심사를 가진 사람들만 참여하여 결과가 편향됨 제가 앞서 예시를 든 경우가 이에 해당합니다. 한국 남자의 평균 푸쉬업 개수를 측정한다고 할 때, 헬스장에서 푸쉬업 개수를 측정한다면 이는 한국 남자를 대표할 수 있을까요? 교란변수 편향 (Confounding Bias) > 원인변수와 결과변수 모두에 영향을 미치는 제3의 변수(교란변수)를 고려하지 않아 발생하는 편향입니다. 인과추론에서 가장 중요한 편향입니다. 예시: 커피 소비와 심장병의 관계 연구에서 흡연을 고려하지 않은 경우 교육과 소득의 관계에서 가정 배경을 고려하지 않은 경우 정보 편향 (Information Bias) > 데이터 수집 과정에서 발생하는 편향으로, 측정 오류나 분류 오류로 인해 발생합니다. 하위 유형: 회상 편향 (Recall Bias): 과거 사건을 기억하는 정확도 차이 관찰자 편향 (Observer Bias): 연구자의 주관적 판단이 개입 측정 편향 (Measurement Bias): 측정 도구나 방법의 결함 예시: 질병 환자가 건강한 사람보다 과거 노출을 더 잘 기억하는 경우 연구자가 가설을 알고 있어 결과를 주관적으로 해석하는 경우 생존 편향 (Survivorship Bias) > 성공하거나 살아남은 사례에만 집중하고, 실패하거나 탈락한 사례를 고려하지 않아 발생하는 편향입니다. 예시: 성공한 기업가들만 인터뷰하여 \"창업 성공 요인\" 분석 졸업생만 대상으로 한 대학 교육 효과 연구 (중도 탈락자 제외) 발표 편향 (Publication Bias) > 통계적으로 유의한 결과나 긍정적인 결과만 발표되고, 그렇지 않은 결과는 발표되지 않아 발생하는 편향입니다. 예시: 약물 효과가 있는 연구만 논문으로 발표되고, 효과가 없는 연구는 발표되지 않음 성공적인 정책 사례만 보고되고, 실패한 정책은 보고되지 않음 시간 관련 편향 (Time-related Bias) > 시간의 흐름과 관련하여 발생하는 편향입니다. 하위 유형: 시대 효과 (Period Effect): 특정 시점의 환경적 요인 코호트 효과 (Cohort Effect): 특정 세대의 공통 경험 연령 효과 (Age Effect): 나이에 따른 자연스러운 변화 탈락 편향 (Attrition Bias) > 연구 진행 중 참여자가 탈락하여 발생하는 편향입니다. 예시: 장기 추적 연구에서 건강한 사람들만 계속 참여하고, 아픈 사람들은 탈락 온라인 교육 효과 연구에서 학습 능력이 높은 사람들만 끝까지 참여 결과를 해석할 때 이런 편향들은 오해를 만듭니다. 따라서 이러한 편향을 고려하고 결과를 해석해야하며, 이런 편향을 찾고 제거하는 방법이 인과추론의 주된 목적입니다. 내가 다루고 있는 데이터에서의 통계적 편향 --- 저는 시력교정병원의 의료 데이터를 분석하고 있습니다. 주요 연구 목적은 환자들에게 다양한 수술 방법을 적용했을 때의 결과를 비교하여, 수술의 효과를 검증하는 것입니다. 당연히 환자마다 눈의 상태가 다르기 때문에 동일한 수술을 해도 결과는 달라집니다. 이러한 개인차를 고려하지 않고 수술 효과를 평가하면, 앞서 설명한 편향이 발생하여 잘못된 결론을 내릴 수 있습니다. 수술마다 환자의 분포가 다르고 이 환자들에 대해 결과를 비교하는 건 문제가 있을 수 있다고 생각해볼 수 있습니다. 그리고 이는 혼동 편향 (Confounding Bias)이라고 생각해볼 수 있습니다. 왜 Confounding Bias인가? 구조: 원인변수: 수술 방법 (A수술 vs B수술) 결과변수: 수술 결과 (시력 개선 정도) 혼동변수: 환자의 눈 상태 (근시 정도, 각막 두께, 나이 등) 문제점: 눈 상태가 수술 방법 선택에 영향을 미침 예: 각막이 얇은 환자는 특정 수술만 가능 근시가 심한 환자는 다른 수술 방법 선택 눈 상태가 수술 결과에도 직접 영향을 미침 예: 각막이 두꺼운 환자가 일반적으로 결과가 좋음 젊은 환자가 회복이 빠름 결과: 눈 상태를 통제하지 않으면, 수술 방법 자체의 효과가 아닌 환자 특성의 차이를 수술 효과로 잘못 해석하게 됩니다. 이를 어떻게 보여줄 수 있을까? --- 요즘 가장 많은 고민을 하고 있는 부분입니다. > 논리적으로 편향이 발생하는 구조이고, 이를 해결하기 위해 인과추론을 도입했다. > 그래서 결과는? 어느정도 좋아졌는데? 어떤 방법을 도입한 후의 결과를 명확하게 보여줘야 하는데, 이를 어떻게 보여줄 수 있을지에 대한 고민입니다. 제가 보고 있는 데이터에 대한 설명을 추가적으로 하자면 다음과 같습니다. 수술방법의 차이 : 렌즈삽입술에서 사용하는 렌즈 크기 [12.1, 12.6, 13.2] 환자 눈의 조건 : [나이, ATA(눈의 가로길이), ACD(눈의 세로길이), CLR(수정체의 상태)] 수술방법 별로 눈의 조건 분포를 확인하기 --- <small>처치 : 인과추론에서 연구자가 조작하는 변수(예: 수술 방법, 약물 투여). 처치 그룹과 대조 그룹을 비교하여 효과를 평가합니다.</small> > 처치(수술방법, 삽입하는 렌즈크기)별로 환자 눈의 분포가 같은가? <figure> <img src=\"/post/what-is-statistical-bias/처치별분포_피처이름제거.png\" alt=\"처치별 Feature Density Plot\"> <figcaption>그림 1: 처치별 Feature 분포 비교</figcaption> </figure> <figure> <img src=\"/post/what-is-statistical-bias/처치별박스플랏_피처이름제거.pngg\" alt=\"Boxplot Grouped by Size\"> <figcaption>그림 2: 크기별 그룹화된 박스플롯</figcaption> </figure> ata와 clr의 경우 분포의 차이가 명확히 보이고 있습니다. > 처치별 환자 눈의 분포가 다르다. 따라서 환자 눈의 조건을 통제하지 않고 수술 효과를 평가하면 오해를 만들 수 있다. 즉 ata와 clr은 처치와 결과변수 모두에 영향을 주는 교란변수 (confounding variable)이라고 할 수 있습니다. 그리고 처치별 결과 차이는 이 교란변수에 의해 왜곡될 수 있습니다. <figure> <img src=\"/post/what-is-statistical-bias/처치별결과분포_피처이름제거.png\" alt=\"처치별 결과 비교\"> <figcaption>그림 3: 처치별 결과 분포 비교</figcaption> </figure> > 단순히 처치별 분포의 차이를 통해 처치가 결과에 미치는 영향을 판단할 수 없다. 같은 크기의 눈에 3가지 종류의 렌즈를 모두 넣은 결과가 있다면, 이를 통해 렌즈 크기가 결과에 미치는 영향을 판단할 수 있습니다. 하지만 그림1과 2에서 확인한 것처럼 렌즈별로 다른 크기의 눈에 렌즈를 넣어 결과를 얻었기 때문에, 이를 통해서는 렌즈 크기가 결과에 미치는 영향을 판단할 수 없습니다. 처치가 결과에 미치는 영향을 실질적으로 분석하기 위해서는 통계적 편향을 제거할 필요가 있습니다. 예측 모델의 한계점 --- 예측 모델은 상관관계에 기반해 패턴을 찾습니다. 인과추론이란? 상관관계와 인과관계 이때 confounding variable은 인과관계가 아닌 상관관계를 만들 수 있습니다. ata가 커질수록 렌즈를 더 큰 걸 사용했고, 따라서 결과 (vaulting)가 더 커진 경향이 있습니다. 이때 모델은 ata가 커지면 vaulting이 커진다고 예측할 수 있습니다. 이런 상황에서 예측 모델에 기반해 처치를 결정하는 건 굉장히 위험합니다. > 처치와 결과변수 간의 인과관계를 모델링할 필요가 있고, 이를 통해 처치를 결정할 때 신뢰할 수 있는 결과를 얻을 수 있습니다. 인과추론으로 이를 해결하는 방법 --- > 교란변수들을 제거하고 처치와 결과변수 간의 인과관계를 모델링해야 합니다. 즉 처치(T)와 결과(Y)에 모두 영향을 미치는 변수를 $X$라고 했을 때, $X$가 $Y$에 미치는 영향을 제거하고 $X$가 $T$에 미치는 영향을 제거하고 $Y$와 $T$ 간의 인과관계를 모델링해야 합니다. 교란변수를 제거하는 방법은 여러가지가 있지만 그 중에 하나가 잔차화 방법입니다. FWL 정리 에서 설명한 것처럼 잔차화 방법을 통해 교란변수를 통제할 수 있습니다. 잔차화를 하게 되면 다음과 같은 결과를 얻을 수 있습니다. <figure> <img src=\"/post/what-is-statistical-bias/사이즈별잔차분포.png\" alt=\"결과에서 교란변수에 대한 영향 제거후 분포\"> <figcaption>그림 4: 잔차화를 통해 교란변수에 대한 영향을 제거한 결과 분포</figcaption> </figure> 그림3과 비교하면, 처치(렌즈 크기)를 제외한 나머지 교란변수들의 영향을 제거한 결과를 확인할 수 있습니다. 그림4를 통해 렌즈 크기가 결과에 미치는 영향을 확인할 수 있습니다. 잔차화가 잘되었는지는 잔차가 정규분포 형태를 띄는지와 잔차가 교란변수와 독립인지를 확인하면 됩니다. 정규분포 형태를 띄고 있다는 부분은 그림4를 통해 확인할 수 있고, 교란변수와 독립인지는 교란변수로 잔차를 예측하는 모델을 만들어 확인할 수 있습니다. <figure> <img src=\"/post/what-is-statistical-bias/조건부독립성검사.png\" alt=\"교란변수로 잔차를 예측하는 모델 결과\"> <figcaption>그림 5: 교란변수로 잔차를 예측하는 모델 결과</figcaption> </figure> 모델이 잔차를 전혀 예측하지 못하는 것을 통해 더 이상 X에 대한 정보가 Y에는 포함되어있지 않다고 생각해볼 수 있습니다. 이 결과를 좀 더 직관적을 해석해보면 <figure> <img src=\"/post/what-is-statistical-bias/같은렌즈크기_다른눈_피처이름제거.png\" alt=\"같은렌즈크기_같은결과_다른눈\"> <figcaption>그림 6: 같은 렌즈 크기에 같은 결과를 얻었지만 다른 눈을 가진 경우</figcaption> </figure> 같은 결과를 얻었고 같은 사이즈를 사용했기 때문에, 눈이 결과에 미치는 영향을 고려하지 않으면 렌즈 크기의 영향을 제대로 판단할 수 없습니다. > 직관적으로 너무나도 당연한 말이지만, 앞서 설명드린 것처럼 이러한 통계적인 편향들을 제거하지 않고 결과를 분석하게 되면 잘못된 결과를 도출할 수 있습니다. 잔차화하는 코드는 잔차화 코드 확인해볼 수 있습니다. 정리 처치 (수술방법, 렌즈크기) 별로 환자의 눈의 분포가 다르다는 것을 확인할 수 있었습니다. 수술을 받은 환자의 눈의 분포가 다르기 때문에 수술 결과를 비교하는 것은 문제가 있습니다. 정확히 수술 결과를 비교하기 위해서는 결과에서 교란변수의 영향 (통계적 편향) 을 제거한 후 비교해야 합니다.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-07-08-optimization-pitfall",
    "title": "최적화의 함정에 대하여",
    "date": "2025-07-08",
    "excerpt": "패배주의에서 벗어나자",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "배경 저는 꽤나 오랜시간 일종의 패배주의에서 벗어나지 못하고 고통스러워하며 시간을 보냈습니다. 저보다 훨씬 뛰어난 사람들을 보며 나는 저런 길을 걸어오지 않았으니까 안될 것 같다는 생각들에 사로잡혀 있었습니다. 이러한 사고방식을 어떻게 정의할 수 있을지 몰랐는데, \"뉴욕털게\"님의 영상들을 보면서 \"최적화의 함정\"에 빠졌다고 정의할 수 있었습니다. 최적화의 함정이란? 인생에는 어떤 최적화된 길이 있고, 이 길을 따라가야하며, 따라가지 못한 나는 패배재자라고 생각하는 것입니다. 그래서 최적화된 길을 찾으려고 하면서 완벽주의에 빠지게 되는 것입니다. 완벽한 계획에 대해 고민하면서 시간을 보내고, 막상 앞으로 한발자국도 나아가지 못하는 상황에 빠지게 됩니다. 뉴욕털게님 영상 링크 >계획이 원대해지면 내 하루는 비참해진다. 가장 인상깊은 말이자, 제 과거 삶을 관통하는 말입니다. 뉴욕털게님의 사고방식 뉴욕털게님이 말씀하시는 부분들을 제 삶에 정말 많은 도움이 되고 있습니다. 여러가지 개념들을 말씀해주시는데, 공통적인 개념은 \"수용의 자세\"가 아닐까 싶습니다. 이는 제가 좋아하는 \"법륜스님\"의 말씀과 비슷한 점이 많습니다. 과거, 어쩌면 지금도 제가 고통받는 이유를 생각해보면 제가 원하는 이상적인 상을 그려놓고 이것과 비교하면서 제 하루하루를 비참하게 생각했기 때문이 아닐까 싶습니다. 역설적으로 제 삶을 부정하고 원대한 계획을 세울수록 삶은 나아지지 않는 것 같습니다. 그저 비참한 하루를 보내며 시간을 보내기만 할 뿐입니다. 그래서 이 수용의 자세는 삶을 살아가는데 굉장히 중요한 자세가 아닐까 싶습니다. 지금 내 모습이 내가 상상했던 모습은 아니지만, 충분히 훌륭하다. 1인분의 삶을 온전히 살아나가고 있고 조금씩 나아가고 있다. 이렇게 지금 삶을 온전히 수용할 때, 비로소 발전할 수 있는 내가 된다고 생각합니다. 나의 삶에 적용해보기 원하는 회사에 지원하고 있지만 계속해서 떨어지고 있습니다. 나의 이상적인 상과 비교하며 나는 안되는 인생인가보다 하며 고정적이고 패배적인 사고방식을 가질 수도 있지만, 지금의 상황을 수용하고 앞으로 나아가기 위한 방법을 찾아보는게 더 좋아 보입니다. 이 수용의 자세를 떠올리며 삶을 살아가보려 합니다.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-07-08-how-to-write-resume",
    "title": "이력서는 어떻게 적어야할까?",
    "date": "2025-07-08",
    "excerpt": "이력서를 적으며 했던 나의 고민들",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "나의 배경 일한지 1년이 조금 넘는 시점에서 이직을 준비하고 있습니다. 이력서를 준비하며 나를 어떻게 표현하면 좋을지에 대한 고민들을 하고 있습니다. 모르는 내용을 공부하고 좋은 코드를 작성하고 좋은 모델을 만드는 일들은 어렵지만 그 길이 나름 명확합니다. 이런 과정을 적어도 5년이상 거쳐왔으니 자신이 있습니다. 하지만 나를 표현하는 부분들은 정말 어렵게 느껴집니다. 나는 진짜 경쟁력이 있는지에 대한 의구심부터, 내가 했던 일들을 어디서부터 어디까지 설명해야하는지 등등 이런 과정에서 했던 고민들을 하나씩 정리해봅니다. 내 이력서에 대한 피드백 사이드프로젝트를 같이하는 개발자분 > 임팩트가 없다. 이력서를 더 짧게 쓰고 포트폴리오를 풍부하게 가져가는게 좋겠다. 이 피드백에 동의하는 부분이 많았습니다. 내가 한 프로젝트의 결과를 수치적으로 표현하는데에만 집중했지, 진짜 어떤 문제를 풀어서 어떤 영향을 줬는지에 대한 내용이 부족한 것 같다고 느꼈습니다. 특히 포트폴리오를 따로 준비하지 않았던 부분은 꼭 수정이 핗요하다고 느껴집니다.",
    "public": true
  },
  {
    "id": "Self_Development/English Study/SPEAK/지나간달/07-08-칭찬하는표현",
    "title": "칭찬하는 표현",
    "date": "2025-07-08",
    "excerpt": "I was really impressed, It couldn`t be any better, I couldn`t have done it without you",
    "category": "English Study",
    "tags": [
      "영어공부꾸준히!",
      "SPEAK",
      "칭찬하는 표현들"
    ],
    "content": "오늘 배운거 I was really impressed I was really impresed with your exhibit It couldnt be any better I couldnt have done it without you I couldnt have graduate without you 이거 사용해서 간단한 작문 Today, I attend side-project 모임. We talked about recommmend system. The topic was, how to delete data noise, how to check '가정', distribute tasks for project. After that 동료 ,who 프로젝트를 같이하는 사람, give me advice. Because nowadays I fall 서류탈락, so I ask them how can I improve my 이력서. I was really impressed with their advice. They 고민하다 for me. How to improve my resume. It couldnt be any better. I was so 감동받다. I always appreciate this project. They gave me so many 도움. I couldnt have be better person without them. 문장수정 (gemini-2.5-pro) Today, I attended a side-project meeting. We talked about recommendation systems. The main topics were how to remove noisy data, how to test our assumptions, and how to distribute tasks for the project. After the meeting, a teammate who is working on the project with me gave me some advice. Recently, I've been getting rejected at the resume screening stage, so I asked them for tips on how to improve my resume. I was really impressed with their advice. They thoughtfully considered how I could improve my resume. It couldn’t have been any better. I was so moved. I always appreciate being a part of this project. They have been so helpful to me. I couldn’t have become a better person without them. 피드백 (gemini-2.5-pro) 전반적으로 전달하려는 의미가 명확해서 좋았습니다! 몇 가지 문법적인 부분과 더 자연스러운 표현을 중심으로 피드백을 정리했습니다. 시제 일치: 과거의 경험에 대해 이야기하고 있으므로 동사는 과거형으로 일치시키는 것이 좋습니다. (예: attend → attended) 자연스러운 표현: side-project 모임 → a side-project meeting 처럼 한국어 단어를 영어로 바꾸고, 명사 앞에 관사(a/an/the)를 적절히 사용하면 문장이 더 자연스러워집니다. '서류 탈락하다'는 fail the resume screening 또는 get rejected at the resume screening stage와 같이 구체적으로 표현할 수 있습니다. '고민하다'는 thoughtfully considered 또는 took the time to think about 등으로 표현하면 '나를 위해 신중하게 생각해 주었다'는 뉘앙스를 잘 살릴 수 있습니다. 문법: '도움(help)'은 셀 수 없는 명사라 many 대신 much를 사용합니다. (so much help) I couldn't have be better person → I couldn't have become a better person 처럼 조동사 뒤에 동사 원형의 과거분사형(p.p.)을 사용하고, 명사 앞에 관사를 붙여주면 문법적으로 완전한 문장이 됩니다. 오타: recommmend → recommendation`과 같은 오타를 주의하면 더 완성도 높은 글이 될 것 같습니다.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-07-08-thoughts-on-review-quality",
    "title": "리뷰 노이즈에 대한 고민",
    "date": "2025-07-08",
    "excerpt": "카카오맵 리뷰 데이터의 노이즈들에 대하여",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템",
      "추천시나리오"
    ],
    "content": "배경 데이터과학자로 1년 조금 넘게 일하면서 가장 크기 느끼는 부분은, 데이터의 중요성입니다. 모델은 데이터 안에서 패턴을 찾을 뿐, 그 안에 패턴이 없거나 노이즈가 크면 모델은 제 역할을 못하게 된다고 생각합니다. 그래서 데이터를 뜯어보고 그 안의 노이즈를 살펴보는 일은 굉장히 중요한 일입니다. > 리뷰 데이터에는 어떤 노이즈가 있는지 살펴보고 이를 정리했습니다 <small>같이 사이즈 프로젝트를 하는 '신보현'님의 분석 결과를 참고해 정리헀습니다.</small> 문제 상황들 여러 노이즈들이 존재하는데 그 중 가장 심각하다고 느끼는 부분들에 대해 정리했습니다. 장소(음식점)과 상관없는 이유인 부정적/긍정적인 리뷰들 가장 눈에 띈 부분은 계엄과 그 후의 시위들과 관련된 리뷰들입니다. 계엄을 찬성하거나 반대할 경우 관련된 사람들이 '댓글 테러'를 하는 경우를 발견했습니다. 그 외에도 제가 다니고 있는 헬스장에 여자 트레이너가 뚱뚱하다는 이유로 부정적인 리뷰를 남기는 경우도 있었습니다. 마케팅 목적으로 작성된 리뷰들 마케팅을 위해 의도적으로 작성된 리뷰들입니다. 예전에 알바했던 음식점에서도 이런 마케팅을 진행했었습니다. AI로 작성해서 문체에 티가 나는 경우라면 다르게 접근할 수도 있겠지만, 사람이 작성한 듯한 댓글이 많아서 이를 어떻게 거를 수 있는지도 고민할 필요가 있습니다. 그래서? 이런 부분을 상쇄할만큼 리뷰가 많으면 이게 상쇄가 되는지, 혹은 모델로 이를 극복할 수 있을지. 데이터를 필터링할 수 있는 부분들을 고민해야할지에 대한 논의가 추가로 필요한 상황입니다. 이 부분들에 대한 내용도 추가로 정리할 예정입니다.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-07-08-thoughts-on-deciding-scenario",
    "title": "시나리오를 정하기 위한 고민",
    "date": "2025-07-08",
    "excerpt": "추천시스템을 구현하는 사이드 프로젝트를 진행하면서 추천 시나리오는 어떻게 정하면 좋을지에 대한 논의를 정리했습니다.",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템",
      "추천시나리오"
    ],
    "content": "배경 사이드 프로젝트를 진행하며 추천 서비스를 구현해보고 있습니다. 구현하면서 필요한 여러 고민 중 하나는 어떤 상황에서 어떻게 추천을 해줄 것인지에 대한 고민입니다. 이를 추천 시나리오라고 정의하고 이를 구체화하는 과정에 있습니다. 콜드 유저에 대해 어떻게 추천할 것인가 서비스를 새롭게 만들게 된다면 이 서비스를 사용하는 유저는 콜드 유저일 것입니다. 이에 대해 어떻게 대처할 수 있을지에 대한 논의가 주된 논의였습니다. 인기도 기반 추천 + 필터링 가장 대표적으로 사용되는 방법입니다. 여기에 추가로 날씨라던지, 위치라던지, 특정 필터링을 붙이는 방식입니다. 유저 프로필 완성 (온보딩 기반 추천) 유저에게 정보를 완성하도록 요청하고, 이를 바탕으로 추천을 하는 방식입니다. 대표적으로 왓챠나 넷플릭스의 경우, 처음에 본인이 좋아하는 영화를 선택하도록 구성하고 그 후 이를 바탕으로 추천을 해줍니다. 이런식으로 유저 정보를 받을 수 있는 서비스에 대한 논의를 했습니다.",
    "public": true
  },
  {
    "id": "Data Science/Recommendation/2025-07-08-cold-start-solution",
    "title": "Cold Start 해결 방법에 대한 고민",
    "date": "2025-07-08",
    "excerpt": "사이드프로젝트에서 진행한 cold start 해결방안과 데이터의 한계점 극복 고민",
    "category": "Recommendation",
    "tags": [
      "사이드프로젝트정리",
      "추천시스템"
    ],
    "content": "배경 모두의 연구소에서 \"쩝쩝LAB\"이라는 이름으로 진행한 사이드 프로젝트에 대한 정리입니다. 맛집 추천 시스템을 구현하고 있습니다. 여러 과제 중 Cold Start를 어떻게 해결할지에 대해 논의한 내용들을 정리해봤습니다. Cold Start Problem이란? > 사용자나 아이템에 대한 정보가 없거나 희소한 문제 이 중에서 유저에 대한 cold start 문제를 어떻게 풀지에 대해 고민하고 있습니다. <small>같이 사이즈 프로젝트를 하는 '이윤선'님의 분석 결과를 참고해 정리헀습니다.</small> 기본적인 추천의 컨셉 > Popularity Model에서 Context를 반영하여, 유저가 만족할만한 음식을 추천해주고 싶다. 콜드 유저에게 인기도 기반 추천을 내주는 것처럼, 계절과 날씨를 고려해 추천을 내주면 좋을 것 같다는 아이디어입니다. (예시) 상황 유저의 생각 추천 가능 음식 --- --- --- 맑고 청명한 날 “밖에 나가서 먹고 싶어” 샌드위치, 김밥, 분식 흐리고 습한 날 “뭔가 시원한 게 땡긴다” 냉면, 물회, 아이스커피 비 올 것 같은 날 “집에 일찍 가고 싶어” 국물 요리, 칼국수, 해장국 겨울철 “대게가 제철이네?” 대게찜, 어탕국수, 전골류 계졀 날씨 데이터를 모델에 포함시켜야 하는 이유로 3가지를 제시했습니다. 왜 계절 날씨 데이터를 모델에 포함해야 할까? 개인화 추천 강화 → 동일한 유저도 날씨에 따라 선택이 달라짐 → ‘유저 + 날씨’ 조합 기반의 더 똑똑한 추천 가능 모델의 정밀도 향상 → 기존 모델에 컨텍스트 데이터를 추가함으로써 예측 정확도 향상 Cold Start 상황에서도 강력한 보완 → 유저 정보가 없을 때도, 그날의 날씨 + 인기 메뉴로 합리적인 추천 가능 여기에 추가로 저는 추천의 근거를 제시해줄 수 있기 때문에, 사용자가 추천의 결과를 더 신뢰할 수 있을 것이라고 생각합니다. 구현상황에서의 문제점, 데이터의 한계 사용하고 있는 데이터는 카카오맵을 크롤링하여 만든 데이터입니다. 카카오맵에 있는 리뷰는 사용자가 서비스를 이용한 후에 바로 작성하는게 아니기 때문에, 작성 시점과 실제 사용한 시점이 다릅니다. 따라서 사용한 시점의 날씨를 알 수 없다는 문제가 있습니다. 이를 확인하기 위한 EDA 결과를 발표자님께서 발표해주셨습니다. > 리뷰 날짜와 방문날짜가 같다는 가정을 타당하게 만들기 위한 방법 리뷰어 중에 매일 리뷰를 쓰는 사람만 고르자 > 작성한 리뷰들의 일자의 차이가 평균적으로 1일정도 나는 유저, 즉 매일 리뷰를 작성하는 리뷰어들을 통해 데이터를 통해 이를 해결하자 즉 리뷰 날짜와 방문날짜가 같다고 추정되는 유저만 필터링하는 방법을 선택합니다. 이렇게 되면 데이터의 한계를 어느정도는 극복할 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/Codility Study/공부방향",
    "title": "코딩테스트 공부 방향",
    "date": "2025-07-08",
    "excerpt": "코딩테스트 공부 방향에 대한 정리",
    "category": "Codility Study",
    "tags": [
      "고민정리"
    ],
    "content": "배경 잠깜 공부하다가 내려놓고 다시 공부를 시작하려고 합니다. 주니어, 혹은 시니어더라도 코딩테스트는 선택이 아닌 필수가 아닐까 싶습니다. (적어도 10년 이하까지는?) 그래서 꾸준히 공부해보려고 하는데 어떤 방향으로 잡을지에 대한 정리를 해보려고 합니다. 또 사이드 프로젝트를 같이 하는 '신보현'님의 경험을 들어보니, 라이브코테는 필수적으로 보고 난이도가 꽤 있어서 꾸준한 준비가 필요하다고 합니다. 지금부터라도 꾸준히 공부해보려고 합니다. 코테 공부하기 좋은 곳 (GPT 정리) > 와 엄청 정리 잘해주네요... 코딩 테스트(코테) 공부에 도움이 되는 좋은 사이트들을 아래에 용도별로 정리해드릴게요. 각 사이트마다 특성이 다르니, 본인의 학습 목적(기초 다지기, 문제 풀이, 실전 대비 등)에 맞춰 선택하시면 됩니다. --- ✅ 1. 문제 풀이 중심 백준 온라인 저지 (BOJ) URL: https://www.acmicpc.net/ 특징: 국내 코테 준비자들이 가장 많이 사용하는 사이트. 다양한 알고리즘 분류, 난이도별 문제 제공. 단계별로 풀어보기, 문제집 기능으로 체계적인 학습 가능. C++, Python, Java 등 다양한 언어 지원. 프로그래머스 (Programmers) URL: https://programmers.co.kr/learn/challenges 특징: 실제 기업 코딩테스트 문제 수록 (카카오, 네이버 등). 실전 감각을 기르기에 적합. 레벨별로 분류되어 있어서 입문자부터 고급자까지 활용 가능. Python, JavaScript 등 실무에 많이 쓰는 언어에 최적화. LeetCode URL: https://leetcode.com/ 특징: 글로벌 IT 기업(FAANG) 코테 문제 다수 수록. 난이도 다양 (Easy, Medium, Hard). 영어 기반이지만, 알고리즘 실력을 실질적으로 끌어올리기에 효과적. 문제마다 디스커션과 솔루션이 잘 되어 있음. --- ✅ 2. 알고리즘 개념/이론 학습 알고리즘 비주얼라이저 URL: https://visualgo.net/ 특징: 자료구조 및 알고리즘 동작 원리를 시각적으로 보여줌. DFS, BFS, 정렬, 힙, 트리 등 기초 개념 정립에 탁월. Tistory / velog 블로그 검색 활용 방법: \"[문제 이름] + 알고리즘 풀이\"로 검색하면, 풀이 과정 + 시간 복잡도 분석까지 설명된 블로그 글이 많음. 이론 + 구현을 연결지어 학습하기 좋음. --- ✅ 3. 모의 코딩 테스트 / 실전 연습 코드업 (CodeUp) URL: https://codeup.kr/ 특징: 입문자에게 적합한 기초 문제부터 알고리즘 문제까지 제공. 단계별 학습으로 알고리즘 실력을 자연스럽게 끌어올림. Codeforces URL: https://codeforces.com/ 특징: 주기적인 온라인 대회 운영. 실전 감각 및 속도, 구현 능력 향상에 매우 효과적. 러시아 기반 커뮤니티이나 영어 지원. --- ✅ 4. 문제 추천/가이드 자료 GitHub 저장소 예시: https://github.com/tony9402/baekjoon 백준 알고리즘 분류 기반 추천 문제 모음 https://github.com/ellynhan/boj-algorithm-practice 기초 → 고급 순서로 정리된 백준 문제집 --- 🔍 코테 준비 방법 요약 (이론적 배경) 기초 개념 습득: 시간 복잡도, 정렬, 탐색, 재귀, DP, 그리디 등 이론 학습. 문제 유형별 학습: 구현 → 정렬 → 탐색(BFS/DFS) → 스택/큐 → DP → 그래프 순서 추천. 문제 풀이 + 복습: 오답 노트 필수. 같은 문제를 여러 번 반복해서 풀어보는 것이 중요. 실전 모의 테스트: 실제 시험 시간처럼 제한 시간 안에 푸는 연습이 효과적. 내 생각은? 기초부터 하나씩 습득하는 것도 좋지만 원하는 회사 기출문제부터 풀어보면서 채워나가는 것도 좋은 방향인 것 같습니다. 당장 7월에 있는 현대 오토에버 지원하고 여기 코테 기출문제부터 풀어보려고 합니다. 관련 내용들을 하나씩 정리해보곘습니다.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2025-07-03-importance-of-numbers",
    "title": "수치의 중요성",
    "date": "2025-07-03",
    "excerpt": "수치로 표현하는게 왜 중요한지에 대한 생각 정리",
    "category": "Reflections",
    "tags": [
      "이런저런 생각들"
    ],
    "content": "",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-fwl",
    "title": "FWL(Frisch-Waugh-Lovell) 정리란?",
    "date": "2025-07-02",
    "excerpt": "통계적 편향을 제거하기 위한 방법론",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "참고자료 실무로 통하는 인과추론 with python 네이버블로그_ (선형모형) 04 Frisch-Waugh-Lovell 정리, 수식 풀이가 잘되어 있음 배경 설명 --- 인과추론이란 \"원인 -> 결과\"의 패턴인 \"인과관계\"를 찾는데에 목적이 있습니다. 이 패턴을 찾는건 어려운 일인데, 그 이유 중 하나는 원인과 결과에 모두 영향을 주는 교란변수 때문입니다. 교란변수(confounding variable)란 종속변수와 독립변수에 모두 영향을 줘 잘못된 인과관계를 찾도록 만드는 변수를 말합니다. 즉 교란변수가 잘못된 패턴을 찾도록 유도하기 때문입니다. !confounding variable 예시그림 가장 대표적인 예시로 \"아이스크림 판매량\"과 \"상어 어택횟수\"를 생각해볼 수 있습니다. \"기온\"이라는 변수는 \"아이스크림 판매량\"과 \"상어의 공격횟수\"에 모두 영향을 줍니다. 그래서 \"기온\"이 올라가서 \"아이스크림 판매량\"이 증가하고 \"상어의 공격횟수\"가 증가한 것인데, \"아이스크림 판매량\"이 증가해서 \"상어의 공격횟수\"가 증가했다는 생각을 하게 됩니다. 즉 \"교란 변수\"는 변수간의 관계를 잘못 해석할 가능성을 만들게 됩니다. 따라서 변수간의 관계를 볼 땐 교란변수를 제거해주는게 굉장히 중요합니다. 그렇다면 다음과 같은 질문을 해볼 수 있습니다. > 교란변수를 없애야 한다는 건 이해했어, 그러면 이걸 어떻게 없앨 건데? 가장 좋은 건 교란변수를 파악하고 실험 설계를 통해 없애는 것입니다. 위의 예시에서는 \"기온\"이라는 교란편수를 파악하고, 이를 통제한 후에 두 변수 간의 관계를 살펴볼 수 있습니다. 하지만 교란변수를 파악하더라도 이를 통제할 수 없을 때가 있습니다. 예를 들어 수술방법(독립변수)과 수술 후 결과(종속변수)의 관계에서는 나이, 생활습관 등의 교란변수가 있습니다. 교란변수를 파악하지만 이를 통제할 수 없습니다. > 이를 위해 교란변수의 영향을 \"통계적\"으로 제거해주는 방법이 제안되었습니다. FWL Theorem 이란? --- > 앞서 설명한 교란변수의 영향, 즉 편향을 보정하기 위한 방법입니다. FWL 정리의 이론적 배경은 다중회귀의 강력함에 있습니다. > 다중 회귀 분석에서 회귀 계수의 의미는 모델의 다른 변수들의 효과를 고려한 후 얻은 동일 설명변수의 이변량계수다. 이는 다중회귀모형에서 특정 관심 변수의 OLS 추정계수는, (1) 모든 설명변수를 한꺼번에 포함하여 회귀분석한 결과와, (2) 통제 변수들에 대해 종속변수와 관심 변수 각각의 잔차를 구한 뒤 이 잔차들 간에 회귀분석한 결과가 동일하다는 의미입니다. 수학적 표현 다중회귀 모델을 다음과 같이 표현할 수 있습니다: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k + \\epsilon$$ 여기서 $Y$는 종속변수, $X_1$은 관심 변수, $X_2, \\ldots, X_k$는 통제 변수들입니다. FWL 정리에 따르면, $\\beta_1$의 OLS 추정치는 다음 두 방법으로 동일하게 구할 수 있습니다: 방법 1: 전체 모델에서 직접 추정 $$\\hat{\\beta}_1 = \\frac{\\text{Cov}(X_1, Y)}{\\text{Var}(X_1)}$$ 방법 2: 잔차화를 통한 추정 $Y$를 $X_2, \\ldots, X_k$에 대해 회귀하여 잔차 $e_Y$ 계산 $X_1$을 $X_2, \\ldots, X_k$에 대해 회귀하여 잔차 $e_{X_1}$ 계산 $e_Y$를 $e_{X_1}$에 대해 회귀하여 $\\hat{\\beta}_1$ 추정 $$e_Y = Y - \\hat{Y}_{X_2,\\ldots,X_k}$$ $$e_{X_1} = X_1 - \\hat{X_1}_{X_2,\\ldots,X_k}$$ $$\\hat{\\beta}_1 = \\frac{\\text{Cov}(e_{X_1}, e_Y)}{\\text{Var}(e_{X_1})}$$ 실제로 수식적으로도 같고 (참고자료 2 참고) 테스트를 해봐도 같습니다. 인과추론에서 다중회귀는 예측 모델이 아닌 인과관계를 찾는 모델로 사용됩니다. R-learner, 인과추론 모델로 구현하기 --- 정리하면 FWL이란 잔차화를 통해 교란변수를 직교화 시키는 방법입니다. FWL을 meta learner 형태로 구현한게 R-learner입니다. 따라서 잔차화를 위한 2개의 nuisance function이 필요하고, CATE를 추정하는 모델이 필요합니다. R-learner의 목적 함수는 다음과 같습니다: $$\\mathcal{L}(\\tau) = \\mathbb{E}\\left[\\left(Y - m(X) - \\tau(X)(T - e(X))\\right)^2\\right]$$ 여기서: $m(X) = \\mathbb{E}[Y X]$: 결과 모델 (outcome model) $e(X) = \\mathbb{E}[T X]$: 처리 모델 (treatment model) $\\tau(X)$: 조건부 평균 처리 효과 (CATE) 다중회귀와 비교해서 FWL은 meta leaner로 구현할 때의 장점은 '비선형적인 패턴'을 학습할 수 있다는 점입니다. 이렇게 구현된 R-learner를 통해 CATE(Conditional Average Treatment Effect)를 추정할 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/correlation-vs-causation",
    "title": "상관관계와 인과관계의 이해",
    "date": "2025-06-28",
    "excerpt": "상관관계와 인과관계의 차이점을 이해하고, 예측 모델의 한계와 인과추론의 중요성에 대한 이야기",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "> 상관관계와 인과관계를 이해하고 구분하는 건 결과를 해석할 때 중요한 점입니다. 상관관계란, 두 변수 사이에 일정한 패턴이나 동반 변동이 존재함을 의미하며, 한 변수가 커질 때 다른 변수가 일정하게 커지거나(양의 상관), 작아지는(음의 상관) 경향을 보이는 통계적 관계를 말합니다. 인과관계란, 한 변수가 변할 때 다른 변수가 그 변화에 의해 직접적·체계적으로 영향을 받아 변화하는 관계를 의미합니다 보통 두 변수간의 관계를 살펴볼 때 상관관계를 살펴보곤 합니다. 하지만 상관관계를 인과관계로 해석하지 않도록 주의해야 합니다. 예를 들어 아이스크림 판매량과 일일 기온 사이에는 높은 기온일수록 판매량이 증가하는 강한 양의 상관관계가 관찰되지만, 이는 기온이 아이스크림 판매량을 직접 “원인”한다고 단정할 수 없으며, 사람들이 더운 날씨에 밖에 나와 판매량이 늘어나는 등 다양한 제3의 요인이 동시에 작용했을 가능성을 배제할 수 없습니다. > 예측 모델은 주어진 데이터에서 변수 간의 동시 변동 패턴, 즉 상관관계를 학습해 결과를 예측합니다. 이 모델의 예측결과는 인과관계를 보장하지 못합니다. 가격 매출 --- --- --- 1 100 1000 2 150 900 3 600 10000 4 700 12000 예를 들어 숙박 가격과 매출이 주어진 데이터에서 가격이 높을수록 매출이 높아 보인다면, 모델은 “가격이 오르면 매출이 오른다”고 예측할 것입니다. 하지만 이 패턴 뒤에는 “성수기/비수기 여부”라는 숨겨진 외부 요인이 있을 수 있습니다. 가격이 비수기에 낮아지고 성수기에 올라가는 동시에 매출도 함께 변했다면, 실제로는 계절성이 매출을 결정짓는 진짜 원인인데, 단순 예측 모델은 이를 구분하지 못합니다. 따라서 상관관계에 기반한 예측 결과를 인과관계로 오해하면 “매출을 올리려면 가격을 올려야 한다”는 잘못된 결론에 이를 수 있고, 실제 효과가 있는 개입(예: 비수기 프로모션) 대신 부적절한 가격 정책을 선택하게 만듭니다. 인과추론은 이러한 숨겨진 교란 요인까지 고려해, 조작 가능한 원인이 실제로 결과에 미치는 영향을 통계적으로 분리·추정함으로써 올바른 의사결정을 가능하게 합니다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/what-is-causal-inference",
    "title": "인과추론이란?",
    "date": "2025-06-20",
    "excerpt": "인과추론의 개념에 대해 소개하는 이야기",
    "category": "Causal Inference",
    "tags": [
      "상관관계",
      "인과관계",
      "인과추론"
    ],
    "content": "“인과추론” (Causal Inference)은 최근에 의료 분야, 마케팅 분야 등 선택에 대한 분석이 필요한 곳에 자주 사용되는 개념입니다. 인과추론은 다음과 같은 질문들에 답을 합니다. “마케팅 비용을 늘렸더니 매출이 증가했네, 마케팅 비용을 늘려서 그런건가?” . “환자에게 A라는 약물을 처방했더니 상태가 괜찮아졌어. A 덕분인가?” 이러한 질문들은 전통적으로 대조군과 실험군을 통해 증명되어왔습니다. 실험하려는 조건 외에 다른 조건은 모두 통제한 후에 실험결과를 비교합니다. 예를 들어 새로 개발한 비료의 효과를 검증하려고 합니다. 같은 품종의 옥수수 묘목 100포기를 두 그룹으로 나누어, 실험군에는 새 비료를 표준량만큼 투입하고 대조군에는 기존 비료(혹은 비료를 전혀 주지 않음)를 동일한 방식으로 처리합니다. 이때 물주기, 일조량, 토양 성분, 온도 등 나머지 재배 조건은 두 그룹에서 완전히 동일하게 유지합니다. 일정 기간 후 두 그룹의 생장 속도, 수확량, 작물 건강 상태를 비교하면, 오로지 “비료 종류”의 차이만이 결과에 영향을 미쳤다고 판단할 수 있게 됩니다. 이러한 접근법은 가장 합리적이지만, 현실 문제에 적용하기엔 어려움이 있습니다. 마블의 멀티버스 세계관이 아니라면, 2025년 7월 1일에 마케팅 비용을 100만원과 1000만원을 모두 사용할 수 없습니다. 한 명의 환자에게 렌즈 사이즈를 12.1를 삽입한 후 결과를 확인하고, 렌즈를 뺀 후 12.6를 삽입해 결과를 확인할 수 없습니다. 즉, 대조군과 실험군을 설정하는데 어려움이 있습니다. 인과추론은 이를 통계적으로 보완하여 결과를 추정하는 방식입니다. 예를 들면, 온라인 쇼핑몰에서 A라는 광고 캠페인이 판매량에 미친 영향을 알고 싶을 때, 실제로는 동일한 고객에게 광고를 보여주지 않은 상태와 보여준 상태를 모두 경험시킬 수 없으므로 인과추론 기법을 활용합니다. 이때 고객의 연령, 성별, 과거 구매 이력 등 다양한 고객 특성을 보고, 광고를 본 그룹과 보지 않은 그룹 간에 이 특성 분포를 비슷하게 맞춰(예: 성향점수 매칭) 가상의 대조군을 만듭니다. 이후 두 그룹의 평균 구매액 차이를 계산하면, “광고 효과”만을 통계적으로 분리해 추정할 수 있습니다. 이렇게 하면 실제 실험을 수행하기 어려운 상황에서도 마치 무작위대조시험과 유사한 수준의 인과적 해석을 얻을 수 있습니다. 렌즈 사이즈를 추천할 때도, 마치 광고 효과를 분석하듯이 환자마다 나이·굴절도·각막 두께 같은 특징들을 고려해 “12.1, 12.6, 13.2 크기 렌즈”를 쓴 환자들과 비슷한 조건의 가상의 대조군을 만들어 비교합니다. 예를 들어, 30대 근시 환자 중에 12.6mm 렌즈를 사용한 그룹과 12.1mm 렌즈를 사용한 그룹이 비슷한 눈 구조와 시력치를 갖고 있도록 통계적으로 조정한 뒤, 두 그룹의 수술 후 시력 개선 정도 차이를 분석하면 “12.6mm 렌즈를 쓰면 시력이 얼마나 더 좋아지는지”를 인과적으로 추정할 수 있습니다. 이렇게 얻은 결과를 바탕으로, 실제로는 써보지 않은 12.3mm 같은 중간 크기의 렌즈가 가져올 효과도 예측할 수 있고, 최종적으로는 각 환자에게 가장 효과적인 렌즈 크기를 과학적으로 추천해 줄 수 있습니다.",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/review-Multi-Study-R-Learner-for-Estimating-Heterogeneous-Treatment-Effects-Across-Studies-Using-Statistical-Machine-Learning",
    "title": "[Paper Review] Multi-Study R-Learner for Estimating Heterogeneous Treatment Effects Across Studies Using Statistical Machine Learning",
    "date": "2025-01-16",
    "excerpt": "Multi-Study R-Learner 논문에 대한 리뷰 및 분석",
    "category": "Causal Inference",
    "tags": [
      "R-Learner",
      "Heterogeneous Treatment Effects",
      "Multi-Study",
      "Statistical Machine Learning",
      "Paper Review"
    ],
    "content": "paper link",
    "public": false
  },
  {
    "id": "Self_Development/Career/Graduate School/OMSCS관련",
    "title": "OMSCS(Georgia Tech 온라인 CS 석사) 지원 준비 계획",
    "date": "2025-01-15",
    "excerpt": "세계 최고 가성비 CS 석사 프로그램 분석과 지원 전략",
    "category": "Career",
    "tags": [
      "대학원",
      "OMSCS",
      "Georgia Tech",
      "컴퓨터사이언스"
    ],
    "content": "배경 데이터 사이언스 분야에서 더 깊이 있는 커리어를 쌓기 위해 석사 과정을 고려하던 중, OMSCS(Georgia Tech Online Master of Science in Computer Science)라는 혁신적인 프로그램을 발견했다. 세계 톱급 CS 프로그램을 온라인으로, 그것도 극도로 저렴한 비용으로 이수할 수 있다는 점이 매력적이다. OMSCS란? 📊 기본 정보 정식명칭: Georgia Institute of Technology Online Master of Science in Computer Science 개설연도: 2014년 (Udacity, AT&T와 협력) 총 비용: 약 $7,000-$8,500 (한화 900만-1,100만원) 학위: 캠퍼스와 동일한 Georgia Tech MS in Computer Science (온라인 표기 없음) 기간: 평균 2-3년 (파트타임으로 진행 가능) 과정 구성: 10개 과정 (30학점) 합격률: 약 70% ✅ 주요 장점 압도적인 가성비 전체 프로그램 비용이 $7,000-$8,500 (한화 900만-1,100만원) 타 명문대 온캠퍼스 프로그램 대비 1/5~1/10 수준 명문대 브랜드 + 동등한 학위 Georgia Tech는 CS 분야 세계 8위 (US News 2024) 졸업장에 \"온라인\" 표기 없음 실리콘밸리에서 인정받는 브랜드 다른 블로그 글들이나 링크드인 봤을 때 job interview 기회도 확실히 더 얻을 수 있다. 해외 이직도 고려하고 있으니까 지금 상황에서 할 수 있는 가장 좋은 선택이 아닐까. 유연성 풀타임 직장과 병행 가능 자신의 페이스로 진도 조절 전 세계 어디서나 수강 가능 보통 1과목당 주당 20~40시간정도 필요하다고 함. 실무 중심 커리큘럼 Machine Learning, AI, Computer Vision 등 현업 엔지니어들이 직접 활용할 수 있는 스킬 프로젝트 기반 학습 ⚠️ 단점 및 고려사항 높은 자기관리 요구 온라인 특성상 강한 자기 동기부여 필요 시간 관리와 학습 계획 수립 능력 중요 졸업이 어려움 네트워킹 제한 교수나 동료들과의 직접적인 만남 부족 연구 기회나 인턴십 연결 어려움 Redis에서 활발하게 이야기가 나오고 있긴 함. 그래도 네트워킹은 많이 부족함. 한국에서의 인식 아직 국내에서는 인지도가 낮음 전통적인 한국 기업에서는 평가 불분명 그래도 알고 있는 사람들은 충분히 좋게 봐주는 걸로 이해 중 언어 장벽 모든 수업이 영어로 진행 TOEFL 100점 이상 필요 토플 준비가 가장 급하다. 지원 일정 및 전략 📅 지원 일정 Spring 학기 지원 마감일: 8월 15일 Fall 학기 지원 마감일: 3월 1일 Priority Decision: 마감일 이전부터 순차적 합격 발표 내년 가을 학기 목표로 3월까지 준비 서류 갖추기 준비해야 하는 것들 [x] OMSCS 프로그램 상세 조사 완료 [ ] TOEFL 기존 점수 확인 또는 응시 일정 잡기 [ ] 추천인 3명에게 연락 (8월 10일까지 제출 요청) 추천인들에게 리마인더 메시지 필요한 정보 즉시 제공 [ ] 대학교 영문 성적증명서 발급 신청 [ ] Personal Statement 초안 작성 시작 [ ] 데이터 사이언스 경험 + OMSCS 지원 동기 명확히 [ ] 온라인 샘플들 참고하여 빠르게 작성 [ ] 이력서 OMSCS 맞춤 업데이트 기술 프로젝트, 프로그래밍 경험 강조 📅 합격 후 계획 (2025.9 ~ 2026.1) [ ] TOEFL 점수 향상 (합격 후에도 계속) 실제 수업에서 필요한 Speaking/Writing 실력 [ ] CS 기초 지식 보강 자료구조/알고리즘 체계적 복습 Python, Java 등 프로그래밍 언어 숙련도 향상 [ ] OMSCS 커뮤니티 활동 합격자 그룹 참여 첫 학기 수강 과목 리서치 [ ] 수학 기초 강화 선형대수, 확률통계, 이산수학 복습 [ ] Foundational Course 선택 처음에는 1개 과목만 수강 권장 CS 6515 (Algorithms) 또는 CS 6601 (AI) 고려 [ ] 학습 환경 구축 온라인 학습에 최적화된 환경 설정 시간 관리 시스템 구축 --- 🎯 핵심 우선순위 🔥 HIGH (필수) TOEFL 100점 이상 - 가장 중요한 정량적 지표 Personal Statement - 합격을 좌우하는 핵심 요소 추천서 3개 - 제3자 관점에서의 검증 📈 MEDIUM (중요) CS 기초 지식 - 면접이나 추가 질문 대비 프로그래밍 프로젝트 - 실무 역량 증명 성적증명서 - 기본 자격 요건 💡 LOW (플러스 요소) OMSCS 커뮤니티 활동 - 추가 정보 수집 기술 블로그 - 학습 의지 표현 --- ⏰ 주간 학습 계획 (예시) 평일 (월-금) 아침 1시간: 영어 공부 (TOEFL 준비) 저녁 1시간: CS 기초 또는 프로그래밍 주말 토요일 3-4시간: 지원서 작성 및 프로젝트 일요일 2-3시간: 복습 및 다음 주 계획 총 주당 약 10-12시간 투자 커리어에 미칠 영향 긍정적 영향 기술적 깊이: 현재 실무 경험에 이론적 백그라운드 추가 글로벌 네트워크: 전 세계 OMSCS 동문들과의 연결 경력 발전: ML 엔지니어, 리서치 사이언티스트 포지션 진출 가능 브랜드 가치: Georgia Tech 학위의 신뢰도 현실적 고려사항 시간 투자: 주당 15-20시간 학습 시간 필요 업무 병행: 현재 직장과의 균형 유지 전략 필요 ROI 관점: 학위 취득 후 실제 연봉 상승 및 커리어 기회 증가 여부 결론 OMSCS는 현재 내 상황에 매우 적합한 선택지로 보인다. 특히: 경제적 부담 최소화: 기존 직장 유지하면서 진행 가능 실무 연계: 현재 데이터 사이언스 업무와 직접적 연관 미래 가치: AI/ML 분야 전문성 강화로 커리어 확장 다만 성공적인 완주를 위해서는 철저한 시간 관리와 학습 계획이 필수적이다. 무엇보다 왜 이 프로그램을 해야 하는지에 대한 명확한 동기부여가 가장 중요할 것 같다. Spring 2026 입학을 목표로 체계적인 준비를 시작해보자! 🎯 --- 참고 자료 OMSCS 공식 웹사이트 OMSCS 지원 가이드라인 OMSCS 커뮤니티 및 후기",
    "public": true
  },
  {
    "id": "Data Science/Causal Inference/Paper Review/test-private-manski",
    "title": "[Private] Test Manski Bounds Analysis",
    "date": "2025-01-01",
    "excerpt": "비공개 테스트 포스트입니다. 관리자 모드에서만 보입니다.",
    "category": "Causal Inference",
    "tags": [
      "Test",
      "Private",
      "Manski"
    ],
    "content": "비공개 테스트 포스트 이 포스트는 public: false 설정으로 인해 관리자 모드에서만 볼 수 있습니다. 내용 URL에 ?admin=true 또는 ?admin=True를 추가하면 이 포스트를 볼 수 있어야 합니다. 테스트 항목 포스트 목록에서 표시 여부 검색 결과에서 표시 여부 직접 URL 접근 시 차단 여부 시각적 표시 (🔒 아이콘, 빨간 테두리) 이 내용이 보인다면 관리자 모드가 정상적으로 작동하고 있는 것입니다!",
    "public": false
  },
  {
    "id": "Data Science/Recommendation/2024-07-05-recommendation-system-interest",
    "title": "추천시스템에 관심있는 이유",
    "date": "2024-07-04 23:30:00 +0900",
    "excerpt": "",
    "category": "Recommendation",
    "tags": [
      "Recommendation"
    ],
    "content": "잠깐 추천시스템을 공부했었는데, 다시 공부를 시작하면서 프로젝트를 하나씩 쌓아가려고 한다. 그 전에 내가 왜 관심이 있는지, 또 어떤 프로젝트를 해보고 싶은지 정리해보려 한다. 추천시스템이란? 추천시스템은 검색과 비슷한 목적을 가지고 있다. > 많은 정보 속에서 필요한 정보를 필터링하는 것 \"진짜 많은 정보들, 컨텐츠들이 있는데 유저에게 어떤 것을 보여줘야할까?\"에 대한 답을 하는 것이다. 왜 중요할까? 추천시스템이 필요한 곳은 대표적으로 OTT 회사들이 떠오른다. 소비자는 언제 구독을 그만둘까? > 당연하게도 더 이상 볼게 없다고 느껴질때 그만둘 것이다. 전체 컨텐츠에 비해 소비한 컨텐츠는 소수일텐데 왜 볼게 없다고 느껴질까? > 뭘 봐야할지 모르겠어서, 어떤게 내 재밌을지 몰라서. 그래서 소비자에게 취향에 맞는 컨텐츠를 꾸준히 노출해줘야하고, 그래야 소비자가 떠나지 않도록 만들 수 있다. 어려운 점 내가 생각했을 때 추천시스템을 구성하는데 가장 어려운 것은 소비자의 평가 데이터가 적기 때문인 것 같다. 어떤 사람이 평가한게 적으면 당연히 취향을 분석하기 어렵고, 그래서 추천해주기도 어렵다. 평가 데이터가 많은 사람에게 잘 추천해주는 것도 굉장히 중요하지만, 평가 데이터가 부족한 사람에게 어떻게 추천해줄지에 대한 문제를 해결하는게 가장 중요한 것 같다. 해보고 싶은 프로젝트 요즘 해보려는 프로젝트는 LLM을 이용해서 소비자의 취향을 알아내는 것이다. 구체적으로는 Multi-Agent-System으로 추천시스템을 구축해보려고 하고, 맨 앞단에 취향을 분석하는 Agent를 구축해 사용해보려고 한다. 먼저 영화로 시작해서 책과 같은 분야로도 확장해보려고 한다. 취향을 분석한다는게 굉장히 애매한 부분인 것 같아서 고민이 많이 필요할 것 같다. 이게 가능해진다면 많은 산업에서 정말 매력적으로 느끼는 기술이 아닐까? 총 3명이서 이 프로젝트를 진행할 것 같은데 그 과정을 블로그에 남겨보려고 한다.",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2024-07-04-interest-concerns",
    "title": "관심사에 대한 고민",
    "date": "2024-07-04 19:49:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "Interest"
    ],
    "content": "현재 나의 상황 이제 데이터과학자로 일한지 3개월정도 되었다. 아이펠이라는 모두의 연구소에서 운영하는 부트캠프를 진행했었는데, 이때 기업연계프로젝트로 진행했던 기업에 취업을 했다. 분야는 안과의료분야이며, 비앤빛 밝은세상 안과로부터 만들어진 스타트업이다. 이때 진행한 프로젝트는 당뇨병성 망막병증의 병변들을 탐지해내는 모델을 만드는 것이었다. 이에 대한 링크는 다음과 같다. 발표자료 발표링크 결과적으로는 의료 데이터를 가지고 Segmentation 작업을 수행하고 발전시켜볼 수 있어서 좋았다. 또한 이게 어떻게 쓰일 수 있을지에 대한 고민도 해볼 수 있어서 좋았다. 그렇게 인공지능을 통해 건강에 기여할 수 있겠다는 꿈을 가지고 입사를 하게 되었다. 대부분의 스토리가 그러하듯, 나 역시 기대와 많이 다른 회사생활을 하게 되었다. 안과 분야에서의 인공지능 개발현황 의료 분야 중에서 특히 안과에 대해서만 알고, 그래서 안과에 대해서만 한정해서 이야기하려고 한다. 비쥬웍스 내가 일하고 있는 비쥬웍스라는 기업은 내가 느끼기에 가장 성취가 적다. 의료인증을 받은 모델도 없고, 받으려면 아직 멀었다. 자세하게 이야기할 수는 없지만, 회사의 방향도 당장의 수익을 만드는 것에 집중하고 있다. 따라서 내가 원했던 부분과는 많이 다르게 일을 하고 있다. 다른 회사로 이직하면 되는 걸까? 선두기업들은 어떻게 하고 있을까? 뷰노 뷰노에서는 안저사진을 통한 질병 분류 모델을 출시하고, 결과에 CAM도 같이 보여준다. (CAM을 통해 모델이 어느 부분을 집중적으로 보고 있는지 '추정'할 수 있음) 병원에 있는 의사분께 듣기로는 전혀 안쓰인다고 한다. 정확도가 낮고, 몇번 정확도가 낮다는 생각이 들면 바로 안쓰게 된다고 한다. 굳이? 이런 마음이 큰 것 같다. 메디비전 가장 재미있어 보이는 기업이다. 안저사진을 통해 심질환 위험도 예측, 콩팥 위험도 예측을 진행한다. 시리즈B(100억)투자를 받고, 쟁쟁한 이사진들과 함께 모델을 개발하고 의료인증을 진행하고 있다. 아마도 국내는 수가가 낮고 보수적이라는 이유 때문에 미국 FDA 승인을 진행 중이며, 콩팥 모델의 경우에는 승인을 받은 것으로 알고 있다. 링크드인을 팔로잉하며 보고 있는데, 학문적인 발표도 많이 하는 것으로 보인다. 홈페이지에서도 발행한 논문들을 확인할 수 있다. 다만 과연 수익이 날까? 에 대해서는 궁금하고 났으면 좋겠다. 나의 진로에 대한 고민 이런 상황속에서 나는 나의 흥미에 대해 한번 더 생각해보게 됐다. 그리고 이런 흥미에 따라 진로를 결정하게 되었다. 건강에 기여하고 싶은가? 과연 나는 진짜 건강에 기여하고 싶을까? 당장에 나의 영향이 전혀 느껴지지 않는 상황에서, 이런 꿈을 가지고 개발을 이어나갈 수 있을까? 나의 대답은 \"아니, 전혀\" . 항상 느끼지만 나는 소비자와 밀접한 분야에서 개발을 하고 싶다. 내가 만든 것들이 바로 사람들에게 영향을 미쳤으면 좋겠다. 그렇다면 어느 분야? 고3때 내 꿈은 PD 였다. 물론 이과였지만, 컨텐츠들을 소비하며 삶을 이어나갈 수 있었고 많은 영감을 받았기 때문이다. 나도 좋은 컨텐츠를 만들어 다른 사람에게 긍정적인 영향을 주고 싶었다. 핑계지만 이런 생각을 발전시킬 상황이 아니었고 자연스럽게 흘러갔다. 시간이 지나 인공지능으로 컨텐츠를 만드는 세상이 왔다. 직접적으로 컨텐츠를 생성하는 일을 하는 것도 좋지만, 내가 원하는 컨텐츠들을 쉽게 생성할 수 있는 도구를 개발하는 것도 재밌을 것 같다는 생각이 많이 든다. 준비할 것들 정리하자면 의료분야 데이터 과학자에서 생성형 인공지능을 개발하는 쪽으로 커리어를 이어나가고 싶다. 혹은 컨텐츠를 다루는 기업의 데이터 과학자도 괜찮을 것 같다. 어쨌든 컨텐츠를 다루는 쪽으로 커리어를 가지고 싶다. 이를 위해 크게 2가지 방향으로 준비하려고 한다. 생성형 인공지능 추천시스템 이를 위해 지금 하고 있는 것은 생성형 인공지능 스터디 (모두의 연구소 풀잎스쿨) 추천시스템 (프로젝트, 개인공부) 정리 앞으로 공부해야할 것들이 많다. 탑을 쌓아가는 과정을 꾸준히 블로그에 남기려고 한다. 하나씩 하다보면 못하는 일은 거의 없다고 생각한다. 꾸준히 준비해서 내가 원하는 커리어를 만들어 나가자!",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2024-04-10-job-hunting-important-things",
    "title": "취준할 때 중요한 것",
    "date": "2024-04-10 12:00:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "취준"
    ],
    "content": "저는 현재 의료인공지능 회사에서 데이터과학자로 일하고 있습니다. 제가 데이터과학자를 준비하면서 가장 중요하게 생각했고, 도움이 됐던 생각을 공유해보려 합니다. > 회사와 비슷한 고민을 하고 내가 가진 자원으로 해결해보는 것 데이터과학자 데이터과학자란 문제를 정의하고 이걸 데이터로 해결해나가는 사람을 뜻합니다. 이는 크게 2가지 과정으로 나눠볼 수 있습니다. 문제를 정의 내가 가진 자원과 기술로 해결 프로젝트를 하고 이를 포트폴리오를 만들 때는 이 2가지 과정이 꼭 잘 담겨있어야 합니다. > 어떻게 문제를 정의했으며, 이를 해결하기 위한 과정에서 어떤 고민들을 헀는지. 회사 회사는 문제를 정의하고 그걸 해결해나가며 수익을 내는 집단입니다. 회사에서 일하는 데이터과학자들은 본인이 정의하거나 운영진에서 정의한 문제를 회사의 자원과 본인의 기술들로 해결해나갑니다. > 데이터과학자를 준비하는 사람과 회사에서 데이터과학자로 일하는 사람의 과정은 완전히 동일하다고 할 수 있습니다. 중요한 부분 결국 중요한 것은 나의 과정들과 회사의 과정이 얼마나 겹치는지 입니다. 이 교집합이 클수록 본인의 프로젝트 혹은 포트폴리오는 매력적으로 보이게 됩니다. 즉, 내가 생각했을 때 이런 문제가 있고 또는 중요하다고 생각한다. 그래서 나는 이런 문제를 이렇게 풀어봤다. 푸는 과정에서는 이런 어려움이 있었고 이렇게 해결해나갔다. 이게 포트폴리오에 잘 담겨있어야하며, 취업을 준비할 때 가장 중요한 점이라고 생각합니다. 실제로 저도 제가 정의하고 해결했던 문제가 현재 제가 회사에서 해결하고 있는 문제입니다. 면접에서도 이걸 가장 좋게 봐주셨고 취업에 가장 도움이 많이 됐다고 생각합니다. 만약 제가 다른 분야로 이직을 준비한다면, 위에서 언급한 것처럼 사이드 프로젝트를 준비할 것입니다. 당연해보이는 생각이지만, 생각보다 많이 놓칠 수 있어 한번 정리해봤습니다. 취업을 준비하시는 분들이 좋은 결과를 얻었으면 좋겠습니다!",
    "public": true
  },
  {
    "id": "Self_Development/Reflections/2024-03-12-aifell-review",
    "title": "아이펠 후기",
    "date": "2024-03-12",
    "excerpt": "6개월 동안의 아이펠 후기",
    "category": "Reflections",
    "tags": [
      "아이펠",
      "이런저런 생각들"
    ],
    "content": "저는 23.9 ~ 24.2 동안 아이펠 6기 리서치 과정을 수료했고, 회고 겸 후기를 적어보려 합니다. !졸업증 신청했을 때의 나의 상황 일단 제가 어떤 상태에서 아이펠을 들었는지 설명드리겠습니다. 일단 저는 산업공학을 전공했고, 통계학을 부전공했습니다. 학부생때 ML에 관심이 많아서 혼자서 공부했었고, 관련 프로젝트도 진행했습니다. DL 부분은 잘 몰랐는데, 시작하기 1달전에 \"밑바닥부터 시작하는 딥러닝\" 2회독정도 했습니다. 제 배경지식은 통계학 + 컴퓨터공학 + DL 조금. 정도로 정리할 수 있겠습니다. 내가 생각하는 난이도 처음 1~2달 정도는 그 전에 공부했던 것들이라 복습하는 느낌으로 공부했고, 나머지는 개념정도만 아는 상태에서 공부를 했습니다. 그래서 따라가는데 어려움은 없었습니다. 다만, 양이 워낙 많아서 평균적으로 추가공부를 3~4시간, 적어도 1시간씩은 했던 것 같습니다. 배경지식이 전혀 없는 분이 리서치 과정을 수료하려면 적어도 매일 6시간씩은 추가공부해야할 것 같습니다. 공부방식 선생님이 있는게 아닌, 공부자료가 주어지고 같이 공부하는 사람들과 함께 알아가는 방식입니다. 공부의 방향을 \"퍼실\" 분들이 잡아주고, 모르는 부분을 해소해주는 방식으로 도움을 주십니다. 이게 보통 스터디 같은 경우 나보다 더 잘아는 사람과 해야지 얻는게 많을 것이라 생각하는데, 경험해보니 반대였던 것 같습니다. > 왜 선생님이 가장 많이 배운다고 하잖아요, 저는 배경지식이 있어 \"선생님\" 역할을 맡을 때가 많았는데 정말 많이 배웠습니다. 어느정도 알던 개념들이 설명을 하면서 완성되곤 했습니다. 특히 아에 모르던 개념들도 어떻게 해야 빠르게 습득하고 하나의 지식으로 만들 수 있는지 많이 배웠습니다. 또 토의, 토론을 정말 많이 했는데, 이를 통해 제 지식을 점검할 수 있었고 오류가 난 부분을 많이 찾을 수 있었습니다. 처음에는 어색했던 방법인데, 정말 만족했고 많이 배울 수 있었습니다. 프로젝트 6개월 과정 중 마지막 1~2개월에 최종 프로젝트(아이펠톤)를 진행하게 됩니다. 그 전 4개월동안 짧게 프로젝트를 진행했는데 (하루 or 3일), 이 때 연습한게 최종 프로젝트에 도움이 많이 됐습니다. 아이펠톤에는 기업이 참여하기도 하며, 참여기업이 있을 경우 주제를 들어보고 괜찮다고 판단되면 참여해 기업프로젝트를 진행하게 됩니다. 저는 해보고 싶었던 아이디어가 있어서 그걸 발제했고, 선정이 됐지만 기업프로젝트를 진행했습니다. 명확한 결과를 내고 싶었고, 기업의 관점이 궁금했습니다. 또 채용 가능성도 있어서 매력적으로 느껴졌습니다. 채용에 대해서는 기업소개때 같이 언급해주십니다. (보통 맘에 들면 채용할 수도 있다는 식입니다) 아이펠에서는 멘토를 각 프로젝트에 배정해주는데, 이 부분이 도움이 많이 됐습니다. 선택의 갈림길에서 조언을 많이 구했고, 피드백을 많이 받았습니다. 다만, 멘토님이 모두 좋지는 않았던 것 같습니다. 다른 팀에서는 되게 불만족스러웠다고 이야기하더라구요. 약 2개월동안 주제를 정하고, 문제를 해결해나가면서 결과를 완성했습니다. 왜 프로젝트 경험이 중요하다고 이야기하는지 정말 많이 느꼈고, 포트폴리오에 하나가 추가되어서 좋았습니다. 회고 시간이 참 빠른 것 같습니다. 6개월이 진짜 빨리 지나갔고, 그 과정에서 많이 배우고 좋은 인연을 얻었습니다. 아이펠에 만족하냐고 묻는다면, 저는 진짜 대만족입니다. 정말 좋은 분들이 계시고, 많은 도움을 받았습니다. 아이펠의 처음 목적은 \"딥러닝의 원리를 이해하는 것\" 이었는데, 목적을 달성했습니다. 단순히 코드 중심으로 결과를 내는게 아닌, 개념에 대해 토론하면서 개념을 깊이 이해할 수 있었습니다. 6개월이 지났고, 이 과정에서 배운 것들을 이용해 문제를 정의하고 해결하면서 꾸준히 성장하려고 합니다.",
    "public": true
  }
]