[
  {
    "id": "Codility Study/í˜„ëŒ€ì˜¤í† ì—ë²„ì½”í…Œ",
    "title": "í˜„ëŒ€ì˜¤í† ì—ë²„ ì½”í…Œ ì¤€ë¹„í•˜ê¸°",
    "date": "2025-08-01",
    "excerpt": "í˜„ëŒ€ì˜¤í† ì—ë²„ ì½”í…Œ ì¤€ë¹„í•˜ê¸°",
    "category": "Codility Study",
    "tags": [
      "ì½”í…Œ ì¤€ë¹„"
    ],
    "content": "ì‹œí—˜ ê¸°ì¶œ ë¬¸ì œ ìœ í˜• ì°¸ê³ í•œ ë¸”ë¡œê·¸ 2025ë…„ ê¸°ì¶œ ë¬¸ì œ ìœ í˜• ë°±ì—”ë“œ ì´ 2ë¬¸ì œ DFS ë¬¸ì œ ê·¸ë˜í”„ê°€ ë‚˜ë‰˜ì–´ì ¸ ìˆëŠ” ê²½ìš°ì™€ ì‚¬ì´í´ì¸ ê²½ìš°ë¥¼ ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë¬¸ì œ. DP ìœ í˜•ì˜ ë¬¸ì œ ëŠ˜ë³´ì½”ì¹˜ì˜ IT ì·¨ì—… ì½”í…Œ + ì„œë¥˜ë¡œ ë³´í†µ 1ì°¨ í‰ê°€ ì˜ˆì • ì•Œê³ ë¦¬ì¦˜ ìœ„ì£¼ì¸ë° ì‹œê°„ë³µì¡ë„ë¥¼ ì¤„ì—¬ì£¼ëŠ” ì•Œê³ ë¦¬ì¦˜ ì„ í˜¸ íˆ¬í¬ì¸í„° (ë°±ì¤€ 1253ë²ˆ) ì™„ì „íƒìƒ‰ìœ¼ë¡œ í’€ë©´ ì‹œê°„ ì´ˆê³¼ ì´ë¶„íƒìƒ‰ (ë°±ì¤€ 6236ë²ˆ) DP ê¸°ë³¸ë¬¸ì œ (ë°±ì¤€ 12865ë²ˆ), ë©”ëª¨ì´ì œì´ì…˜ (ë°±ì¤€ 9184ë²ˆ) ìš°ì„ ìˆœìœ„ í (ë°±ì¤€ 1766ë²ˆ) ëˆ„ì í•© (ë°±ì¤€ 2143ë²ˆ)"
  },
  {
    "id": "Codility Study/ì½”ë”©í…ŒìŠ¤íŠ¸í•©ê²©ìë˜ê¸°/ê·¸ë˜í”„/ê·¸ë˜í”„_ê°œë…ì •ë¦¬",
    "title": "ê·¸ë˜í”„ ê°œë…ì •ë¦¬",
    "date": "2025-08-01",
    "excerpt": "ê·¸ë˜í”„ ê°œë…ì •ë¦¬",
    "category": "Codility Study",
    "tags": [
      "ì½”ë”© í…ŒìŠ¤íŠ¸ í•©ê²©ì ë˜ê¸°",
      "Codility Study"
    ],
    "content": "ê·¸ë˜í”„ë€? ì •ì˜ : > ê·¸ë˜í”„ëŠ” ë…¸ë“œ (vertex)ì™€ ê°„ì„  (edge)ì„ ì´ìš©í•œ ë¹„ì„ í˜• ë°ì´í„° êµ¬ì¡° ë°ì´í„°ì˜ ê´€ê³„ë¥¼ í‘œí˜„í•˜ëŠ”ë° ì‚¬ìš© ê·¸ë˜í”„ì˜ íŠ¹ì§•ê³¼ ì¢…ë¥˜ : ë°©í–¥ì„± ì¡´ì¬ ì—¬ë¶€ ë°©í–¥ ê·¸ë˜í”„ (directed graph) : ê°„ì„ ì— ë°©í–¥ì„±ì´ ìˆëŠ” ê·¸ë˜í”„ ë¬´ë°©í–¥ ê·¸ë˜í”„ (undirected graph) : ê°„ì„ ì— ë°©í–¥ì„±ì´ ì—†ëŠ” ê·¸ë˜í”„ ê°„ì„ ì˜ ê°€ì¤‘ì¹˜ ì¡´ì¬ ì—¬ë¶€ ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ (weighted graph) : ê°„ì„ ì— ê°€ì¤‘ì¹˜ê°€ ìˆëŠ” ê·¸ë˜í”„ ë¬´ê°€ì¤‘ì¹˜ ê·¸ë˜í”„ (unweighted graph) : ê°„ì„ ì— ê°€ì¤‘ì¹˜ê°€ ì—†ëŠ” ê·¸ë˜í”„ ìˆœí™˜ ì—¬ë¶€ ìˆœí™˜ ê·¸ë˜í”„ (cyclic graph) : ê°„ì„ ì„ ë”°ë¼ ì´ë™í•˜ë©´ ì¶œë°œì ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ê²½ë¡œê°€ ì¡´ì¬í•˜ëŠ” ê·¸ë˜í”„ ë¹„ìˆœí™˜ ê·¸ë˜í”„ (acyclic graph) : ê°„ì„ ì„ ë”°ë¼ ì´ë™í•˜ë©´ ì¶œë°œì ìœ¼ë¡œ ëŒì•„ì˜¤ëŠ” ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê·¸ë˜í”„ ê·¸ë˜í”„ êµ¬í˜„ : ê·¸ë˜í”„ì˜ í‘œí˜„ ë°©ë²• ì¸ì ‘ í–‰ë ¬ (adjacency matrix) : 2ì°¨ì› ë°°ì—´ë¡œ ê·¸ë˜í”„ì˜ ì—°ê²° ê´€ê³„ë¥¼ í‘œí˜„ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ (adjacency list) : ì—°ê²° ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ë˜í”„ì˜ ì—°ê²° ê´€ê³„ë¥¼ í‘œí˜„ > ì…ë ¥ê°’ì´ ì ê±°ë‚˜ íŠ¹ì • ì •ì  ì‚¬ì´ì˜ ê°„ì„  ì—¬ë¶€ë¥¼ ë¹ ë¥´ê²Œ íŒë‹¨í•´ì•¼ í•  ë•ŒëŠ” ì¸ì ‘ í–‰ë ¬ì„, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤. ì¸ì ‘í–‰ë ¬ ë…¸ë“œ ê°„ì˜ ê´€ê³„ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•˜ëŠ” ê²ƒ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ [ì •ì (v), ê°€ì¤‘ì¹˜(w), ë‹¤ìŒ ë…¸ë“œ(next)] ë¡œ êµ¬ì„±ëœ ë¦¬ìŠ¤íŠ¸ë¡œ ê·¸ë˜í”„ë¥¼ í‘œí˜„. ë…¸ë“œì— ëŒ€í•œ ë°°ì—´ì´ ìˆê³ , ë…¸ë“œë³„ë¡œ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì§„ë‹¤. ì¥ë‹¨ì  ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œê°„ë³µì¡ë„ ê¸°íƒ€ --- --- --- --- ì¸ì ‘ í–‰ë ¬ $O(N^2)$ $O(1)$ êµ¬í˜„ì´ ìƒëŒ€ì ìœ¼ë¡œ ì‰¬ì›€ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ $O(N+E)$ $O(N)$ ì¸ì ‘ í–‰ë ¬ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ì˜ ì¥ì  : ì •ë³´ë¥¼ í™•ì¸í•  ë–„ ì‹œê°„ ë³µì¡ë„ê°€ O(1)ë¡œ ì¢‹ìŒ ì¸ì ‘ í–‰ë ¬ì˜ ë‹¨ì  : ì¸ì ‘ í–‰ë ¬ë¡œ í¬ì†Œ ê·¸ë˜í”„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš° -> êµ‰ì¥íˆ ë¹„íš¨ìœ¨ì . ë©”ëª¨ë¦¬ ë‚­ë¹„. ë…¸ë“œë“¤ì˜ ê°’ì˜ ì°¨ì´ê°€ ë§¤ìš° í° ê·¸ë˜í”„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš° -> ê°€ì¥ í° ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì¸ì ‘ í–‰ë ¬ì˜ í¬ê¸°ë¥¼ ì¡ì•„ì•¼ í•˜ë‹ˆê¹Œ. ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ (ì¸ì ‘ í–‰ë ¬ê³¼ ë°˜ëŒ€) ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ì˜ ì¥ì  : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ìŒ ë…¸ë“œë“¤ì˜ ê°’ì˜ ì°¨ì´ê°€ ë§¤ìš° í° ê·¸ë˜í”„ë¥¼ í‘œí˜„í•˜ëŠ” ê²½ìš° ë©”ëª¨ë¦¬ ë‚­ë¹„ê°€ ì ìŒ ì¸ì ‘ ë¦¬ìŠ¤íŠ¸ì˜ ë‹¨ì  : ì •ë³´ë¥¼ í™•ì¸í•  ë–„ ì‹œê°„ ë³µì¡ë„ê°€ O(n)ìœ¼ë¡œ ë¹„íš¨ìœ¨ì ."
  },
  {
    "id": "English Study/SPEAK/07-31-ì¢€ë¶ˆí¸í•´ë„ë§ˆìŠ¤í¬ì“¸ê±°ì•¼",
    "title": "ì¢€ ë¶ˆí¸í•´ë„ ë§ˆìŠ¤í¬ ì“¸ ê±°ì•¼",
    "date": "2025-07-31",
    "excerpt": "ì¢€ ë¶ˆí¸í•´ë„ ë§ˆìŠ¤í¬ ì“¸ ê±°ì•¼",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ What if I catch the flu? Even if it's inconvinient, I will still wear a mask. Even if you mess up, we'll still believe in you. No matter what happens, I'll always believe in you. be there for you. ê²ë‚˜ ë¡œë§¨í‹±í•˜ê²Œ ëŠê»´ì§€. Failures are okay as long as youy learn from then. ê°„ë‹¨í•œ ì‘ë¬¸ Even if I'm tired, I'll practice English. No matter what happens, I'll always be there for you."
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-30-ê³ ê¹ƒì§‘ì—ê°€ë„ë‚œìƒê´€ì—†ì–´",
    "title": "ê³ ê¹ƒì§‘ì— ê°€ë„ ë‚œìƒê´€ì—†ì–´",
    "date": "2025-07-30",
    "excerpt": "ê³ ê¹ƒì§‘ì— ê°€ë„ ë‚œìƒê´€ì—†ì–´",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Sharing is caring. ê´€ì‹¬ ìˆìœ¼ë©´ ì•Œë ¤ì¤˜ ê·¸ë ‡ê²Œ í•´ë„ ìƒê´€ì—†ì–´ Let me know. ì•Œë ¤ì¤˜. ì—°ë½í•´. Let me know if you're interested. ê´€ì‹¬ ìˆìœ¼ë©´ ì•Œë ¤ì¤˜. change your mind. hange out : ê°™ì´ ì‹œê°„ì„ ë³´ë‚´ë‹¤. I don't mind if we go clubbing. ê°„ë‹¨í•œ ì‘ë¬¸ I totally agree about the sentence \"Sharing is caring.\" I'm willing to share my insights or knowledge with my pioneers and friends. Today I learned the phares that is used in the conversation. Let me know if you wonder my codes. Then I will explain it. And my food type (ì‹ì„±) is so normal. I like almost foods. So I always say to my friends that I don't mind if we have chinese foods. í”¼ë“œë°± ë¬¸ë²• ì˜¤ë¥˜ \"pioneers\" â†’ \"peers\" (ë™ë£Œë¥¼ ì˜ë¯¸í•  ë•Œ) \"phares\" â†’ \"phrases\" (ì² ì ì˜¤ë¥˜) \"that is\" â†’ \"that are\" (phrasesëŠ” ë³µìˆ˜) \"wonder my codes\" â†’ \"wonder about my code\" (codeëŠ” ë³´í†µ ë‹¨ìˆ˜í˜• ì‚¬ìš©) \"almost foods\" â†’ \"almost all foods\" (ê±°ì˜ ëª¨ë“  ìŒì‹) \"chinese foods\" â†’ \"Chinese food\" (ìŒì‹ì€ ë³´í†µ ë‹¨ìˆ˜í˜•) í‘œí˜„ ê°œì„  \"I totally agree about\" â†’ \"I totally agree with\" \"I'm willing to share\" â†’ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ê°œì„  \"Let me know if you wonder\" â†’ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ìœ¼ë¡œ ê°œì„  ê°œì„ ëœ ë²„ì „ I totally agree with the saying \"Sharing is caring.\" I'm always happy to share my insights and knowledge with my peers and friends. Today I learned some useful phrases that are commonly used in conversations. If you're curious about my code, feel free to ask. I'd be happy to explain it to you. As for my food preferences, I'm quite flexible. I enjoy almost all types of food. That's why I always tell my friends that I don't mind going to Chinese restaurants."
  },
  {
    "id": "Causal Inference/residualization-fwl-theorem-test",
    "title": "FWLì— ê¸°ë°˜í•œ ì”ì°¨í™” ë°©ë²•ë¡  êµ¬í˜„ ë…¸íŠ¸ë¶",
    "date": "2025-07-10T00:00:00.000Z",
    "excerpt": "ì”ì°¨í™”ë¥¼ í•˜ê¸° ìœ„í•´ nusiance functionì„ ë§Œë“¤ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë…¸íŠ¸ë¶",
    "category": "Causal Inference",
    "tags": [
      "residualization",
      "FWL"
    ],
    "content": "--- title: \"FWLì— ê¸°ë°˜í•œ ì”ì°¨í™” ë°©ë²•ë¡  êµ¬í˜„ ë…¸íŠ¸ë¶\" date: 2025-07-10 category: \"Causal Inference\" tags: [\"residualization\", \"FWL\"] excerpt: \"ì”ì°¨í™”ë¥¼ í•˜ê¸° ìœ„í•´ nusiance functionì„ ë§Œë“¤ê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë…¸íŠ¸ë¶\" --- # Load Data # from src.dataloader.vault.main import VaultDatasetLoader from src.dataloader.vault.utils import split_df_od_os from src.utils.db import SOFTCRM_DBINFO, get_conn import warnings import pandas as pd import numpy as np warnings.filterwarnings(\"ignore\") # pandas display ì˜µì…˜ ì„¤ì • - ëª¨ë“  ì—´ ë³´ì´ê¸° pd.set_option('display.max_columns', None) pd.set_option('display.width', None) pd.set_option('display.max_colwidth', None) loader = VaultDatasetLoader() feature_df, y_df = loader.run(start_date=\"2022-01-01\", end_date=\"2024-12-31\", piol_data_path=\"./data/PIOLë Œì¦ˆì£¼ë¬¸ì„œ.xlsx\", refresh=False, db=\"crm_emr\") # y_dfì˜ oper_dateì™€ feature_dfì˜ dateë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë³‘í•© merged_df = pd.merge(y_df, feature_df, left_on=['cust_num', 'oper_date'], right_on=['cust_num', 'date'], how='left') merged_df = split_df_od_os(merged_df) query = \"\"\"SELECT CUST_NUM, BIRTH_DAY FROM VIEW_CUSTOM_JUMIN\"\"\" with get_conn(SOFTCRM_DBINFO) as conn, conn.cursor() as cursor: cursor.execute(query) result = cursor.fetchall() age_df = pd.DataFrame([list(res) for res in result], columns=[\"cust_num\", \"birthday\"]) age_df = age_df.applymap(lambda x: np.nan if isinstance(x, str) and x.strip() == \"\" else x) age_df[\"birthday\"] = pd.to_datetime(age_df[\"birthday\"], errors=\"coerce\", format=\"%Y-%m-%d\") age_df = age_df.dropna().reset_index(drop=True) age_df[\"cust_num\"] = age_df[\"cust_num\"].apply(lambda x: x.strip()) cust_age_dict = dict(zip(age_df[\"cust_num\"], age_df[\"birthday\"])) merged_df['birthday'] = merged_df['cust_num'].map(cust_age_dict) merged_df['age'] = merged_df.apply(lambda row: pd.to_datetime(row['oper_date']).year - row['birthday'].year if pd.notnull(row['birthday']) else np.nan, axis=1) # merged_df.to_csv(\"./merged_df_2022_01_01_2024_12_31_with_age.csv\", index=False) # Modules from src.preprocessing.constants import ALL_RANGES_NO_SIDE import pandas as pd import numpy as np def load_data() -> tuple[pd.DataFrame, list, list, list]: \"\"\" ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•˜ì—¬ ë¶„ì„ì— í•„ìš”í•œ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤. Returns: tuple[pd.DataFrame, list, list, list]: df_clean (pd.DataFrame): ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„ features (list): íŠ¹ì„± ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] treatment (list): ì²˜ì¹˜ ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ [\"size\"] y (list): ê²°ê³¼ ë³€ìˆ˜ëª… ë¦¬ìŠ¤íŠ¸ [\"meas_vault\"] Note: NaN ê°’ì´ ìˆëŠ” í–‰ì€ ì œê±°ë©ë‹ˆë‹¤ sizeê°€ [12.1, 12.6, 13.2]ê°€ ì•„ë‹Œ ê²½ìš°ëŠ” ì œê±°ë©ë‹ˆë‹¤ \"\"\" df = pd.read_csv(\"./data/merged_df_2022_01_01_2024_12_31_with_age.csv\") df.head() features = [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] treatment = [\"size\"] y = [\"meas_vault\"] # df ì¤‘ features, treatment, y ì»¬ëŸ¼ë§Œ ë‚¨ê¸°ê³  ì¹¼ëŸ¼ ì œê±°, ì´ ì¹¼ëŸ¼ë“¤ì— ëŒ€í•œ ê°’ì´ NaNì¸ ê²ƒë„ ì œê±° selected_columns = features + treatment + y df_clean = df[selected_columns].copy() # df['size']ê°€ [12.1, 12.6, 13.2] ì´ ì•„ë‹Œ ê²½ìš° ì œê±° df_clean = df_clean[df_clean[\"size\"].isin([12.1, 12.6, 13.2])].reset_index(drop=True) # ì´ìƒì¹˜ ì œê±° ë¡œì§ # ëª¨ë“  ì»¬ëŸ¼ì— ëŒ€í•´ ë²”ìœ„ ì ìš© for column, range_obj in ALL_RANGES_NO_SIDE.items(): if column in df_clean.columns: # ì»¬ëŸ¼ ê°’ì´ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ë©´ np.nanìœ¼ë¡œ ë³€ê²½ df_clean[column] = df_clean[column].where((df_clean[column] >= range_obj.min) & (df_clean[column] <= range_obj.max), np.nan) # Cas_cctë¡œ vault ë²”ìœ„ ë§Œë“¤ê³ , ë„˜ì–´ê°€ë©´ ì—†ì• ì£¼ê¸° max_vault = ALL_RANGES_NO_SIDE[\"Cas_cct\"].max 1.5 df_clean = df_clean[df_clean[\"meas_vault\"] <= max_vault].reset_index(drop=True) # NaN ê°’ì´ ìˆëŠ” í–‰ ì œê±° df_clean = df_clean.dropna() # ë‚˜ì´ê°€ 60ì´ìƒ ì œê±° df_clean = df_clean[df_clean[\"age\"] < 60].reset_index(drop=True) df_clean[y] = df_clean[y].abs() return df_clean, features, treatment, y # ì”ì°¨í™” ì§„í–‰í•˜ê¸° > R-learnerëŠ” FWL theoremì— ì´ë¡ ì  ë°°ê²½ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤. FWL ì •ë¦¬ëŠ” ë‹¤ì¤‘ íšŒê·€ë¶„ì„ì—ì„œ íŠ¹ì • ë³€ìˆ˜ì˜ ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ë™ì¼í•œ ê²°ê³¼ë¥¼ ì¤€ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤: ì§ì ‘ íšŒê·€: ëª¨ë“  ë³€ìˆ˜ë¥¼ í¬í•¨í•œ ì „ì²´ íšŒê·€ëª¨í˜• ì”ì°¨í™” í›„ íšŒê·€: ê´€ì‹¬ ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ë¥¼ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ê°ê° íšŒê·€í•˜ì—¬ ì”ì°¨ë¥¼ êµ¬í•œ í›„, ì´ ì”ì°¨ë“¤ ê°„ì˜ íšŒê·€ ìˆ˜í•™ì  í‘œí˜„: ëª¨í˜•: Y = XÎ² + ZÎ³ + Îµì—ì„œ Î²ë¥¼ êµ¬í•˜ê³  ì‹¶ë‹¤ë©´: ë°©ë²• 1: ì „ì²´ íšŒê·€ `` Î²Ì‚ = (X'M_Z X)^(-1) X'M_Z Y ` ë°©ë²• 2: ì”ì°¨í™” ` XÌƒ = M_Z X (Xë¥¼ Zì— íšŒê·€í•œ ì”ì°¨) á»¸ = M_Z Y (Yë¥¼ Zì— íšŒê·€í•œ ì”ì°¨) Î²Ì‚ = (XÌƒ'XÌƒ)^(-1) XÌƒ'á»¸ ` ì—¬ê¸°ì„œ M_Z = I - Z(Z'Z)^(-1)Z'ëŠ” Zì— ìˆ˜ì§ì¸ íˆ¬ì˜í–‰ë ¬ì…ë‹ˆë‹¤. ì”ì°¨í™”ê°€ ê°€ëŠ¥í•´ì§€ëŠ” ì´ìœ : ì§êµì„±: ì”ì°¨í™”ëŠ” ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ ì˜í–¥ì„ ì œê±°í•˜ì—¬ ìˆœìˆ˜í•œ íš¨ê³¼ë§Œ ë¶„ë¦¬ ì°¨ì› ì¶•ì†Œ: ë³µì¡í•œ ë‹¤ë³€ëŸ‰ ë¬¸ì œë¥¼ ë‹¨ìˆœí•œ ì´ë³€ëŸ‰ ë¬¸ì œë¡œ ë³€í™˜ í•´ì„ ìš©ì´ì„±: ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì„ í†µì œí•œ í›„ì˜ ìˆœìˆ˜í•œ ê´€ê³„ íŒŒì•… > R-learnerëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‘ ë‹¨ê³„ë¡œ êµ¬í˜„ë©ë‹ˆë‹¤: Propensity Score: Treatmentë¥¼ Featuresë¡œ ì˜ˆì¸¡ Outcome Function: Outcomeì„ Featuresë¡œ ì˜ˆì¸¡ Step 1: Propensity Score (Treatment ì”ì°¨í™”): `python X â†’ T íšŒê·€ (Treatmentë¥¼ Featuresë¡œ ì˜ˆì¸¡) model = LogisticRegression() model.fit(X_train, y_train) propensity_scores = model.predict_proba(X_val) ` Step 2: Outcome Function (Y ì”ì°¨í™”): `python X â†’ Y íšŒê·€ (Outcomeì„ Featuresë¡œ ì˜ˆì¸¡) Y_residual = Y - E[Y X] T_residual = T - E[T X] (propensity score í™œìš©) ` ì™œ ì”ì°¨í™”ê°€ \"ê°€ëŠ¥\"í•´ì§€ëŠ”ê°€? : A. ì¡°ê±´ë¶€ ë…ë¦½ì„± í™•ë³´ ` E[T_residual X] = 0 E[Y_residual X] = 0 ` Xì˜ ì˜í–¥ì„ ì œê±°í•¨ìœ¼ë¡œì¨ ìˆœìˆ˜í•œ Tâ†’Y íš¨ê³¼ë§Œ ë¶„ë¦¬ B. í¸í–¥ ì œê±° (Bias Elimination) Selection bias: ê´€ì¸¡ë˜ì§€ ì•Šì€ êµë€ë³€ìˆ˜ì˜ ì˜í–¥ Confounding: Xê°€ Tì™€ Y ëª¨ë‘ì— ì˜í–¥ì„ ì£¼ëŠ” ë¬¸ì œ C. ê³¼ì í•© ë°©ì§€ `python Cross-fitting ì‚¬ìš© kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) ` Out-of-sample ì˜ˆì¸¡ìœ¼ë¡œ ê³¼ì í•© ë°©ì§€ ì‹¤ì œ ì¸ê³¼íš¨ê³¼ ì¶”ì • ìµœì¢…ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì€ íšŒê·€ë¥¼ ìˆ˜í–‰: ` Y_residual = Ï„ Ã— T_residual + noise ` ì—¬ê¸°ì„œ Ï„`ê°€ ìˆœìˆ˜í•œ treatment effectì…ë‹ˆë‹¤. ê²°ë¡ : FWL ì •ë¦¬ì—ì„œ ì”ì°¨í™”ê°€ ê°€ëŠ¥í•´ì§€ëŠ” ì´ìœ ëŠ”: ìˆ˜í•™ì : ì§êµ íˆ¬ì˜ì˜ ì„±ì§ˆë¡œ ì¸í•œ ê³„ìˆ˜ ë¶ˆë³€ì„± í†µê³„ì : êµë€ë³€ìˆ˜ í†µì œë¥¼ í†µí•œ í¸í–¥ ì œê±° ì‹¤ìš©ì : ë³µì¡í•œ ë‹¤ë³€ëŸ‰ ë¬¸ì œë¥¼ ë‹¨ìˆœí•œ ì´ë³€ëŸ‰ ë¬¸ì œë¡œ ë³€í™˜ > R-learner êµ¬í˜„ì€ ì´ ì´ë¡ ì  ê¸°ë°˜ì„ ì¸ê³¼ì¶”ë¡ ì— ì‹¤ì œ ì ìš©í•œ ì‚¬ë¡€ë¡œ, íŠ¹íˆ ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼(Heterogeneous Treatment Effects) ì¶”ì •ì— í™œìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. # 1. propensity score function > P(T X) Xì¼ ë•Œ Tì¼ í™•ë¥ ì„ êµ¬í•´ì„œ, ì´ë¥¼ í†µí•´ ì”ì°¨í™” ì§„í–‰ propensity scoreë¥¼ í†µì œí•˜ëŠ” ê²ƒê³¼ Të¥¼ í†µì œí•˜ëŠ” ê²ƒì€ ê°™ì€ ì˜ë¯¸ì´ê¸° ë•Œë¬¸ì— df, features, treatment, y = load_data() df[treatment] = df[treatment].astype(\"category\") (treatment,) = treatment print(treatment) df.head() ì¤‘ìš”í•œì ! > ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ cross-fitting (ë˜ëŠ” sample splitting)ì„ ì‚¬ìš©í•´ì„œ out-of-sample ì˜ˆì¸¡ê°’ì„ í™œìš© import numpy as np import pandas as pd from sklearn.model_selection import StratifiedKFold from sklearn.linear_model import LogisticRegression from sklearn.metrics import accuracy_score, roc_auc_score from sklearn.preprocessing import StandardScaler RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) ì¹´í…Œê³ ë¦¬í˜• treatmentë¥¼ ìˆ«ìë¡œ ë³€í™˜ X_features = df[features] y_treatment_cat = df[treatment].cat.codes # ì¹´í…Œê³ ë¦¬ë¥¼ ìˆ«ìë¡œ ë³€í™˜ print(f\"Treatment ì¹´í…Œê³ ë¦¬: {df[treatment].cat.categories}\") print(f\"Treatment ë¶„í¬:\\n{df[treatment].value_counts()}\") print(f\"ìˆ«ì ë³€í™˜ëœ ë¶„í¬:\\n{pd.Series(y_treatment_cat).value_counts()}\") ê° í´ë˜ìŠ¤ë³„ propensity score ì €ì¥ìš© ë°°ì—´ class_categories = df[treatment].cat.categories n_classes = len(class_categories) propensity_scores_all = np.zeros((len(df), n_classes)) StratifiedKFoldë¡œ ì¹´í…Œê³ ë¦¬ ë¹„ìœ¨ ë§ì¶¤ kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) fold_performances = [] for fold, (train_idx, val_idx) in enumerate(kf.split(X_features, y_treatment_cat)): print(f\"\\n=== Fold {fold + 1} ===\") X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx] y_train, y_val = y_treatment_cat[train_idx], y_treatment_cat[val_idx] # ê° foldì˜ treatment ë¶„í¬ í™•ì¸ print(f\"Train ë¶„í¬: {pd.Series(y_train).value_counts().to_dict()}\") print(f\"Val ë¶„í¬: {pd.Series(y_val).value_counts().to_dict()}\") # ë°ì´í„° ìŠ¤ì¼€ì¼ë§ (ìˆ˜ë ´ ì„±ëŠ¥ í–¥ìƒ) scaler = StandardScaler() X_train_scaled = scaler.fit_transform(X_train) X_val_scaled = scaler.transform(X_val) # ëª¨ë¸ í›ˆë ¨ (ìˆ˜ë ´ ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ì„¤ì •) model = LogisticRegression( random_state=42, max_iter=1000, # ë°˜ë³µ íšŸìˆ˜ ì¦ê°€ (ê¸°ë³¸ê°’: 100) solver=\"liblinear\", # ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ì— ì í•©í•œ ì†”ë²„ ) model.fit(X_train_scaled, y_train) # í›ˆë ¨ ì„±ëŠ¥ í™•ì¸ train_pred = model.predict(X_train_scaled) train_accuracy = accuracy_score(y_train, train_pred) # ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ëª¨ë“  í´ë˜ìŠ¤ë³„ propensity score ê³„ì‚° val_propensity = model.predict_proba(X_val_scaled) propensity_scores_all[val_idx] = val_propensity # ëª¨ë“  í´ë˜ìŠ¤ í™•ë¥  ì €ì¥ # ê²€ì¦ ì„±ëŠ¥ í™•ì¸ val_pred = model.predict(X_val_scaled) val_accuracy = accuracy_score(y_val, val_pred) fold_performances.append( {\"fold\": fold + 1, \"train_accuracy\": train_accuracy, \"val_accuracy\": val_accuracy, \"train_size\": len(X_train), \"val_size\": len(X_val)} ) print(f\"í›ˆë ¨ ì •í™•ë„: {train_accuracy:.3f}\") print(f\"ê²€ì¦ ì •í™•ë„: {val_accuracy:.3f}\") ì›ë³¸ dfì— ê° í´ë˜ìŠ¤ë³„ propensity_score ì»¬ëŸ¼ ì¶”ê°€ for i, category in enumerate(class_categories): df[f\"propensity_score_{category}\"] = propensity_scores_all[:, i] print(f\"\\n=== ì „ì²´ ê²°ê³¼ ===\") performance_df = pd.DataFrame(fold_performances) print(f\"í‰ê·  í›ˆë ¨ ì •í™•ë„: {performance_df['train_accuracy'].mean():.3f} Â± {performance_df['train_accuracy'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ ì •í™•ë„: {performance_df['val_accuracy'].mean():.3f} Â± {performance_df['val_accuracy'].std():.3f}\") print(f\"\\nì²˜ìŒ 10ê°œ ìƒ˜í”Œ:\") propensity_cols = [f\"propensity_score_{cat}\" for cat in class_categories] print(df[[\"meas_vault\", treatment] + propensity_cols].head(10)) print(f\"\\nìƒì„±ëœ propensity score ì»¬ëŸ¼ë“¤:\") for col in propensity_cols: print(f\"{col}: í‰ê·  {df[col].mean():.3f}\") df # 2. Outcome function >m(Y X) ì´ë¥¼ í†µí•´ Yì—ì„œ Xì˜ ì˜í–¥ì„ ì œê±°í•œ ì”ì°¨ë¥¼ êµ¬í•  ìˆ˜ ìˆìŒ. df, features, treatment, y = load_data() df[treatment] = df[treatment].astype(\"category\") (treatment,) = treatment print(treatment) df.head() ì¤‘ìš”í•œì ! > ê³¼ì í•©ì„ í”¼í•˜ê¸° ìœ„í•´ cross-fitting (ë˜ëŠ” sample splitting)ì„ ì‚¬ìš©í•´ì„œ out-of-sample ì˜ˆì¸¡ê°’ì„ í™œìš© import numpy as np import pandas as pd import lightgbm as lgb from sklearn.model_selection import KFold from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error import matplotlib.pyplot as plt í•œê¸€ í°íŠ¸ ì„¤ì • (ë§¥ í™˜ê²½) import matplotlib.font_manager as fm plt.rcParams[\"font.family\"] = \"AppleGothic\" # ë§¥ìš© í•œê¸€ í°íŠ¸ plt.rcParams[\"axes.unicode_minus\"] = False # ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€ RANDOM_SEED = 42 np.random.seed(RANDOM_SEED) Outcome function (featuresë¡œ y ì˜ˆì¸¡) print(\"=== Outcome Function í•™ìŠµ (features â†’ y) ===\") X_features = df[features] y_outcome = df[y[0]] # yëŠ” ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œ ì¶”ì¶œ print(f\"Features: {features}\") print(f\"Target: {y[0]}\") print(f\"ë°ì´í„° í¬ê¸°: X={X_features.shape}, y={y_outcome.shape}\") print(f\"Target í†µê³„:\") print(f\" í‰ê· : {y_outcome.mean():.2f}\") print(f\" í‘œì¤€í¸ì°¨: {y_outcome.std():.2f}\") print(f\" ë²”ìœ„: [{y_outcome.min():.2f}, {y_outcome.max():.2f}]\") Out-of-sample ì˜ˆì¸¡ê°’ ì €ì¥ìš© ë°°ì—´ outcome_predictions = np.zeros(len(df)) KFoldë¡œ ë¶„í•  (íšŒê·€ ë¬¸ì œì´ë¯€ë¡œ StratifiedKFold ëŒ€ì‹  KFold ì‚¬ìš©) kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED) fold_performances = [] LightGBM íŒŒë¼ë¯¸í„° ì„¤ì • (ê³¼ì í•© ë°©ì§€ì— ì¤‘ì ) lgb_params = { \"objective\": \"regression\", \"metric\": \"rmse\", \"boosting_type\": \"gbdt\", \"num_leaves\": 15, # ì‘ê²Œ ì„¤ì •í•˜ì—¬ ê³¼ì í•© ë°©ì§€ \"learning_rate\": 0.1, \"feature_fraction\": 0.8, \"bagging_fraction\": 0.8, \"bagging_freq\": 5, \"lambda_l1\": 0.1, # L1 ì •ê·œí™” \"lambda_l2\": 0.1, # L2 ì •ê·œí™” \"min_data_in_leaf\": 20, \"min_split_gain\": 0.1, \"max_depth\": 6, \"verbose\": -1, \"random_state\": RANDOM_SEED, } for fold, (train_idx, val_idx) in enumerate(kf.split(X_features, y_outcome)): print(f\"\\n=== Fold {fold + 1} ===\") X_train, X_val = X_features.iloc[train_idx], X_features.iloc[val_idx] y_train, y_val = y_outcome.iloc[train_idx], y_outcome.iloc[val_idx] print(f\"Train í¬ê¸°: {len(X_train)}, Val í¬ê¸°: {len(X_val)}\") print(f\"Train target í‰ê· : {y_train.mean():.2f}, Val target í‰ê· : {y_val.mean():.2f}\") # LightGBM ë°ì´í„°ì…‹ ìƒì„± train_data = lgb.Dataset(X_train, label=y_train) val_data = lgb.Dataset(X_val, label=y_val, reference=train_data) # ëª¨ë¸ í›ˆë ¨ model = lgb.train( lgb_params, train_data, valid_sets=[train_data, val_data], valid_names=[\"train\", \"eval\"], num_boost_round=500, callbacks=[ lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=0), # ë¡œê·¸ ì¶œë ¥ ë¹„í™œì„±í™” ], ) # ì˜ˆì¸¡ train_pred = model.predict(X_train, num_iteration=model.best_iteration) val_pred = model.predict(X_val, num_iteration=model.best_iteration) # Out-of-sample ì˜ˆì¸¡ê°’ ì €ì¥ outcome_predictions[val_idx] = val_pred # ì„±ëŠ¥ ê³„ì‚° train_rmse = np.sqrt(mean_squared_error(y_train, train_pred)) val_rmse = np.sqrt(mean_squared_error(y_val, val_pred)) train_r2 = r2_score(y_train, train_pred) val_r2 = r2_score(y_val, val_pred) train_mae = mean_absolute_error(y_train, train_pred) val_mae = mean_absolute_error(y_val, val_pred) fold_performances.append( { \"fold\": fold + 1, \"train_rmse\": train_rmse, \"val_rmse\": val_rmse, \"train_r2\": train_r2, \"val_r2\": val_r2, \"train_mae\": train_mae, \"val_mae\": val_mae, \"best_iteration\": model.best_iteration, \"train_size\": len(X_train), \"val_size\": len(X_val), } ) print(f\"í›ˆë ¨ RMSE: {train_rmse:.3f}, ê²€ì¦ RMSE: {val_rmse:.3f}\") print(f\"í›ˆë ¨ RÂ²: {train_r2:.3f}, ê²€ì¦ RÂ²: {val_r2:.3f}\") print(f\"í›ˆë ¨ MAE: {train_mae:.3f}, ê²€ì¦ MAE: {val_mae:.3f}\") print(f\"ìµœì  ë°˜ë³µíšŸìˆ˜: {model.best_iteration}\") ì›ë³¸ dfì— outcome ì˜ˆì¸¡ê°’ ì¶”ê°€ df[\"outcome_prediction\"] = outcome_predictions print(f\"\\n=== ì „ì²´ Outcome Function ê²°ê³¼ ===\") performance_df = pd.DataFrame(fold_performances) print(f\"í‰ê·  í›ˆë ¨ RMSE: {performance_df['train_rmse'].mean():.3f} Â± {performance_df['train_rmse'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ RMSE: {performance_df['val_rmse'].mean():.3f} Â± {performance_df['val_rmse'].std():.3f}\") print(f\"í‰ê·  í›ˆë ¨ RÂ²: {performance_df['train_r2'].mean():.3f} Â± {performance_df['train_r2'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ RÂ²: {performance_df['val_r2'].mean():.3f} Â± {performance_df['val_r2'].std():.3f}\") print(f\"í‰ê·  í›ˆë ¨ MAE: {performance_df['train_mae'].mean():.3f} Â± {performance_df['train_mae'].std():.3f}\") print(f\"í‰ê·  ê²€ì¦ MAE: {performance_df['val_mae'].mean():.3f} Â± {performance_df['val_mae'].std():.3f}\") ê³¼ì í•© ì²´í¬ overfitting_score = performance_df[\"train_r2\"].mean() - performance_df[\"val_r2\"].mean() print(f\"\\nê³¼ì í•© ì§€í‘œ (Train RÂ² - Val RÂ²): {overfitting_score:.3f}\") if overfitting_score > 0.1: print(\"âš ï¸ ì‹¬ê°í•œ ê³¼ì í•©ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤. ì •ê·œí™”ë¥¼ ê°•í™”í•˜ì„¸ìš”.\") elif overfitting_score > 0.05: print(\"âš ï¸ ì•½ê°„ì˜ ê³¼ì í•©ì´ ìˆìŠµë‹ˆë‹¤.\") else: print(\"âœ… ê³¼ì í•©ì´ ì˜ ì œì–´ë˜ê³  ìˆìŠµë‹ˆë‹¤.\") print(f\"\\nì²˜ìŒ 10ê°œ ìƒ˜í”Œì˜ ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’:\") comparison_df = df[[y[0], \"outcome_prediction\"]].head(10) comparison_df[\"residual\"] = comparison_df[y[0]] - comparison_df[\"outcome_prediction\"] print(comparison_df) print(f\"\\nOutcome ì˜ˆì¸¡ê°’ í†µê³„:\") print(f\" í‰ê· : {df['outcome_prediction'].mean():.2f}\") print(f\" í‘œì¤€í¸ì°¨: {df['outcome_prediction'].std():.2f}\") print(f\" ë²”ìœ„: [{df['outcome_prediction'].min():.2f}, {df['outcome_prediction'].max():.2f}]\") ì”ì°¨ ê³„ì‚° (R-learnerì˜ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ìœ„í•´) df[\"outcome_residual\"] = df[y[0]] - df[\"outcome_prediction\"] print(f\"\\nOutcome ì”ì°¨ í†µê³„:\") print(f\" í‰ê· : {df['outcome_residual'].mean():.2f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") print(f\" í‘œì¤€í¸ì°¨: {df['outcome_residual'].std():.2f}\") ì‹œê°í™” plt.figure(figsize=(12, 4)) ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’ plt.subplot(1, 3, 1) plt.scatter(df[y[0]], df[\"outcome_prediction\"], alpha=0.6, s=10) plt.plot([df[y[0]].min(), df[y[0]].max()], [df[y[0]].min(), df[y[0]].max()], \"r--\", lw=2) plt.xlabel(\"ì‹¤ì œê°’\") plt.ylabel(\"ì˜ˆì¸¡ê°’\") plt.title(\"ì‹¤ì œê°’ vs ì˜ˆì¸¡ê°’\") ì”ì°¨ ë¶„í¬ plt.subplot(1, 3, 2) plt.hist(df[\"outcome_residual\"], bins=30, alpha=0.7, edgecolor=\"black\") plt.xlabel(\"ì”ì°¨\") plt.ylabel(\"ë¹ˆë„\") plt.title(\"ì”ì°¨ ë¶„í¬\") plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) Foldë³„ ì„±ëŠ¥ plt.subplot(1, 3, 3) plt.plot(performance_df[\"fold\"], performance_df[\"train_r2\"], \"o-\", label=\"Train RÂ²\", alpha=0.7) plt.plot(performance_df[\"fold\"], performance_df[\"val_r2\"], \"s-\", label=\"Val RÂ²\", alpha=0.7) plt.xlabel(\"Fold\") plt.ylabel(\"RÂ² Score\") plt.title(\"Foldë³„ ì„±ëŠ¥\") plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() ì„±ëŠ¥ ìš”ì•½ í…Œì´ë¸” print(f\"\\n=== Foldë³„ ì„±ëŠ¥ ìš”ì•½ ===\") print(performance_df[[\"fold\", \"train_rmse\", \"val_rmse\", \"train_r2\", \"val_r2\", \"best_iteration\"]].round(3)) df.iloc[[4799, 4800]] import matplotlib.pyplot as plt import numpy as np í•œê¸€ í°íŠ¸ ì„¤ì • plt.rcParams[\"font.family\"] = \"AppleGothic\" plt.rcParams[\"axes.unicode_minus\"] = False Sizeë³„ ì”ì°¨ ë¶„í¬ ì‹œê°í™” fig, axes = plt.subplots(2, 3, figsize=(18, 10)) Sizeë³„ íˆìŠ¤í† ê·¸ë¨ (ê²¹ì¹˜ê²Œ) plt.subplot(2, 3, 1) for size in sorted(df[\"size\"].unique()): residuals = df[df[\"size\"] == size][\"outcome_residual\"] plt.hist(residuals, bins=30, alpha=0.7, label=f\"Size {size}\", density=True) plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.xlabel(\"Outcome ì”ì°¨\") plt.ylabel(\"ë°€ë„\") plt.title(\"Sizeë³„ ì”ì°¨ ë¶„í¬ (íˆìŠ¤í† ê·¸ë¨)\") plt.legend() plt.grid(True, alpha=0.3) Sizeë³„ ë°•ìŠ¤í”Œë¡¯ plt.subplot(2, 3, 2) size_groups = [df[df[\"size\"] == size][\"outcome_residual\"] for size in sorted(df[\"size\"].unique())] size_labels = [f\"Size {size}\" for size in sorted(df[\"size\"].unique())] plt.boxplot(size_groups, labels=size_labels) plt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.ylabel(\"Outcome ì”ì°¨\") plt.title(\"Sizeë³„ ì”ì°¨ ë¶„í¬ (ë°•ìŠ¤í”Œë¡¯)\") plt.grid(True, alpha=0.3) ë°”ì´ì˜¬ë¦° í”Œë¡¯ plt.subplot(2, 3, 3) parts = plt.violinplot(size_groups, positions=range(1, len(size_groups) + 1)) plt.axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.xticks(range(1, len(size_labels) + 1), size_labels) plt.ylabel(\"Outcome ì”ì°¨\") plt.title(\"Sizeë³„ ì”ì°¨ ë¶„í¬ (ë°”ì´ì˜¬ë¦° í”Œë¡¯)\") plt.grid(True, alpha=0.3) 4-6. Sizeë³„ ê°œë³„ íˆìŠ¤í† ê·¸ë¨ for i, size in enumerate(sorted(df[\"size\"].unique())): plt.subplot(2, 3, 4 + i) residuals = df[df[\"size\"] == size][\"outcome_residual\"] plt.hist(residuals, bins=30, alpha=0.7, edgecolor=\"black\") plt.axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) plt.axvline(residuals.mean(), color=\"blue\", linestyle=\"-\", alpha=0.7, label=f\"í‰ê· : {residuals.mean():.1f}\") plt.xlabel(\"Outcome ì”ì°¨\") plt.ylabel(\"ë¹ˆë„\") plt.title(f\"Size {size} ì”ì°¨ ë¶„í¬ (n={len(residuals)})\") plt.legend() plt.grid(True, alpha=0.3) plt.tight_layout() plt.show() Sizeë³„ í†µê³„ ìš”ì•½ print(\"=== Sizeë³„ ì”ì°¨ í†µê³„ ===\") for size in sorted(df[\"size\"].unique()): residuals = df[df[\"size\"] == size][\"outcome_residual\"] print(f\"\\nSize {size} (n={len(residuals)}):\") print(f\" í‰ê· : {residuals.mean():8.2f}\") print(f\" í‘œì¤€í¸ì°¨: {residuals.std():8.2f}\") print(f\" ì¤‘ì•™ê°’: {residuals.median():8.2f}\") print(f\" ë²”ìœ„: [{residuals.min():6.1f}, {residuals.max():6.1f}]\") ì „ì²´ ì”ì°¨ í†µê³„ print(f\"\\nì „ì²´ (n={len(df)}):\") print(f\" í‰ê· : {df['outcome_residual'].mean():8.2f}\") print(f\" í‘œì¤€í¸ì°¨: {df['outcome_residual'].std():8.2f}\") ANOVA ê²€ì • (sizeë³„ ì”ì°¨ í‰ê·  ì°¨ì´ ê²€ì •) from scipy.stats import f_oneway size_12_1 = df[df[\"size\"] == 12.1][\"outcome_residual\"] size_12_6 = df[df[\"size\"] == 12.6][\"outcome_residual\"] size_13_2 = df[df[\"size\"] == 13.2][\"outcome_residual\"] f_stat, p_value = f_oneway(size_12_1, size_12_6, size_13_2) print(f\"\\n=== ANOVA ê²€ì • (sizeë³„ ì”ì°¨ í‰ê·  ì°¨ì´) ===\") print(f\"F-í†µê³„ëŸ‰: {f_stat:.4f}\") print(f\"p-value: {p_value:.6f}\") if p_value < 0.05: print(\"âŒ Sizeë³„ë¡œ ì”ì°¨ í‰ê· ì— ìœ ì˜í•œ ì°¨ì´ê°€ ìˆìŒ (ë¬¸ì œ)\") else: print(\"âœ… Sizeë³„ ì”ì°¨ í‰ê· ì— ìœ ì˜í•œ ì°¨ì´ ì—†ìŒ (ì¢‹ìŒ)\") > ì”ì°¨í™”ê°€ ì˜ ëëŠ”ì§€ëŠ” ì–´ë–»ê²Œ íŒë‹¨í•  ìˆ˜ ìˆëŠ”ê±°ì§€? ì£¼ìš” í‰ê°€ ê¸°ì¤€: ì¡°ê±´ë¶€ ë…ë¦½ì„± (ê°€ì¥ ì¤‘ìš”) E[ì”ì°¨ X] â‰ˆ 0 RÂ² < 0.01 ì´ë©´ ìš°ìˆ˜ RÂ² > 0.05 ì´ë©´ ë¬¸ì œ í¸í–¥ ì œê±° ì”ì°¨ í‰ê·  â‰ˆ 0 ì²´ê³„ì ì¸ ê³¼ëŒ€/ê³¼ì†Œì˜ˆì¸¡ ì—†ìŒ ì •ê·œì„± ì”ì°¨ê°€ ì •ê·œë¶„í¬ í˜•íƒœ ì™œë„, ì²¨ë„ê°€ 0ì— ê°€ê¹Œì›€ ì”ì°¨ ê°„ ë…ë¦½ì„± Treatment ì”ì°¨ì™€ Outcome ì”ì°¨ ê°„ ë‚®ì€ ìƒê´€ê´€ê³„ ìƒê´€ê³„ìˆ˜ < 0.1 ì´ë©´ ì´ìƒì  import numpy as np import pandas as pd import matplotlib.pyplot as plt import matplotlib.font_manager as fm from sklearn.linear_model import LinearRegression from sklearn.metrics import r2_score from scipy import stats í•œê¸€ í°íŠ¸ ì„¤ì • plt.rcParams[\"font.family\"] = \"AppleGothic\" plt.rcParams[\"axes.unicode_minus\"] = False def check_conditional_independence_comprehensive(df): \"\"\" Outcome ì”ì°¨ì˜ ì¡°ê±´ë¶€ ë…ë¦½ì„±ì„ ì¢…í•©ì ìœ¼ë¡œ í™•ì¸ \"\"\" features = [\"Cas_ata\", \"Cas_acd_endo\", \"Cas_clr\", \"age\"] residual_col = \"outcome_residual\" print(\"=\" 60) print(\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ê²€ì‚¬: E[outcome_residual X] = 0\") print(\"=\" 60) X = df[features] residuals = df[residual_col] # ===== 1. ì „ì²´ RÂ² í™•ì¸ ===== print(\"\\n1. ì „ì²´ ì¡°ê±´ë¶€ ë…ë¦½ì„± ê²€ì‚¬\") print(\"-\" 40) model = LinearRegression() model.fit(X, residuals) predictions = model.predict(X) r2_total = r2_score(residuals, predictions) print(f\"Featuresë¡œ ì”ì°¨ ì˜ˆì¸¡ RÂ²: {r2_total:.6f}\") # RÂ² í‰ê°€ if r2_total < 0.001: print(\"ğŸ‰ ìš°ìˆ˜: ë§¤ìš° ê°•í•œ ì¡°ê±´ë¶€ ë…ë¦½ì„±\") grade = \"A+\" elif r2_total < 0.01: print(\"âœ… ìš°ìˆ˜: ê°•í•œ ì¡°ê±´ë¶€ ë…ë¦½ì„±\") grade = \"A\" elif r2_total < 0.05: print(\"âš ï¸ ì–‘í˜¸: ì•½ê°„ì˜ ì˜ì¡´ì„± ì¡´ì¬\") grade = \"B\" elif r2_total < 0.1: print(\"âš ï¸ ë³´í†µ: ì¤‘ê°„ ì •ë„ì˜ ì˜ì¡´ì„±\") grade = \"C\" else: print(\"âŒ ë¬¸ì œ: ê°•í•œ ì˜ì¡´ì„± - ëª¨ë¸ ê°œì„  í•„ìš”\") grade = \"D\" # ===== 2. ê° featureë³„ ìƒê´€ê´€ê³„ ===== print(f\"\\n2. ê° Featureë³„ ìƒê´€ê´€ê³„\") print(\"-\" 40) correlations = {} for feature in features: corr = np.corrcoef(df[feature], residuals)[0, 1] correlations[feature] = corr if abs(corr) < 0.05: status = \"âœ… ë§¤ìš° ë‚®ìŒ\" elif abs(corr) < 0.1: status = \"âš ï¸ ë‚®ìŒ\" elif abs(corr) < 0.2: status = \"âš ï¸ ì¤‘ê°„\" else: status = \"âŒ ë†’ìŒ\" print(f\"{feature:15}: {corr:8.4f} {status}\") # ===== 3. ì”ì°¨ì˜ ê¸°ë³¸ í†µê³„ ===== print(f\"\\n3. ì”ì°¨ ê¸°ë³¸ í†µê³„\") print(\"-\" 40) mean_residual = residuals.mean() std_residual = residuals.std() skew_residual = stats.skew(residuals) kurt_residual = stats.kurtosis(residuals) print(f\"í‰ê· : {mean_residual:8.4f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") print(f\"í‘œì¤€í¸ì°¨: {std_residual:8.4f}\") print(f\"ì™œë„: {skew_residual:8.4f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") print(f\"ì²¨ë„: {kurt_residual:8.4f} (0ì— ê°€ê¹Œì›Œì•¼ í•¨)\") # ===== 4. í†µê³„ì  ê²€ì • ===== print(f\"\\n4. í†µê³„ì  ê²€ì •\") print(\"-\" 40) # F-test for overall significance n = len(residuals) k = len(features) f_stat = (r2_total / k) / ((1 - r2_total) / (n - k - 1)) f_p_value = 1 - stats.f.cdf(f_stat, k, n - k - 1) print(f\"F-ê²€ì • (ì „ì²´ ëª¨ë¸ ìœ ì˜ì„±):\") print(f\" F-í†µê³„ëŸ‰: {f_stat:.4f}\") print(f\" p-value: {f_p_value:.6f}\") if f_p_value > 0.05: print(\" âœ… ì”ì°¨ì™€ features ê°„ ìœ ì˜í•œ ê´€ê³„ ì—†ìŒ (ì¢‹ìŒ)\") else: print(\" âŒ ì”ì°¨ì™€ features ê°„ ìœ ì˜í•œ ê´€ê³„ ì¡´ì¬ (ë¬¸ì œ)\") # Shapiro-Wilk test for normality (ìƒ˜í”Œì´ ë„ˆë¬´ í¬ë©´ ìƒëµ) if len(residuals) <= 5000: shapiro_stat, shapiro_p = stats.shapiro(residuals) print(f\"\\nShapiro-Wilk ì •ê·œì„± ê²€ì •:\") print(f\" í†µê³„ëŸ‰: {shapiro_stat:.4f}\") print(f\" p-value: {shapiro_p:.6f}\") if shapiro_p > 0.05: print(\" âœ… ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¦„\") else: print(\" âš ï¸ ì”ì°¨ê°€ ì •ê·œë¶„í¬ë¥¼ ë²—ì–´ë‚¨\") # ===== 5. ì‹œê°í™” ===== print(f\"\\n5. ì‹œê°ì  ì§„ë‹¨\") print(\"-\" 40) fig, axes = plt.subplots(2, 3, figsize=(18, 12)) fig.suptitle(\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ì§„ë‹¨\", fontsize=16) # 5-1. ê° feature vs ì”ì°¨ for i, feature in enumerate(features): row = i // 2 col = i % 2 axes[row, col].scatter(df[feature], residuals, alpha=0.6, s=10) axes[row, col].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) # ì¶”ì„¸ì„  z = np.polyfit(df[feature], residuals, 1) p = np.poly1d(z) x_line = np.linspace(df[feature].min(), df[feature].max(), 100) axes[row, col].plot(x_line, p(x_line), \"r-\", alpha=0.8, linewidth=2) axes[row, col].set_xlabel(feature) axes[row, col].set_ylabel(\"Outcome ì”ì°¨\") axes[row, col].set_title(f\"{feature} vs ì”ì°¨ (ìƒê´€ê³„ìˆ˜: {correlations[feature]:.3f})\") axes[row, col].grid(True, alpha=0.3) # 5-2. ì˜ˆì¸¡ê°’ vs ì”ì°¨ axes[1, 0].scatter(df[\"outcome_prediction\"], residuals, alpha=0.6, s=10) axes[1, 0].axhline(0, color=\"red\", linestyle=\"--\", alpha=0.7) z = np.polyfit(df[\"outcome_prediction\"], residuals, 1) p = np.poly1d(z) x_line = np.linspace(df[\"outcome_prediction\"].min(), df[\"outcome_prediction\"].max(), 100) axes[1, 0].plot(x_line, p(x_line), \"r-\", alpha=0.8, linewidth=2) axes[1, 0].set_xlabel(\"ì˜ˆì¸¡ê°’\") axes[1, 0].set_ylabel(\"ì”ì°¨\") axes[1, 0].set_title(\"ì˜ˆì¸¡ê°’ vs ì”ì°¨ (ë“±ë¶„ì‚°ì„± ê²€ì‚¬)\") axes[1, 0].grid(True, alpha=0.3) # 5-3. ì”ì°¨ ë¶„í¬ axes[1, 1].hist(residuals, bins=50, density=True, alpha=0.7, edgecolor=\"black\") # ì •ê·œë¶„í¬ ê³¡ì„  ì˜¤ë²„ë ˆì´ mu, sigma = residuals.mean(), residuals.std() x_norm = np.linspace(residuals.min(), residuals.max(), 100) axes[1, 1].plot(x_norm, stats.norm.pdf(x_norm, mu, sigma), \"r-\", lw=2, label=\"ì •ê·œë¶„í¬\") axes[1, 1].axvline(0, color=\"red\", linestyle=\"--\", alpha=0.7) axes[1, 1].set_xlabel(\"ì”ì°¨\") axes[1, 1].set_ylabel(\"ë°€ë„\") axes[1, 1].set_title(\"ì”ì°¨ ë¶„í¬\") axes[1, 1].legend() axes[1, 1].grid(True, alpha=0.3) plt.tight_layout() plt.show() # ===== 6. Q-Q Plot ===== plt.figure(figsize=(8, 6)) stats.probplot(residuals, dist=\"norm\", plot=plt) plt.title(\"Q-Q Plot: ì”ì°¨ ì •ê·œì„± ê²€ì •\") plt.grid(True, alpha=0.3) plt.show() # ===== 7. ì¢…í•© í‰ê°€ ===== print(f\"\\n\" + \"=\" 60) print(\"ì¢…í•© í‰ê°€\") print(\"=\" 60) issues = [] if r2_total > 0.05: issues.append(\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ìœ„ë°˜\") if abs(mean_residual) > 0.1: issues.append(\"í¸í–¥ ì¡´ì¬\") if any(abs(corr) > 0.2 for corr in correlations.values()): issues.append(\"ê°•í•œ feature ì˜ì¡´ì„±\") if f_p_value < 0.05: issues.append(\"í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ì˜ì¡´ì„±\") print(f\"ì¡°ê±´ë¶€ ë…ë¦½ì„± ë“±ê¸‰: {grade}\") print(f\"ì „ì²´ RÂ²: {r2_total:.6f}\") print(f\"ë°œê²¬ëœ ë¬¸ì œ: {len(issues)}ê°œ\") if issues: print(\"\\në¬¸ì œì :\") for issue in issues: print(f\" âŒ {issue}\") print(f\"\\nê°œì„  ë°©ì•ˆ:\") print(f\" ğŸ”§ ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ (ë” ë§ì€ features, interaction terms)\") print(f\" ğŸ”§ ë¹„ì„ í˜• ë³€í™˜ ì ìš©\") print(f\" ğŸ”§ ì •ê·œí™” íŒŒë¼ë¯¸í„° ì¡°ì •\") print(f\" ğŸ”§ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ ì‹œë„ (XGBoost, Neural Network)\") else: print(\"\\nğŸ‰ ì¡°ê±´ë¶€ ë…ë¦½ì„±ì´ ì˜ ë§Œì¡±ë©ë‹ˆë‹¤!\") print(\"âœ… R-learnerì˜ ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰ ê°€ëŠ¥\") return { \"r2_total\": r2_total, \"correlations\": correlations, \"mean_residual\": mean_residual, \"f_p_value\": f_p_value, \"grade\": grade, \"issues\": issues, } ì‹¤í–‰ results = check_conditional_independence_comprehensive(df) # 3. CATE ì¶”ì • ëª¨ë¸ > CATE(X, T) class MultiTreatmentRLearner: def __init__(self, model_y, model_t, model_final): self.model_y = model_y # Y ëª¨ë¸ self.model_t = model_t # Treatment propensity ëª¨ë¸ self.model_final = model_final # Final stage ëª¨ë¸ def fit(self, Y, T, X, W=None): # Step 1: Estimate E[Y X,W] if W is not None: XW = np.concatenate([X, W], axis=1) else: XW = X self.model_y.fit(XW, Y) Y_pred = self.model_y.predict(XW) Y_res = Y - Y_pred # Y residuals # Step 2: Multi-treatment propensity estimation # One-hot encode treatments T_onehot = pd.get_dummies(T, drop_first=True) T_pred = np.zeros_like(T_onehot) for k, col in enumerate(T_onehot.columns): # Each treatment vs rest self.model_t.fit(XW, T_onehot[col]) T_pred[:, k] = self.model_t.predict_proba(XW)[:, 1] T_res = T_onehot - T_pred # Treatment residuals # Step 3: Final stage - regress Y_res on T_res X # Create interaction terms features = [] for k in range(T_res.shape[1]): # Î¸_k(X) = X Î±_k (linear case) interaction = T_res[:, k : k + 1] X features.append(interaction) final_features = np.concatenate(features, axis=1) self.model_final.fit(final_features, Y_res) def predict(self, X, T0, T1): # Predict treatment effects between T0 and T1 # Î¸(X) (T1 - T0) pass from econml.dml import DML from econml.metalearners import XLearner, TLearner EconMLì˜ meta-learnersëŠ” multi-treatment ì§€ì› xl = XLearner(models=RandomForestRegressor()) xl.fit(Y, T, X=X) # T can be multi-valued from causalml.inference.meta import XLearner Multi-treatment support"
  },
  {
    "id": "Career/site-recommendation",
    "title": "ì·¨ì—… ì¤€ë¹„ ì‚¬ì´íŠ¸ ì¶”ì²œ",
    "date": "2025-07-30",
    "excerpt": "ì·¨ì—… ì¤€ë¹„ ì‚¬ì´íŠ¸ ì¶”ì²œ",
    "category": "Career",
    "tags": [
      "ì·¨ì—…"
    ],
    "content": "ì·¨ì—… ì¤€ë¹„ ì‚¬ì´íŠ¸ ì¶”ì²œ zighang offercent bzpp ë§í¬ë“œì¸ ì±„ìš©ê³µê³ ë‚˜ ì±„ìš©í•˜ëŠ” í”¼ë“œ ì°¸ê³ í•˜ê¸°"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-29-ë„ˆì•„ë‹ˆì—ˆìœ¼ë©´",
    "title": "ë„ˆ ì•„ë‹ˆì—ˆìœ¼ë©´",
    "date": "2025-07-29",
    "excerpt": "ë„ˆ ì•„ë‹ˆì—ˆìœ¼ë©´, ë„ˆë¼ë©´ ì–´ë–»ê²Œ í–ˆê² ì–´?",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ê±° ì •ë¦¬ match maker ë¬¸ë²•. í˜„ì¬ì™€ ë‹¤ë¥¸ ì–´ë–¤ ìƒí™©ì„ ê°€ì •í•  ë•ŒëŠ” ê³¼ê±°ì™„ë£Œë¥¼ ì‚¬ìš©. If it hadnt been for you, I wouldn't have made it. would've -> /wÊŠdÉ™v/ (woul-duv) wouldn't -> /wÊŠdnt/ (woul-dnt) wou-ov ë°œìŒ ê¸°í˜¸ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤! ğŸ“ ë°œìŒ ê°€ì´ë“œ would've: /wÊŠdÉ™v/ (woul-duv) wÊŠ = \"woul\" (woolê³¼ ë¹„ìŠ·) dÉ™v = \"duv\" (duh + v ì†Œë¦¬) wouldn't: /wÊŠdnt/ (woul-dnt) wÊŠ = \"woul\" (woolê³¼ ë¹„ìŠ·) dnt` = \"dnt\" (d + n + të¥¼ ë¹ ë¥´ê²Œ ì—°ê²°) ì‹¤ì œë¡œëŠ” \"would have\"ì™€ \"would not\"ì´ ì¶•ì•½ë˜ì–´ ë” ì§§ê³  ë¹ ë¥´ê²Œ ë°œìŒë˜ëŠ” í˜•íƒœì…ë‹ˆë‹¤. Would you have moved to New York?"
  },
  {
    "id": "Career/how-to-write-resume",
    "title": "ì´ë ¥ì„œ, ê²½ë ¥ê¸°ìˆ ì„œ ì‘ì„± ë°©ë²• ì •ë¦¬",
    "date": "2025-07-29",
    "excerpt": "ì´ë ¥ì„œ, ê²½ë ¥ê¸°ìˆ ì„œ ì ëŠ” ë°©ë²• ì •ë¦¬",
    "category": "Career",
    "tags": [
      "ì´ë ¥ì„œ"
    ],
    "content": "ì°¸ê³ ë§í¬: ìœ íŠœë¸Œ ì´í˜• - ê²½ë ¥ê¸°ìˆ ì„œ ì‘ì„±ë²• ì´ì •ë¦¬ ì´ì§ì´ ì²˜ìŒì´ë¼ë©´ ì´ê²ƒë¶€í„° ë³´ì„¸ìš” ì´ì§ë°”ì´ë¸” 137p ~ 187p ê²½ë ¥ê¸°ìˆ ì„œ ì‘ì„±ë°©ë²• : ìµœê·¼ê²½ë ¥ - ì§€ì›ì§ë¬´ì— ë§ê²Œ ë¨¼ì € ì œì¶œ ì§ë¬´ì™€ ë¬´ê´€í•œ ê²½í—˜ì— ëŒ€í•´ì„œëŠ” ê³¼ê°í•˜ê²Œ ì‚­ì œ ì—°ì°¨ë³„ 1ì¹¸ì”©ì€ í•„ìˆ˜ ê°€ê¸‰ì  10ê°œ ë¯¸ë§Œ ê²½ë ¥ê¸°ìˆ ì„œì˜ ë‚´ìš© : êµ¬ë¶„ ì‘ì„± ë°©ë²• --- --- ê¸°ê°„ í”„ë¡œì íŠ¸ ì¤‘ì‹¬ìœ¼ë¡œ ì„¤ëª… ì„±ê³¼ (1ì¤„ ì´ë‚´) How + Result -> KPI ì—­í•  (5ì¤„ ì´ë‚´) 1. ë¬¸ì œ í•´ê²° ê´€ì : í•µì‹¬ ë¬¸ì œ í•´ê²° ê³¼ì •ì—ì„œì˜ ì—­í• <br>2. ìƒì‚°ì„± í–¥ìƒ ê´€ì : ì—…ë¬´ íš¨ìœ¨ì„± ê°œì„ ì—ì„œì˜ ì—­í• <br>3. ì˜ì‚¬ì†Œí†µ ê´€ì : íŒ€ í˜‘ì—… ë° ì»¤ë®¤ë‹ˆì¼€ì´ì…˜ì—ì„œì˜ ì—­í•  ê¸°ìˆ  1. ë„êµ¬/íˆ´: ì‚¬ìš©í•œ ê¸°ìˆ  ìŠ¤íƒ<br>2. í”„ë¡œì„¸ìŠ¤ ì´í•´: ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì´í•´ë„ ë¬¼ê²½ë ¥ ì§„ë‹¨ ë¦¬ìŠ¤íŠ¸: ìˆ«ìë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” ì„±ê³¼ê°€ ì—†ë‹¤. ë‚´ ì§ë¬´ KPIê°€ ë¬´ì—‡ì¸ì§€ ëª¨ë¥¸ë‹¤ ì§„í–‰í•œ í”„ë¡œì íŠ¸/ì—…ë¬´ì˜ Before/Afterê°€ ëª…í™•í•˜ì§€ ì•Šë‹¤. ì„±ê³¼ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ êµ¬ì²´ì ìœ¼ë¡œ ì–´ë–¤ ì—­í• ì„ ìˆ˜í–‰í–ˆëŠ”ì§€ ì„¤ëª…í•  ìˆ˜ ì—†ë‹¤. ì—°ì† ê·¼ë¬´ ê¸°ê°„ì´ 1ë…„ ë¯¸ë§Œì´ë‹¤. ìµœê·¼ 3ë…„ ë‚´ 2ê³³ ì´ìƒìœ¼ë¡œ ì´ì§í–ˆë‹¤. ë™ê¸°ë‚˜ ë™ì¼ ì—°ì°¨ì— ë¹„í•´ ìŠ¹ì§„ì´ ëŠ¦ë‹¤. ê²½ë ¥ ê¸°ìˆ ì„œ ì‘ì„± í”„ë¡œì„¸ìŠ¤ : ê²½ë ¥ ê¸°ìˆ ì„œë¥¼ ìœ„í•œ ì†Œì¬ ì¤€ë¹„í•˜ê¸° : ê²½í—˜ ë¦¬ìŠ¤íŠ¸ì—… ê²½í—˜ ë¦¬ìŠ¤íŠ¸ì—… í…œí”Œë¦¿ì„ í™œìš©í•˜ì—¬ ì£¼ìš” í™œë™ ì •ë¦¬í•˜ê¸° KPI ë§¤ì¹­í•˜ê¸° ê²½ë ¥ ê¸°ìˆ ì„œ ì‘ì„±í•˜ê¸° í”„ë¡œì íŠ¸ëª… ê¸°ê°„ ì„±ê³¼ ì—­í•  ê¸°ìˆ  > ê°„í˜¹ ê²½ë ¥ ê¸°ìˆ ì„œë¥¼ êµ¬ì²´ì ì´ê³  ê¸¸ê²Œ ì„œìˆ í•˜ëŠ” ê²½ìš°ê°€ ìˆëŠ”ë°, ê²½ë ¥ ê¸°ìˆ ì„œì˜ ëª©ì  ìì²´ê°€ í•µì‹¬ì„ ê°„ê²°í•˜ê²Œ ë³´ê¸° ìœ„í•¨ì´ê¸° ë•Œë¬¸ì— > ìš°ë¦¬ëŠ” ë©´ì ‘ê´€ì´ ê¶ê¸ˆí•´ì•¼í•  ë‚´ìš©ë§Œ ì‘ì„±í•˜ì. ì˜ˆì‹œ ì§ë¬´ : MD í”„ë¡œì íŠ¸ëª…/ì—…ë¬´ëª… : ëª…ì ˆ ì‹œì¦Œ ë¼ì´ë¸Œ íŒë§¤ ë°©ì†¡ìœ¼ë¡œ ë§¤ì¶œ 154% ê²¬ì¸ ê¸°ê°„ : 2010ë…„ 9ì›” ~ 2020ë…„ 10ì›” (2ê°œì›”) ì„±ê³¼ : 5000ëª…ì˜ ê´€ì‹¬ê³ ê° CRM ë¶„ì„ì„ í†µí•œ ëª…ì ˆ íŒë§¤ ë°©ì†¡ìœ¼ë¡œ ë¼ì´ë¸Œ ë§¤ì¶œ 1ì–µ ë‹¬ì„± ì—­í•  : ê²½ìŸì‚¬ 100ê°œ ìƒí’ˆì¡°ì‚´ë¥´ ë°”íƒ•ìœ¼ë¡œ 3ë§Œì› ëŒ€ ì„¸íŠ¸ ìƒí’ˆì„ ì œì•ˆí•´ ê°ë‹¨ê°€ 50% ì¦ê°€ 5000ëª… ê´€ì‹¬ê³ ê° ëŒ€ìƒìœ¼ë¡œ CRM ë§ˆì¼€íŒ… ì§„í–‰ìœ¼ë¡œ ìœ ì… 300% ì¦ê°€ 100ê°œ ë²¤ë”ì‚¬ ë¶„ì„ì„ í†µí•´ ë²¤ë”ì‚¬ ì„ ì •ìœ¼ë¡œ ê¸°íš ë‹¨ê°€ 30% ì ˆê° ê¸°ìˆ  : Jira, Data Studio, Slack ë¼ì´ë¸Œ ì»¤ë¨¸ìŠ¤ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì´í•´ ì§ë¬´: í’ˆì§ˆê´€ë¦¬ í”„ë¡œì íŠ¸ëª…/ì—…ë¬´ëª… : ëŒ€ë§Œ ê³µì¥ ìƒì‚° ê°€ë™ ê¸°ê°„ : 2019.10 ~ 2020.10 (1ë…„) ì„±ê³¼ : ì‹ ê·œ í’ˆì§ˆê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶•ì„ í†µí•´ ëŒ€ë§Œ ê³µì¥ ìƒì‚° ì œí’ˆ ë¶ˆëŸ‰ë¥  80% ê°ì†Œ ì—­í•  : ë¶ˆëŸ‰í’ˆ 100% ì „ìˆ˜ê²€ì‚¬ë¥¼ í†µí•´ ì™„ì œí’ˆ ê²°í•¨ ë¹„ìš© 30% ê°ì†Œ ë°˜ë³µë˜ëŠ” ì ‘ì°© ê²°í•¨ ì´ìŠˆ í”„ë¡œì„¸ìŠ¤ ê°œì„ ì„ í†µí•œ ê²€ì‚¬ ì‹œê°„ 10% ê°ì†Œ ìœ íš¨ê¸°ê°„ ë§Œë£Œëœ ì™„ì œí’ˆì„ ê²°í•¨ í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ ì•…ì„± ì¬ê³  70% ì²˜ë¦¬ ë¶ˆëŸ‰í’ˆ ë°ì´í„° 100ê°œ ë¶„ì„ì„ í†µí•´ ìƒì‚° ë©”ë‰´ì–¼ ê°œë°œ ë° êµìœ¡ ë¶ˆëŸ‰ ë°œìƒ ì›ì¸ íŒŒì•…ì„ í†µí•´ ì‹ ê·œ ì¥ë¹„ êµ¬ë§¤ë¡œ ìƒì‚°ë¹„ 3% ì ˆê° ê¸°ìˆ  : QMS, ISO 9001 ëŒ€ë§Œ ë²•ë¥ ì— ëŒ€í•œ ì´í•´ ì§ë¬´: ì¬ë¬´íšŒê³„ í”„ë¡œì íŠ¸ëª…/ì—…ë¬´ëª… : ë¶„ê¸° ì¬ë¬´ ì‹¤ì‚¬ ê¸°íš ë° ë¶„ì„ ë³´ê³  ê¸°ê°„ : 2022ë…„ 8ì›” ~ 2022ë…„ 10ì›” (3ê°œì›”) ì„±ê³¼: ë¶„ê¸°ë³„ ì¬ê³ ì¡°ì‚¬ ì´ 10íšŒ ì‹¤ì‹œë¥¼ í†µí•´ ì¬ë¬´ ë°ì´í„° ì¼ì¹˜ìœ¨ 10%p ìƒìŠ¹ ì—­í•  í˜„ì¥ ì¬ê³ ì¡°ì‚¬ë¥¼ í†µí•´ ìë£Œ ì˜¤ì°¨ìœ¨ 0% ë‹¬ì„± ì‹¤ì‚¬ ë‚´ì—­ ë¶„ì„ì„ í†µí•´ ì„¸ë¬´ ì¦ë¹™ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ ë³€ê²½ ë° ë©”ë‰´ì–¼ êµìœ¡ ì˜ëª» ê¸°ì¬ëœ ê³„ì • 70% ìˆ˜ì • ì¬ê³ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ ê°œì„ ì„ í†µí•´ ì¬ê³  ê´€ë¦¬ ë¹„ìš© 10% ì ˆê° ê¸°ìˆ  : ERP ìì‚° ìœ í˜•ë³„ ì¬ë¬´ê´€ë¦¬ ë²•ë¥ ì— ëŒ€í•œ ì´í•´ ê²½ë ¥ ê¸°ìˆ ì„œ íƒ¬í”Œë¦¿ ì‚¬ìš© ê°€ì´ë“œ ê¸°ê°„ : ë˜ê²Œ ì¤‘ìš”í•œ ë¶€ë¶„. ê¼­ ë²ˆê±°ë¡­ì§€ ì•Šê²Œ ëª‡ê°œì›” í•œê±´ì§€ë¥¼ ì ì–´ì£¼ì. ê¸°ê°„ : 2022.08 ~ 2022.10 (3ê°œì›”) ì„±ê³¼ : > ê²½ë ¥ê¸°ìˆ ì„œëŠ” ë‚˜ì˜ ì†Œì†ê³¼ ì§ë¬´ë¥¼ ì ëŠ” ì¹¸ì´ ì•„ë‹ˆë‹¤. > ë©´ì ‘ê´€ì€ ê·¼ê±°ë¥¼ í† ëŒ€ë¡œ ì§€ì›ìë“¤ì˜ í•©ë¶ˆì„ ê°€ë¥¸ë‹¤. > ê·¸ ê·¼ê±°ë¡œ ê²°ì •ì ì¸ ìš”ì†Œê°€ ë°”ë¡œ ì„±ê³¼ì´ë‹¤. > ì•ì—ì„œ ì•„ë§ˆì¶”ì–´ì™€ í”„ë¡œí˜ì…”ë„ì˜ ì°¨ì´ë¥¼ ë…¸ë ¥ì„ ê°•ì¡°í•˜ëŠ”ê°€ ê²°ê³¼ë¥¼ ê°•ì¡°í•˜ëŠ”ê°€ë¼ê³  ì„¤ëª…í–ˆë‹¤. > ê²½ë ¥ê¸°ìˆ ì„œì˜ ì„±ê³¼ë¥¼ ì‘ì„±í•˜ëŠ”ê²Œ ë°”ë¡œ ê²°ê³¼ë¥¼ ê°•ì¡°í•˜ëŠ” í–‰ìœ„ì´ë‹¤. > ì„±ê³¼ë¥¼ ë³´ê³  ì§€ì›ìë¥¼ ë½‘ê³  ì‹¶ì€ì§€ë¥¼ ê²°ì •í•œë‹¤ê³  í•´ë„ ê³¼ì–¸ì´ ì•„ë‹ˆë‹¤. ì„±ê³¼ëŠ” ê°„ê²°í•˜ê²Œ ì‘ì„±í• ìˆ˜ë¡ ì¢‹ë‹¤. ë°˜ë“œì‹œ í•œ ì¤„ë¡œ. How + Result. ì•„ì‰¬ìš´ ì˜ˆì‹œ ì‹ ê·œë¸Œëœë“œ ëŸ°ì¹­ 100ë§Œì› ë‹¬ì„± ì¢‹ì€ ì˜ˆì‹œ ë¸Œëœë“œ ì½œë¼ë³´ë ˆì´ì…˜ì„ í†µí•œ ì‹ ì¬í’ˆ ì¶œì‹œë¡œ ê°ë‹¨ê°€ 1.5ë°° ìƒìŠ¹ ë° ë§¤ì¶œ 300% ìƒìŠ¹ ìˆ˜ìš” ì—ì¸¡ ì˜¤ë¥˜ ë°œê²¬ ë° ë¡œì§ ë³€ê²½ í†µí•œ íŒë§¤ ìˆœìœ„ 100ìœ„ê¶Œ ë‚´ forecast accuracy 5% ìƒìŠ¹ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„. > ì§€ì›ìë“¤ì´ ìì£¼ í•˜ëŠ” ì‹¤ìˆ˜ëŠ” ë„ˆë¬´ ë§ì€ ë‚´ìš©ì„ ë‹´ìœ¼ë ¤ëŠ” ê²ƒì´ë‹¤. > ì„±ê³¼ëŠ” ìì„¸í•¨ë³´ë‹¤ í•µì‹¬ì„ ì˜ í‘œí˜„í–ˆëŠ”ê°€ê°€ ì¤‘ìš”í•˜ë‹¤. ì—­í•  : ì–´ë–»ê²Œ ì„±ê³¼ë¥¼ ë‹¬ì„±í–ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë¶€ë¶„. > ì—­í• ì´ë¼ê³  ì ì—ˆì§€ë§Œ ì‚¬ì‹¤ì€ ì „ëµì´ë¼ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ ë” ì í•©í•˜ë‹¤. ë¬¸ì œë¥¼ í•´ê²°í•œ ì•¡ì…˜ ìƒì‚°ì„±ì„ ë†’ì¸ ì•¡ì…˜ ì˜ì‚¬ì†Œí†µì„ ì˜í•œ ì•¡ì…˜ > ì—­í• ì„ í›¨ì”¬ ì—£ì§€ìˆê²Œ ì‘ì„±í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤. > ê° ì•¡ì…˜ë“¤ì— ìˆ˜ì¹˜í™”ë¥¼ ê°€ë¯¸í•˜ëŠ” ê²ƒì´ë‹¤. ì•„ì‰¬ìš´ ì˜ˆì‹œ ì—­í•  : ì œí’ˆ ì½˜ì…‰íŠ¸ì™€ ìŠ¤í† ë¦¬ë³´ë“œ ë„ì¶œ, ì‹œì¥ë¶„ì„ -> ë‹¹ì—°íˆ í•´ì•¼í•˜ëŠ” ì¼ë“¤ì„ ì ê³  ìˆìŒ ì„±ê³¼ : ì‹ ì œí’ˆ ì¶œì‹œë¥¼ í†µí•´ ê³ ê°ë§Œì¡±ë„ 100% ìƒìŠ¹ ì—­í•  : í•´ì™¸ í”Œë«í¼ ì‹œì¥ ë§ˆì¼€íŒ… ë°©ì•ˆ ì¡°ì‚¬ ê²½ìŸì‚¬ ì½˜í…ì¸ ë¥¼ ë²¤ì¹˜ë§ˆí‚¹í•´ ì œí’ˆ ëœë”© í˜ì´ì§€ ë¦¬ë‰´ì–¼ ë° ê´‘ê³  ì†Œì¬ ê¸°íš ë§ˆì¼€íŒ… ì„±ê³¼ ë¶„ì„ -> ì‹ ì œí’ˆ ì¶œì‹œë¼ëŠ” ì•¡ì…˜ì´ ëª¨í˜¸í•˜ê¸° ë•Œë¬¸ì—, ì—­í• ì´ ì„±ê³¼ì™€ ì—°ê²°ë˜ì–´ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤. ê° ì—­í• ë„ ìˆ˜ì¹˜í™”ê°€ ë˜ì§€ ì•Šì•„ì„œ ë‹¹ì—°í•œ ì—…ë¬´ í™œë™ìœ¼ë¡œ í•´ì„ëœë‹¤. ì—­í• ì— ê¸°ìˆ ëœ ë‚´ìš©ì´ ì„±ê³¼ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ ìˆ˜ì¹˜ë¡œ ì„¤ëª…í•´ë³´ì. ì¢‹ì€ ì˜ˆì‹œ ì—­í•  : ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë¶„ì„ì„ í†µí•œ ìƒì‚°ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ ë¡œì§ì„ ì„¤ê²Œí•˜ì—¬ ë¶ˆëŸ‰í’ˆ ê²€ì¦ ìë™í™” ê¸°ëŠ¥ ê°œë°œ ë¶ˆëŸ‰í’ˆ ê²€ì¦ ìë™í™” ì‹œìŠ¤í…œ ê°œë°œì„ í†µí•´ ë¶ˆëŸ‰í’ˆ ê²€ì‚¬ ì‹œê°„ 300% ë‹¨ì¶• ì œí’ˆë³„, ê³ ê°ì‚¬ë³„ 100ê°€ì§€ ë¶ˆëŸ‰í’ˆ ì¼€ì´ìŠ¤ ë¶„ì„ì„ í†µí•´ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ë§Œë“¤ì–´ ë¶ˆëŸ‰í’ˆ ê²€ì‚¬ ë¹„ìš© 30% ì ˆê° -> ë¬¸ì œë¥¼ í•´ê²°í•œ í•µì‹¬ ì•¡ì…˜ê³¼ ìƒì‚°ì„±ì„ ë†’ì¸ ì•¡ì…˜ì„ ìˆ«ìë¥¼ ê·¼ê±°ë¡œ ì •í™•í•˜ê²Œ ë‚˜íƒ€ëƒˆë‹¤. ì—­í•  : ì‹ ê·œ ìœ ì…ì 30%ê°€ ëœë”© í˜ì´ì§€ ì²´ë¥˜ì‹œê°„ì´ 10ì´ˆ ì´ë‚´ì¸ ì ì„ ë¶„ì„ í™ˆí˜ì´ì§€ ìœ ì…ì 5000ëª… ë°ì´í„° ë¶„ì„ìœ¼ë¡œ ëœë”© í˜ì´ì§€ ê°œì„ ì•ˆ ë„ì¶œ A/B í…ŒìŠ¤íŠ¸ë¡œ ê°œì„ ì•ˆ ë””ìì¸ ê²€ì¦ì„ í†µí•´ í´ë¦­ë¥  ë†’ì€ ë¬¸êµ¬, ë²„íŠ¼ ìœ„ì¹˜ íŒŒì•… ë³€ê²½ëœ ëœë”© í˜ì´ì§€ë¡œ ì—°ê³„ ë§¤ì¶œ 100% ì¦ê°€ -> íˆíŠ¸ë§µ ë¶„ì„œê¸°ë¼ëŠ” ë¶„ëª…í•œ ì•¡ì…˜ì´ ìˆê³  ì—­í•­ì— ê¸°ìˆ ëœ ë‚´ìš©ì´ ì—°ê²°ëœë‹¤. ì—­í• ì— ê¸°ìˆ ëœ ë‚´ìš©ì´ ì„±ê³¼ì— ì–´ë–¤ ì˜í–¥ì„ ì£¼ì—ˆëŠ”ì§€ ìˆ«ìë¡œ ì˜ í‘œí˜„ë˜ì–´ ìˆë‹¤. > ì—­í• ì„ ì ì„ ë•Œ ê°€ì¥ í”¼í•´ì•¼ í•  ì¼ì€, ë‹¹ì—°íˆ í•´ì•¼ í•˜ëŠ” ì¼ë“¤ì„ ë‚˜ì—´í•˜ëŠ” ê²ƒì´ë‹¤. > ëª©í‘œì™€ ìƒê´€ì—†ì´ ë°˜ë³µí•˜ëŠ” ì¼ë“¤ì€ ì ì„ í•„ìš”ê°€ ì—†ë‹¤. ê¸°ìˆ  : > ë‚´ê°€ ì§€ì›í•˜ëŠ” íšŒì‚¬ì˜ ì •ë³´ë¥¼ íŒŒì•…í•˜ì—¬ ê·¸ì— ë§ê²Œ ì‘ì„±í•˜ëŠ” ê²ƒ í•„ìˆ˜ì ì¸ ê±´ ì•„ë‹˜ ê²½ë ¥ê¸°ìˆ ì„œ ì²´í¬ë¦¬ìŠ¤íŠ¸ ê¸°ê´€, ì„±ê³¼, ì—­í• , íšë“ ê¸°ìˆ ì„ ì‘ì„±í•˜ì˜€ëŠ”ê°€? ì„±ê³¼ë¥¼ ì„œìˆ í˜•ì´ ì•„ë‹Œ ê°œì¡°ì‹ìœ¼ë¡œ ì‘ì„±í•˜ì˜€ëŠ”ê°€? í•µì‹¬ì ì¸ ë‚´ìš© ì™¸ì— ë‹¤ë¥¸ ë‚´ìš©ì´ ë§ì§€ëŠ” ì•Šì€ê°€? í•´ì„ì´ í•„ìš” ì—†ëŠ” ê°ê´€ì ì´ê³  ì§ê´€ì ì¸ ìˆ«ì í˜¹ì€ ê²°ê³¼ë¬¼ë¡œ ì‘ì„±í•˜ì˜€ëŠ”ê°€? ì„±ê³¼ë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•œ ë‚˜ì˜ ì—­í• ì„ 3-5ê°€ì§€ë¡œ ê¸°ì¬í–ˆëŠ”ê°€? ì§€ì›í•˜ëŠ” ì§ë¬´, ì‚°ì—…, íšŒì‚¬ì™€ ì¼ì¹˜ë„ê°€ ë–¨ì–´ì§€ëŠ” ê²½ë ¥ ë‚´ìš©ì„ ì œê±°í•˜ì˜€ëŠ”ê°€? ê²½ë ¥ ê¸°ìˆ ì„œì— ê¸°ìˆ í•œ ê²½ë ¥ìˆ˜ê°€ ì´ ê²½ë ¥ê¸°ê°„ / 2 ë³´ë‹¤ ë§ì€ê°€?"
  },
  {
    "id": "Career/Resumes/25-07-29-hyundai-autoever-ml-engineer-resume",
    "title": "Hyundai Autoever - ML Engineer Resume",
    "date": "2025-07-29",
    "excerpt": "í˜„ëŒ€ìë™ì°¨ ì´ë ¥ì„œ",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³ ",
      "ì´ë ¥ì„œ"
    ],
    "content": "ì±„ìš©ê³µê³ ì™€ ë¶„ì„ë‚´ìš© í•„ìš”í•œ ê²½í—˜ë§Œ ì‚´ë ¤ì„œ ì§§ê³  êµµê²Œ ì“°ê¸°. 2ì¥ì •ë„ ë¶„ëŸ‰ í•„ì‚´ê¸°? ìœ ì‚¬ê²½í—˜ ğŸ” ì´ëŸ° ë¶„ê³¼ í•¨ê»˜ í•˜ê³  ì‹¶ì–´ìš” -> ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ í”„ë¡œì íŠ¸ëª…: ì±—ë´‡ ì‹œìŠ¤í…œ ê°œë°œ ë° ì„œë¹„ìŠ¤ ìš´ì˜ ê¸°ê°„: 2024.11 ~ 2025.02 (4ê°œì›”) ì„±ê³¼: RAG íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë° ìµœì í™”ë¡œ ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ëŒ€ì²´ ë° ì‚¬ìš©ì ë§Œì¡±ë„ 90% ì´ìƒ ë‹¬ì„± ì—­í• : ì¿¼ë¦¬ ë¶„í•´ ë° ì¬ì‘ì„±, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê¸°ë°˜ RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶•ìœ¼ë¡œ ê²€ìƒ‰ ì •í™•ë„ 90% ì´ìƒ ë‹¬ì„± ì§ˆë¬¸ ë¶„ë¥˜ ëª¨ë¸ ë„ì…ìœ¼ë¡œ ë¶ˆì™„ì „ ì‘ë‹µë¥ ì„ ë‚®ì¶° ì‚¬ìš©ì ë¶ˆë§Œ 70% ê°ì†Œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§(Instruct, Role, Context ê¸°ë°˜) ìµœì í™”ë¡œ ìƒë‹´ í†¤ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë‹µ ìƒì„± ë° ë‹µë³€ ì •í™•ë„ 90% ì´ìƒ ë‹¬ì„± Qdrant ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ìš´ì˜, ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§ ì‹œìŠ¤í…œ êµ¬í˜„ìœ¼ë¡œ ê²€ìƒ‰ ì„±ëŠ¥ 40% í–¥ìƒ LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ ìë™ í‰ê°€ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ë¹„ìš© 80% ê°ì†Œ ì¹´ì¹´ì˜¤í†¡ ì±„ë„ ì—°ë™ì„ í†µí•œ ì‹¤ì œ ë³‘ì› ìƒë‹´ ì„œë¹„ìŠ¤ ìš´ì˜ìœ¼ë¡œ ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ì±—ë´‡ ëŒ€ì²´ ê¸°ìˆ : Python, FastAPI, Transformers, Qdrant, Docker, AWS RAG ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜, ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶• ë° ìš´ì˜, ì±—ë´‡ ì„œë¹„ìŠ¤ í’ˆì§ˆ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ì— ëŒ€í•œ ì´í•´ --- ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì í”„ë¡œì íŠ¸ëª…: OCR íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê°œë°œ ê¸°ê°„: 2024.08 ~ 2024.10 (3ê°œì›”) ì„±ê³¼: OCR íŒŒì´í”„ë¼ì¸ ìµœì í™”ë¡œ ì •í™•ë„ 99% ë‹¬ì„± ë° ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œ ìš´ì˜, ìë™í™”ëœ ë°ì´í„° ê²€ì¦ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§ ìë™í™” ì—­í• : ì´ë¯¸ì§€ ìœ„ì¹˜ ê³ ì • íŠ¹ì„±ì„ í™œìš©í•œ ì „ì²˜ë¦¬ ë° text recognition íŠ¹í™” ëª¨ë¸ ë„ì…, í›„ì²˜ë¦¬ ë¡œì§ êµ¬í˜„ìœ¼ë¡œ OCR ì •í™•ë„ 99% ë‹¬ì„± ì´ë¯¸ì§€ ì†¡ìˆ˜ì‹ ë¶€í„° OCR ì²˜ë¦¬, DB ì ì¬ê¹Œì§€ ê° ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶• ë° ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•´ ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œ ë‹¬ì„± ë° ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨ 10ë°° ì¦ê°€ ëª¨ë¸ ì •í™•ë„ ë° ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìë™í™”ë¥¼ ìœ„í•´ í†µê³„ì  ë¶„í¬ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê°œë°œ ê³ ê°ë²ˆí˜¸ ë“± ì¼ê´€ì„± ê²€ì¦ ë¡œì§ ê°œë°œ ë° ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶• ìë™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ê°œë°œë¡œ ê° ê°ì²´ë³„ ì‹¤ì‹œê°„ í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ ìë™í™” ê¸°ìˆ : Python, Transformers, OpenCV, gRPC, FastAPI, Triton Server, Docker OCR ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ ìš´ì˜, ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬ ë° ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•œ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ì— ëŒ€í•œ ì´í•´ --- ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì í”„ë¡œì íŠ¸ëª…: ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ ë³‘ë³€ íƒì§€ ëª¨ë¸ë§ ë° ì„œë¹„ìŠ¤ ê°œë°œ ê¸°ê°„: 2024.04 ~ 2024.07 (4ê°œì›”) ì„±ê³¼: Contrastive Learning ë° Multi-Task Learning ì ìš©ìœ¼ë¡œ ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ ì¼ë°˜í™” ì„±ëŠ¥ 10% ì´ìƒ í–¥ìƒ ë° Dice Score 0.8 ë‹¬ì„± ì—­í• : U-Net ê¸°ë°˜ segmentation ëª¨ë¸ ì„¤ê³„ ë° ìµœì í™”ë¥¼ í†µí•´ ê¸°ì¡´ ëŒ€ë¹„ ì„±ëŠ¥ 10-15% í–¥ìƒ CLAHE ì „ì²˜ë¦¬ ê¸°ë²• ì ìš©ìœ¼ë¡œ ë°ì´í„° íŠ¹ì§• ëª…í™•í™”ì„ í†µí•´ ë¯¼ê°ë„ 10% í–¥ìƒ Contrastive Learningê³¼ Multi-Task Learningì„ í™œìš©í•´ ì œí•œëœ ë°ì´í„°(1,500ê°œ) í™˜ê²½ì—ì„œ ëª¨ë¸ ì¼ë°˜í™” ì„±ëŠ¥ 10%ì´ìƒ ê°œì„  Triton Serverë¥¼ í†µí•œ ê³ ì„±ëŠ¥ ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œ êµ¬ì¶•ìœ¼ë¡œ ì¶”ë¡  ì†ë„ 20% í–¥ìƒ ê¸°ìˆ : U-Net, OpenCV, CLAHE, PyTorch, Contrastive Learning, Multi-Task Learning, Triton Server, TensorRT, gRPC ì œí•œëœ ë°ì´í„° í™˜ê²½ì—ì„œì˜ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ ë°©ë²•ë¡  ë° ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œì— ëŒ€í•œ ì´í•´ --- ì‚¬ì „ì§ˆë¬¸ (ì„ íƒ) ì´ë ¥ì„œ ë‚´ìš© ì™¸ì— ì¶”ê°€ì ìœ¼ë¡œ ì„¤ëª…í•˜ê±°ë‚˜ ì–´í•„í•˜ê³ ì í•˜ëŠ” ë‚´ìš©ì´ ìˆìœ¼ì‹œë©´ ììœ ë¡­ê²Œ ì‘ì„± ë¶€íƒë“œë¦½ë‹ˆë‹¤. (ìµœëŒ€ 1,000ì) ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” ë°ì´í„°ê³¼í•™ìë¡œì„œ ì¼í•˜ë©´ì„œ ì±—ë´‡ ê°œë°œë¶€í„° ìˆ˜ìˆ  ê²½ê³¼ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œê¹Œì§€ ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ë‹¤ë£¨ë©° ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ê³  ìš´ì˜í•˜ëŠ” ê²½í—˜ì„ í•˜ë©´ì„œ ì¦ê²ê²Œ ì¼ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ê³  ëª¨ë¸ì„ ë§Œë“¤ë©´ì„œ ê°€ì¥ ë§ì´ ì‹œê°„ì„ ì“°ëŠ” ë¶€ë¶„ì€ ì—­ì‹œ ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³  ì •ì œí•˜ëŠ” ê³¼ì •ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. íŠ¹íˆ ë°ì´í„° í’ˆì§ˆì„ ê´€ë¦¬í•˜ëŠ” ì¼ì€ ê°€ì¥ ë§ì€ ë…¸ë ¥ì„ ìŸëŠ” ì¼ì…ë‹ˆë‹¤. ì´ìƒì¹˜ëŠ” ì–´ë–»ê²Œ ì •ì˜í•  ê²ƒì´ë©° ì–´ë–»ê²Œ ì²˜ë¦¬í•  ê²ƒì¸ì§€ì™€ ë°ì´í„° ì•ˆì— ì¡´ì¬í•˜ëŠ” ë…¸ì´ì¦ˆëŠ” ì–´ë–»ê²Œ ê´€ë¦¬í•´ì•¼í•˜ëŠ”ì§€ ë“±ë“± ë§ì€ ê³ ë¯¼ê³¼ ì‹œí–‰ì°©ì˜¤ ëì— í•˜ë‚˜ì”© í•´ê²°í•´ ë‚˜ê°€ëŠ” ê³¼ì •ì„ ê²½í—˜í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì œê°€ êµ¬ì¶•í•œ OCR íŒŒì´í”„ë¼ì¸ì„ í†µí•´ì„œ ìˆ˜ì§‘í•œ ë°ì´í„°ì™€ ê·¸ ì™¸ì— ìˆ˜ì§‘ë˜ëŠ” ë°ì´í„°ë“¤ì„ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ì— ì ì¬í•˜ê³  ì´ë¥¼ ì •ì œí•´ ë°ì´í„° ë§ˆíŠ¸ì™€ í”¼ì²˜ ìŠ¤í† ì–´ë¥¼ ê°œë°œí•˜ëŠ” ê²½í—˜ì„ í•˜ë©° ë°ì´í„°ë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ë‹¤ë£¨ê¸° ìœ„í•œ ì‹œìŠ¤í…œì„ ì„¤ê³„í•˜ëŠ” ë²•ì„ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ê¸°ìˆ ì´ë‚˜ ê°œë…ì„ ë°°ì›Œ ì„œë¹„ìŠ¤ì— ì ìš©í•˜ë©° ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì¼ì€ ê½¤ë‚˜ ì„¤ë ˆëŠ” ì¼ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. segmentation ëª¨ë¸ì„ ê°œë°œí•˜ë©´ì„œ ë°ì´í„° ë¶€ì¡± ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ contrast learningê³¼ multi-task learningì— ëŒ€í•œ ê°œë…ì„ ê³µë¶€í•˜ê³  ë…¼ë¬¸ë“¤ì„ ì°¸ê³ í•´ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ìµœì í™”í•˜ëŠ” ê²½í—˜ì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•´ì£¼ëŠ” ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí•˜ë©´ì„œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„ íƒ ë¬¸ì œì— ê°€ì§€ëŠ” í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡ ì— ëŒ€í•´ ê³µë¶€í–ˆê³ , ì´ë¥¼ ì„œë¹„ìŠ¤ì— ì ìš©í•œ ê²½í—˜ì„ í–ˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ ë‹¤ë¥¸ í”„ë¡œì íŠ¸ì—ì„œë„ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ë§‰íˆëŠ” ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ëŠ” ê²½í—˜ì€ ê½¤ë‚˜ ì¦ê²ê³  ì¬ë¯¸ìˆì—ˆìŠµë‹ˆë‹¤. ì´ì²˜ëŸ¼ ì €ëŠ” ê¸¸ì§€ ì•Šì€ ê¸°ê°„ì´ì§€ë§Œ ë‹¤ì–‘í•œ í”„ë¡œì íŠ¸ë¥¼ ê²½í—˜í•˜ë©° ë§ì€ ë¬¸ì œë¥¼ ë§ˆì£¼í–ˆê³  ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì´í•´í•˜ê³  í™œìš©í•˜ë©´ì„œ ì´ë¥¼ í•´ê²°í•˜ëŠ” ê²½í—˜ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. í›„ê¸° ë–¨ë¦¬ëŠ” êµ¬ë§Œ... ì¼ë‹¨ ê¸°ëŒ€ëŠ” 0... ë–¨ì–´ì§€ë©´ ë–¨ì–´ì§€ëŠ”ê±°ì§€ ë­ ã….ã…"
  },
  {
    "id": "Career/Job Transition/25-07-29-hyundai-autoever-ml-engineer",
    "title": "Hyundai Autoever - ML Engineer",
    "date": "2025-07-29",
    "excerpt": "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³  ë¶„ì„ê³¼ ì¤€ë¹„ê³¼ì •",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ìë™ì°¨ ì±„ìš©ê³µê³ "
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´</strong></summary> ë§í¬ [Tech] Machine Learning Engineer - AI ë°ì´í„° ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ğŸ“ƒ ëˆ„êµ¬ë‚˜ ë§ˆìŒ ì†ì— ì´ë ¥ì„œ í•œ ì¥ì€ ìˆìœ¼ë‹ˆê¹Œ, âš¡ 1ë¶„ ì»· ì§€ì›ìœ¼ë¡œ í˜„ëŒ€ì˜¤í† ì—ë²„ë¡œì˜ ì—¬ì •ì„ ì‹œì‘í•˜ì„¸ìš”. (ì´ë ¥ì„œ ììœ¨ì–‘ì‹) âœ… AI â€‹ë°ì´í„° â€‹ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ ğŸš€ í•©ë¥˜í•˜ì‹¤ â€‹íŒ€ì„ â€‹ì†Œê°œí•´ìš” ì¸ê³µì§€ëŠ¥ê¸°ìˆ ì‹¤ì€ â€‹AI ì‹ ê¸°ìˆ ì„ ë°”íƒ•ìœ¼ë¡œ â€‹ì „ì‚¬ â€‹AI ì£¼ì œë¥¼ ë¦¬ë“œí•˜ë©° â€‹ê³µí†µ â€‹í™œìš© â€‹ê¸°ìˆ ì— ëŒ€í•´ ë‹¤ë£¹ë‹ˆë‹¤. â€‹ì—°êµ¬ê°œë°œì„ í†µí•´ â€‹'ì‚´ì•„ìˆëŠ” â€‹AI' ê¸°ìˆ ì„ â€‹ë§Œë“¤ì–´ ì‚¬ë‚´ â€‹ì£¼ìš” â€‹ì„œë¹„ìŠ¤ ë° ì œí’ˆì´ â€‹ë”ìš± ê²½ìŸë ¥ â€‹ìˆë„ë¡ ë§Œë“¤ì–´ ê°‘ë‹ˆë‹¤. ìŒì„±/ì–¸ì–´, ë¹„ì „/ì¸ì§€ ë“± í­ë„“ì€ AI ì£¼ì œ ì „ë°˜ì„ ë‹¤ë£¨ê³  ìˆìœ¼ë©°, ê°œë°œ ê²°ê³¼ëŠ” ìš°ë¦¬ íšŒì‚¬ ë° ê·¸ë£¹ì˜ ì¤‘ìš” ì„œë¹„ìŠ¤ì— ì ìš©ë˜ê³  ìˆìŠµë‹ˆë‹¤. AI ì‹ ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ì™€ AI ëª¨ë¸ ì—°êµ¬ê°œë°œ ì—­ëŸ‰ì„ ë°”íƒ•ìœ¼ë¡œ ë¯¸ë˜ ëª¨ë¹Œë¦¬í‹° ë° ìœ ê´€ ë¶„ì•¼ì˜ ì£¼ì¸ê³µì´ ë  ê°œë°œì/ì—°êµ¬ì ë¶„ë“¤ì„ ëª¨ì‹­ë‹ˆë‹¤. ğŸ’» í•©ë¥˜í•˜ì‹œë©´ í•¨ê»˜ í•  ì—…ë¬´ì˜ˆìš” AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤ë¥¼ ìœ„í•œ ë°ì´í„° ë¶„ì„ ë° ì„¤ê³„ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ / ë°ì´í„° ì¦ê°• AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤ ë°ì´í„° ê´€ë ¨ ìš´ì˜ ë° í’ˆì§ˆ ê´€ë¦¬ ğŸ” ì´ëŸ° ë¶„ê³¼ í•¨ê»˜ í•˜ê³  ì‹¶ì–´ìš” ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ NLP ê´€ë ¨ Task ë°ì´í„° êµ¬ì¶• ë° ì„¤ê³„ ê²½í—˜ Python ê°œë°œ ê²½ë ¥ 1ë…„ ì´ìƒ í˜¹ì€ ê·¸ì— ì¤€í•˜ëŠ” ì‹¤ë ¥ ğŸ” ì´ëŸ° ë¶„ì´ë¼ë©´ ë”ìš± ì¢‹ì•„ìš” ì „ì‚°ì–¸ì–´í•™/ìì—°ì–´ì²˜ë¦¬ ì „ê³µ ì„ì‚¬ ì´ìƒ LLM ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ê²½í—˜ì LLM / GPT ê¸°ë°˜ ë°ì´í„° ì¦ê°• ê²½í—˜ì ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì âŒ› ì´ë ‡ê²Œ í•©ë¥˜í•´ìš” ì„œë¥˜ ì ‘ìˆ˜ â†’ ì„œë¥˜ ê²€í†  â†’ ì§ë¬´ì—­ëŸ‰í…ŒìŠ¤íŠ¸(ì½”ë”© ë˜ëŠ” ê³¼ì œí…ŒìŠ¤íŠ¸) ë° ì¸ì„±ê²€ì‚¬ â†’ 1ì°¨ë©´ì ‘ â†’ 2ì°¨ë©´ì ‘ â†’ ì²˜ìš°í˜‘ì˜ ë° ì±„ìš©ê²€ì§„ â†’ ìµœì¢… í•©ê²©ğŸ‰ ğŸ“ ë§Œë‚˜ê²Œ ë  ê·¼ë¬´ì§€ëŠ” ì—¬ê¸°ì˜ˆìš” ì„œìš¸ ê°•ë‚¨ ğŸ“£ ë™ë£Œì˜ í•œ ë§ˆë”” ì¸ê³µì§€ëŠ¥ê¸°ìˆ ì‹¤ì˜ ê¸°ë‘¥ Lì±…ì„ì…ë‹ˆë‹¤. ìš°ë¦¬ íŒ€ì€ ììœ¨ì„±ì´ íŠ¹ì§•ì´ì—ìš”. ì„œë¹„ìŠ¤ì—ì„œë¶€í„° ì œì¡°/ìƒì‚°ê¹Œì§€, ê¸°ìˆ ì´ ì ìš©ë˜ëŠ” ë¶„ì•¼ê°€ ë‹¤ì–‘í•˜ê³  ë˜ íŒ”ë¡œì—… í•´ì•¼ í•˜ëŠ” ì£¼ì œê°€ ê´‘ë²”ìœ„í•˜ë‹¤ ë³´ë‹ˆ ììœ¨ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì„±ì¥ì„ ì§€ì›í•˜ê³  ìˆì–´ìš”. ì „ë°˜ì ì¸ ë¶„ì•¼ë¥¼ ì˜ ì•„ëŠ” ë©˜í† ë“¤ì´ ìˆì–´ ë§ì€ ë„ì›€ì„ ë°›ê³  ìˆê³ , ë˜ ê° ë¶„ì•¼ë³„ ì£¼ì œì— ë”°ë¼ í™œë°œí•œ ììœ¨ì  í•™ìŠµ ê·¸ë£¹ êµ¬ì„±ì„ í†µí•´ ì—…ë¬´ ì™¸ì— ì—­ëŸ‰ì  ì„±ì¥ì„ ê¾¸ì¤€íˆ í•˜ê³  ìˆë‹µë‹ˆë‹¤ :) ì—°êµ¬ê°œë°œ ê²°ê³¼ëŠ” ê·¸ë£¹ ì„œë¹„ìŠ¤ë‚˜ ì œí’ˆì— ë°˜ì˜ë˜ê¸°ë„ í•˜ê³ , ê´€ë ¨ í•™íšŒë‚˜ í–‰ì‚¬ ë“±ì— ì´ˆëŒ€ë¥¼ ë°›ì•„ ë°œí‘œí•˜ê¸°ë„ í•´ìš”. ì‚¬ë¬´ì‹¤ì€ ì‚¼ì„±ì—­ ì¸ê·¼ì— ìˆê³ , ë¡¯ë°íƒ€ì›Œê°€ ì¡°ë§ë˜ëŠ” ë·°ë„ í™˜ê²½ì  ì¥ì ì´ëë‹ˆë‹¤! í•¨ê»˜ ì‹ ê¸°ìˆ ì„ ê³µë¶€í•˜ê³  êµ¬ì²´ì  ê²°ê³¼ê°€ ìˆëŠ” ì‹¤ì§ˆì  ê¸°ìˆ  ì‘ìš© ê²½í—˜ì„ í†µí•´ ì»¤ë¦¬ì–´ì ìœ¼ë¡œ ì„±ì¥í•˜ê³  ì‹¶ì€ ë¶„ë“¤ì„ ê¸°ë‹¤ë¦¬ê³  ìˆì–´ìš”! (íŒ€ ë¦¬ë”) ì¸ê³µì§€ëŠ¥ê¸°ìˆ ì‹¤ì€ ì‹ ê¸°ìˆ ì— ëŒ€í•œ ì´í•´ì™€ í™œìš©ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ì—´ì •ì„ ê°€ì§„ ì‚¬ëŒë“¤ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. AIë¥¼ ìœ„í•œ ì¸í”„ë¼, ë°ì´í„°, ë¶„ì„ ëª¨ë¸ë§ ë° ì•Œê³ ë¦¬ì¦˜, ê¸°ìˆ  ê¸°ë°˜ ê³µí†µ ì„œë¹„ìŠ¤ ê°œë°œê¹Œì§€ ë‹¤ì–‘í•œ ë¶„ì•¼ë¥¼ ë‹¤ë£¨ë©°, ì´ëŸ° ë…¸ë ¥ì„ ë°”íƒ•ìœ¼ë¡œ ê·¸ë™ì•ˆ ì˜ë¯¸ ìˆê³  ê²°ê³¼ë“¤ì„ ë§Œë“¤ì–´ë‚´ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ë¡œ ì£¼ìš” ê° ë¶„ì•¼ AI í”„ë ˆì„ì›Œí¬ë¥¼ ì´ìš©í•´ ê°œë°œì„ í•˜ë©° Python ë“±ì˜ ì–¸ì–´ë¥¼ ì£¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. ìš°ë¦¬ íŒ€ì€ ë©€í‹°ëª¨ë‹¬ ê¸°ë°˜ì˜ ê¶ê·¹ì  AI ê°œë°œì„ ì§€í–¥í•˜ê³ , ì´ë¥¼ í†µí•´ íšŒì‚¬ì˜ ì„±ì¥ê³¼ ê° êµ¬ì„±ì›ì˜ ì»¤ë¦¬ì–´ ì„±ì¥ì„ í•¨ê»˜ ì´ë£° ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ì‹¬ì„ ê°€ì§€ê³  ì„±ê³µì˜ ì—¬ì •ì„ í•¨ê»˜í•´ ì£¼ì„¸ìš”! Join Us! ğŸ“Œ ì°¸ê³ í•´ ì£¼ì„¸ìš” ì±„ìš© ì‹œ ë§ˆê°ë˜ëŠ” ìƒì‹œ ì±„ìš© ê³µê³ ë¡œ ìš´ì˜ë˜ë©°, ì±„ìš© ì ˆì°¨ì™€ ì¼ì •ì€ ë³€ë™ë  ìˆ˜ ìˆì–´ìš”. ì‚¬íšŒì  ë°°ë ¤ ëŒ€ìƒì(ë³´í›ˆ ì·¨ì—…ì§€ì›ëŒ€ìƒì, ì¥ì• ì¸)ëŠ” ê´€ê³„ ë²•ë ¹ê³¼ ë‚´ê·œì— ë”°ë¼ ìš°ëŒ€í•´ìš”. ëª¨ì§‘ ë¶„ì•¼ ë° ë‹´ë‹¹ ì—…ë¬´ì— ë”°ë¼ ì˜ì–´ êµ¬ìˆ í‰ê°€, ë ˆí¼ëŸ°ìŠ¤ ì²´í¬, ë˜ëŠ” ê¸°íƒ€ ì „í˜•ì´ ì‹¤ì‹œë  ìˆ˜ ìˆì–´ìš”. ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ë¥¸ í¬ì§€ì…˜ì´ ë” ì í•©í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ê²½ìš° ì§€ì› ë¶„ì•¼ê°€ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ë°°ì¹˜ ë¶€ì„œ ë° ê·¼ë¬´ì§€ëŠ” íšŒì‚¬ ì‚¬ì •ì— ë”°ë¼ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ì•„ë˜ì˜ ê²½ìš°, í•©ê²©ì´ ì·¨ì†Œë˜ê±°ë‚˜ ì „í˜• ì§„í–‰ì—ì„œ ë¶ˆì´ìµì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì–´ìš”. ì§€ì›ì„œê°€ ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ê±°ë‚˜ ì¦ë¹™ì´ ë¶ˆê°€í•  ê²½ìš° í•´ì™¸ì—¬í–‰ ê²°ê²© ì‚¬ìœ ê°€ ìˆëŠ” ê²½ìš° (ë‚¨ì„±ì˜ ê²½ìš°, íšŒì‚¬ê°€ ì§€ì •í•œ ì…ì‚¬ì¼ê¹Œì§€ ë³‘ì—­ í•„ ë˜ëŠ” ë©´ì œ í•„ìš”) ìµœì¢… í•©ê²© í›„ íšŒì‚¬ê°€ ì§€ì •í•˜ëŠ” ì…ì‚¬ì¼ì— ì…ì‚¬ ë¶ˆê°€í•œ ê²½ìš° </details> ì´ë ¥ì„œ ì „ëµ í•„ìš”ê²½í—˜ ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ -> ì±—ë´‡ ê°œë°œí•˜ê³  ì„œë¹„ìŠ¤ ìš´ì˜í•´ë³¸ ê²½í—˜ì´ ìˆìŒ ì„ í˜¸ ê²½í—˜ ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì NLP taskëŠ” ì•„ë‹ˆì§€ë§Œ segmentation ëª¨ë¸ ê°œë°œ ê²½í—˜ì´ë‘ --- ğŸ¯ í•„ìš” ê²½í—˜ (Must Have) ê²€ìƒ‰ / ì±—ë´‡ ê´€ë ¨ ë°ì´í„° êµ¬ì¶• ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ âœ… Chatbot í”„ë¡œì íŠ¸: RAG ê¸°ë°˜ ì§€ëŠ¥í˜• ì±—ë´‡ ì‹œìŠ¤í…œ ê°œë°œ ë° ì„œë¹„ìŠ¤ ìš´ì˜ âœ… ì‹¤ì œ ì„œë¹„ìŠ¤: ì¹´ì¹´ì˜¤í†¡ ì±„ë„ì„ í†µí•œ ë³‘ì› ìƒë‹´ ìë™í™” ì„œë¹„ìŠ¤ ğŸŒŸ ì„ í˜¸ ê²½í—˜ (Nice to Have) ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ ê²½í—˜ì âœ… Computer Vision: OCR Pipelineì—ì„œ TrOCR ëª¨ë¸ í™œìš© âœ… NLP: Chatbotì—ì„œ BGE-m3 ì„ë² ë”© ëª¨ë¸, GPT ê¸°ë°˜ ë‹µë³€ ìƒì„± âœ… ëª¨ë¸ ì„œë¹™: Triton Server, BentoMLì„ í†µí•œ í”„ë¡œë•ì…˜ ë°°í¬ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ í”„ë¡œì„¸ìŠ¤ ê²½í—˜ì âœ… ìë™í™”ëœ í’ˆì§ˆ í‰ê°€: LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ ìë™ í‰ê°€ ì‹œìŠ¤í…œ âœ… ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: Slack ê¸°ë°˜ ì—ëŸ¬ ì•Œë¦¼ ë° ì‘ì—… ìš”ì•½ âœ… í’ˆì§ˆ ì§€í‘œ ì •ì˜: ë‹µë³€ ì¶œì²˜ ì í•©ì„±, ì§ˆë¬¸-ë¬¸ì„œ ì—°ê´€ì„± í‰ê°€ âœ… ì„ê³„ì¹˜ ê¸°ë°˜ ê´€ë¦¬: í’ˆì§ˆ ì ìˆ˜ ì„ê³„ì¹˜ ì´í•˜ ì‹œ ìë™ ì•Œë¦¼ âœ… ë°ì´í„° ê²€ì¦ ë¡œì§: OCR ê²°ê³¼ ì •ì œ, ìˆ˜ê¸° ì…ë ¥ ë°ì´í„° ì™„ì „ì„± ê²€ì¦ ğŸ“‹ í¬íŠ¸í´ë¦¬ì˜¤ ë§¤ì¹­ë„ ìš”êµ¬ì‚¬í•­ í”„ë¡œì íŠ¸ ê²½í—˜ ìˆ˜ì¤€ --------- --------- ---------- ì±—ë´‡ ì„œë¹„ìŠ¤ ê°œë°œ Chatbot â­â­â­â­â­ ë°ì´í„° í’ˆì§ˆ ê²€ì¦ Chatbot + OCR â­â­â­â­â­ ë”¥ëŸ¬ë‹ ëª¨ë¸ë§ OCR + Chatbot â­â­â­â­ Python ê°œë°œ ëª¨ë“  í”„ë¡œì íŠ¸ â­â­â­â­â­ ğŸ¯ ê°•ì¡° í¬ì¸íŠ¸ AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤: ì‹¤ì œ ë³‘ì› ìƒë‹´ ì±—ë´‡ ì„œë¹„ìŠ¤ ìš´ì˜ ê²½í—˜ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬: ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œìœ¼ë¡œ í’ˆì§ˆ ê´€ë¦¬ í”„ë¡œì„¸ìŠ¤ êµ¬ì¶• í”„ë¡œë•ì…˜ ë°°í¬: Docker, AWS, ì˜¨í”„ë ˆë¯¸ìŠ¤ í™˜ê²½ì—ì„œ ì‹¤ì œ ì„œë¹„ìŠ¤ ìš´ì˜ ì„±ê³¼ ì§€í‘œ: ì‚¬ìš©ì ë§Œì¡±ë„ 90%+, ë‹µë³€ ì •í™•ë„ 95%+ ë‹¬ì„±"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-28-ë‚˜ë§ì¹˜ë©´ì–´ë–¡í•˜ì§€",
    "title": "ë‚˜ë§ì¹˜ë©´ ì–´ë–¡í•˜ì§€?",
    "date": "2025-07-28",
    "excerpt": "ë‚˜ë§ì¹˜ë©´ ì–´ë–¡í•˜ì§€?",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "--- ë°°ìš´ê±° ~í•˜ë©´ ì–´ì©Œì§€? ~ë©´ ì–´ë–»ê²Œ í• ê±°ì•¼. What if I mess up? What if the flight is delayed? What if + ê³¼ê±°í˜• -> í˜„ì¬ì™€ ì¼ì¹˜í•˜ê³  ìˆì§€ ì•Šì€ ìƒí™©ì„ ê°€ì • What if you were in Seoul? What if he weren't married? (í˜„ì¬ì™€ ë‹¤ë¥¸ ê°€ì •ì„ í•  ë•ŒëŠ” ë¬´ì¡°ê±´ wereì„ ì”€) What would you do if you were in Seoul? anxious ê°„ë‹¨í•œ ì‘ë¬¸ I think that what if questions are good to check value-thinking (ê°€ì¹˜ê´€). What if I got a lot of money? Will I work continue? It then, it means I love my job. What would I do If I were fluent in English? This imaginations make me more ëª…í™•í•˜ê²Œ. --- ìˆ˜ì •ëœ ë²„ì „ I think that \"what if\" questions are good for checking values and priorities. What if I got a lot of money? Would I continue working? If so, it means I love my job. What would I do if I were fluent in English? These imaginations help me think more clearly. --- í”¼ë“œë°± ë¬¸ë²• ìˆ˜ì •ì‚¬í•­: \"what if questions\" â†’ \"what if\" questions (ë”°ì˜´í‘œ ì¶”ê°€) \"check value-thinking\" â†’ \"check values and priorities\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"Will I work continue?\" â†’ \"Would I continue working?\" (ì¡°ê±´ë¬¸ì—ì„œëŠ” would ì‚¬ìš©) \"It then\" â†’ \"If so\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"What would I don\" â†’ \"What would I do\" (ì˜¤íƒ€ ìˆ˜ì •) \"This imaginations\" â†’ \"These imaginations\" (ë³µìˆ˜í˜•ì— ë§ëŠ” ì§€ì‹œì‚¬) \"make me more ëª…í™•í•˜ê²Œ\" â†’ \"help me think more clearly\" (ì˜ì–´ë¡œ ì™„ì „íˆ í‘œí˜„) í‘œí˜„ ê°œì„ ì‚¬í•­: \"value-thinking\" ëŒ€ì‹  \"values and priorities\" ì‚¬ìš© \"It then\" ëŒ€ì‹  \"If so\" ì‚¬ìš©í•˜ì—¬ ë” ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²° ë§ˆì§€ë§‰ ë¬¸ì¥ì„ ë” êµ¬ì²´ì ì´ê³  ëª…í™•í•˜ê²Œ í‘œí˜„ ì „ì²´ì ì¸ í”¼ë“œë°±: ì¡°ê±´ë¬¸ì„ ì‚¬ìš©í•œ ê°€ì¹˜ê´€ íƒêµ¬ë¼ëŠ” ì¢‹ì€ ì•„ì´ë””ì–´ì…ë‹ˆë‹¤! ë¬¸ë²•ì ìœ¼ë¡œ ëª‡ ê°€ì§€ ìˆ˜ì •ì´ í•„ìš”í–ˆì§€ë§Œ, í•µì‹¬ ë©”ì‹œì§€ëŠ” ì˜ ì „ë‹¬ë˜ê³  ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ \"What if\" ì§ˆë¬¸ì„ í†µí•´ ìì‹ ì˜ ì§„ì§œ ê°€ì¹˜ê´€ì„ íŒŒì•…í•˜ëŠ” ë°©ë²•ì€ ë§¤ìš° íš¨ê³¼ì ì¸ í•™ìŠµë²•ì…ë‹ˆë‹¤."
  },
  {
    "id": "Engineering/íšŒì‚¬ì—ì„œí•˜ëŠ”ê³ ë¯¼_íš¨ìœ¨ì ì¸MLêµ¬ì¡°",
    "title": "íš¨ìœ¨ì ì¸ ML êµ¬ì¡° ì„¤ê³„ëŠ” ì–´ë–»ê²Œ í•˜ëŠ”ê±¸ê¹Œ?",
    "date": "2025-07-28",
    "excerpt": "íš¨ìœ¨ì ì¸ ML êµ¬ì¡° ì„¤ê³„ì— ëŒ€í•œ ê³ ë¯¼",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design"
    ],
    "content": "ì‚¬ìˆ˜ê°€ ì—†ëŠ” íšŒì‚¬ì—ì„œ ì¼ì„ í•˜ë‹¤ë³´ë‹ˆ ì‹¤ì œë¡œ ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•˜ë©´ì„œ ë§ë‹¥ëœ¨ë¦¬ëŠ” ë¬¸ì œë“¤ì„ ì–´ë–»ê²Œ í•´ê²°í•˜ëŠ”ê²Œ ì¢‹ì€ì§€ì— ëŒ€í•œ ê³ ë¯¼ì´ ëŠ˜ ìƒê¸´ë‹¤. ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Designing Machine Learning Systems ì±…ì„ ì½ê³  ê³µë¶€í•˜ë©´ì„œ ì ìš©í•´ë³´ë ¤ê³  í•œë‹¤. ì½ì„ ì±…ë“¤ Designing Machine Learning Systems Machine Learning System Design Interview AI Engineering Designing Data-Intensive Applications íšŒì‚¬ì˜ use caseë³„ë¡œ MLì„ ì–´ë–»ê²Œ ë°°í¬í•˜ê³  ìˆëŠ”ì§€. ì•„í‚¤í…ì²˜ ì–´ë–»ê²Œ ì„¤ê³„í• ì§€. ê³ ë¯¼í–ˆë˜ ë¶€ë¶„ í¬íŠ¸í´ë¦¬ì˜¤ë¡œë„ ë…¹ì—¬ë‚´ì."
  },
  {
    "id": "Engineering/Designing Machine Learning Systems/ch3_ë°ì´í„°ì—”ì§€ë‹ˆì–´ë§ê¸°ì´ˆ",
    "title": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ",
    "date": "2025-07-28",
    "excerpt": "ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ê¸°ì´ˆ",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> í”„ë¡œë•ì…˜ìš© ML ì‹œìŠ¤í…œì„ êµ¬ì¶•í•˜ë ¤ë©´ ì¦ê°€í•˜ëŠ” ë°ì´í„°ë¥¼ ìˆ˜ì§‘, ì²˜ë¦¬, ì €ì¥, ê²€ìƒ‰ ë° ì²˜ë¦¬í•˜ëŠ” ë°©ë²™ã…‚ì„ ìˆ™ì§€í•´ì•¼ í•©ë‹ˆë‹¤. > ì‹œìŠ¤í…œ ê´€ì ì˜ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ì„ ë” ìì„¸íˆ ì•Œê³  ì‹¶ë‹¤ë©´ ë§ˆí‹´ í´ë ˆí”„ë§Œì˜ \"ë°ì´í„° ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„\"ë¥¼ ì½ì–´ë³´ê¸° ë°”ëë‹ˆë‹¤. ë°ì´í„° ì¤‘ì‹¬ ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ê³„ ë°ì´í„° ì†ŒìŠ¤ --- > ML ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ì˜¨ ë°ì´í„°ë¡œ ì‘ë™í•©ë‹ˆë‹¤. > ë°ì´í„°ë§ˆë‹¤ íŠ¹ì„±, ëª©ì , ì²˜ë¦¬ ë°©ë²•ì´ ë‹¤ë¥´ë©° ë°ì´í„° ì†ŒìŠ¤ë¥¼ íŒŒì•…í•˜ë©´ ë°ì´í„°ë¥¼ ë³´ë‹¤ íš¨ìœ¨ì ìœ¼ë¡œ ì‚¬ìš©í•˜ëŠ”ë° ë„ì›€ì´ ë©ë‹ˆë‹¤. user input data system-generated data ë°ì´í„° í¬ë§· --- JSON CSV (í–‰ ìš°ì„ ) Parquet (ì—´ ìš°ì„ ) í–‰ ìš°ì„ ì¸ ë°ì´í„°ì— í–‰ìœ¼ë¡œ ì ‘ê·¼í•˜ë©´ ë¹ ë¥´ê³ , ì—´ ìš°ì„  ë°ì´í„°ì— ì—´ë¡œ ì ‘ê·¼í•˜ë©´ ë¹ ë¥´ë‹¤. ë°ì´í„° ëª¨ë¸ --- ë°ì´í„°ê°€ ì–´ë–»ê²Œ í‘œí˜„ë˜ëŠ”ì§€ ê´€ê³„í˜• ëª¨ë¸ : > ê´€ê³„í˜• ëª¨ë¸ì—ì„œëŠ” ë°ì´í„°ëŠ” ê´€ê³„ (relation)ë¡œ êµ¬ì„±ë˜ë©° ê° ê´€ê³„ëŠ” íŠœí”Œì˜ ì§‘í•©ì…ë‹ˆë‹¤. > ê´€ê³„ëŠ” ìˆœì„œê°€ ì—†ìŠµë‹ˆë‹¤. ê´€ê³„ì—ì„œ í–‰ì˜ ìˆœì„œë‚˜ ì—´ì˜ ìˆœì„œë¥¼ ì„ë”ë¼ë„ ì—¬ì „íˆ ë™ì¼í•œ ê´€ê³„ì…ë‹ˆë‹¤. ì •ê·œí™”í•˜ëŠ” í¸ì´ ì¢‹ë‹¤. ì •ê·œí™”ë¥¼ í†µí•´ ë°ì´í„° ì¤‘ë³µì„ ì¤„ì´ê³  ë°ì´í„° ë¬´ê²°ì„±ì„ í–¥ìƒ ë‹¤ë§Œ, ë°ì´í„°ê°€ ë¶„ì‚°ë˜ì–´ ê´€ë¦¬ë˜ê³  ì¡°ì¸ ë¹„ìš© ë“±ì´ ì¦ê°€. > ê´€ê³„í˜• ë°ì´í„° ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ êµ¬ì¶•ëœ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê´€ê³„í˜• ë°ì´í„°ë² ì´ìŠ¤ë¼ê³  í•©ë‹ˆë‹¤. ì¿¼ë¦¬ ì–¸ì–´. SQL. > ì£¼ëª©í•´ì•¼ í•  ì ì€ SQLì´ ì„ ì–¸ì  ì–¸ì–´ë¼ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤. > ëª…ë ¹í˜• ì–¸ì–´ì¸ íŒŒì´ì¬ê³¼ ëŒ€ë¹„ë˜ì£ . ì„ ì–¸ì  ì–¸ì–´ (Declarative Language) -> ë¬´ì—‡ì„ ì›í•˜ëŠ”ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ì–¸ì–´ (ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œ ìµœì í™” ìˆ˜í–‰) ëª…ë ¹í˜• ì–¸ì–´ (Imperative Language) -> ì–´ë–»ê²Œ ì›í•˜ëŠ” ê²ƒì„ ë‹¬ì„±í• ì§€ë¥¼ í‘œí˜„í•˜ëŠ” ì–¸ì–´ ì„ ì–¸ì  ë°ì´í„° ì‹œìŠ¤í…œì—ì„œ ì„ ì–¸ì  ML ì‹œìŠ¤í…œìœ¼ë¡œ : > ì„ ì–¸ì  ë°ì´í„° ì‹œìŠ¤í…œì´ ì„±ê³µí•œ ë°ì„œ ì˜ê°ì„ ë°›ì€ ë§ì€ ì‚¬ëŒë“¤ì€ ì„ ì–¸ì  MLì„ ê¸°ëŒ€í•´ì™”ìŠµë‹ˆë‹¤. > ì„ ì–¸ì  ML ì‹œìŠ¤í…œì„ ì‚¬ìš©í•˜ë©´ ì‚¬ìš©ìëŠ” í”¼ì²˜ì˜ ìŠ¤í‚¤ë§ˆì™€ ì‘ì—…ë§Œ ì„ ì–¸í•˜ë©´ ë©ë‹ˆë‹¤. > ê·¸ëŸ¬ë©´ ì‹œìŠ¤í…œì€ ì£¼ì–´ì§„ í”¼ì²˜ë¡œ í•´ë‹¹ ì‘ì—…ì„ ìˆ˜í–‰í•˜ëŠ”ë° ê°€ì¥ ì í•©í•œ ëª¨ë¸ì„ ì°¾ì•„ëƒ…ã„´ë””ã…. ALudwig, AutoML. ëª¨ë¸ ê°œë°œ ë¶€ë¶„ì€ ë§ì´ ì¶”ìƒí™”ë˜ëŠ” ì¤‘. ê·¸ë ‡ê²Œ ì–´ë ¤ìš´ ì‘ì—…ì´ ì•„ë‹ˆë‹¤. NoSQL : ìŠ¤ë¯¸ì¹´ ê´€ë¦¬ì— ëŒ€í•œ ë¶ˆë§Œ. > ë¹„ê´€ê³„í˜• ëª¨ë¸ì˜ ì£¼ìš” ìœ í˜• ë‘ ê°€ì§€ëŠ” ë¬¸ì„œ ëª¨ë¸ê³¼ ê·¸ë˜í”„ ëª¨ë¸ì…ë‹ˆë‹¤. ë¬¸ì„œ ëª¨ë¸ ê·¸ë˜í”„ ëª¨ë¸ > ë°ì´í„° ëª¨ë¸ì— ë”°ë¼ ìˆ˜í–‰í•˜ê¸° ì‰¬ìš´ ì¿¼ë¦¬ê°€ ìˆê³  ì–´ë ¤ìš´ ì¿¼ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. > ë”°ë¼ì„œ ì• í”Œë¦¬ì¼€ì´ì…˜ì— ì í•©í•œ ë°ì´í„° ëª¨ë¸ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ë°”ëŒì§í•˜ì£ . ì •í˜• ë°ì´í„° vs ë¹„ì •í˜• ë°ì´í„° ì •í˜• ë°ì´í„°ëŠ” ìŠ¤í‚¤ë§ˆë¥¼ ë”°ë¥¸ë‹¤. > ì •í˜• ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì €ì¥ì†Œë¥¼ ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¼ê³  í•˜ë©° ë¹„ì •í˜• ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ” ì €ì¥ì†Œë¥¼ ë°ì´í„° ë ˆì´í¬ë¼ê³  í•©ë‹ˆë‹¤. > ë°ì´í„° ë ˆì´í¬ëŠ” ì¿ë‚˜ì ìœ¼ë¡œ ì²˜ë¦¬ ì „ ì›ì‹œ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ”ë° ì‚¬ìš©í•˜ë©° ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ëŠ” ì‚¬ìš© ê°€ëŠ¥í•œ í˜•ì‹ìœ¼ë¡œ ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ì €ì¥í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ë°ì´í„° ìŠ¤í† ë¦¬ì§€ ì—”ì§„ ë° ì²˜ë¦¬ --- íŠ¸ëœì­ì…˜ ì²˜ë¦¬ì™€ ë¶„ì„ ì²˜ë¦¬ : transaction : ì˜¨ê°– ì¢…ë¥˜ì˜ ì‘ì—… ìƒì„±ë  ë•Œ ì‚½ì…ë˜ê³  ë³€ê²½ë  ë•Œ ì—…ë°ì´íŠ¸ ë˜ë©´ í•„ìš”í•˜ì§€ ì•Šìœ¼ë©´ ì‚­ì œë¨. OLTP (Online Transaction Processing) > íŠ¸ëœì­ì…˜ ë°ì´í„°ë² ì´ìŠ¤ëŠ” ì˜¨ë¼ì¸ íŠ¸ëœì­ì…˜ì„ ì²˜ë¦¬í•˜ê³  ë‚®ì€ ë ˆì´í„´ì‹œì™€ ê³ ê°€ìš©ì„± ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ë„ë¡ ì„¤ê³„ëìŠµë‹ˆë‹¤. ACID (Atomicity, Consistency, Isolation, Durability) Atomicity(ì›ìì„±) : > íŠ¸ëœì­ì…˜ì˜ ëª¨ë“  ë‹¨ê³„ê°€ í•˜ë‚˜ì˜ ê·¸ë£¹ìœ¼ë¡œì„œ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. Consistency(ì¼ê´€ì„±) : > ë“¤ì–´ì˜¤ëŠ” ëª¨ë“  íŠ¸ëœì­ì…˜ì´ ë¯¸ë¦¬ ì •ì˜ëœ ê·œì¹™ì„ ë”°ë¼ì•¼ í•¨ì„ ë³´ì¥í•©ë‹ˆë‹¤. Isolation(ê²©ë¦¬ì„±) : > ë‘ íŠ¸ëœì­ì…˜ì´ ë§ˆì¹˜ ê²©ë¦¬ëœ ê²ƒì²˜ëŸ¼ ë™ì‹œì— ë°œìƒí•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. Durability(ì§€ì†ì„±) : > íŠ¸ëœì­ì…˜ì´ ì»¤ë°‹ëœ í›„ì—ëŠ” ì‹œìŠ¤í…œ ì¥ì• ê°€ ë°œìƒí•˜ë”ë¼ë„ ì»¤ë°‹ëœ ìƒíƒœë¥¼ ìœ ì§€í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤. íŠ¸ëœì­ì…˜ ë°ì´í„°ë² ì´ìŠ¤ëŠ” í–‰ ìš°ì„ ì¼ ë•Œê°€ ë§ìŒ -> ë¶„ì„ì— ì ì ˆí•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŒ OLAP (Online Analytical Processing) ë¶„ì„ì— ë” ì ì ˆí•œ database. ì—´ ì¡°íšŒì— ìµœì í™” ë¨. ìš”ì¦˜ì€ êµ¬ë¶„í•´ì„œ ì´ì•¼ê¸°ì•ˆí•¨. ë‘˜ ë‹¤ ì§€ì›í•˜ëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ. ETL: Extract, Transform, Load : > ETLì€ ë°ì´í„°ë¥¼ ë²”ìš© ì²˜ë¦¬ ë° ì›í•˜ëŠ” ëª¨ì–‘ê³¼ í¬ë§·ìœ¼ë¡œ ì§‘ê³„í•¨ì„ ì˜ë¯¸ <figure> <img src=\"/post/DesigningMachineLaerningSystem/ETL.webp\" alt=\"ETL Process\" /> <figcaption> ì¶œì²˜: <a href=\"https://www.getdbt.com/blog/etl-vs-elt\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.getdbt.com/blog/etl-vs-elt</a> </figcaption> </figure> ELT: Extract, Load, Transform > ë°ì´í„°ë¥¼ ë¨¼ì € ìŠ¤í† ë¦¬ì§€ì— ì ì¬í•œ ë’¤ ë‚˜ì¤‘ì— ì²˜ë¦¬í•˜ëŠ” í”„ë¡œì„¸ìŠ¤ <figure> <img src=\"/post/DesigningMachineLaerningSystem/ELT.webp\" alt=\"ELT Process\" /> <figcaption> ì¶œì²˜: <a href=\"https://www.getdbt.com/blog/etl-vs-elt\" target=\"_blank\" rel=\"noopener noreferrer\">https://www.getdbt.com/blog/etl-vs-elt</a> </figcaption> </figure> ë°ì´í„°í”Œë¡œ ëª¨ë“œ --- > ë°ì´í„°ê°€ í•œ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ë‹¤ë¡œë”œ ë•Œ ë°ì´í„°ê°€ í•œ í”„ë¡œì„¸ìŠ¤ì—ì„œ ë‹¤ë¥¸ í”„ë¡œì„¸ìŠ¤ë¡œ íë¥¸ë‹¤ê³  í•©ë‹ˆë‹¤. > ì¦‰, ë°ì´í„° í”Œë¡œê°€ ìƒê¹ë‹ˆë‹¤. ë°ì´í„°ë² ì´ìŠ¤ë¥¼ í†µí•œ ë°ì´í„° ì „ë‹¬ ì„œë¹„ìŠ¤ë¥¼ í†µí•œ ë°ì´í„° ì „ë‹¬ (REST, RPC APIì—ì„œ ì œê³µí•˜ëŠ” ìš”ì²­) ì‹¤ì‹œê°„ ì „ì†¡ì„ í†µí•œ ë°ì´í„° ì „ë‹¬ (ì•„íŒŒì¹˜ ì¹´í”„ì¹´, ì•„ë§ˆì¡´ í‚¤ë„¤ì‹œìŠ¤)"
  },
  {
    "id": "Engineering/Designing Machine Learning Systems/ch2_ë¨¸ì‹ ëŸ¬ë‹ì‹œìŠ¤í…œì„¤ê³„ì†Œê°œ",
    "title": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ì„¤ê³„ ì†Œê°œ",
    "date": "2025-07-28",
    "excerpt": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ì„¤ê³„ ì†Œê°œ",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "ML ì‹œìŠ¤í…œ ì„¤ê³„ëŠ” ì–´ë–»ê²Œ í•˜ëŠ”ê±¸ê¹Œ? MLOpsì— ì‹œìŠ¤í…œìœ¼ë¡œ ì ‘ê·¼ > ì‹œìŠ¤í…œì„ ë¹„ì¦ˆë‹ˆìŠ¤ìš©ìœ¼ë¡œ ê°œë°œí•œë‹¤ë©´ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì— ë”°ë¼ ì—…ë¬´ë¥¼ ì¶”ì§„í•´ì•¼ í•˜ë©°, ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì€ ë‹¤ì‹œ ML ëª©ì ìœ¼ë¡œ ë°”ê¿”ì„œ ML ëª¨ë¸ ê°œë°œì˜ ì´ì •í‘œë¡œ ì‚¼ì•„ì•¼ í•©ë‹ˆë‹¤. ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í•˜ëŠ” ë¶€ë¶„. ë¹„ì¦ˆë‹ˆìŠ¤ ëª©ì ì„ ì´í•´í•˜ê¸° ì´ì— ë§ê²Œ ML ëª©ì ì„ ì„¸ìš°ëŠ” ê³¼ì •. ì‹ ë¢°ì„±, í™•ì¥ì„±, ìœ ì§€ë³´ìˆ˜ì„±, ì ì‘ì„± ë¹„ì¦ˆë‹ˆìŠ¤ì™€ ë¨¸ì‹ ëŸ¬ë‹ì˜ ëª©ì  --- > ì‚¬ì‹¤ ê¸°ì—…ë“¤ì€ ëŒ€ë¶€ë¶„ ì´ëŸ° í™”ë ¤í•œ ML ì§€í‘œì— í¬ê²Œ ê´€ì‹¬ì´ ì—†ìŠµë‹ˆë‹¤. > ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œì— ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” í•œ ëª¨ë¸ ì •í™•ë„ë¥¼ 94%ì—ì„œ 94.2%ë¡œ ë†’ì´ëŠ” ì¼ ë”°ìœ„ì— ì‹ ê²½ì“°ì§€ ì•Šì£ . > ë¹„ì¦ˆë‹ˆìŠ¤ ì¡°ì§ ë‚´ì—ì„œ ML í”„ë¡œì íŠ¸ê°€ ì„±ê³µí•˜ë ¤ë©´ ML ì‹œìŠ´í…œ ì„±ê³¼ë¥¼ ì „ì²´ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ì™€ ì—°ê²°í•´ì•¼ í•©ë‹ˆë‹¤. > ì‹ ê·œ ML ì‹œìŠ¤í…œì€ ê´‘ê³  ìˆ˜ìµ, ì›”ê°„ í™œì„± ì‚¬ìš©ì ìˆ˜ ë“± ì–´ë–¤ ë¹„ì¦ˆë‹ˆìŠ¤ ì„±ê³¼ ì§€í‘œì— ì˜í–¥ì„ ë¯¸ì¹ ê¹Œìš”? > ë§ì€ íšŒì‚¬ì—ì„œ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œë¥¼ ML ì§€í‘œì— ì—°ê²°í•˜ê¸° ìœ„í•´ ìì²´ ì§€í‘œë¥¼ ë§Œë“­ë‹ˆë‹¤. ë„·í”Œë¦­ìŠ¤ - ì¶”ì²œì‹œìŠ¤í…œ. ì±„íƒë¥  (take-rate) ì˜ìƒ ì¬ìƒ íšŸìˆ˜ / ì‚¬ìš©ìê°€ ë³¸ ì¶”ì²œ ì¸ë‚´ì¼ ê°œìˆ˜ ì¶”ì²œì‹œìŠ¤í…œì˜ ì±„íƒë¥ ì„ ë‹¤ë¥¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì§€í‘œ, ì˜ˆì»¨ëŒ€ ì´ ìŠ¤íŠ¸ë¦¬ë° ì‹œê°„ì´ë‚˜ êµ¬ë… ì·¨ì†Œìœ¨ì˜ ë§¥ë½ì—ì„œë„ ê³ ë ¤. ì‹¤ì œ ë¹„ì¦ˆë‹ˆìŠ¤ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•œ ì‹œë„ë“¤. -> ë„·í”Œë¦­ìŠ¤ëŠ” ìƒˆë¡œìš´ í‰ê°€ì§€í‘œë¥¼ ê³„ì‚° ì§ì ‘ì ìœ¼ë¡œ íŒŒì•…í•˜ëŠ”ê±´ ê½¤ë‚˜ ì–´ë µê³  ë§ì€ ì‹¤í—˜ì´ í•„ìš”í•¨ ëŒ€ì•ˆìœ¼ë¡œ A/B testë¥¼ ì§„í–‰í•´ ì–´ë–»ê²Œ ì—°ê´€ë˜ëŠ”ì§€ëŠ” ëª¨ë¥´ì§€ë§Œ, ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ ê´€ì ì—ì„œë§Œ ë¶„ì„í•´ë³¼ ìˆ˜ë„ ìˆìŒ > ë§ì€ ê¸°ì—…ì—ì„œ 'AI ê¸°ë°˜'ì´ë¼ëŠ” ë¬¸êµ¬ê°€ ê³ ê° ìœ ì¹˜ì— ë„ì›€ì´ ë˜ê¸° ë•Œë¬¸ì— MLì„ ì‚¬ìš©í•œë‹¤ê³  ì´ì•¼ê¸°í•©ë‹ˆë‹¤. > AIê°€ ì‹¤ì œë¡œ ìœ ìš©í•œì§€ëŠ” ìƒê´€ì—†ì´ ë§ì´ì£ . MLì´ ë§ˆë²•ê°™ì€ ë³€í™”ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ëŠ” ìˆì§€ë§Œ, í•˜ë£¨ì•„ì¹¨ì— ë˜ëŠ”ê±´ ì ˆëŒ€ ì•„ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ì ìœ¼ë¡œ MLì´ ê°€ì ¸ì˜¬ ìˆ˜ìµì— ëŒ€í•´ì„œ í˜„ì‹¤ì ìœ¼ë¡œ ìƒê°í•´ì•¼ í•¨. > ML ê´€ë ¨í•œ íˆ¬ì ìˆ˜ìµì€ ë„ì… ì´í›„ ì„±ìˆ™ ë‹¨ê³„ì— ë‹¤ë‹¤ë¼ì„œì•¼ ëˆˆì— ë„ê¸° ì‹œì‘í•©ë‹ˆë‹¤. > MLì„ ë„ì…í•œ ê¸°ê°„ì´ ê¸¸ìˆ˜ë¡ íŒŒì´í”„ë¼ì¸ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì‹¤í–‰ë˜ê³  ê°œë°œ ì£¼ê¸°ê°€ ë¹¨ë¼ì§€ë©° ì—”ì§€ë‹ˆì–´ë§ ì‹œê°„ê³¼ í´ë¼ìš°ë“œ ë¹„ìš©ì´ ì¤„ì–´ë“¤ì–´ ìˆ˜ìµì´ ë†’ì•„ì§€ê²Œ ë©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ìš”êµ¬ ì‚¬í•­ --- ML ì‹œìŠ¤í…œì´ ê°–ì¶°ì•¼ í•˜ëŠ” ìš”êµ¬ì‚¬í•­ ì‹ ë¢°ì„± : > ML ì‹œìŠ¤í…œì€ 'ì‹œìŠ¤í…œ ë™ì‘ì˜ ì˜¬ë°”ë¦„'ì„ íŒë‹¨í•˜ê¸° ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ì‹¤ì œ ìƒí™©ì—ì„œëŠ” ì •ë‹µì´ ì—†ëŠ”ë°, í‹€ë ¸ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì–´ë–»ê²Œ íŒŒì•…í•  ìˆ˜ ìˆì„ê¹Œ? í™•ì¥ì„± : > ML ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤. > ì‹œìŠ¤í…œ ë³µì¡ë„ê°€ ì¦ê°€í•˜ë“ , íŠ¸ë˜í”½ ì–‘ì´ ì¦ê°€í•˜ë“ , ML ëª¨ë¸ ê°œìˆ˜ê°€ ì¦ê°€í•˜ë“  ìƒê´€ì—†ì´ ê·œëª¨ ì¦ê°€ë¥¼ ì²˜ë¦¬í•  í•©ë¦¬ì ì¸ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. resource scaling. artfiact managment. ìœ ì§€ë³´ìˆ˜ì„± : > ì›Œí¬ë¡œë“œë¥¼ êµ¬ì¡°í™”í•˜ê³  ì¸í”„ë¼ë¥¼ ì„¤ì •í•˜ëŠ” ì¼ì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì ì‘ì„± : > ì‹œìŠ¤í…œì€ ë³€í™”í•˜ëŠ” ë°ì´í„° ë¶„í¬ì™€ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ì‚¬í•­ì— ì ì‘í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. > ê·¸ëŸ¬ë ¤ë©´ ì„±ëŠ¥ í–¥ìƒì— ì˜í–¥ì„ ì£¼ëŠ” ìš”ì†Œë¥¼ ì°¾ì•„ë‚¼ ìˆ˜ ìˆì–´ì•¼ í•˜ë©° ì„œë¹„ìŠ¤ ì¤‘ë‹¨ ì—†ì´ ì—…ë°ì´íŠ¸ê°€ ê°€ëŠ¥í•´ì•¼ í•©ë‹ˆë‹¤. > ML ì‹œìŠ¤í…œì€ ë°˜ì€ ì½”ë“œì´ê³  ë°˜ì€ ë°ì´í„°ì…ë‹ˆë‹¤. > ë°ì´í„°ëŠ” ë¹ ë¥´ê²Œ ë³€í™”í•˜ë¯€ë¡œ ML ì‹œìŠ¤í…œ ë˜í•œ ìì²´ì ìœ¼ë¡œ ë¹ ë¥´ê²Œ ì§„í™”í•  ìˆ˜ ìˆì–´ì•¼ í•˜ì£ . ë°˜ë³µ í”„ë¡œì„¸ìŠ¤ --- > ML ì‹œìŠ¤í…œ ê°œë°œì€ ë°˜ë³µì ì´ë©° ëŒ€ë¶€ë¶„ ëì´ ì—†ëŠ” í”„ë¡œì„¸ìŠ¤ì…ë‹ˆë‹¤. > ì‹œìŠ¤í…œì„ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•˜ë©´ ì§€ì†ì ìœ¼ë¡œ ëª¨ë‹ˆí„°ë§í•˜ê³  ì—…ë°ì´íŠ¸í•´ì•¼ í•©ë‹ˆë‹¤. í”„ë¡œì íŠ¸ ë²”ìœ„ ì‚°ì • ë²”ìœ„ë¥¼ ì‚°ì •í•˜ê³  ëª©í‘œ, ëª©ì ê³¼ ì œì•½ì‚¬í•­ì„ ì„¤ì •í•˜ëŠ” ì¼ ë°ì´í„° ì—”ì§€ë‹ˆì–´ë§ ML ëª¨ë¸ ê°œë°œ ë°°í¬ > ML ì‹œìŠ¤í…œ ê°œë°œì€ ê¸€ì“°ê¸°ì™€ ê°™ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì€ ê²°ì½” ì™„ë²½íˆ ì™„ì„±ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. > í•˜ì§€ë§Œ ì–´ëŠ ì‹œì ì—ëŠ” ì‹œìŠ¤í…œì„ ì™¸ë¶€ì— ë‚´ë³´ì—¬ì•¼ í•©ë‹ˆë‹¤. ëª¨ë‹ˆí„°ë§ê³¼ ì—°ì† í•™ìŠµ > ëª¨ë¸ì„ í”„ë¡œë•ì…˜ í™˜ê²½ì— ë°°í¬í•œ ë’¤ì—ëŠ” ì§€ì†ì ìœ¼ë¡œ ì„±ëŠ¥ ì €í•˜ë¥¼ ëª¨ë‹ˆí„°ë§í•˜ê³  ë³€í™”í•˜ëŠ” í™˜ê²½ê³¼ ìš”êµ¬ ì‚¬í•­ì— ì ì‘í•˜ë„ë¡ ìœ ì§€ ê´€ë¦¬í•´ì•¼ í•©ë‹ˆë‹¤. ë¹„ì¦ˆë‹ˆìŠ¤ ë¶„ì„ > ëª¨ë¸ ì„±ëŠ¥ì„ ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œ ê´€ì ì—ì„œ ë¶„ì„í•´ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œ êµ¬ì¡°í™”í•˜ê¸° --- > ê³ ê° ì§€ì› ì†ë„ê°€ ëŠë¦¬ë‹¤ëŠ” ì ì€ ë¬¸ì œì§€ë§Œ MLë¡œ ë°”ë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œëŠ” ì•„ë‹™ë‹ˆë‹¤. > ML ë¬¸ì œëŠ” ì…ë ¥ê³¼ ì¶œë ¥, í•™ìŠµ í”„ë¡œì„¸ìŠ¤ë¥¼ ì´ëŒì–´ë‚˜ê°€ëŠ” ëª©ì  í•¨ìˆ˜ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ë¶„ë¥˜ vs íšŒê·€ ì´ì§„ë¶„ë¥˜ vs ë‹¤ì¤‘í´ë˜ìŠ¤ ë¶„ë¥˜ ë‹¤ì¤‘ í´ë˜ìŠ¤ vs ë‹¤ì¤‘ ë ˆì´ë¸” ë¶„ë¥˜ ë¬¸ì œë¥¼ êµ¬ì¡°í™”í•˜ëŠ” ë‹¤ì–‘í•œ ë°©ë²• : ë¬¸ì œ: ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ì—´ì–´ë³¼ ì•± ì˜ˆì¸¡í•˜ê¸° êµ¬ì¡°í™” ë°©ë²•: ë¶„ë¥˜ -> ì•±ì´ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ì¬í•™ìŠµì„ í•´ì•¼í•˜ëŠ” ì´ìŠˆ ë¬¸ì œ: ì‚¬ìš©ìê°€ ë‹¤ìŒìœ¼ë¡œ ì—´ì–´ë³¼ ì•± ì˜ˆì¸¡í•˜ê¸° êµ¬ì¡°í™” ë°©ë²•: íšŒê·€ ì…ë ¥ìœ¼ë¡œ ì•±ê´€ë ¨ í”¼ì²˜ë¥¼ ë„£ì–´ì£¼ê³  ì—´ì–´ë³¼ í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¡œ ì •ì˜í•˜ë©´, ë‹¤ì‹œ í•™ìŠµí•´ì•¼í•˜ëŠ” ì†Œìš”ê°€ ì¤„ì–´ë“¬ ì§€ì„± (mind) vs ë°ì´í„° (data) --- > ë°ì´í„°ë³´ë‹¤ ì§€ì„±ì´ ìœ„ìœ„ë¼ê³  ìƒê°í•˜ëŠ” ì§„ì˜ì—ëŠ” íŠœë§ìƒ ìˆ˜ìƒìì¸ ì£¼ë°ì•„ í„ ë°•ì‚¬ê°€ ìˆìŠµë‹ˆë‹¤. > í„ì€ ì¸ê³¼ ê´€ê³„ ì¶”ë¡ ê³¼ ë² ì´ì¦ˆ ë„¤íŠ¸ì›Œí¬ì— ëŒ€í•œ ì—°êµ¬ë¡œ ì˜ ì•Œë ¤ì¡Œìœ¼ë©° ê·¸ì˜ ì €ì„œ ì¸ê³¼ì„±ì— ëŒ€í•´ì— ì‹¤ë¦° ì„œë¬¸ ì œëª©ì´ 'ë°ì´í„°ë³´ë‹¤ ìš°ìœ„ì¸ ì§€ì„±'ì…ë‹ˆë‹¤."
  },
  {
    "id": "Engineering/Designing Machine Learning Systems/ch1_ë¨¸ì‹ ëŸ¬ë‹ì‹œìŠ¤í…œê°œìš”",
    "title": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ê°œìš”",
    "date": "2025-07-28",
    "excerpt": "ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ ê°œìš”",
    "category": "Engineering",
    "tags": [
      "MLOps",
      "System Design",
      "Designing Machine Learning Systems"
    ],
    "content": "> 'ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ'ì´ë¼ê³  í•˜ë©´ ë§ì€ ì‚¬ëŒì´ ë¡œì§€ìŠ¤í‹± íšŒê·€ë‚˜ ë‹¤ì–‘í•œ ìœ í˜•ì˜ ì‹ ê²½ë§ ë“± ML ì•Œê³ ë¦¬ì¦˜ë§Œ ë– ì˜¬ë¦½ë‹ˆë‹¤. > ê·¸ëŸ¬ë‚˜ í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì•Œê³ ë¦¬ì¦˜ì€ ML ì‹œìŠ¤í…œì˜ ì¼ë¶€ì¼ ë¿ì…ë‹ˆë‹¤. > ì‹œìŠ¤í…œì€ ML í”„ë¡œì íŠ¸ì˜ ì¶œë°œì ì´ ëœ ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ ì‚¬í•­, ì‚¬ìš©ìì™€ ê°œë°œìê°€ ì‹œìŠ¤í…œê³¼ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ì¸í„°í˜ì´ìŠ¤, ë°ì´í„° ìŠ¤íƒ, ëª¨ë¸ì„ ê°œë°œ ë° ëª¨ë‹ˆí„°ë§í•˜ê³  ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ë¡œì§ì€ ë¬¼ë¡  í•´ë‹¹ ë¡œì§ì„ ì „ë‹¬í•  ìˆ˜ ìˆëŠ” ì¸í”„ë¼ë¥¼ í¬í•¨í•©ë‹ˆë‹¤. MLì„ ì‚¬ìš©í•´ì•¼í•˜ëŠ” ê²½ìš° (use case)ëŠ” ì–¸ì œì¸ê°€? ì‚¬ìš©í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ê²ƒë“¤ì€? ì—°êµ¬ì™€ í”„ë¡œë•ì…˜ ìƒí™©ì—ì„œ ë¨¸ì‹ ëŸ¬ë‹ì„ ê°œë°œí•˜ëŠ”ê²Œ ì–´ë–¤ ì°¨ì´ê°€ ìˆëŠ”ì§€ì— ëŒ€í•œ ë‚´ìš©ë“¤. ì—°êµ¬ëŠ” ì—°êµ¬ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°ì— ëª©í‘œê°€ ìˆì§€ë§Œ, ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•  ë•ŒëŠ” ì´í•´ê´€ê³„ë¥¼ ë§Œì¡±í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ëª©í‘œì— ë§ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì´ ëª©í‘œ. > ML ì‹œìŠ¤í…œì€ ë‹¤ì–‘í•œ ìš”ì†Œë¡œ êµ¬ì„±ëœ ë³µì¡í•œ ì‹œìŠ¤í…œì…ë‹ˆë‹¤. > í”„ë¡œë•ì…˜ì—ì„œ ML ì‹œìŠ¤í…œì„ ì‘ì—…í•˜ëŠ” ë°ì´í„° ê³¼í•™ìì™€ ML ì—”ì§€ë‹ˆì–´ëŠ” ML ì•Œê³ ë¦¬ì¦˜ì—ë§Œ ì§‘ì¤‘í•˜ëŠ” ê±¸ë¡œëŠ” ì ˆëŒ€ ì¶©ë¶„ì¹˜ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¹¨ë‹¬ì„ ê²ë‹ˆë‹¤. > ì•Œê³ ë¦¬ì¦˜ ì™¸ì— ì‹œìŠ¤í…œì˜ ë‹¤ë¥¸ ì¸¡ë©´, ì˜ˆì»¨ëŒ€ ë°ì´í„° ìŠ¤íƒ, ë°°í¬, ëª¨ë‹ˆí„°ë§, ìœ ì§€ ê´€ë¦¬, ì¸í”„ë¼ì— ê´€í•´ ì•„ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‚´ê°€ ì„œë¹„ìŠ¤ë¥¼ êµ¬ì„±í•œ í›„ ì´ë¥¼ ì„¤ëª…í•  ë•Œë„ ë‚´ê°€ ë””ìì¸í•œ ì‹œìŠ¤í…œì— ëŒ€í•´ì„œ, ì´ëŸ° ë¶€ë¶„ì„ ê³ ë ¤í•´ ì„¤ëª…í•  í•„ìš”ê°€ ìˆê² ë‹¤. ì—°êµ¬ìš© ë¨¸ì‹ ëŸ¬ë‹ vs í”„ë¡œë•ì…˜ìš© ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬ìš© ML í”„ë¡œë•ì…˜ìš© ML --- --- --- ìš”êµ¬ì‚¬í•­ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ì—ì„œ ìµœì ì˜ ëª¨ë¸ ì„±ëŠ¥ ë‹¬ì„±í•˜ê¸° ì´í•´ê´€ê³„ìë§ˆë‹¤ ë‹¤ë¦„ ê³„ì‚° ìš°ì„ ìˆœìœ„ ë¹ ë¥¸ í›ˆë ¨, ë†’ì€ ìŠ¤ë£¨í’‹ ë¹ ë¥¸ ì¶”ë¡ , ë‚®ì€ ë ˆì´í„´ì„¸ ë°ì´í„° ì •ì ì„ ëŠì„ì—†ì´ ë³€ë™í•¨ ê³µì •ì„± ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŒ ë°˜ë“œì‹œ ê³ ë ¤í•´ì•¼ í•¨ í•´ì„ ê°€ëŠ¥ì„± ì¤‘ìš”í•˜ì§€ ì•Šì€ ê²½ìš°ê°€ ë§ìŒ ë°˜ë“œì‹œ ê³ ë ¤í•´ì•¼ í•¨ ë‹¤ì–‘í•œ ì´í•´ê´€ê³„ìì™€ ìš”êµ¬ì‚¬í•­ : > ML ì‹œìŠ¤í…œì„ í”„ë¡œë•ì…˜ì— ì ìš©í•˜ëŠ” ê³¼ì •ì—ëŠ” ë§ì€ ì´í•´ê´€ê³„ìê°€ ì–½í˜€ ìˆìœ¼ë©° ê°ê° ìš”êµ¬ ì‚¬í•­ì´ ë‹¤ë¦…ë‹ˆë‹¤. > ìš”êµ¬ ì‚¬í•­ë“¤ì€ ì„œë¡œ ì¶©ëŒí•  ë•Œë„ ë§ìœ¼ë¯€ë¡œ ëª¨ë“  ìš”êµ¬ ì‚¬í•­ì„ ì¶©ì¡±í•˜ëŠ” ML ëª¨ë¸ì„ ì„¤ê³„, ê°œë°œ, ì„ íƒí•˜ê¸°ëŠ” ì–´ë ¤ìš¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ìš©ì ëª©í‘œë¥¼ ì´í•´í•˜ê³  ëª©í‘œê°€ ì—¬ëŸ¬ê°œë¼ë©´ ìª¼ê°œì„œ ê°ê°ì˜ ëª¨ë¸ì„ ë§Œë“¤ í•„ìš”ê°€ ìˆë‹¤. ì‚¬ìš©ì ëª©í‘œë¥¼ ë§Œì¡±í•˜ì§€ë§Œ ëª¨ë¸ì˜ ëª©í‘œê°€ ë‹¤ë¥¼ ìˆ˜ë„ ìˆê³ . > ML í”„ë¡œì íŠ¸ë¥¼ ê°œë°œí•  ë•Œ ML ì—”ì§€ë‹ˆì–´ëŠ” ëª¨ë“  ì´í•´ê´€ê³„ìì˜ ìš”êµ¬ ì‚¬í•­ì„ íŒŒì•…í•˜ê³  ê°ê°ì´ ì–¼ë§ˆë‚˜ ì—„ê²©í•œì§€ ì´í•´í•´ì•¼ í•©ë‹ˆë‹¤. > í”„ë¡œë•ì…˜ìš© MLì€ ì—°êµ¬ìš© MLê³¼ ìš”êµ¬ì‚¬í•­ì´ ë‹¤ë¦…ë‹ˆë‹¤. ê³„ì‚° ìš°ì„ ìˆœìœ„ : > ML ì‹œìŠ¤í…œì„ ë°°í¬í•œ ê²½í—˜ì´ ì—†ëŠ” ë¶„ë“¤ì€ ì‹œìŠ¤í…œì„ ì„¤ê³„í•  ë•Œ ì¢…ì¢… ëª¨ë¸ ê°œë°œì—ë§Œ ë„ˆë¬´ ì§‘ì¤‘í•˜ê³  ë°°í¬ì™€ ìœ ì§€ ê´€ë¦¬ë¥¼ ì†Œí™€íˆ í•˜ëŠ” ì‹¤ìˆ˜ë¥¼ í•©ë‹ˆë‹¤. ì—°êµ¬ì—ì„œëŠ” í•™ìŠµë‹¨ê³„ê°€ ë³‘ëª©ì´ì§€ë§Œ ë°°í¬ ë‹¨ê³„ì—ì„œëŠ” ì¶”ë¡ ì´ ë³‘ëª©ì´ë‹¤. > í•œê°€ì§€ ê²°ë¡ ì€ ì—°êµ¬ì—ì„œëŠ” ë†’ì€ ìŠ¤ë£¨í’‹ (throughput)ì„ ìš°ì„ ì‹œ í•˜ëŠ” ë°˜ë©´ì— í”„ë¡œë•ì…˜ì—ì„œëŠ” ë‚®ì€ ë ˆì´í„´ì‹œë¥¼ ìš°ì„ ì‹œí•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. > ë ˆì´í„´ì‹œëŠ” ì¿¼ë¦¬ë¥¼ ìˆ˜ì‹ í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ì˜ë¯¸í•˜ë©° ìŠ¤ë£¨í’‹ì€ íŠ¹ì • ê¸°ê°„ ë‚´ì— ì²˜ë¦¬ëœ ì¿¼ë¦¬ ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ìŠ¤ë£¨í’‹(Throughput): íŠ¹ì • ì‹œê°„ ë‚´ì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‘ì—…ì˜ ì´ëŸ‰ ë ˆì´í„´ì‹œ(Latency): í•˜ë‚˜ì˜ ìš”ì²­ì„ ì²˜ë¦¬í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ > ì‹¤í—˜ ë‹¨ê³„ì—ì„œ ëŒ€ëŸ‰ì˜ ì‹¤í—˜ì´ í•„ìš”í•¨. ``markdown ì—°êµ¬ ì‹œë‚˜ë¦¬ì˜¤: í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: 1000ê°œ ì¡°í•© ì‹œë„ ëª¨ë¸ ì•„í‚¤í…ì²˜ ì‹¤í—˜: 50ê°€ì§€ êµ¬ì¡° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¦ê°• ì‹¤í—˜: 10ê°€ì§€ ë°©ë²• ë¹„êµ ëª©í‘œ: ê°€ëŠ¥í•œ ë§ì€ ì‹¤í—˜ì„ ë¹ ë¥´ê²Œ ì™„ë£Œ â†’ ìŠ¤ë£¨í’‹ì´ ì¤‘ìš” (ì‹œê°„ë‹¹ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì‹¤í—˜ ìˆ˜) ` > í”„ë¡œë•ì…˜ì—ì„œëŠ” ì‚¬ìš©ì ê²½í—˜ì„ ê°œì„ í•˜ê³  ë¹„ì¦ˆë‹ˆìŠ¤ ì„íŒ©íŠ¸ë¥¼ ì¤˜ì•¼í•¨. `markdown ì‹¤ì‹œê°„ ì„œë¹„ìŠ¤ ì‹œë‚˜ë¦¬ì˜¤: ì¶”ì²œ ì‹œìŠ¤í…œ: ì‚¬ìš©ìê°€ í˜ì´ì§€ ë¡œë“œ í›„ 100ms ë‚´ì— ì¶”ì²œ ì œê³µ ì±—ë´‡: ì‚¬ìš©ì ë©”ì‹œì§€ì— 2ì´ˆ ë‚´ì— ì‘ë‹µ ììœ¨ì£¼í–‰: ì„¼ì„œ ë°ì´í„°ë¥¼ 50ms ë‚´ì— ì²˜ë¦¬ ëª©í‘œ: ê°œë³„ ìš”ì²­ì„ ë¹ ë¥´ê²Œ ì²˜ë¦¬ â†’ ë ˆì´í„´ì‹œê°€ ì¤‘ìš” (ì‘ë‹µ ì‹œê°„) ì˜ˆì‹œ: ì´ì»¤ë¨¸ìŠ¤ ì¶”ì²œ ì‹œìŠ¤í…œ ë ˆì´í„´ì‹œ 100ms â†’ ì „í™˜ìœ¨ 5% ë ˆì´í„´ì‹œ 500ms â†’ ì „í™˜ìœ¨ 3% (40% ê°ì†Œ) ë ˆì´í„´ì‹œ 1ì´ˆ â†’ ì „í™˜ìœ¨ 1% (80% ê°ì†Œ) ê²°ê³¼: 1ì´ˆ ì§€ì—° = ë§¤ì¶œ 80% ê°ì†Œ `` > ë ˆì´í„´ì‹œëŠ” ê°œë³„ ìˆ˜ì¹˜ê°€ ì•„ë‹Œ ë¶„í¬ì„ì„ ê¸°ì–µí•©ì‹œë‹¤. ë°ì´í„° : > í”„ë¡œë•ì…˜ ë‹¨ê³„ì˜ ë°ì´í„°ëŠ” í›¨ì”¬ ë” ë³µì¡í•©ë‹ˆë‹¤. ì¡ìŒì´ ë§ê³  ë¹„ì •í˜•ì¼ ìˆ˜ ìˆìœ¼ë©° ëŠì„ì—†ì´ ë³€í™”í•˜ì£ . > ê²Œë‹¤ê°€ ë°ì´í„°ê°€ í¸í–¥ëì„ ìˆ˜ ìˆëŠ”ë° ì–´ë–»ê²Œ í¸í–¥ëëŠ”ì§€ ëª¨ë¥¼ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. ì§„ì§œ raw dataë¶€í„° ë‹¤ë¤„ë³¸ ê²½í—˜ì´ ê½¤ ë§ì€ë°, ë‹¤ ì§„ì§œ ë„ˆë¬´ ë”ëŸ¬ì› ë‹¤ ã….ã….. ê³µì •ì„± : > ì—¬ëŸ¬ë¶„ í˜¹ì€ ì£¼ë³€ì— ìˆëŠ” ëˆ„êµ°ê°€ëŠ” ìì‹ ë„ ëª¨ë¥´ëŠ” ì‚¬ì´ì— í¸í–¥ëœ ìˆ˜í•™ ì•Œê³ ë¦¬ì¦˜ì˜ í¬ìƒìì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•´ì„ ê°€ëŠ¥ì„± : > ì‚°ì—…ê³„ ëŒ€ë¶€ë¶„ì˜ ML ìœ ìŠ¤ ì¼€ì´ìŠ¤ì—ì„œ í•´ì„ ê°€ëŠ¥ì„±ì€ ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ ì‹œìŠ¤í…œ vs ì „í†µì ì¸ ì†Œí”„íŠ¸ì›¨ì–´ > ê¸°ì¡´ SWEì—ì„œëŠ” ì½”ë“œ í…ŒìŠ¤íŠ¸ ë° ë²„ì „ ê´€ë¦¬ì—ë§Œ ì§‘ì¤‘í•˜ë©´ ë˜ì§€ë§Œ MLì„ ì‚¬ìš©í•˜ë©´ ë°ì´í„° ë˜í•œ í…ŒìŠ¤íŠ¸í•˜ê³  ë²„ì „ì„ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤. > í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì´ëŸ¬í•œ ëª¨ë¸ì„ ëª¨ë‹ˆí„°ë§í•˜ê³  ë””ë²„ê¹…í•˜ëŠ” ì¼ ë˜í•œ ì‚¬ì†Œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. > ML ëª¨ë¸ì´ ë³µì¡í•´ì§€ê³  ì‘ì—…ì— ëŒ€í•œ ê°€ì‹œì„±ì´ ë–¨ì–´ì§ì— ë”°ë¼ ë¬´ì—‡ì´ ì˜ëª»ëëŠ”ì§€ íŒŒì•…í•˜ê±°ë‚˜ ë¬¸ì œê°€ ë°œìƒí–ˆì„ ë–„ ì‹ ì†í•˜ê²Œ ê²½ê³ ë¥¼ ë°›ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤."
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-27-ì„ íƒœì˜ìƒí™©ì—ì„œ_ReviewDay",
    "title": "ì„ íƒì˜ ìƒí™©ì—ì„œ ì“°ëŠ” í‘œí˜„ë“¤ Review Day",
    "date": "2025-07-27",
    "excerpt": "ì„ íƒì˜ ìƒí™©ì—ì„œ",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ One one hand One the other hand On one hand the note has really big screen. On the other the iPhone has a really good camera. On one hand, I havent had chinees food in a while. On the other hand, I'm craving fried chicken. Whether we eat Chinese food or fried chicken, we should order dilivery. The best thing about iPhone is its design. When it comes to online shopping, Amazon might be the best option. Although its a bit pricey, the bar has amazing view at night. Even though i`m not a big fan of animated films, I love Frozen. The dress is too expensive. It's pretty though. It's not about design. It's about performance. In which case, I'll probably go with the new Galaxy."
  },
  {
    "id": "Guitar/Practice Log/20250726_ì¢‹ì€ë°¤ì¢‹ì€ê¿ˆ",
    "title": "ì¢‹ì€ë°¤ ì¢‹ì€ê¿ˆ ì¸íŠ¸ë¡œ ì—°ìŠµ",
    "date": "2025-07-26",
    "excerpt": "ì¢‹ì€ë°¤ ì¢‹ì€ê¿ˆ ì¸íŠ¸ë¡œ ë¶€ë¶„ í•‘ê±°ìŠ¤íƒ€ì¼ ì•„ë¥´í˜ì§€ì˜¤ ì—°ìŠµ",
    "category": "Practice Log",
    "tags": [
      "ê¸°íƒ€ì—°ìŠµ",
      "Guitar Practice",
      "Arpeggio"
    ],
    "content": "@audioì¢‹ì€ë°¤ ì¢‹ì€ê¿ˆ ì¸íŠ¸ë¡œ ì—°ìŠµ ë­”ê°€ ë§›ì´ ì—†ë‹¤ ã…‹ã…‹ Fë‘ Fmê°€ ì™„ë²½í•˜ê²Œ ì•ˆëˆ„ë¦¼ íŠ¹íˆ 3ë²ˆì¤„ì´."
  },
  {
    "id": "Guitar/Practice Log/20250726_ê³ ë°±_ìŠ¤íŠ¸ë¡œí¬",
    "title": "ë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°± ìŠ¤íŠ¸ë¡œí¬ ì—°ìŠµ",
    "date": "2025-07-26",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [
      "ê¸°íƒ€ì—°ìŠµ",
      "Guitar Practice",
      "Strokes"
    ],
    "content": "@audioë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°± ìŠ¤íŠ¸ë¡œí¬ ì—°ìŠµ C#7ì´ ì˜ ì•ˆì¡í˜. íŠ¹íˆ G#m -> C#7ì´ ì˜ ì•ˆë¨. ê¸°íƒ€ í†¤ì€ ë‚«ë°°ë“œí•œ ë“¯? AC clean + front pickup + ì‹±ì‹±ì‹±"
  },
  {
    "id": "Guitar/Practice Log/20250726_love",
    "title": "ì›¨ì´ë¸Œ íˆ¬ì–´ìŠ¤ ì‚¬ë‘ìœ¼ë¡œ ì¸íŠ¸ë¡œ ì»¤ë²„",
    "date": "2025-07-26",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [
      "ê¸°íƒ€ì—°ìŠµ",
      "Guitar Practice"
    ],
    "content": "@audioì›¨ì´ë¸Œ íˆ¬ì–´ìŠ¤ ì‚¬ë‘ìœ¼ë¡œ ì¸íŠ¸ë¡œ ì»¤ë²„ í™•ì‹¤íˆ ë…¹ìŒí•´ì„œ ë“¤ì–´ë³´ë‹ˆê¹Œ ëŠê¸°ëŠ” ë¶€ë¶„ì´ ë§ì´ ë“¤ë¦¼. ë°•ì ë‚˜ê°€ëŠ” ê²ƒë„ ë³´ì´ê³ ."
  },
  {
    "id": "Data Science/vDB_code_notebook",
    "title": "vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤",
    "date": "2025-07-25",
    "excerpt": "vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤",
    "category": "Data Science",
    "tags": [
      "VectorDB",
      "Qdrant",
      "ChromaDB",
      "FAISS",
      "Milvus"
    ],
    "content": "--- title: \"vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤\" date: \"2025-07-25\" category: \"Data Science\" tags: [\"VectorDB\", \"Qdrant\", \"ChromaDB\", \"FAISS\", \"Milvus\"] excerpt: \"vDBê´€ë ¨ ìƒ˜í”Œ ì½”ë“œë“¤\" --- # Qdrant íŒŒì´ì¬ í´ë¼ì´ì–¸íŠ¸ ì„¤ì¹˜ í•„ìš” pip install qdrant-client from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np Qdrant ì„œë²„ì— ì—°ê²° (ë¡œì»¬ ì„œë²„ ì˜ˆì‹œ) client = QdrantClient(host=\"localhost\", port=6333) ì»¬ë ‰ì…˜ ìƒì„± (ë²¡í„° ì°¨ì› 1536, cosine ê±°ë¦¬) client.recreate_collection( collection_name=\"my_collection\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) ë²¡í„° ë°ì´í„° ì‚½ì… vectors = np.random.rand(3, 1536).tolist() # 3ê°œ ë²¡í„°, 1536ì°¨ì› payloads = [ {\"doc_id\": \"A1\", \"text\": \"ë¬¸ì„œ1\"}, {\"doc_id\": \"A2\", \"text\": \"ë¬¸ì„œ2\"}, {\"doc_id\": \"A3\", \"text\": \"ë¬¸ì„œ3\"}, ] points = [ PointStruct(id=i, vector=vectors[i], payload=payloads[i]) for i in range(3) ] client.upsert(collection_name=\"my_collection\", points=points) ìœ ì‚¬ë„ ê²€ìƒ‰ (ì„ì˜ ì¿¼ë¦¬ ë²¡í„°) query_vector = np.random.rand(1536).tolist() results = client.search( collection_name=\"my_collection\", query_vector=query_vector, limit=2, ) for hit in results: print(f\"ID: {hit.id}, Score: {hit.score}, Payload: {hit.payload}\") from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np client = QdrantClient(host=\"localhost\", port=6333) íšŒì‚¬A ì»¬ë ‰ì…˜ ìƒì„± client.recreate_collection( collection_name=\"companyA_docs\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) íšŒì‚¬B ì»¬ë ‰ì…˜ ìƒì„± client.recreate_collection( collection_name=\"companyB_docs\", vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) íšŒì‚¬A ë°ì´í„° ì‚½ì… vectors_a = np.random.rand(2, 1536).tolist() payloads_a = [{\"doc_id\": \"A1\"}, {\"doc_id\": \"A2\"}] points_a = [PointStruct(id=i, vector=vectors_a[i], payload=payloads_a[i]) for i in range(2)] client.upsert(collection_name=\"companyA_docs\", points=points_a) íšŒì‚¬B ë°ì´í„° ì‚½ì… vectors_b = np.random.rand(2, 1536).tolist() payloads_b = [{\"doc_id\": \"B1\"}, {\"doc_id\": \"B2\"}] points_b = [PointStruct(id=i, vector=vectors_b[i], payload=payloads_b[i]) for i in range(2)] client.upsert(collection_name=\"companyB_docs\", points=points_b) íšŒì‚¬A ë°ì´í„°ë§Œ ê²€ìƒ‰ query_vector = np.random.rand(1536).tolist() results = client.search( collection_name=\"companyA_docs\", query_vector=query_vector, limit=1, ) for hit in results: print(f\"íšŒì‚¬A ê²°ê³¼: {hit.payload}\") íšŒì‚¬B ë°ì´í„°ë§Œ ê²€ìƒ‰ results = client.search( collection_name=\"companyB_docs\", query_vector=query_vector, limit=1, ) for hit in results: print(f\"íšŒì‚¬B ê²°ê³¼: {hit.payload}\") # Multi tenancy ë¥¼ ì˜ êµ¬í˜„í•˜ëŠ” ë²• # 1. ì»¬ë ‰ì…˜ ë„¤ì´ë° ì»¨ë²¤ì…˜ from qdrant_client import QdrantClient from qdrant_client.models import VectorParams, Distance, PointStruct import numpy as np client = QdrantClient(host=\"localhost\", port=6333) í…Œë„ŒíŠ¸ë³„ ì»¬ë ‰ì…˜ ë„¤ì´ë° íŒ¨í„´ def get_collection_name(tenant_id: str, data_type: str = \"docs\") -> str: \"\"\"í…Œë„ŒíŠ¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±\"\"\" return f\"{tenant_id}_{data_type}\" ì˜ˆì‹œ: íšŒì‚¬ë³„ ì»¬ë ‰ì…˜ ìƒì„± tenants = [\"company_a\", \"company_b\", \"company_c\"] data_types = [\"docs\", \"images\", \"products\"] for tenant in tenants: for data_type in data_types: collection_name = get_collection_name(tenant, data_type) client.recreate_collection( collection_name=collection_name, vectors_config=VectorParams(size=1536, distance=Distance.COSINE), ) print(f\"Created collection: {collection_name}\") # 2. ë©€í‹° í…Œë„Œì‹œ ë§¤ë‹ˆì € í´ë˜ìŠ¤ class QdrantMultiTenantManager: def __init__(self, client: QdrantClient): self.client = client self.tenant_configs = { \"company_a\": { \"collections\": [\"docs\", \"images\"], \"vector_size\": 1536, \"distance\": Distance.COSINE }, \"company_b\": { \"collections\": [\"docs\", \"products\"], \"vector_size\": 1536, \"distance\": Distance.COSINE } } def get_collection_name(self, tenant_id: str, collection_type: str) -> str: \"\"\"í…Œë„ŒíŠ¸ë³„ ì»¬ë ‰ì…˜ ì´ë¦„ ìƒì„±\"\"\" return f\"{tenant_id}_{collection_type}\" def ensure_collection_exists(self, tenant_id: str, collection_type: str): \"\"\"ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒì„±\"\"\" if tenant_id not in self.tenant_configs: raise ValueError(f\"Unknown tenant: {tenant_id}\") collection_name = self.get_collection_name(tenant_id, collection_type) config = self.tenant_configs[tenant_id] # ì»¬ë ‰ì…˜ ì¡´ì¬ ì—¬ë¶€ í™•ì¸ collections = self.client.get_collections() collection_names = [c.name for c in collections.collections] if collection_name not in collection_names: self.client.create_collection( collection_name=collection_name, vectors_config=VectorParams( size=config[\"vector_size\"], distance=config[\"distance\"] ), ) print(f\"Created collection: {collection_name}\") def upsert_for_tenant(self, tenant_id: str, collection_type: str, vectors, payloads, ids): \"\"\"í…Œë„ŒíŠ¸ë³„ ë°ì´í„° ì‚½ì…\"\"\" self.ensure_collection_exists(tenant_id, collection_type) collection_name = self.get_collection_name(tenant_id, collection_type) points = [ PointStruct(id=id_, vector=vector, payload=payload) for id_, vector, payload in zip(ids, vectors, payloads) ] return self.client.upsert( collection_name=collection_name, points=points ) def search_for_tenant(self, tenant_id: str, collection_type: str, query_vector, limit=10, kwargs): \"\"\"í…Œë„ŒíŠ¸ë³„ ê²€ìƒ‰\"\"\" collection_name = self.get_collection_name(tenant_id, collection_type) return self.client.search( collection_name=collection_name, query_vector=query_vector, limit=limit, kwargs ) def get_tenant_collections(self, tenant_id: str): \"\"\"í…Œë„ŒíŠ¸ì˜ ëª¨ë“  ì»¬ë ‰ì…˜ ì¡°íšŒ\"\"\" collections = self.client.get_collections() tenant_prefix = f\"{tenant_id}_\" return [ c.name for c in collections.collections if c.name.startswith(tenant_prefix) ] def delete_tenant_data(self, tenant_id: str): \"\"\"í…Œë„ŒíŠ¸ì˜ ëª¨ë“  ë°ì´í„° ì‚­ì œ\"\"\" collections = self.get_tenant_collections(tenant_id) for collection_name in collections: self.client.delete_collection(collection_name) print(f\"Deleted collection: {collection_name}\") # 3. ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” manager = QdrantMultiTenantManager(client) íšŒì‚¬Aì— ë¬¸ì„œ ë°ì´í„° ì‚½ì… vectors_a = np.random.rand(3, 1536).tolist() payloads_a = [ {\"doc_id\": \"A1\", \"title\": \"íšŒì‚¬A ë¬¸ì„œ1\", \"tenant\": \"company_a\"}, {\"doc_id\": \"A2\", \"title\": \"íšŒì‚¬A ë¬¸ì„œ2\", \"tenant\": \"company_a\"}, {\"doc_id\": \"A3\", \"title\": \"íšŒì‚¬A ë¬¸ì„œ3\", \"tenant\": \"company_a\"} ] ids_a = [\"A1\", \"A2\", \"A3\"] manager.upsert_for_tenant(\"company_a\", \"docs\", vectors_a, payloads_a, ids_a) íšŒì‚¬Bì— ì œí’ˆ ë°ì´í„° ì‚½ì… vectors_b = np.random.rand(2, 1536).tolist() payloads_b = [ {\"product_id\": \"B1\", \"name\": \"íšŒì‚¬B ì œí’ˆ1\", \"tenant\": \"company_b\"}, {\"product_id\": \"B2\", \"name\": \"íšŒì‚¬B ì œí’ˆ2\", \"tenant\": \"company_b\"} ] ids_b = [\"B1\", \"B2\"] manager.upsert_for_tenant(\"company_b\", \"products\", vectors_b, payloads_b, ids_b) í…Œë„ŒíŠ¸ë³„ ê²€ìƒ‰ query_vector = np.random.rand(1536).tolist() íšŒì‚¬A ë¬¸ì„œ ê²€ìƒ‰ results_a = manager.search_for_tenant(\"company_a\", \"docs\", query_vector) print(\"íšŒì‚¬A ê²€ìƒ‰ ê²°ê³¼:\", [hit.payload for hit in results_a]) íšŒì‚¬B ì œí’ˆ ê²€ìƒ‰ results_b = manager.search_for_tenant(\"company_b\", \"products\", query_vector) print(\"íšŒì‚¬B ê²€ìƒ‰ ê²°ê³¼:\", [hit.payload for hit in results_b]) # ë°±ì—… ë³µêµ¬ import json import os class TenantBackup: def __init__(self, manager: QdrantMultiTenantManager): self.manager = manager def backup_tenant(self, tenant_id: str, backup_path: str): \"\"\"í…Œë„ŒíŠ¸ë³„ ë°±ì—…\"\"\" collections = self.manager.get_tenant_collections(tenant_id) for collection_name in collections: # ì»¬ë ‰ì…˜ ë°ì´í„° ë‚´ë³´ë‚´ê¸° points = self.manager.client.scroll( collection_name=collection_name, limit=10000 # ì ì ˆí•œ ë°°ì¹˜ í¬ê¸° )[0] # ë°±ì—… íŒŒì¼ì— ì €ì¥ backup_file = f\"{backup_path}/{collection_name}_backup.json\" with open(backup_file, 'w') as f: json.dump(points, f, indent=2) print(f\"Backed up {collection_name} to {backup_file}\") def restore_tenant(self, tenant_id: str, backup_path: str): \"\"\"í…Œë„ŒíŠ¸ë³„ ë³µêµ¬\"\"\" collections = self.manager.get_tenant_collections(tenant_id) for collection_name in collections: backup_file = f\"{backup_path}/{collection_name}_backup.json\" if os.path.exists(backup_file): with open(backup_file, 'r') as f: points_data = json.load(f) # ë°ì´í„° ë³µêµ¬ self.manager.client.upsert( collection_name=collection_name, points=points_data ) print(f\"Restored {collection_name} from {backup_file}\")"
  },
  {
    "id": "Data Science/compare-vDB-libraries",
    "title": "VectorDBì— ì‚¬ìš©ë˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤ ë¹„êµ",
    "date": "2025-07-25",
    "excerpt": "ì±—ë´‡ êµ¬í˜„í•˜ë©´ì„œ RAG ë¶€ë¶„ì— ì‚¬ìš©í•  ë²¡í„°í™”ëœ ë¬¸ì„œë“¤ì„ ê´€ë¦¬í•  DBë“¤ì„ ë¹„êµ",
    "category": "Data Science",
    "tags": [
      "vector-database",
      "chatbot",
      "RAG"
    ],
    "content": "Qdrant vs ChromaDB vs FAISS vs Milvus DB ì£¼ìš” íŠ¹ì§• ì¥ì  ë‹¨ì  ------------ ---------------------------------------- -------------------------------------------------------------- -------------------------------------------------------------- Qdrant Rust ê¸°ë°˜, í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤, API ë‹¤ì–‘ ë¹ ë¥¸ ì¿¼ë¦¬, í™•ì¥ì„±, ë©”íƒ€ë°ì´í„° í•„í„°ë§, í´ë¼ìš°ë“œ ì§€ì› í´ëŸ¬ìŠ¤í„° ê´€ë¦¬ í•„ìš”, ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥ì€ ë¯¸í¡ ChromaDB Python ê¸°ë°˜, ë‚´ì¥í˜•, LangChain ì—°ë™ ì‰¬ìš´ ì„¤ì¹˜, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, íŒŒì´ì¬ ì¹œí™”ì , ì‹¤ì‹œê°„ ê²€ìƒ‰ ëŒ€ê·œëª¨ ë°ì´í„°/ë¶„ì‚° ë¯¸ì§€ì›, ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì´ìŠˆ, ë¬¸ì„œí™” ë¶€ì¡± FAISS Facebook ê°œë°œ, ë¼ì´ë¸ŒëŸ¬ë¦¬, GPU ì§€ì› ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤, ëŒ€ê·œëª¨ ë²¡í„° ì²˜ë¦¬, ì˜¤í”ˆì†ŒìŠ¤ DB ì•„ë‹˜(ì €ì¥/ìš´ì˜ ì§ì ‘ êµ¬í˜„ í•„ìš”), í™•ì¥ì„±/ìš´ì˜ ìë™í™” ë¯¸í¡ Milvus ë¶„ì‚°í˜•, í´ë¼ìš°ë“œ/ì˜¨í”„ë ˆë¯¸ìŠ¤, MLOps ì—°ë™ ëŒ€ê·œëª¨ ë¶„ì‚°ì²˜ë¦¬, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤, RBAC ë“± ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ ë³µì¡í•œ ë°°í¬(ì¿ ë²„ë„¤í‹°ìŠ¤ ë“±), ë¦¬ì†ŒìŠ¤ ìš”êµ¬ ë†’ìŒ, ëŸ¬ë‹ì»¤ë¸Œ ìˆìŒ --- Qdrant íŠ¹ì§•: Rustë¡œ ê°œë°œëœ ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° DB. í•˜ì´ë¸Œë¦¬ë“œ ì¸ë±ìŠ¤(ë²¡í„°+ì†ì„±), ë‹¤ì–‘í•œ ê±°ë¦¬ ì¸¡ë„(ìœ í´ë¦¬ë“œ, ì½”ì‚¬ì¸ ë“±), REST/gRPC API, í´ë¼ìš°ë“œ ë° ì˜¨í”„ë ˆë¯¸ìŠ¤ ì§€ì›. ì¥ì : ë¹ ë¥¸ ì¿¼ë¦¬ ì„±ëŠ¥(HNSW, PQ ë“± ê³ ê¸‰ ì¸ë±ìŠ¤) ë©”íƒ€ë°ì´í„° ê¸°ë°˜ í•„í„°ë§(ì†ì„±+ë²¡í„° ë™ì‹œ ê²€ìƒ‰) í´ëŸ¬ìŠ¤í„° í™•ì¥ì„±, í•˜ì´ë¸Œë¦¬ë“œ í´ë¼ìš°ë“œ ì§€ì› ë¬¸ì„œí™” ë° ì»¤ë®¤ë‹ˆí‹° í™œë°œ ë‹¨ì : í´ëŸ¬ìŠ¤í„° ê´€ë¦¬/ìš´ì˜ ë³µì¡ì„± ì¼ë¶€ ê³ ê¸‰ ê¸°ëŠ¥(ì˜ˆ: ìë™ í´ëŸ¬ìŠ¤í„°ë§, ì¿¼ë¦¬ ë¦¬ë°¸ëŸ°ì‹± ë“±) ë¯¸í¡ ì¶”ì²œ ì‚¬ìš©ì²˜: RAG, ì¶”ì²œì‹œìŠ¤í…œ, ëŒ€ê·œëª¨ ê²€ìƒ‰, ì†ì„±+ë²¡í„° ë™ì‹œ í•„í„°ë§ì´ í•„ìš”í•œ ì„œë¹„ìŠ¤ ChromaDB íŠ¹ì§•: Python ê¸°ë°˜ ë‚´ì¥í˜• ë²¡í„° DB. LangChain ë“± LLM/RAG íŒŒì´í”„ë¼ì¸ì— ìµœì í™”. ë‚´ì¥ ì„ë² ë”© ëª¨ë¸, SQLite/ClickHouse ìŠ¤í† ë¦¬ì§€ ì§€ì›. ì¥ì : ì„¤ì¹˜/ì‚¬ìš©ì´ ë§¤ìš° ê°„ë‹¨, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘ íŒŒì´ì¬ ì¹œí™”ì (ML/AI ì›Œí¬í”Œë¡œìš°ì™€ ì—°ë™ ìš©ì´) ì‹¤ì‹œê°„ ê²€ìƒ‰, ë™ì  ë©”íƒ€ë°ì´í„° í•„í„°ë§ LangChain, OpenAI API ë“±ê³¼ ì—°ë™ ì‰¬ì›€ ë‹¨ì : ëŒ€ê·œëª¨ ë°ì´í„°/ë¶„ì‚° ë¯¸ì§€ì›(ì‹±ê¸€ë…¸ë“œ í•œê³„) ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë° ì„œë¹„ìŠ¤ í¬ë˜ì‹œ ì´ìŠˆ(2024ë…„ ê¸°ì¤€) ë¬¸ì„œí™”/ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ ë¶€ì¡± ì¶”ì²œ ì‚¬ìš©ì²˜: ì†Œê·œëª¨ RAG, LLM ì‹¤í—˜, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…, íŒŒì´ì¬ ê¸°ë°˜ í”„ë¡œì íŠ¸ FAISS íŠ¹ì§•: Facebook AIê°€ ê°œë°œí•œ ë²¡í„° ê²€ìƒ‰ ë¼ì´ë¸ŒëŸ¬ë¦¬. ë‹¤ì–‘í•œ ì¸ë±ìŠ¤(HNSW, IVF, PQ ë“±), GPU ì§€ì›, ëŒ€ê·œëª¨ ë²¡í„° ì²˜ë¦¬ì— ìµœì í™”. ì¥ì : ë§¤ìš° ë¹ ë¥¸ ê²€ìƒ‰ ì†ë„(íŠ¹íˆ GPU í™œìš© ì‹œ) ë‹¤ì–‘í•œ ì¸ë±ìŠ¤/ê±°ë¦¬ ì¸¡ë„ ì§€ì› ëŒ€ê·œëª¨ ë²¡í„° ë°ì´í„° ì²˜ë¦¬ì— ê°•ì  ì˜¤í”ˆì†ŒìŠ¤, ì»¤ë®¤ë‹ˆí‹° í™œë°œ ë‹¨ì : DBê°€ ì•„ë‹ˆë¼ ë¼ì´ë¸ŒëŸ¬ë¦¬(ì €ì¥/ìš´ì˜ ì§ì ‘ êµ¬í˜„ í•„ìš”) ë¶„ì‚°/í™•ì¥ì„±, ìš´ì˜ ìë™í™” ë¯¸í¡ ë©”íƒ€ë°ì´í„° í•„í„°ë§ ë“± DBì  ê¸°ëŠ¥ ë¶€ì¡± ì¶”ì²œ ì‚¬ìš©ì²˜: ëŒ€ê·œëª¨ ë²¡í„° ê²€ìƒ‰ ì—”ì§„ ì§ì ‘ êµ¬ì¶•, ML/AI ì‹¤í—˜, GPU ê¸°ë°˜ ëŒ€ìš©ëŸ‰ ê²€ìƒ‰ Milvus íŠ¹ì§•: ë¶„ì‚°í˜• ì˜¤í”ˆì†ŒìŠ¤ ë²¡í„° DB. í´ë¼ìš°ë“œ/ì˜¨í”„ë ˆë¯¸ìŠ¤, ë‹¤ì–‘í•œ ì¸ë±ìŠ¤(HNSW, IVF ë“±), RBAC, MLOps ì—°ë™, ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬ì— ìµœì í™”. ì¥ì : ëŒ€ê·œëª¨ ë¶„ì‚°ì²˜ë¦¬(ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ë²¡í„°) ë‹¤ì–‘í•œ ì¸ë±ìŠ¤, GPU ê°€ì† ì§€ì› ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥(RBAC, ê°ì‚¬ë¡œê·¸, GDPR ë“±) MLOps, Grafana/Prometheus ë“±ê³¼ ì—°ë™ ë‹¨ì : ë³µì¡í•œ ë°°í¬(ì¿ ë²„ë„¤í‹°ìŠ¤ ë“± ì¸í”„ë¼ í•„ìš”) ë†’ì€ ë¦¬ì†ŒìŠ¤ ìš”êµ¬, ëŸ¬ë‹ì»¤ë¸Œ ìˆìŒ ì†Œê·œëª¨/ë‹¨ì¼ ë…¸ë“œì—ëŠ” ê³¼í•œ ì„ íƒì¼ ìˆ˜ ìˆìŒ ì¶”ì²œ ì‚¬ìš©ì²˜: ëŒ€ê·œëª¨ RAG, ì—”í„°í”„ë¼ì´ì¦ˆ ê²€ìƒ‰, ë³´ì•ˆ/ê°ì‚¬/í™•ì¥ì„± ì¤‘ìš”í•œ ì„œë¹„ìŠ¤ --- ì—¬ëŸ¬ íšŒì‚¬/ë¬¸ì„œ ê·¸ë£¹ êµ¬ë¶„ ê´€ë¦¬: ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤/íŒŒí‹°ì…˜ ë¹„êµ DB ë°ì´í„° ë¶„ë¦¬ ì§€ì› ë¶„ë¦¬ ë°©ì‹(ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë“±) ë©€í‹°í…Œë„Œì‹œ/ê¶Œí•œê´€ë¦¬ ì¶”ì²œë„(íšŒì‚¬ë³„ êµ¬ë¶„) ------------ ------------------ ------------------------------- -------------------- -------------------- Qdrant O ì»¬ë ‰ì…˜(collection) O â˜…â˜…â˜…â˜…â˜† ChromaDB O ì»¬ë ‰ì…˜(collection) X â˜…â˜…â˜†â˜†â˜† FAISS X(ì§ì ‘êµ¬í˜„) ì—†ìŒ(ì½”ë“œë¡œ ì¸ë±ìŠ¤ ë¶„ë¦¬) X â˜…â˜†â˜†â˜†â˜† Milvus O ì»¬ë ‰ì…˜/íŒŒí‹°ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤ O â˜…â˜…â˜…â˜…â˜… --- ì‹¤ë¬´/ìš´ì˜ ê´€ì ì—ì„œì˜ ì¶”ì²œ Qdrant: ì»¬ë ‰ì…˜ ë‹¨ìœ„ë¡œ íšŒì‚¬A, íšŒì‚¬B ë“± ì™„ì „íˆ ë¶„ë¦¬ëœ ë°ì´í„° ê´€ë¦¬ ê°€ëŠ¥. API/ê¶Œí•œê´€ë¦¬ë„ ìš°ìˆ˜. ìš´ì˜/í™•ì¥ì„± ëª¨ë‘ ê°•ì . Milvus: ì»¬ë ‰ì…˜, íŒŒí‹°ì…˜, ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ë“± ë‹¤ì–‘í•œ ë¶„ë¦¬ ë°©ì‹ ì œê³µ. ëŒ€ê·œëª¨ ë©€í‹°í…Œë„Œì‹œ, RBAC ë“± ì—”í„°í”„ë¼ì´ì¦ˆ í™˜ê²½ì— ìµœì . ChromaDB: ì»¬ë ‰ì…˜ìœ¼ë¡œ êµ¬ë¶„ì€ ê°€ëŠ¥í•˜ë‚˜, ì‹±ê¸€ë…¸ë“œ/ë‚´ì¥í˜•ì´ë¼ ëŒ€ê·œëª¨ ìš´ì˜ì—ëŠ” í•œê³„. ì‹¤í—˜/ì†Œê·œëª¨ì— ì í•©. FAISS: DBê°€ ì•„ë‹ˆë¼ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¼ ë°ì´í„° ë¶„ë¦¬/ê¶Œí•œê´€ë¦¬ ì§ì ‘ êµ¬í˜„ í•„ìš”. ìš´ì˜/í™•ì¥ì„± ëª©ì ì—ëŠ” ë¹„ì¶”ì²œ. ê²°ë¡ : íšŒì‚¬ë³„, í”„ë¡œì íŠ¸ë³„ ë“± ì—¬ëŸ¬ ê·¸ë£¹ì˜ ë¬¸ì„œë¥¼ í•˜ë‚˜ì˜ ë²¡í„° DBì—ì„œ ì•ˆì „í•˜ê²Œ êµ¬ë¶„/ìš´ì˜í•˜ë ¤ë©´ Qdrant ë˜ëŠ” Milvusê°€ ê°€ì¥ ì í•©í•©ë‹ˆë‹¤. (ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤/ê¶Œí•œê´€ë¦¬ ë“± ì§€ì›) ChromaDBëŠ” ì†Œê·œëª¨/ì‹¤í—˜ì—, FAISSëŠ” ì§ì ‘ êµ¬í˜„ì´ í•„ìš”í•˜ë¯€ë¡œ ëŒ€ê·œëª¨ ìš´ì˜ì—ëŠ” ë¶€ì í•©í•©ë‹ˆë‹¤. ë©€í‹°í…Œë„Œì‹œë€? ë©€í‹°í…Œë„Œì‹œ(Multi-tenancy)ëŠ” í•˜ë‚˜ì˜ ë°ì´í„°ë² ì´ìŠ¤(í˜¹ì€ ì‹œìŠ¤í…œ) ì•ˆì—ì„œ ì—¬ëŸ¬ ì‚¬ìš©ì ê·¸ë£¹(íšŒì‚¬, ì¡°ì§, í”„ë¡œì íŠ¸ ë“±)ì´ ì„œë¡œì˜ ë°ì´í„°ì— ì˜í–¥ì„ ì£¼ì§€ ì•Šê³  ì•ˆì „í•˜ê²Œ ë¶„ë¦¬ëœ ê³µê°„ì—ì„œ ë°ì´í„°ë¥¼ ê´€ë¦¬í•  ìˆ˜ ìˆëŠ” êµ¬ì¡°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. í…Œë„ŒíŠ¸(tenant): í•˜ë‚˜ì˜ ê³ ê°, íšŒì‚¬, ì¡°ì§, í”„ë¡œì íŠ¸ ë“± ë…ë¦½ì ìœ¼ë¡œ ë°ì´í„°ë¥¼ ê´€ë¦¬í•´ì•¼ í•˜ëŠ” ë‹¨ìœ„ ë©€í‹°í…Œë„Œì‹œ: ì—¬ëŸ¬ í…Œë„ŒíŠ¸(íšŒì‚¬A, íšŒì‚¬B, â€¦)ê°€ í•œ DB ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê³µìœ í•˜ì§€ë§Œ, ê° í…Œë„ŒíŠ¸ì˜ ë°ì´í„°ëŠ” ì™„ì „íˆ ë¶„ë¦¬ë˜ì–´ ìˆê³ , ì„œë¡œì˜ ë°ì´í„°ì— ì ‘ê·¼í•˜ê±°ë‚˜ ì˜í–¥ì„ ì¤„ ìˆ˜ ì—†ìŒ ë²¡í„° DBì—ì„œì˜ ë©€í‹°í…Œë„Œì‹œ ì˜ˆì‹œ íšŒì‚¬Aì™€ íšŒì‚¬Bê°€ ê°™ì€ Qdrant/Milvus ì„œë²„ë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, íšŒì‚¬AëŠ” ìì‹ ì˜ ì»¬ë ‰ì…˜/ë„¤ì„ìŠ¤í˜ì´ìŠ¤/íŒŒí‹°ì…˜ì—ë§Œ ì ‘ê·¼ ê°€ëŠ¥ íšŒì‚¬Bë„ ë§ˆì°¬ê°€ì§€ë¡œ ìì‹ ì˜ ë°ì´í„°ë§Œ ì ‘ê·¼ ê°€ëŠ¥ ì„œë¡œì˜ ë°ì´í„°ëŠ” ì ˆëŒ€ ë³´ê±°ë‚˜ ìˆ˜ì •í•  ìˆ˜ ì—†ìŒ ê¶Œí•œ ê´€ë¦¬(RBAC), API ì¸ì¦ ë“±ìœ¼ë¡œ ë³´ì•ˆë„ ë³´ì¥ ì™œ ì¤‘ìš”í• ê¹Œ? ìš´ì˜ë¹„ ì ˆê°: íšŒì‚¬ë§ˆë‹¤ DBë¥¼ ë”°ë¡œ ë„ìš°ì§€ ì•Šì•„ë„ ë¨ ë³´ì•ˆ/ê²©ë¦¬: ë°ì´í„°ê°€ ì„ì´ì§€ ì•Šê³ , ì‹¤ìˆ˜ë¡œë„ ì ‘ê·¼ ë¶ˆê°€ í™•ì¥ì„±: í…Œë„ŒíŠ¸ê°€ ëŠ˜ì–´ë‚˜ë„ í•˜ë‚˜ì˜ ì‹œìŠ¤í…œì—ì„œ ê´€ë¦¬ ê°€ëŠ¥ ì •ë¦¬: ë©€í‹°í…Œë„Œì‹œëŠ” â€œì—¬ëŸ¬ íšŒì‚¬/ì¡°ì§ì´ í•˜ë‚˜ì˜ DBë¥¼ ì“°ë”ë¼ë„, ê°ìì˜ ë°ì´í„°ê°€ ì™„ì „íˆ ë¶„ë¦¬Â·ê²©ë¦¬ë˜ì–´ ì•ˆì „í•˜ê²Œ ê´€ë¦¬ë˜ëŠ” êµ¬ì¡°â€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. (SaaS ì„œë¹„ìŠ¤ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ê°œë…!) ë¬´ë£Œ ì„œë¹„ìŠ¤/ì˜¤í”ˆì†ŒìŠ¤ ì—¬ë¶€ DB ì˜¤í”ˆì†ŒìŠ¤ ìì²´ ì„œë²„ ë¬´ë£Œ ê³µì‹ í´ë¼ìš°ë“œ ë¬´ë£Œ í”Œëœ ë¹„ê³  ------------ ---------- --------------- ------------------------ --------------------------- Qdrant O O O (Qdrant Cloud Free) ì˜¤í”ˆì†ŒìŠ¤, í´ë¼ìš°ë“œ ë¬´ë£Œ í‹°ì–´(ì œí•œ ìˆìŒ) ChromaDB O O X ì˜¤í”ˆì†ŒìŠ¤, ìì²´ ì„œë²„ë§Œ ì§€ì› FAISS O O X ì˜¤í”ˆì†ŒìŠ¤ ë¼ì´ë¸ŒëŸ¬ë¦¬, í´ë¼ìš°ë“œ ì—†ìŒ Milvus O O O (Zilliz Cloud Free) ì˜¤í”ˆì†ŒìŠ¤, Zilliz Cloud ë¬´ë£Œ í‹°ì–´(ì œí•œ ìˆìŒ) --- ìƒì„¸ ì„¤ëª… Qdrant ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ)ë¡œ ì§ì ‘ ì„œë²„ ìš´ì˜ ê°€ëŠ¥ ê³µì‹ Qdrant Cloudì—ì„œ ë¬´ë£Œ í”Œëœ(ì œí•œì  ìš©ëŸ‰/ì¿¼ë¦¬) ì œê³µ ChromaDB ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ)ë¡œ ì§ì ‘ ì„œë²„ ìš´ì˜ë§Œ ê°€ëŠ¥ ê³µì‹ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤/ë¬´ë£Œ í”Œëœ ì—†ìŒ FAISS ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ) ë¼ì´ë¸ŒëŸ¬ë¦¬ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ ì—†ìŒ(ì§ì ‘ ì„œë²„/ì½”ë“œë¡œë§Œ ì‚¬ìš©) Milvus ì˜¤í”ˆì†ŒìŠ¤(ë¬´ë£Œ)ë¡œ ì§ì ‘ ì„œë²„ ìš´ì˜ ê°€ëŠ¥ ê³µì‹ Zilliz Cloudì—ì„œ ë¬´ë£Œ í”Œëœ(ì œí•œì  ìš©ëŸ‰/ì¿¼ë¦¬) ì œê³µ --- ì •ë¦¬: 4ê°œ ëª¨ë‘ ì˜¤í”ˆì†ŒìŠ¤ë¼ ì§ì ‘ ì„œë²„ë¥¼ ë„ìš°ë©´ ë¬´ë£Œë¡œ ì‚¬ìš© ê°€ëŠ¥ Qdrant, MilvusëŠ” ê³µì‹ í´ë¼ìš°ë“œì—ì„œ ë¬´ë£Œ í”Œëœë„ ì œê³µ ChromaDB, FAISSëŠ” í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ê°€ ì—†ê³ , ì§ì ‘ ì„¤ì¹˜/ìš´ì˜ë§Œ ê°€ëŠ¥ --- Qdrant vs ChromaDB ì»¬ë ‰ì…˜ì˜ ì°¨ì´ í•­ëª© Qdrant ì»¬ë ‰ì…˜ ChromaDB ì»¬ë ‰ì…˜ --------------------- ------------------------------------- ---------------------------------- ë¶„ë¦¬ ìˆ˜ì¤€ ì™„ì „ ë¶„ë¦¬(ë¬¼ë¦¬ì /ë…¼ë¦¬ì ) ë…¼ë¦¬ì  ë¶„ë¦¬(ë‚´ì¥ DB ë‚´ í…Œì´ë¸”) í™•ì¥ì„± ë¶„ì‚°/í´ëŸ¬ìŠ¤í„° ì§€ì›, ëŒ€ê·œëª¨ ìš´ì˜ ê°€ëŠ¥ ì‹±ê¸€ë…¸ë“œ, ëŒ€ê·œëª¨ ìš´ì˜ í•œê³„ ë³´ì•ˆ/ê¶Œí•œê´€ë¦¬ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê¶Œí•œ/RBAC ì§€ì› ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê¶Œí•œê´€ë¦¬ ë¯¸í¡ ìš´ì˜ ì‹ ë¢°ì„± ì¥ì•  ê²©ë¦¬, ë°ì´í„° ì†ìƒ ìµœì†Œí™” ì „ì²´ DB ì¥ì•  ì‹œ ëª¨ë“  ì»¬ë ‰ì…˜ ì˜í–¥ API/ê´€ë¦¬ë„êµ¬ REST/gRPC ë“± ê³µì‹ API, ê´€ë¦¬ë„êµ¬ í’ë¶€ íŒŒì´ì¬ API ìœ„ì£¼, ê´€ë¦¬ë„êµ¬ ì œí•œì  ë°±ì—…/ë³µêµ¬ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ë°±ì—…/ë³µêµ¬ ì§€ì› ì „ì²´ DB ë‹¨ìœ„ ë°±ì—…/ë³µêµ¬ ë©€í‹°í…Œë„Œì‹œ ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë©€í‹°í…Œë„Œì‹œ êµ¬í˜„ ìš©ì´ ì‹¤í—˜/ì†Œê·œëª¨ ë©€í‹°í…Œë„Œì‹œì— ì í•© ìƒì„¸ ì„¤ëª… Qdrant ì»¬ë ‰ì…˜ì´ ì™„ì „íˆ ë¶„ë¦¬ëœ ë°ì´í„°ë² ì´ìŠ¤ì²˜ëŸ¼ ë™ì‘í•©ë‹ˆë‹¤. ê° ì»¬ë ‰ì…˜ì€ ë³„ë„ì˜ ì¸ë±ìŠ¤, ì„¤ì •, ê¶Œí•œ, ë°±ì—…/ë³µêµ¬ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤. ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤, ë©€í‹°í…Œë„Œì‹œ, ë³´ì•ˆì´ ì¤‘ìš”í•œ í™˜ê²½ì— ì í•©í•©ë‹ˆë‹¤. ì¥ì• ê°€ ë‚˜ë„ í•œ ì»¬ë ‰ì…˜ë§Œ ì˜í–¥ë°›ê³ , ë‚˜ë¨¸ì§€ëŠ” ì •ìƒ ë™ì‘í•©ë‹ˆë‹¤. ChromaDB ì»¬ë ‰ì…˜ì€ ë…¼ë¦¬ì  ë¶„ë¦¬(í…Œì´ë¸” ìˆ˜ì¤€)ë¡œ, í•˜ë‚˜ì˜ ë‚´ì¥ DB íŒŒì¼ì— ì—¬ëŸ¬ ì»¬ë ‰ì…˜ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. ì‹±ê¸€ë…¸ë“œ ê¸°ë°˜ì´ë¼ ëŒ€ê·œëª¨ ìš´ì˜, ì¥ì•  ê²©ë¦¬, ê¶Œí•œ ë¶„ë¦¬ì—ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. ì‹¤í—˜, ì†Œê·œëª¨ ì„œë¹„ìŠ¤, ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…ì—ëŠ” ì¶©ë¶„íˆ ìœ ìš©í•©ë‹ˆë‹¤. Qdrant vs Milvus ë¹„êµ í•­ëª© Qdrant Milvus --------------------- ---------------------------------------------- --------------------------------------------- ê°œë°œ ì–¸ì–´/ì—”ì§„ Rust ê¸°ë°˜, ê²½ëŸ‰/ê³ ì„±ëŠ¥ C++/Go ê¸°ë°˜, ëŒ€ê·œëª¨ ë¶„ì‚° ì•„í‚¤í…ì²˜ ì•„í‚¤í…ì²˜ ì‹±ê¸€ë…¸ë“œ/í´ëŸ¬ìŠ¤í„° ëª¨ë‘ ì§€ì›, ê²½ëŸ‰í™” ë¶„ì‚°í˜•(í´ëŸ¬ìŠ¤í„° ì „ìš©), ëŒ€ê·œëª¨ í™•ì¥ì„± ì„¤ì¹˜/ìš´ì˜ ì„¤ì¹˜/ìš´ì˜ ê°„ë‹¨, ë¡œì»¬/í´ë¼ìš°ë“œ ëª¨ë‘ ìš©ì´ ì¿ ë²„ë„¤í‹°ìŠ¤ ë“± ì¸í”„ë¼ í•„ìš”, ë³µì¡í•œ ë°°í¬ í™•ì¥ì„± ì¤‘~ëŒ€ê·œëª¨ê¹Œì§€ ë¬´ë‚œ, í´ëŸ¬ìŠ¤í„° ì§€ì› ì´ˆëŒ€ê·œëª¨(ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ë²¡í„°) ë¶„ì‚°ì²˜ë¦¬ ìµœì í™” API/ì–¸ì–´ ì§€ì› REST/gRPC, Python ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì› REST/gRPC, Python/Java/Go ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì› ì¸ë±ìŠ¤/ê²€ìƒ‰ HNSW, PQ ë“± ê³ ì„±ëŠ¥ ì¸ë±ìŠ¤, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ HNSW, IVF, DiskANN ë“± ë‹¤ì–‘í•œ ì¸ë±ìŠ¤ ë©”íƒ€ë°ì´í„°/í•„í„° ë²¡í„°+ì†ì„± ë™ì‹œ í•„í„°ë§(í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê°•ì ) ë²¡í„°+ì†ì„± ë™ì‹œ í•„í„°ë§, ë³µì¡í•œ ì¿¼ë¦¬ ì§€ì› ê¶Œí•œ/ë³´ì•ˆ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ê¶Œí•œ, RBAC(í´ë¼ìš°ë“œ) RBAC, ê°ì‚¬ë¡œê·¸, GDPR ë“± ì—”í„°í”„ë¼ì´ì¦ˆ ê¸°ëŠ¥ ë©€í‹°í…Œë„Œì‹œ ì»¬ë ‰ì…˜ ë‹¨ìœ„ ì™„ì „ ë¶„ë¦¬, ë©€í‹°í…Œë„Œì‹œ ìš©ì´ ë„¤ì„ìŠ¤í˜ì´ìŠ¤/ì»¬ë ‰ì…˜/íŒŒí‹°ì…˜ ë“± ë¶„ë¦¬, ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ í´ë¼ìš°ë“œ ì„œë¹„ìŠ¤ Qdrant Cloud(ë¬´ë£Œ/ìœ ë£Œ í”Œëœ) Zilliz Cloud(ë¬´ë£Œ/ìœ ë£Œ í”Œëœ) ì»¤ë®¤ë‹ˆí‹°/ë¬¸ì„œ í™œë°œ, ë¬¸ì„œ/ê°€ì´ë“œ í’ë¶€ í™œë°œ, ì—”í„°í”„ë¼ì´ì¦ˆ ë¬¸ì„œ/ì‚¬ë¡€ í’ë¶€ ì¶”ì²œ ì‚¬ìš©ì²˜ RAG, ì¶”ì²œì‹œìŠ¤í…œ, ì¤‘~ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤, ë¹ ë¥¸ êµ¬ì¶• ì´ˆëŒ€ê·œëª¨ ê²€ìƒ‰, ì—”í„°í”„ë¼ì´ì¦ˆ, ë³´ì•ˆ/ê°ì‚¬ í•„ìˆ˜ í™˜ê²½ ì‹¤ë¬´ì  ì„ íƒ í¬ì¸íŠ¸ Qdrant ì„¤ì¹˜/ìš´ì˜ì´ ì‰½ê³ , ì¤‘~ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ì— ì í•© ë¹ ë¥¸ í”„ë¡œí† íƒ€ì´í•‘, RAG, ì¶”ì²œì‹œìŠ¤í…œ, ì†ì„±+ë²¡í„° ë™ì‹œ í•„í„°ë§ì´ ì¤‘ìš”í•œ ì„œë¹„ìŠ¤ í´ë¼ìš°ë“œ/ì˜¨í”„ë ˆë¯¸ìŠ¤ ëª¨ë‘ ìœ ì—°í•˜ê²Œ ì‚¬ìš© ê°€ëŠ¥ ëŒ€ê·œëª¨ ë¶„ì‚°ì´ ê¼­ í•„ìš”í•˜ì§€ ì•Šë‹¤ë©´ Qdrantê°€ ë” ê°„í¸ Milvus ì´ˆëŒ€ê·œëª¨(ìˆ˜ì–µ~ìˆ˜ì‹­ì–µ ë²¡í„°) ë°ì´í„°, ë¶„ì‚°ì²˜ë¦¬, ì—”í„°í”„ë¼ì´ì¦ˆê¸‰ ë³´ì•ˆ/ê°ì‚¬/ê¶Œí•œì´ í•„ìš”í•œ í™˜ê²½ ì¿ ë²„ë„¤í‹°ìŠ¤ ë“± ì¸í”„ë¼ êµ¬ì¶•ì´ ê°€ëŠ¥í•˜ê³ , ë³µì¡í•œ ìš´ì˜ì„ ê°ìˆ˜í•  ìˆ˜ ìˆì„ ë•Œ MLOps, ëŒ€ê·œëª¨ AI/ê²€ìƒ‰ í”Œë«í¼, ë³´ì•ˆ/ê°ì‚¬/ê·œì œ ì¤€ìˆ˜ê°€ ì¤‘ìš”í•œ ê¸°ì—… í™˜ê²½ ê²°ë¡ : ë¹ ë¥¸ êµ¬ì¶•, ì¤‘~ëŒ€ê·œëª¨, ìš´ì˜ í¸ì˜ì„± â†’ Qdrant ì¶”ì²œ ì´ˆëŒ€ê·œëª¨, ì—”í„°í”„ë¼ì´ì¦ˆ, ë³´ì•ˆ/ê°ì‚¬/ë¶„ì‚° í•„ìˆ˜ â†’ Milvus ì¶”ì²œ ë‘˜ ë‹¤ ë¬´ë£Œ/í´ë¼ìš°ë“œ í”Œëœì´ ìˆìœ¼ë‹ˆ, ì‹¤ì œ PoC(íŒŒì¼ëŸ¿)ë¡œ í…ŒìŠ¤íŠ¸í•´ë³´ê³  ê²°ì •í•˜ëŠ” ê²ƒë„ ì¢‹ìŠµë‹ˆë‹¤. ìƒ˜í”Œ ì½”ë“œë“¤ compare-vDB-libraries"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-24-ë¬¼ê±´ì—ëŒ€í•œë¶„ì„",
    "title": "ë¬¼ê±´ì— ëŒ€í•œ ë¶„ì„",
    "date": "2025-07-24",
    "excerpt": "ë¬¼ê±´ì— ëŒ€í•œ ë¶„ì„",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ It's not about performance. It's about design. It's not about customizabiltiy. It's about compatibility. In which case, I'll probably go with the new Galaxy. go with : ì„ íƒí•˜ë‹¤"
  },
  {
    "id": "Causal Inference/how-can-evaluate-casual-models",
    "title": "how to evaluate CATE estimation model",
    "date": "2025-07-24",
    "excerpt": "CATE estimation model í‰ê°€ ë°©ë²•ì— ëŒ€í•´ ì •ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "ì¸ê³¼ì¶”ë¡ ",
      "metrics"
    ],
    "content": "ì°¸ê³ í•œ ìë£Œ Causal Inference for The Brave and True > When Prediction Fails chapter, they generate data on both $Y_{0i}$ and $Y_{1i}$ so that they can check if their model is correctly capturing the treatment effect $Y_{1i} - Y_{0i}$. > Thatâ€™s fine for academic purposes, but in the real world, we donâ€™t have that luxury. CATE estimation modelì€ ì–´ë–»ê²Œ í‰ê°€í•  ìˆ˜ ìˆì„ê¹Œìš”? í•™ë¬¸ì ìœ¼ë¡œëŠ” ì„ì˜ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•´ì„œ ì‹¤ì œë¡œ ì´ë¥¼ ì˜ ì¶”ì •í•˜ëŠ”ì§€ í™•ì¸í•´ë³¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. > The quality of your model will have to be grounded on something more concrete than a beautiful theory. > Unfortunately, it isnâ€™t obvious at all how we achieve anything like a train-test paradigm in the case of causal inference. > Thatâ€™s because causal inference is interested in estimating an unobservable quantity, $\\frac{\\delta y}{\\delta t}$ í•˜ì§€ë§Œ ì‹¤ì œ ë°ì´í„°ì—ì„œëŠ” CATEë¥¼ ì§ì ‘ í™•ì¸í•  ìˆ˜ ì—†ìœ¼ë‹ˆ, ë‹¤ë¥¸ ë°©ë²•ì„ ì¨ì•¼í•©ë‹ˆë‹¤. íšŒì‚¬ì— ì¸ê³¼ì¶”ë¡ ì„ ì ìš©í•˜ë©´ì„œ ê°€ì¥ ë§ì´ ê³ ë¯¼í—€ë˜ ë¶€ë¶„ì´ê¸°ë„ í•©ë‹ˆë‹¤. ë‚´ê°€ ë§Œë“  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì–´ë–»ê²Œ ì„¤ëª…í•˜ê³  ì„¤ë“í•  ìˆ˜ ìˆì„ê¹Œìš”? ë¨¸ì‹ ëŸ¬ë‹ì€ train-test paradigmì„ í†µí•´ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆê³ , ì´ëŠ” ê½¤ë‚˜ ì§ê´€ì ìœ¼ë¡œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¦ëª…í•´ì£¼ê³  ìˆìŠµë‹ˆë‹¤. CATE estimation modelì—ë„ ì´ëŸ¬í•œ ë°©ë²•ì´ ì¡´ì¬í• ê¹Œìš”? > Is not a definitive one, but it works in practice and it has that concreteness, which I hope will approach causal inference from a train-test paradigm similar to the one we have with machine learning. > The trick is to use aggregate measurements of sensitivity. > Even if you canâ€™t estimate sensitivity individually, you can do it for a group and that is what we will leverage here. ê°œê°œì¸ì˜ CATEëŠ” ì•Œ ìˆ˜ ì—†ì§€ë§Œ ê·¸ë£¹ì˜ ATEëŠ” ì•Œ ìˆ˜ ìˆìœ¼ë‹ˆ ì´ë¥¼ í™œìš©í•´ì„œ CATEë¥¼ í‰ê°€í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. Sensitivity by Model Band --- > A good causal model should help us find which customers will respond better and worse to a proposed treatment. > They should be able to separate units into how elastic or sensitive they are to the treatment. CATE estimationì˜ ëª©ì ì€ ì²˜ì¹˜ì— ëŒ€í•œ ë¯¼ê°ë„ë¥¼ ê°œì¸ë³„ë¡œ ì¶”ì •í•˜ì—¬, ê°œì¸í™”ëœ ì²˜ì¹˜ë¥¼ í•˜ëŠ”ë°ì— ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ê°œì¸ë³„ë¡œ ë¯¼ê°ë„ë¥¼ ëª…í™•í•˜ê²Œ êµ¬ë¶„í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. > If that is the goal, it would be very useful if we could somehow order units from more sensitive to less sensitive. CATE estimationì„ í†µí•´ ê°œì¸ë³„ ë¯¼ê°ë„ë¥¼ ê³„ì‚°í•˜ê³  ì´ë¥¼ í™œìš©í•´ ì •ë ¬ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ë ¬ì„ ì–¼ë§ˆë‚˜ ì˜í•˜ëŠ”ì§€ë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•´ë³¼ ìˆ˜ ìˆì§€ ì•Šì„ê¹Œìš”? > Sadly, we canâ€™t evaluate that ordering on a unit level. > But, what if we donâ€™t need to? > What if, instead, we evaluate groups defined by the ordering? > If our treatment is randomly distributed (and here is where randomness enters), estimating sensitivity for a group of units is easy. > All we need is to compare the outcome between the treated and untreated. <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/cate_metric_using_ate.png\" alt=\"cate_metric_using_ate\" /> <figcaption> ê·¸ë¦¼1. CATE í‰ê°€ ë°©ë²•ì— ëŒ€í•œ ì§ê´€ì  ì„¤ëª… (low: ë¹„ì²˜ì¹˜ëŒ€ìƒ, high: ì²˜ì¹˜ëŒ€ìƒ-ê°€ê²©ì„ ë†’ì˜€ì„ ë•Œ)</figcaption> </figure> ê·¸ë¦¼1ì´ ì˜ë¯¸í•˜ëŠ” ê²ƒì€ ì¶”ì •ëœ cateë¡œ ì •ë ¬í–ˆì„ ë•Œ, ì •ë ¬ëœ ê²ƒë“¤ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹ì„ ë‚˜ëˆ  ATEë¥¼ êµ¬í•˜ë©´ ê·¸ ì°¨ì´ê°€ ë¶„ëª…í•  ê²ƒì´ë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ë§¨ ì˜¤ë¥¸ìª½ì— ìˆëŠ”, ëœë¤í•˜ê²Œ cateë¥¼ ë°°ì •í•œ ê²½ìš° ATEê°€ ì™„ì „í•˜ê²Œ ê°™ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ model-2ì˜ ê²½ìš° ATEê°€ ë¶„ëª…í•˜ê²Œ ì°¨ì´ê°€ ë‚˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > Just by looking at these plots, you can get a feeling of which model is better. > The more ordered the sensitivities look like and the more different they are between bands, the better. > Here, model 2 is probably better than model 1, which is probably better than the random model. Cumulative Gain Curve ê³„ì†í•´ì„œ ê°€ì ¸ê°€ê³  ìˆëŠ” ì»¨ì…‰ì€, CATE estimationì€ ê°œì¸ë³„ë¡œ cateë¥¼ êµ¬ë¶„ë˜ê²Œ ì¶”ì •í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ê³ , ì´ë¥¼ ì‚¬ìš©í•´ ì •ë ¬í•œ í›„ ê·¸ë£¹ì„ ë‚˜ëˆ  ATEë¥¼ êµ¬í•˜ë©´ ì°¨ì´ê°€ ë‚˜ì•¼í•œë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/cumulative_gain_curve.png\" alt=\"cumulative_gain_curve\" /> <figcaption> ê·¸ë¦¼2. Cumulative gain curve</figcaption> </figure> ê·¸ë¦¼2ëŠ” ê·¸ë¦¼1ì˜ ê·¸ë£¹ë³„ ATEê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í•œ í›„ì—, ëˆ„ì ë˜ëŠ” ATEë¥¼ ì‚´í´ë³¸ ê²ƒì…ë‹ˆë‹¤. ë¹„êµëŒ€ìƒì¸ ëœë¤í•œ ê²½ìš°ë¥¼ ë³´ë©´, ATE ëˆ„ì  ë¶„í¬ê°€ ì§ì„ ì˜ ëª¨ìŠµì„ ë³´ì´ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°˜ëŒ€ë¡œ ì •ë ¬ì´ ì˜ëœ ê²½ìš°ë¼ë©´ ì´ ëœë¤í•œ ëª¨ë¸ì´ ê·¸ë¦¬ëŠ” ì§ì„ ë³´ë‹¤ ë” ë†’ê²Œ íœ˜ì–´ì§„ ì§ì„ ì„ ê·¸ë¦¬ê³  ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > Once we have the theoretic random curve, we can use it as a benchmark and compare our other models against it. > All curves will start and end at the same point. > However, the better the model at ordering sensitivity, the more the curve will diverge from the random line in the points between zero and one. ì´ëŸ¬í•œ ë°©ë²•ì„ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-23-ë­ë­í•˜ê¸´í•´ë„",
    "title": "ë­ë­í•˜ê¸´ í•´ë„",
    "date": "2025-07-23",
    "excerpt": "ë­ë­í•˜ê¸´ í•´ë„ ì“°ëŠ” í‘œí˜„ë“¤",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Although, its a bit pricey, the bar has an amazing view at night. Although there is a long wait, it has the best bubble tea in town. Even though, Im not as big fan of the movie, I still enjoyed it. I like chocolate, even though I dont like sweet. I love burgers. Its not healthy though. The coat is too expensive. but its pretty though. Fair enough : ì¸ì • ê°„ë‹¨í•œ ì‘ë¬¸ Although seoul is confused, full of people, its a great place to live. Many í¸ì˜ì‹œì„¤ in here. Even though Im not a big fan of anime, I love ê·€ë©¸ì˜ ì¹¼ë‚ . I really love burgers. Its not healthy thought. ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë¬¸ & í”¼ë“œë°± Although Seoul is confusing and crowded, it's a great place to live. â†’ í”¼ë“œë°±: \"confused\" ëŒ€ì‹  \"confusing and crowded\"ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ìˆ˜ì •. \"Many í¸ì˜ì‹œì„¤ in here.\"ëŠ” \"There are many convenient facilities here.\"ë¡œ ë°”ê¾¸ë©´ ì¢‹ì•„ìš”. Even though I'm not a big fan of anime, I love Demon Slayer. â†’ í”¼ë“œë°±: \"ê·€ë©¸ì˜ ì¹¼ë‚ \"ì€ ì˜ì–´ ì œëª© \"Demon Slayer\"ë¡œ ì“°ë©´ ìì—°ìŠ¤ëŸ¬ì›Œìš”. I really love burgers. It's not healthy though. â†’ í”¼ë“œë°±: \"thought\"ëŠ” ì˜¤íƒ€, \"though\"ê°€ ë§ì•„ìš”."
  },
  {
    "id": "Data Science/law-of-total-expectaion",
    "title": "Law of total expectation",
    "date": "2025-07-23",
    "excerpt": "ì „ì²´ê¸°ëŒ€ë²•ì¹™ì— ëŒ€í•´ ì •ë¦¬",
    "category": "Data Science",
    "tags": [
      "math"
    ],
    "content": "ì „ì²´ê¸°ëŒ€ë²•ì¹™, ì§„ì§œ ìƒí™œ ë²„ì „ > í•µì‹¬ ì•„ì´ë””ì–´ > â€œì „ì²´ í‰ê·  = â€˜ê° ìƒí™©ë³„ í‰ê· â€™ë“¤ì„ í™•ë¥ ë¡œ ê°€ì¤‘í•œ í‰ê· â€ --- ë§ˆíŠ¸ ê³„ì‚°ëŒ€ ì´ì•¼ê¸°ë¡œ ì§ê´€ ì¡ê¸° ë§ˆíŠ¸ì—ëŠ” \\\\ë¹ ë¥¸ ê³„ì‚°ëŒ€(ì…€í”„)\\\\ì™€ ì¼ë°˜ ê³„ì‚°ëŒ€(ì§ì›) ë‘ ì¤„ì´ ìˆë‹¤. ì…€í”„ ê³„ì‚°ëŒ€ë¥¼ ì„ íƒí•  í™•ë¥  70 %, ì¼ë°˜ ê³„ì‚°ëŒ€ëŠ” 30 %. ì…€í”„ì—ì„œ ê±¸ë¦¬ëŠ” í‰ê·  ì‹œê°„ì€ 2ë¶„, ì¼ë°˜ ê³„ì‚°ëŒ€ëŠ” 6ë¶„. â€œì „ì²´ ì¤„ ì„œê¸° í‰ê·  ì‹œê°„â€ì€? ì¡°ê±´ë¶€ í‰ê· : ì…€í”„ = 2ë¶„, ì¼ë°˜ = 6ë¶„ ê°€ì¤‘ í‰ê· : $2 \\times 0.7 + 6 \\times 0.3 = 3.2$ ë¶„ 3.2 ë¶„ì´ Law of Total Expectation ê²°ê³¼ë‹¤. ([Wikipedia][1]) > í•œ ì¤„ ìš”ì•½ â€• â€œìƒí™©ë³„ í‰ê· ì„ êµ¬í•œ ë’¤, ê° ìƒí™©ì´ ì¼ì–´ë‚  í™•ë¥ ë¡œ ë‹¤ì‹œ í•œ ë²ˆ í‰ê· ì„ ë‚´ë©´ ì „ì²´ í‰ê· ì´ ëœë‹¤.â€ --- ê³µì‹ì„ ëœ¯ì–´ë³´ë©´ $$ \\mathbb{E}[X] = \\mathbb{E}\\big[\\;\\mathbb{E}[X \\mid Y]\\;\\big] $$ $X$: ì•Œê³  ì‹¶ì€ ëŒ€ìƒ(ì¤„ ì„œëŠ” ì‹œê°„). $Y$: ìƒí™©ì„ ë‚˜ëˆ„ëŠ” ì—´ì‡ (ì„ íƒí•œ ê³„ì‚°ëŒ€). $\\mathbb{E}[X\\mid Y]$: â€œìƒí™© Y ê°€ ì£¼ì–´ì¡Œì„ ë•Œ í‰ê· â€ â€” ì…€í”„ë©´ 2ë¶„, ì¼ë°˜ì´ë©´ 6ë¶„. $\\mathbb{E}[\\cdot]$: ê·¸ ê°’ë“¤ì„ ë‹¤ì‹œ í‰ê·  â€” í™•ë¥  0.7, 0.3ìœ¼ë¡œ ê°€ì¤‘. ([Wikipedia][1]) --- â€˜3ë‹¨ ìš”ë¦¬ë²•â€™ìœ¼ë¡œ ê¸°ì–µí•˜ê¸° ë‹¨ê³„ í•´ì•¼ í•  ì¼ ê²°ê³¼ --------- -------------------------- --------- â‘  ìƒí™© ë‚˜ëˆ„ê¸° ë¬¸ì œë¥¼ ì‰½ê²Œ ìª¼ê°¤ ë³€ìˆ˜Â·ì‚¬ê±´ $Y$ ê³ ë¥´ê¸° â€˜ì…€í”„ / ì¼ë°˜â€™ â‘¡ ìƒí™©ë³„ ê¸°ëŒ€ê°’ $\\mathbb{E}[X\\mid Y=y]$ ê³„ì‚° 2ë¶„, 6ë¶„ â‘¢ ê°€ì¤‘ í‰ê·  â‘¡ì— $P(Y=y)$ ê³±í•´ ë‹¤ ë”í•˜ê¸° 3.2ë¶„ > ì–´ë””ì— ì“°ë‚˜? > > ê³ ê° ì„¸ê·¸ë¨¼íŠ¸ë³„ êµ¬ë§¤ì•¡ ì‚°ì • > - ë³´í—˜ë£Œ ì˜ˆìƒ(ë‚˜ì´ëŒ€Â·ì§€ì—­ë³„) > - A/B í…ŒìŠ¤íŠ¸ ë‘ ê·¸ë£¹ ê²°ê³¼ í•©ì¹˜ê¸° --- íŠ¸ë¦¬ ê·¸ë¦¼ìœ¼ë¡œ ê·¸ë ¤ ë³´ê¸° `` ì „ì²´ ê³ ê° / \\ (70%) (30%) ì…€í”„ ê³„ì‚°ëŒ€ ì¼ë°˜ ê³„ì‚°ëŒ€ í‰ê·  2ë¶„ í‰ê·  6ë¶„ `` ì•„ë˜ì¸µ í‰ê· (2, 6)ì„ ìœ„ì¸µ í™•ë¥ (0.7, 0.3)ë¡œ ëŒì–´ì˜¬ë ¤ ìœ„ì¸µ í‰ê· (3.2)ì„ ë§Œë“ ë‹¤. ê·¸ë˜ì„œ ì´ ë²•ì¹™ì„ \\\\Tower Rule(íƒ‘ ê·œì¹™)\\\\ì´ë¼ê³ ë„ ë¶€ë¥¸ë‹¤. ([Mathematics Stack Exchange][2]) --- â€˜ì¡°ê±´ì„ í•œ ë²ˆ ë” ì£¼ì–´ë„ OKâ€™ $$ \\mathbb{E}\\!\\big[\\;\\mathbb{E}[X \\mid Y,Z]\\;\\big]=\\mathbb{E}[X] $$ ì¸µì„ ì—¬ëŸ¬ ì¸µ ìŒ“ì•„ë„, ë§¨ ê¼­ëŒ€ê¸°ë¡œ ì˜¬ë¼ê°€ë©´ ì–¸ì œë‚˜ $X$ì˜ ì›ë˜ í‰ê· . ì •ë³´ë¥¼ ì¡°ê¸ˆ ì•Œê³ (í•œ ì¸µ ë‚´ë ¤ê°€ê³ ) ë‹¤ì‹œ ëª¨ë¥¸ ì²™(í‰ê· )í•´ë„, ê²°êµ­ ì œìë¦¬ë‹¤. ([The Book of Statistical Proofs][3]) --- í•œëˆˆì— ëë‚´ëŠ” ê¸°ì–µ ì¥ì¹˜ > â€œì¡°ê±´ë¶€ í‰ê· ì€ ê³§ â€˜ë¶€ë¶„ í‰ê· í‘œâ€™, > ê·¸ í‘œë¥¼ ë‹¤ì‹œ í‰ê· ë‚´ë©´ ì „ì²´ í‰ê· í‘œ.â€ --- ìì£¼ ë¬»ëŠ” Q\\&A ì§ˆë¬¸ ë‹µë³€ ------------------- ----------------------------------------------------------------- ì™œ êµ³ì´ ì´ë ‡ê²Œ ëŒë ¤ ê³„ì‚°? ë³µì¡í•œ ë¬¸ì œë¥¼ ì‘ì€ ë¸”ë¡ìœ¼ë¡œ ë‚˜ëˆŒ ë•Œ ê³„ì‚°ì´ í›¨ì”¬ ì‰½ë‹¤. í™•ë¥  ëŒ€ì‹  ë¹„ìœ¨Â·ê°€ì¤‘ì¹˜ ì¨ë„ ë˜ë‚˜? ì´í•©ì´ 1ì´ë©´ ì–´ë–¤ ê°€ì¤‘ì¹˜ë“  ë¬´ë°©. ì—°ì†í˜• ë³€ìˆ˜ë„ ë˜ë‚˜? ì‚¬ê±´ í•© ëŒ€ì‹  ì ë¶„ìœ¼ë¡œ ë°”ë€” ë¿ ë˜‘ê°™ì´ ì„±ë¦½í•œë‹¤. ([Everything Explained Today][4]) --- í•„ìš”í•˜ë©´ ì „ì²´ë¶„ì‚°ë²•ì¹™(Var ë²„ì „)ì´ë‚˜ ë² ì´ì§€ì•ˆ ì—…ë°ì´íŠ¸ì—ì„œì˜ í™œìš© ì˜ˆë„ ì´ì–´ì„œ ë³´ì—¬ì¤„ ìˆ˜ ìˆë‹¤. [1]: https://en.wikipedia.org/wiki/Law_of_total_expectation?utm_source=chatgpt.com \"Law of total expectation\" [2]: https://math.stackexchange.com/questions/41536/intuitive-explanation-of-the-tower-property-of-conditional-expectation?utm_source=chatgpt.com \"Intuitive explanation of the tower property of conditional expectation\" [3]: https://statproofbook.github.io/P/mean-tot.html?utm_source=chatgpt.com \"Law of total expectation The Book of Statistical Proofs\" [4]: https://everything.explained.today/Law_of_total_expectation/?utm_source=chatgpt.com \"Law of total expectation explained\""
  },
  {
    "id": "Causal Inference/what-is-r-learner",
    "title": "R-learnerë€?",
    "date": "2025-07-23",
    "excerpt": "R-learnerì˜ ê°œë…ì— ëŒ€í•´ ì •ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "R-learner",
      "ì¸ê³¼ì¶”ë¡ "
    ],
    "content": "R-learnerëŠ” CATE estimationì— ì‚¬ìš©ë˜ëŠ” meta-learner ì¤‘ í•˜ë‚˜ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•´ í•˜ë‚˜ì”© ì •ë¦¬í—€ìŠµë‹ˆë‹¤. CATE estimationì´ë€? --- ì¸ê³¼ì¶”ë¡  (Causal Inference)ì˜ ëª©ì ì€ ì²˜ì¹˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ”ë°ì— ìˆìŠµë‹ˆë‹¤. ì²˜ì¹˜ê°€ ê²°ê³¼ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ì§€ í™•ì¸í•˜ê³  ì´ë¥¼ í†µí•´ ë” ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 1.1 ATEë€? > ATE(Average Treatment Effect)ëŠ” ì „ì²´ ì§‘ë‹¨ì—ì„œ ì²˜ì¹˜(ì˜ˆ: ì‹ ì•½, ì •ì±… ë“±)ê°€ ë¯¸ì¹˜ëŠ” í‰ê· ì ì¸ ì¸ê³¼ íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. $$ \\text{ATE} = \\mathbb{E}[Y^{(t)} - Y^{(0)}] $$ ì—¬ê¸°ì„œ $$Y^{(t)}$$ì€ ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼ (Potential Outcome)ì´ê³ , $$Y^{(0)}$$ì€ ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼ì…ë‹ˆë‹¤. ì¦‰ ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ í‰ê· ì ì¸ ì¸ê³¼íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 1.2 CATEë€? > CATE(Conditional Average Treatment Effect)ëŠ” íŠ¹ì • ì¡°ê±´ì—ì„œ ì²˜ì¹˜ê°€ ë¯¸ì¹˜ëŠ” í‰ê· ì ì¸ ì¸ê³¼ íš¨ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. $$ \\text{CATE} = \\tau(x) = \\mathbb{E}[Y^{(t)} - Y^{(0)} X = x] $$ ì—¬ê¸°ì„œ $$X$$ëŠ” ì¡°ê±´ì„ ì˜ë¯¸í•˜ê³ , $$Y^{(t)}$$ëŠ” ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼, $$Y^{(0)}$$ëŠ” ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ì ì¬ì  ê²°ê³¼ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ATEë¥¼ í†µí•´ ì²˜ì¹˜ì˜ í‰ê· ì ì¸ ì¸ê³¼íš¨ê³¼(ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥)ì„ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ê°œê°œì¸ë³„ë¡œ ì¸ê³¼íš¨ê³¼ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì€ ì§ê´€ì ì¸ë°ìš”, ì´ëŸ¬í•œ ê°œë³„ ê°œì²´ì˜ ì²˜ì¹˜íš¨ê³¼ë¥¼ ITEë¼ê³  í•©ë‹ˆë‹¤. > ITE (Individual Treatment Effect) > - ì •ì˜: ê°œë³„ ê°œì²´ì˜ ì²˜ì¹˜ íš¨ê³¼ > - ìˆ˜ì‹: $$\\text{ITE}_i = Y_i^{(1)} - Y_i^{(0)}$$ > - ì˜ë¯¸: íŠ¹ì • ê°œì¸ì´ ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì™€ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²°ê³¼ ì°¨ì´ > - íŠ¹ì§•: ì‹¤ì œë¡œëŠ” ê´€ì°° ë¶ˆê°€ëŠ¥ (fundamental problem of causal inference) ê·¸ë¦¬ê³  ì´ë¥¼ ì¶”ì •í•˜ëŠ” ê°’ì´ CATE ì…ë‹ˆë‹¤. > Estimating heteronenous treatment effects is fundamental in causal inference and provides insights into various fields. > > -> Towards R-learner with Continuous Treatment ê°œì¸ë³„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ì¼ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. í•™ìƒë³„ êµìœ¡íš¨ê³¼ë¥¼ ì¶”ì •í•´ ê°œì¸í™”ëœ êµìœ¡ í”„ë¡œê·¸ë¨ì„ ì‹¤í–‰í•˜ê±°ë‚˜, í™˜ìë³„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•´ ê°œì¸í™”ëœ ì²˜ì¹˜ë¥¼ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. CATE estimationì— ì‚¬ìš©ë˜ëŠ” meta-learnerë“¤ --- ë©”íƒ€ëŸ¬ë„ˆ (meta-learner)ëŠ” ê¸°ì¡´ ì˜ˆì¸¡ ë¨¸ì‹ ëŸ¬ë‹ ì•Œê³ ë¦¬ì¦˜ì„ í™œìš©í•´ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ê°„ë‹¨í•œ ë°©ë²•ì…ë‹ˆë‹¤. cate estimationì„ ìœ„í•´ ì‚¬ìš©ë˜ëŠ” meta-learnerì—ëŠ” ëŒ€í‘œì ìœ¼ë¡œ T-learner, X-learner, S-learnerê°€ ìˆìŠµë‹ˆë‹¤. 2.1 T-learner ë²”ì£¼í˜• ì²˜ì¹˜ë¥¼ ë‹¤ë£° ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë©”íƒ€ëŸ¬ë„ˆì…ë‹ˆë‹¤. $$ \\hat{\\mu}_0(x) = \\mathbb{E}[Y T=0, X = x] \\\\ \\hat{\\mu}_1(x) = \\mathbb{E}[Y T=1, X = x] \\\\ \\hat{\\tau}(x) = \\hat{\\mu}_1(x) - \\hat{\\mu}_0(x) $$ <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/T-learner.png\" alt=\"T-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> ê·¸ë¦¼ 1 : T-learner êµ¬ì¡°<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">ì¶œì²˜: Causal Inference for The Brave and True</a> </figcaption> </figure> ê·¸ë¦¼1ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼, ì²˜ì¹˜ë³„ë¡œ ê°ê°ì˜ ì˜ˆì¸¡ ëª¨ë¸ì„ ë§Œë“¤ê³  ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê²°ê³¼ë¥¼ ë¹„êµí•´ CATEë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ``python m0 = LGBMRegressor(max_depth=2, min_child_samples=60) m1 = LGBMRegressor(max_depth=2, min_child_samples=60) m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y]) estimate the CATE t_learner_cate_train = m1.predict(train[X]) - m0.predict(train[X]) t_learner_cate_test = test.assign(cate=m1.predict(test[X]) - m0.predict(test[X])) ` 2.2 S-learner (aka, the Go-Horse Learner) SëŸ¬ë„ˆëŠ” ê°€ì¥ ê¸°ë³¸ì ì¸ ë°©ì‹ìœ¼ë¡œ, ë‹¨ì¼ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ì •í•©ë‹ˆë‹¤. $$ \\hat{\\mu}(x) = E[Y X = x] \\\\ \\hat{\\tau}(x) = \\hat{\\mu}(x, T=1) - \\hat{\\mu}(x, T=0) $$ <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/S-learner.png\" alt=\"S-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> ê·¸ë¦¼ 2 : S-learner êµ¬ì¡°<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">ì¶œì²˜: Causal Inference for The Brave and True</a> </figcaption> </figure> ê·¸ë¦¼2ì—ì„œ ë³´ì´ëŠ” ê²ƒì²˜ëŸ¼ ëª¨ë“  ì²˜ì¹˜ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ê³ , ì´ ëª¨ë¸ì„ í†µí•´ CATEë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. `python s_learner = LGBMRegressor(max_depth=3, min_child_samples=30) s_learner.fit(train[X+[T]], train[y]); s_learner_cate_train = (s_learner.predict(train[X].assign({T: 1})) - s_learner.predict(train[X].assign({T: 0}))) s_learner_cate_test = test.assign( cate=(s_learner.predict(test[X].assign({T: 1})) - # predict under treatment s_learner.predict(test[X].assign({T: 0}))) # predict under control ) ` ì´ ëª¨ë¸ì€ CATE estimationì— ì‚¬ìš©ë˜ëŠ” ë©”íƒ€ëŸ¬ë„ˆ ì¤‘ì— ê°€ì¥ ì‰½ê²Œ ì ìš©í•´ë³¼ ìˆ˜ ìˆëŠ” ëª¨ë¸ì´ì§€ë§Œ, í¸í–¥ì— ê°€ì¥ ì·¨ì•½í•œ ëª¨ë¸ì´ê¸°ë„ í•©ë‹ˆë‹¤. 2.3 X-learner <figure style=\"text-align: center;\"> <img src=\"/post/Causal_Inference/X-learner.png\" alt=\"X-learner\" style=\"display: block; margin: 0 auto;\" /> <figcaption style=\"text-align: center;\"> ê·¸ë¦¼ 3 : X-learner êµ¬ì¡°<br/> <a href=\"https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html\" target=\"_blank\" style=\"font-size: 0.8em; color: #fff;\">ì¶œì²˜: Causal Inference for The Brave and True</a> </figcaption> </figure> X-learnerëŠ” 2ë‹¨ê³„ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤. 1ë‹¨ê³„ì—ì„œëŠ” T-learnerì™€ ë§ˆì°¬ê°€ì§€ë¡œ ì²˜ì¹˜ë³„ë¡œ ë°ì´í„°ë¥¼ ë‚˜ëˆ  ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. $$ \\hat{M}_0(X) \\approx \\mathbb{E}[Y T=0, X] \\\\ \\hat{M}_1(X) \\approx \\mathbb{E}[Y T=1, X] $$ 2ë‹¨ê³„ì—ì„œëŠ” 1ë‹¨ê³„ì—ì„œ í•™ìŠµì‹œí‚¨ ëª¨ë¸ë¡œ ì¶”ì •ëœ CATEë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤. $$ \\hat{\\tau}(X, T=0) = \\hat{M}_0(X) - Y_{T=0} \\\\ \\hat{\\tau}(X, T=1) = \\hat{M}_1(X) - Y_{T=1} $$ ê·¸ í›„ì— ì´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ í•™ìŠµì‹œí‚µë‹ˆë‹¤. $$ \\hat{M}_{\\tau 0}(X) = E[\\hat\\tau(X) T = 0] \\\\ \\hat{M}_{\\tau 1}(X) = E[\\hat\\tau(X) T = 1] $$ ë§ˆì§€ë§‰ìœ¼ë¡œ ì´ ëª¨ë¸ì˜ ê²°ê³¼ë¥¼ ë³´ì™„í•´ì£¼ê¸° ìœ„í•´ì„œ propensity scoreë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. í•™ìŠµì„ ìœ„í•œ $\\hat{\\tau}(X, T=0)$ ì„ ì¶”ì •í•  ë•Œ $Y_1$ ì´ ì—†ê¸° ë•Œë¬¸ì— $\\hat{M}_1$ì„ ì‚¬ìš©í•´ì„œ ì´ë¥¼ ì¶”ì •í–ˆìŠµë‹ˆë‹¤. ì´ë•Œ Xê°€ 0ì¼ í™•ë¥ ì´ ë†’ë‹¤ë©´ $\\hat{M}_1$ì˜ ì˜ˆì¸¡ê°’ì˜ ì„±ëŠ¥ì´ ë–¨ì–´ì§ˆ ê²ƒì´ê³ , ì´ë¡œ ì¸í•´ $\\hat{\\tau}(X, T=0)$ì˜ ì¶”ì •ê°’ë„ ì‹ ë¢°ê°€ ë–¨ì–´ì§ˆ ê²ƒì…ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ \\hat{\\tau}(X, T=0)$ì˜ ì¶”ì •ê°’ì˜ ì‹ ë¢°ë„ëŠ” ì˜¬ë¼ê°ˆ ê²ƒì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§ê´€ì„ ê°€ì§€ê³  propensity scoreë¥¼ ì‚¬ìš©í•´ì„œ ë³´ì™„í•©ë‹ˆë‹¤. $$ \\hat{\\tau}(X) = \\hat{M}_{\\tau 0}(X)\\hat e(x) - \\hat{M}_{\\tau 1}(X)(1-\\hat e(x)) $$ `python from sklearn.linear_model import LogisticRegression np.random.seed(123) first stage models m0 = LGBMRegressor(max_depth=2, min_child_samples=30) m1 = LGBMRegressor(max_depth=2, min_child_samples=30) propensity score model g = LogisticRegression(solver=\"lbfgs\", penalty='none') m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y]) m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y]) g.fit(train[X], train[T]); ` `python d_train = np.where(train[T]==0, m1.predict(train[X]) - train[y], train[y] - m0.predict(train[X])) second stage mx0 = LGBMRegressor(max_depth=2, min_child_samples=30) mx1 = LGBMRegressor(max_depth=2, min_child_samples=30) mx0.fit(train.query(f\"{T}==0\")[X], d_train[train[T]==0]) mx1.fit(train.query(f\"{T}==1\")[X], d_train[train[T]==1]); ` `python def ps_predict(df, t): return g.predict_proba(df[X])[:, t] x_cate_train = (ps_predict(train,1)mx0.predict(train[X]) + ps_predict(train,0)mx1.predict(train[X])) x_cate_test = test.assign(cate=(ps_predict(test,1)mx0.predict(test[X]) + ps_predict(test,0)mx1.predict(test[X]))) `` R-learnerë€? ë‹¤ë¥¸ meta-learnerë“¤ê³¼ ë¹„êµ --- R-learnerëŠ” FWL-theoremì„ meta-learner í˜•íƒœë¡œ êµ¬í˜„í•œ ëª¨ë¸ì…ë‹ˆë‹¤. FWL theoremì´ë€? ì¡°ê¸ˆ ë” ì—„ë°€í•˜ê²Œ ì´ì•¼ê¸°í•˜ë©´ double/debiased machine learningì´ë¼ê³  ë§í•  ìˆ˜ ìˆê³ , ì—¬ê¸°ì— propensity scoreë¥¼ í•¨ê»˜ ì‚¬ìš©í•´ í¸í–¥ì„ ë³´ì •í•˜ëŠ” ëª¨ë¸ì´ R-learnerì…ë‹ˆë‹¤. í¸í–¥ì„ nuisance functionì„ í†µí•´ ì”ì°¨í™”í•˜ì—¬ ì œê±°í•˜ê³ , CATEë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ì´ë•Œ nuisance functionë“¤ì€ ê¸°ì¡´ì˜ ì˜ˆì¸¡ ëª¨ë¸ì„ í†µí•´ ì‰½ê²Œ êµ¬í˜„ë˜ë©°, CATE estimationì— ì‚¬ìš©ë˜ëŠ” ëª¨ë¸ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ì¤‘íšŒê·€ëª¨ë¸ê³¼ ë‹¤ë¥´ê²Œ ë¹„ì„ í˜•ì„±ì„ í¬ì°©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. R-learnerëŠ” ë‹¤ë¥¸ meta-learnerë“¤ê³¼ ë‹¤ë¥´ê²Œ ì´ ëª¨ë¸ì€ CATEë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë‹¤ë¥¸ ëª¨ë¸ë³´ë‹¤ CATEë¥¼ ë” ì˜ ì¶”ì •í•œë‹¤ê³  ì•Œë ¤ì ¸ ìˆìŠµë‹ˆë‹¤. > CATE estimationì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì—ëŠ” 2ê°€ì§€ì •ë„ê°€ ìˆëŠ”ë°, > í•˜ë‚˜ëŠ” ì„ì˜ë¡œ ë°ì´í„°ì…‹ì„ ë§Œë“¤ì–´ ì‹¤ì œë¡œ ì˜ ì¶”ì •í•˜ëŠ”ì§€ í™•ì¸í•˜ëŠ” ê²ƒê³¼ > ë‹¤ë¥¸ í•˜ë‚˜ëŠ” cateë¡œ ì •ë ¬í•´ ê·¸ë£¹ë³„ë¡œ ATEë¥¼ êµ¬í•´ ë¹„êµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. -> how to evaluate CATE estimation model ë˜í•œ íšŒê·€ëª¨ë¸ì„ í†µí•œ ì”ì°¨í™”ì™€ propensity scoreë¥¼ í†µí•œ ì”ì°¨í™”ë¥¼ ì§„í–‰í•´ doubly robust í•˜ë‹¤ëŠ” ì¥ì ë„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. Double robustnessë€? --- í¸í–¥ì„ ì œê±°í•˜ê¸° ìœ„í•œ ë°©ë²•ì—ëŠ” íšŒê·€ë¥¼ í†µí•œ ì”ì°¨í™”ì™€ propensity scoreë¥¼ í†µí•œ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. íšŒê·€ë¥¼ í†µí•œ ì”ì°¨í™”ëŠ” FWL-theoremì— ê¸°ë°˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. propensity scoreë¥¼ í†µí•œ ì”ì°¨í™”ì˜ ì»¨ì…‰ì€ Xë¥¼ í†µì œí•  ìˆ˜ ì—†ë‹¤ë©´ e(X)ë¥¼ í†µì œí•´ì„œ, Xë¥¼ í†µì œí•œ ê²ƒê³¼ ê°™ì€ ìƒíƒœë¥¼ ë§Œë“¤ìëŠ” ê²ƒì…ë‹ˆë‹¤. ì°¸ê³ ìë£Œ PropensityScoreNotebook.ipynb Causal Inference for The Brave and True - Propensity Score R-leanerëŠ” ì´ 2ê°€ì§€ ëª¨ë‘ë¥¼ ì‚¬ìš©í•´ í¸í–¥ì„ ì—†ì• ê³  ìˆê³ , ì´ë¥¼ ìœ„í•œ 2ê°œì˜ ëª¨ë¸ ì¤‘ í•˜ë‚˜ì˜ ì„±ëŠ¥ë§Œ ë³´ì¥ë˜ë©´ ë‚˜ë¨¸ì§€ í•˜ë‚˜ì˜ ì„±ëŠ¥ì´ ë¶€ì¡±í•˜ë”ë¼ë„ CATE estimationì˜ ì„±ëŠ¥ì´ ë³´ì¥ëœë‹¤ê³  í•©ë‹ˆë‹¤. 4.1 ATEì—ì„œ doubly robust estimation $$ \\hat{\\text{ATE}} = \\frac{1}{N} \\sum (\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)} + \\hat \\mu_1(X_i)) - \\frac{1}{N} \\sum (\\frac{(1-T_i)(Y_i - \\hat \\mu_0(X_i))}{1 - \\hat P(X_i)} + \\hat \\mu_0(X_i)) \\tag{1} $$ 1ë²ˆ ìˆ˜ì‹ì—ì„œ ì•ì— ë¶€ë¶„ì€ $\\hat E[Y X, T=1]$ ì˜ ì¶”ì •ê°’ì´ê³ , ë’¤ì— ë¶€ë¶„ì€ $\\hat E[Y X, T=0]$ ì˜ ì¶”ì •ê°’ì…ë‹ˆë‹¤. $\\hat E[Y X, T=1]$ì— ëŒ€í•´ì„œ doubly robustê°€ ì„±ë¦½í•˜ëŠ”ì§€ ì‚´í´ë³´ê³  ì´ë¥¼ ê¸°ë°˜í•´ì„œ $\\hat E[Y X, T=0]$ì— ëŒ€í•´ì„œëŠ” ìœ ì¶”í•´ë³´ë¡ í•˜ê² ìŠµë‹ˆë‹¤. $$ \\hat E[Y_1] = \\frac{1}{N} \\sum (\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)} + \\hat \\mu_1(X_i)) \\tag{2} $$ $\\hat \\mu_1(X_i)$ ê°€ ì •í™•í•˜ê³  propensity scoreê°€ ë¶€ì •í™•í•  ë•Œ. ì‹ 2ì—ì„œ $\\frac{T_i(Y_i - \\hat \\mu_1(X_i))}{\\hat P(X_i)}$ ë¶€ë¶„ì´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°€ ë³´ì¥ë©ë‹ˆë‹¤. $$ \\begin{align} \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i (Y_i - \\hat{\\mu}_1(X_i))}{\\hat{P}(X_i)} + \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\frac{T_i \\hat{\\mu}_1(X_i)}{\\hat{P}(X_i)} + \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\left( \\frac{T_i}{\\hat{P}(X_i)} - 1 \\right) \\hat{\\mu}_1(X_i) \\right) \\\\\\\\ \\hat{\\mathbb{E}}[Y_1] &= \\frac{1}{N} \\sum \\left( \\frac{T_i Y_i}{\\hat{P}(X_i)} - \\left( \\frac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)} \\right) \\hat{\\mu}_1(X_i) \\right) \\end{align} \\tag{3} $$ propensity scoreê°€ ì •í™•í•˜ê³  $\\hat \\mu_1(X_i)$ ê°€ ë¶€ì •í™•í•  ë•Œ. ì‹ 3ì—ì„œ $\\frac{T_i - \\hat{P}(X_i)}{\\hat{P}(X_i)}$ ë¶€ë¶„ì´ 0ì´ ë˜ê¸° ë•Œë¬¸ì— ê²°ê³¼ê°€ ë³´ì¥ë©ë‹ˆë‹¤. 4.2 R-learnerì—ì„œ doubly robustê°€ ë³´ì¥ë˜ëŠ” ë°©ì‹ Quasi-Oracle Estimation of Heterogeneous Treatment Effectsì—ì„œ ì´ë¥¼ ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤. paper link paper review ê¸°ë³¸ì ì¸ ì»¨ì…‰ì€ 2ê°œì˜ nuisance functionì´ ì´ìƒì ì¼ ë•Œì˜ error boundì™€ 2ê°œ ì¤‘ 1ê°œë§Œ ì´ìƒì ì¼ ë•Œì˜ error boundê°€ ê°™ë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. R-lossê°€ ì •ì˜ë˜ëŠ” ë°©ì‹ --- Generalized R-lossëŠ” Towards R-learner with Continuous Treatment ë…¼ë¬¸ì—ì„œ ì œì•ˆëœ ì†ì‹¤í•¨ìˆ˜ì…ë‹ˆë‹¤. paper link paper review $$ L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right] \\tag{generalized R-loss} $$ ì´ëŠ” ë‹¤ìŒ ìˆ˜ì‹ë“¤ì—ì„œ ìœ ë„ë©ë‹ˆë‹¤. ì´ ìˆ˜ì‹ë“¤ì€ ì¸ê³¼ì¶”ë¡ ì˜ ê¸°ë³¸ ê°€ì •ì¸ unconfoundednessì™€ stable unit treatment value assumptionì„ ê°€ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. full conditional outcome mean model $$ \\mu(x, t) = \\mathbb{E}[Y X = x, T = t] $$ conditional outcome mean $$ \\mu(x) = \\mathbb{E}[Y X = x] $$ generalized propensity score $$ \\varpi(x) = \\mathbb{E}[T X = x] $$ ìˆ˜ì‹ ì „ê°œ $$ Y_i^{(T_i)} = \\mu(X_i, T_i) + \\epsilon_i = \\mu(X_i, 0) + \\tau(X_i, T_i) + \\epsilon_i \\\\ m(X_i) = E(Y_i^{(T_i)} X=X_i) = \\mu(X_i, 0) + E_{\\varpi}\\{\\tau(X, T) X=X_i\\} + \\epsilon_i \\\\ Y_i^{(T_i)} - m(X_i) = \\tau(X_i, T_i) - E_{\\varpi}\\{\\tau(X, T) X=X_i\\} + \\epsilon_i \\\\ $$ ë§ˆì§€ë§‰ ì‹ì„ $\\epsilon_i$ ì— ëŒ€í•´ì„œ ì •ë¦¬í•˜ë©´ loss functionì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ $h(X, t)$ëŠ” $tau(X, T)$ë¥¼ ì¶”ì •í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. 5.1 Binary treatmentì— ëŒ€í•œ loss function > Quasi-Oracle Estimation of Heterogeneous Treatment Effectsì—ì„œ ì •ì˜í•œ binary treatmentì— ëŒ€í•œ loss functionì€ generalized R-lossì˜ íŠ¹ìˆ˜í•œ ê²½ìš°ì…ë‹ˆë‹¤. $$ L_b(h) =E[Y - m(X) - {T-e(X)}h(X, 1)]^2 \\tag{binary treatment} $$ Tê°€ 0 ë˜ëŠ” 1ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì´ê¸° ë–„ë¬¸ì— Të¥¼ í’€ì–´ì„œ ì“¸ ìˆ˜ ìˆê³ , $h(x, 0) = 0$ì´ë¼ëŠ” ê°€ì •ì„ loss functionì— ë…¹ì—¬ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Causal Inference/my-fail-experience-graduate-project",
    "title": "ì¡¸ì—… ê³¼ì œë¡œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ",
    "date": "2025-07-23",
    "excerpt": "ì¡¸ì—… ê³¼ì œë¡œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ íšŒê³ ",
    "category": "Causal Inference",
    "tags": [
      "ì¸ê³¼ì¶”ë¡ ",
      "í”„ë¡œì íŠ¸"
    ],
    "content": ""
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-22-ì„ íƒì˜ìƒí™©",
    "title": "ì„ íƒì˜ ìƒí™©",
    "date": "2025-07-22",
    "excerpt": "ì„ íƒì˜ ìƒí™©ì—ì„œ ì“°ëŠ” í‘œí˜„ë“¤",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ best-thing ~ì˜ ê°€ì¥ ì¢‹ì€ ì ì€ ~ì´ë‹¤. The best thing about iPhone is its camera. best-thing -> í•œë²ˆì— ì¼ê¸° aBOUT when it comes to ~ì— ëŒ€í•´ì„œëŠ”, ~ê°€ ê°€ì¥ ì¢‹ì„ ê²ƒì´ë‹¤. When it comes to online shopping, Amazon might be the best option. ê°„ë‹¨í•œ ì‘ë¬¸ When it comes to smart-watch, Galaxy Fit is the best option. The best thing about Galaxy Fit is price. It is ver cheap, but have good design and good quality. --- ë” ìì—°ìŠ¤ëŸ¬ìš´ ë²„ì „ & í”¼ë“œë°± ìì—°ìŠ¤ëŸ¬ìš´ ë²„ì „ When it comes to smartwatches, the Galaxy Fit is the best option. The best thing about the Galaxy Fit is its price. It is very cheap, but it also has a good design and high quality. í”¼ë“œë°± When it comes to smart-watch, Galaxy Fit is the best option. ìˆ˜ì •: When it comes to smartwatches, the Galaxy Fit is the best option. í”¼ë“œë°±: \"smart-watch\"ëŠ” ë³´í†µ ë³µìˆ˜í˜• \"smartwatches\"ë¡œ ì“°ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ê³ , \"the\"ë¥¼ ë¶™ì—¬ íŠ¹ì • ì œí’ˆì„ ì§€ì¹­í•˜ëŠ” ê²ƒì´ ë” ëª…í™•í•©ë‹ˆë‹¤. The best thing about Galaxy Fit is price. ìˆ˜ì •: The best thing about the Galaxy Fit is its price. í”¼ë“œë°±: \"its price\"ë¡œ ì†Œìœ ê²©ì„ ì¨ì„œ ë” ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"the\"ë¥¼ ë¶™ì—¬ ì œí’ˆì„ ëª…í™•íˆ ì§€ì¹­í•˜ì„¸ìš”. It is ver cheap, but have good design and good quality. ìˆ˜ì •: It is very cheap, but it also has a good design and high quality. í”¼ë“œë°±: \"ver\"ëŠ” ì˜¤íƒ€ë¡œ \"very\"ê°€ ë§ìŠµë‹ˆë‹¤. \"have\" ëŒ€ì‹  \"has\"ë¥¼ ì¨ì•¼ í•˜ê³ , \"good design and good quality\" ëŒ€ì‹  \"a good design and high quality\"ë¡œ ë” ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"also\"ë¥¼ ì¶”ê°€í•˜ë©´ ë¬¸ì¥ì´ ë” ë¶€ë“œëŸ½ê²Œ ì—°ê²°ë©ë‹ˆë‹¤."
  },
  {
    "id": "Data Science/parametric-vs-non-parametric",
    "title": "Parametric vs Non-Parametric, GBDTëŠ” ì™œ non-parametricì¼ê¹Œ?",
    "date": "2025-07-22",
    "excerpt": "Parametricê³¼ Non-Parametricì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…",
    "category": "Data Science",
    "tags": [
      "parametric",
      "non-parametric",
      "machine-learning",
      "statistics"
    ],
    "content": "Parametricì´ë€? ì˜ì–´ í•œêµ­ì–´ ì„¤ëª… ------------------------- ----------- -------------------------------- parametric model ëª¨ìˆ˜ì  ëª¨ë¸ ê³ ì •ëœ ìˆ˜ì˜ íŒŒë¼ë¯¸í„°(ëª¨ìˆ˜)ë¥¼ ê°–ëŠ” ëª¨ë¸ parametric assumption ëª¨ìˆ˜ì  ê°€ì • ë°ì´í„°ê°€ íŠ¹ì •í•œ ë¶„í¬ í˜•íƒœ(ì˜ˆ: ì •ê·œë¶„í¬)ë¥¼ ë”°ë¥¸ë‹¤ëŠ” ê°€ì • non-parametric model ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ íŒŒë¼ë¯¸í„° ìˆ˜ë‚˜ í•¨ìˆ˜ í˜•íƒœë¥¼ ê°€ì •í•˜ì§€ ì•ŠëŠ” ìœ ì—°í•œ ëª¨ë¸ ë°ì´í„°ì—ì„œ íŒ¨í„´ì„ ê·¼ì‚¬í•´ë‚´ëŠ”ê²Œ Machine Learningì˜ í•µì‹¬ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ë°ì´í„°ê°€ íŠ¹ì •í•œ í•¨ìˆ˜ í˜•íƒœë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê±°ë‚˜, ëª¨ë¸ì˜ êµ¬ì¡°(ì˜ˆ: ì„ í˜• íšŒê·€ì²˜ëŸ¼)ê°€ ê³ ì •ë˜ì–´ ìˆëŠ” ê²½ìš°ë¥¼ ëª¨ìˆ˜ì (parametric) ëª¨ë¸ì´ë¼ê³  í•©ë‹ˆë‹¤. Parametric vs Non-Parametric êµ¬ë¶„ Parametric Non-parametric ------------------- -------------- ------------------------ ëª¨ë¸ êµ¬ì¡° ì‚¬ì „ì— ê³ ì •ë¨ ìœ ì—°í•˜ê³  ë°ì´í„° ê¸°ë°˜ ëª¨ìˆ˜(parameter) ìˆ˜ ê³ ì • ë°ì´í„°ê°€ ë§ì•„ì§€ë©´ ì¦ê°€í•¨ ë³µì¡ë„ ì œì–´ íŒŒë¼ë¯¸í„° ì¡°ì • ëª¨ë¸ ìì²´ì˜ êµ¬ì¡° ë³€í™”ë¡œ ì¡°ì • ì˜ˆì‹œ ì„ í˜• íšŒê·€, ë¡œì§€ìŠ¤í‹± íšŒê·€ GBDT, KNN, ëœë¤í¬ë ˆìŠ¤íŠ¸, ì»¤ë„ íšŒê·€ ë°˜ëŒ€ë¡œ Non-parametric ëª¨ë¸ì€ ëª¨ë¸ì˜ í˜•íƒœë‚˜ êµ¬ì¡°ë¥¼ ì‚¬ì „ì— ê°€ì •í•˜ì§€ ì•Šê³ , ë°ì´í„°ë¥¼ í†µí•´ ëª¨ë¸ì˜ êµ¬ì¡°ë¥¼ ìœ ì—°í•˜ê²Œ ë§Œë“¤ì–´ë‚˜ê°€ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì™œ GBDTëŠ” non-parametricì¼ê¹Œ? ì´ìœ  ì„¤ëª… --------- --------------------------------- íŒŒë¼ë¯¸í„° ìˆ˜ ì¦ê°€ ë°ì´í„°ê°€ ëŠ˜ë©´ íŠ¸ë¦¬ ê°œìˆ˜ë‚˜ ê¹Šì´ê°€ ì»¤ì ¸ì„œ êµ¬ì¡° ìì²´ê°€ ë³€í™”í•¨ êµ¬ì¡° ìœ ì—°ì„± íŠ¹ì • í•¨ìˆ˜ í˜•íƒœë¥¼ ê°€ì •í•˜ì§€ ì•ŠìŒ (ì˜ˆ: ì„ í˜•ì„±) ììœ ë„ ê³¼ì í•© ìœ„í—˜ë„ ìˆì§€ë§Œ, ê·¸ë§Œí¼ ë³µì¡í•œ í•¨ìˆ˜ë„ í•™ìŠµ ê°€ëŠ¥ í•™ìŠµ ëŒ€ìƒ ë°ì´í„°ê°€ ê°€ë¥´ì³ì£¼ëŠ” ë°©ì‹ì— ë”°ë¼ í•¨ìˆ˜ êµ¬ì¡° ìì²´ê°€ ê²°ì •ë¨ GBDTëŠ” ì”ì°¨ë¥¼ ë³´ì™„í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ì—¬ëŸ¬ ê°œì˜ ì•½í•œ ê²°ì • íŠ¸ë¦¬ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í•™ìŠµí•˜ê³  ëˆ„ì í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ì§€ë§Œ, íŠ¸ë¦¬ì˜ ê°œìˆ˜, ê¹Šì´, ë…¸ë“œ ìˆ˜, ë¶„í•  ê·œì¹™ ë“±ì€ ë°ì´í„° ì–‘ê³¼ ë³µì¡ë„ì— ë”°ë¼ ê³„ì† ëŠ˜ì–´ë‚˜ê±°ë‚˜ ë°”ë€” ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ì ì—ì„œ GBDTëŠ” non-parametric ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Data Science/bias-variance",
    "title": "Bias-Variance Tradeoff",
    "date": "2025-07-22",
    "excerpt": "Bias-Variance Tradeoffì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…",
    "category": "Data Science",
    "tags": [
      "bias-variance",
      "machine-learning",
      "statistics"
    ],
    "content": "Bias-Variance Tradeoff ë¨¸ì‹ ëŸ¬ë‹ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ê°œë… ì¤‘ í•˜ë‚˜ê°€ Bias-Variance Tradeoffì…ë‹ˆë‹¤. ì´ëŠ” ëª¨ë¸ì˜ ë³µì¡ë„ì™€ ì¼ë°˜í™” ì„±ëŠ¥ ì‚¬ì´ì˜ ê· í˜•ì„ ì„¤ëª…í•˜ëŠ” í•µì‹¬ ê°œë…ì…ë‹ˆë‹¤. ê¸°ë³¸ ê°œë… Bias (í¸í–¥) > í•™ìŠµëœ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ í‰ê· ê³¼ ì‹¤ì œ ê°’ ê°„ì˜ ì°¨ì´ ì •ì˜: ëª¨ë¸ì´ ì‹¤ì œ ë°ì´í„° ìƒì„± ê³¼ì •ì„ ì–¼ë§ˆë‚˜ ì˜ ê·¼ì‚¬í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ ë†’ì€ Bias: ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í•¨ ë‚®ì€ Bias: ëª¨ë¸ì´ ë³µì¡í•´ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì˜ í•™ìŠµí•  ìˆ˜ ìˆìŒ Variance (ë¶„ì‚°) > ì˜ˆì¸¡ê°’ì´ ì–¼ë§ˆë‚˜ í©ì–´ì¡ŒëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ ì •ì˜: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì˜ ì‘ì€ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•œì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ ë†’ì€ Variance: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ ë‚®ì€ Variance: ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì˜ ë…¸ì´ì¦ˆì— ëœ ë¯¼ê°í•¨ ìˆ˜í•™ì  í‘œí˜„ ì˜ˆì¸¡ ì˜¤ì°¨ì˜ ë¶„í•´ $$E[(y - \\hat{f}(x))^2] = \\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Error}$$ ì—¬ê¸°ì„œ: $y$: ì‹¤ì œ ê°’ $\\hat{f}(x)$: ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ $\\text{Bias}^2 = E[(\\hat{f}(x) - f(x))^2]$: í¸í–¥ì˜ ì œê³± $\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$: ë¶„ì‚° $\\text{Irreducible Error}$: ë°ì´í„° ìì²´ì˜ ë…¸ì´ì¦ˆë¡œ ì¸í•œ ì˜¤ì°¨ í¸í–¥ê³¼ ë¶„ì‚°ì˜ ê³„ì‚° í¸í–¥ê³¼ ë¶„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë©ë‹ˆë‹¤: $$\\text{Bias} = E[\\hat{f}(x)] - f(x)$$ $$\\text{Variance} = E[(\\hat{f}(x) - E[\\hat{f}(x)])^2]$$ Bias-Variance Tradeoff í•µì‹¬ ì•„ì´ë””ì–´ ëª¨ë¸ì´ ë‹¨ìˆœí• ìˆ˜ë¡: ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° ëª¨ë¸ì´ ë³µì¡í• ìˆ˜ë¡: ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° ì‹œê°ì  ì˜ˆì‹œ <figure style=\"text-align: center;\"> <img src=\"/post/bias_variance/bias_variance_1.png\" alt=\"Bias-Variance Tradeoff\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto;\"/> <figcaption>ê·¸ë¦¼ 1: Bias-Variance ê°œë… ì„¤ëª… ê·¸ë¦¼</figcaption> </figure> ê·¸ë¦¼1ì€ í¸í–¥ê³¼ ë¶„ì‚°ì˜ ê°œë…ì„ ì„¤ëª…í•˜ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. í¸í–¥ì€ ë°ì´í„°ì˜ íŒ¨í„´ì„ ì–¼ë§ˆë‚˜ ì˜ í•™ìŠµí–ˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ í¸í–¥ì´ ë‚®ìœ¼ë©´ íŒ¨í„´ì„ ì˜ í•™ìŠµí•´ íŒŒë€ ì› ì•ˆì— ì•ˆì— ì˜ˆì¸¡ê°’ì´ ëª¨ì—¬ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¶„ì‚°ì€ ì˜ˆì¸¡ê°’ì´ ì–¼ë§ˆë‚˜ í©ì–´ì ¸ìˆëŠ”ì§€ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë¶„ì‚°ì´ ë‚®ìœ¼ë©´ í•œ ê³³ì— ë­‰ì³ìˆê³ , ë¶„ì‚°ì´ ë†’ìœ¼ë©´ ë„“ê²Œ í´ì ¸ìˆëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure style=\"text-align: center;\"> <img src=\"/post/bias_variance/bias_variance_2.png\" alt=\"Bias-Variance Tradeoff\" style=\"width: 100%; max-width: 600px; display: block; margin: 0 auto;\"/> <figcaption>ê·¸ë¦¼ 2: ì‹¤ì œ ì˜ˆì¸¡ì—ì„œì˜ Bias-Variance Tradeoff - ë‹¨ìˆœí•œ ëª¨ë¸(ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚°) vs ë³µì¡í•œ ëª¨ë¸(ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚°)</figcaption> </figure> ê·¸ë¦¼2ëŠ” ì‹¤ì œ ì˜ˆì¸¡ì—ì„œì˜ Bias-Variance Tradeoffë¥¼ ì„¤ëª…í•˜ëŠ” ê·¸ë¦¼ì…ë‹ˆë‹¤. í•™ìŠµí•˜ëŠ” ë°ì´í„°ì…‹ ì•ˆì—ì„œ íŒ¨í„´ì„ ì˜ ì°¾ìœ¼ë©´ (í¸í–¥ì´ ë‚®ì„ìˆ˜ë¡) ëª¨ë¸ì€ ë³µì¡í•´ì ¸ì•¼ í•˜ê³ , ì´ë•Œ ì˜ˆì¸¡ ê°’ì´ ë„“ê²Œ í¼ì ¸ìˆê²Œ ë©ë‹ˆë‹¤ (ë¶„ì‚°ì´ ë†’ì•„ì§) ë°˜ëŒ€ë¡œ í•™ìŠµí•˜ëŠ” ë°ì´í„°ì…‹ ì•ˆì—ì„œ íŒ¨í„´ì„ ì˜ ì°¾ì§€ ëª»í•˜ë©´ (í¸í–¥ì´ ë†’ì„ìˆ˜ë¡) ëª¨ë¸ì€ ë‹¨ìˆœí•´ì ¸ì•¼ í•˜ê³ , ì´ë•Œ ì˜ˆì¸¡ ê°’ì´ í•œ ê³³ì— ëª¨ì—¬ìˆê²Œ ë©ë‹ˆë‹¤ (ë¶„ì‚°ì´ ë‚®ì•„ì§) `` ë‹¨ìˆœí•œ ëª¨ë¸ (ì˜ˆ: ì„ í˜• íšŒê·€) â”œâ”€â”€ ë†’ì€ Bias: ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ë†“ì¹¨ â””â”€â”€ ë‚®ì€ Variance: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì•ˆì •ì  ë³µì¡í•œ ëª¨ë¸ (ì˜ˆ: ê³ ì°¨ ë‹¤í•­ì‹, ë”¥ëŸ¬ë‹) â”œâ”€â”€ ë‚®ì€ Bias: ë°ì´í„°ì˜ ë³µì¡í•œ íŒ¨í„´ì„ ì˜ í•™ìŠµ â””â”€â”€ ë†’ì€ Variance: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ë¶ˆì•ˆì • `` ì‹¤ì œ ì˜ˆì‹œ ì„ í˜• íšŒê·€ vs ë‹¤í•­ íšŒê·€ ì„ í˜• íšŒê·€ (1ì°¨ ë‹¤í•­ì‹) ë‚®ì€ ë³µì¡ë„ â†’ ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° ë°ì´í„°ê°€ ë¹„ì„ í˜• íŒ¨í„´ì„ ê°€ì§€ê³  ìˆë‹¤ë©´ ì„±ëŠ¥ì´ ë–¨ì–´ì§ ê³ ì°¨ ë‹¤í•­ íšŒê·€ (ì˜ˆ: 10ì°¨ ë‹¤í•­ì‹) ë†’ì€ ë³µì¡ë„ â†’ ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° í›ˆë ¨ ë°ì´í„°ì—ëŠ” ì™„ë²½í•˜ê²Œ ì í•©í•˜ì§€ë§Œ ìƒˆë¡œìš´ ë°ì´í„°ì—ëŠ” ì„±ëŠ¥ì´ ë–¨ì–´ì§ ëª¨ë¸ ì„ íƒì—ì„œì˜ ì ìš© Underfitting vs Overfitting Underfitting (ê³¼ì†Œì í•©) ë†’ì€ í¸í–¥, ë‚®ì€ ë¶„ì‚° ëª¨ë¸ì´ ë„ˆë¬´ ë‹¨ìˆœí•´ì„œ ë°ì´í„°ì˜ íŒ¨í„´ì„ í•™ìŠµí•˜ì§€ ëª»í•¨ í•´ê²°ì±…: ëª¨ë¸ ë³µì¡ë„ ì¦ê°€ Overfitting (ê³¼ì í•©) ë‚®ì€ í¸í–¥, ë†’ì€ ë¶„ì‚° ëª¨ë¸ì´ í›ˆë ¨ ë°ì´í„°ì— ê³¼ë„í•˜ê²Œ ì í•©ë˜ì–´ ì¼ë°˜í™” ì„±ëŠ¥ì´ ë–¨ì–´ì§ í•´ê²°ì±…: ëª¨ë¸ ë³µì¡ë„ ê°ì†Œ, ì •ê·œí™”, ë” ë§ì€ ë°ì´í„° ìµœì  ëª¨ë¸ ì„ íƒ êµì°¨ ê²€ì¦ (Cross-Validation) í›ˆë ¨ ë°ì´í„°ì™€ ê²€ì¦ ë°ì´í„°ë¥¼ ë¶„ë¦¬í•˜ì—¬ ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€ í¸í–¥ê³¼ ë¶„ì‚°ì˜ ê· í˜•ì ì„ ì°¾ëŠ” ë° ë„ì›€ ì •ê·œí™” (Regularization) Ridge Regression (L2 ì •ê·œí™”): ë¶„ì‚°ì„ ì¤„ì—¬ ê³¼ì í•© ë°©ì§€ Lasso Regression (L1 ì •ê·œí™”): íŠ¹ì„± ì„ íƒì„ í†µí•œ ëª¨ë¸ ë‹¨ìˆœí™” ì‹¤ë¬´ì  ê³ ë ¤ì‚¬í•­ ë°ì´í„° í¬ê¸°ì˜ ì˜í–¥ ì‘ì€ ë°ì´í„°ì…‹: ë¶„ì‚°ì´ ë†’ì•„ ê³¼ì í•© ìœ„í—˜ ì¦ê°€ í° ë°ì´í„°ì…‹: í¸í–¥ì´ ë” ì¤‘ìš”í•œ ê³ ë ¤ì‚¬í•­ íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ì ˆí•œ íŠ¹ì„± ì„ íƒê³¼ ë³€í™˜ì´ í¸í–¥-ë¶„ì‚° ê· í˜•ì— í° ì˜í–¥ ë„ë©”ì¸ ì§€ì‹ì„ í™œìš©í•œ íŠ¹ì„± ìƒì„±ì´ ì¤‘ìš” ê²°ë¡  Bias-Variance TradeoffëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì„¤ê³„ì˜ í•µì‹¬ ì›ì¹™ì…ë‹ˆë‹¤. ìµœì ì˜ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë°ì´í„°ì™€ ë¬¸ì œì— ëŒ€í•´ í¸í–¥ê³¼ ë¶„ì‚°ì´ ì ì ˆíˆ ê· í˜•ì„ ì´ë£¨ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œëŠ”: ë°ì´í„°ì˜ íŠ¹ì„± ì´í•´: ë°ì´í„°ì˜ ë³µì¡ë„ì™€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ íŒŒì•… ëª¨ë¸ ë³µì¡ë„ ì¡°ì ˆ: ë¬¸ì œì— ì í•©í•œ ëª¨ë¸ ì„ íƒ ì •ê·œí™” ê¸°ë²• í™œìš©: ê³¼ì í•© ë°©ì§€ë¥¼ ìœ„í•œ ê¸°ë²• ì ìš© êµì°¨ ê²€ì¦: ëª¨ë¸ì˜ ì¼ë°˜í™” ì„±ëŠ¥ í‰ê°€ ì´ëŸ¬í•œ ì›ì¹™ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ë¬´ì—ì„œ ë” ë‚˜ì€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Causal Inference/Paper Review/Double-Debiased-Machine-Learning-for-Treatment-and-Structural-Parameters",
    "title": "[Paper Review] Double Debiased Machine Learning for Treatment and Structural Parameters",
    "date": "2025-07-22",
    "excerpt": "Double Debiased Machine Learning for Treatment and Structural Parameters",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-21-ê³ ë¯¼í• ë•Œ",
    "title": "ê³ ë¯¼í•˜ëŠ” í‘œí˜„ë“¤",
    "date": "2025-07-21",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "On one hand, the Note has a really big screen. On one hand, I havent had Korean food in a while On one hand, taking a cab would be really convenient On the other hand, the new iPhone has a really good camera. On the other hand, Im craving fried chicken. On the other hand, I can walk there in 15 minute. Whether I buy the Note or the new iPhone, I should buy a phone tommorow."
  },
  {
    "id": "Career/Job Transition/25-07-21-ë‚¨ì² ë‹˜-clabi-ì»¤í”¼ì±—",
    "title": "Clabi coffee chat with ë‚¨ì² ",
    "date": "2025-07-21",
    "excerpt": "Clabi ì±„ìš©ê³µê³  ê´€ë ¨í•´ì„œ ë‚¨ì² ë‹˜ê³¼ ì»¤í”¼ì±— ì§„í–‰",
    "category": "Career",
    "tags": [
      "Coffee Chat",
      "Clabi"
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì»¤í”¼ì±— ìƒì„¸ì •ë³´</strong></summary> í´ë¼ë¹„ ê¸°ìˆ ì—°êµ¬ì†Œ AI ê°œë°œì ì±„ìš© (RAG, LLM, NLP, ì¸ê³µì§€ëŠ¥) ëª¨ì§‘ë¶€ë¬¸ ë° ìƒì„¸ë‚´ìš© ê³µí†µ ìê²©ìš”ê±´ í•™ë ¥: ë¬´ê´€ ì±„ìš© í”„ë¡œì„¸ìŠ¤ ì„œë¥˜ ì§€ì› > ê¸°ìˆ  ë©´ì ‘(íŒ€ì¥) > 2ì°¨ ë©´ì ‘ (ì—°êµ¬ì†Œì¥ë‹˜) > 3ì°¨ ë©´ì ‘ (ëŒ€í‘œë‹˜) AIOps AI ê°œë°œì ëª¨ì§‘ (ê¸°ìˆ ì—°êµ¬ì†Œ 0ëª…) ì£¼ìš”ì—…ë¬´ LLM ë° RAG ì—°êµ¬ë¥¼ í†µí•œ AI ì„œë¹„ìŠ¤ ê³ ë„í™” ë° ì‹ ê·œ ê¸°ëŠ¥ ê°œë°œ í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ì„ í™œìš©í•œ ì±—ë´‡ ë° AI ëª¨ë¸ ê°œë°œ AI ëª¨ë¸ ì„±ëŠ¥ íŒŒì¸íŠœë‹ ë° ìµœì í™” RAG íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë° ê´€ë ¨ ì• í”Œë¦¬ì¼€ì´ì…˜ ê°œë°œ ëŒ€ê·œëª¨ ë°ì´í„° ìˆ˜ì§‘ ì „ì²˜ë¦¬ ë° ê´€ë¦¬ ê·¼ë¬´ì¡°ê±´ ê·¼ë¬´í˜•íƒœ: ì •ê·œì§(ìˆ˜ìŠµê¸°ê°„ 3ê°œì›”) ê·¼ë¬´ì¼ì‹œ: ì£¼ 5ì¼ (ì›”-ê¸ˆ) 10:00~17:00 ê·¼ë¬´ì§€ì—­: (05854) ì„œìš¸ ì†¡íŒŒêµ¬ ë²•ì›ë¡œ 114 ì— ìŠ¤í…Œì´íŠ¸ Bë™ 1108í˜¸(ë¬¸ì •ë™) ì„œìš¸ 8í˜¸ì„  ë¬¸ì •ì—­ì—ì„œ 200m ì´ë‚´ ì§€ì›ë°©ë²• ì´ë ¥ì„œ ë° í¬íŠ¸í´ë¦¬ì˜¤ ì œì¶œ ì§€ì› ë° ë¬¸ì˜ : DM ë˜ëŠ” ì´ë©”ì¼(ncher@clabi.co.kr) </details> ì»¤í”¼ì±— ì§„í–‰ ë°©ì‹ ì•„ì´í ì—ì„œ ê³µë¶€í•  ë•Œ í¼ì‹¤ë‹˜ì´ì—ˆë˜ ë‚¨ì² ë‹˜ê³¼ ì±„ìš©ê´€ë ¨ ì»¤í”¼ì±— ì§„í–‰. ì˜¨ë¼ì¸ìœ¼ë¡œ 1ì‹œê°„ì •ë„ ì§„í–‰í•¨. ê°„ë‹¨í•œ ê·¼í™© ì´ì•¼ê¸°ì™€ ë‚´ê°€ ì§„í–‰í–ˆë˜ í”„ë¡œì íŠ¸ì— ëŒ€í•´ì„œ ì„¤ëª…ë“œë¦¬ê³  íšŒì‚¬ì— ëŒ€í•´ ê¶ê¸ˆí•œ ì  ì§ˆë¬¸. ìœ ì‚¬ ê²½í—˜ (í•„ì‚´ê¸°) ì„¤ëª… íšŒì‚¬ì—ì„œ í•˜ëŠ” ì¼ê³¼ ë‚´ê°€ ì§„í–‰í–ˆë˜ ì¼ì˜ ìœ ì‚¬ì„±ì„ ì„¤ëª…. ìœ ì‚¬ ê²½í—˜ì´ ìˆê³  íšŒì‚¬ì—ì„œ í•  ì¼ê³¼ ë¹„ìŠ·í•œ ê³ ë¯¼ë“¤ì„ í–ˆì—ˆë‹¤. ì±—ë´‡ í”„ë¡œì íŠ¸ Clabiì˜ ì£¼ìš” ì—…ë¬´ê°€ RAG SIì¸ë° ì‹¤ì œë¡œ RAGë¥¼ í†µí•´ ë¹„ì¦ˆë‹ˆìŠ¤ ë¬¸ì œë¥¼ í•´ê²°í–ˆë˜ ê²½í—˜ì´ ìˆìœ¼ë‹ˆê¹Œ ì´ë¥¼ ê°•ì¡°. -> ê¸ì •ì ì¸ ë°˜ì‘. ì•„ì‰¬ìš´ ì ì€ ì§„í–‰í–ˆë˜ ì¼ì˜ ë‚œì´ë„ê°€ ê·¸ë ‡ê²Œ ë†’ì§€ ì•Šì•˜ë‹¤ëŠ” ì . ë§Œì•½ ê¸°ìˆ ë©´ì ‘ì„ ì¤€ë¹„í•œë‹¤ë©´ ì´ ë¶€ë¶„ì„ ë¯¸ë¦¬ ê³µë¶€í•˜ê³  ê°€ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. ë‚´ê°€ ê¶ê¸ˆí–ˆë˜ ë¶€ë¶„ RAG SI ì—…ë¬´ëŠ” ì§„ì§œ ì „í˜€ ì•ˆëŒë¦¼. ê·¸ë˜ì„œ ì´ ë¶€ë¶„ì„ ë²—ì–´ë‚˜ì„œ í™•ì¥í•˜ë ¤ëŠ” ë¶€ë¶„ì´ ìˆëŠ”ì§€ -> í˜„ì¬ ìˆ˜ìµìœ¼ë¡œ ìƒì„±í˜• AI, SasS ë¶€ë¶„ìœ¼ë¡œ í™•ì¥í•˜ê³  ìˆë‹¤ê³  í•¨ ëŒ€ê·œëª¨ ë°ì´í„° ì²˜ë¦¬. ë¹…ë°ì´í„° í”Œë«í¼. ìŠ¤ë§ˆíŠ¸íŒœ ë¶€ë¶„ì€ ì–´ë–»ê²Œ ì§„í–‰ë˜ëŠ”ì§€ -> ì•„ë§ˆ ì™¸ë¶€ì—…ì²´ê°€ ì°¸ì—¬í•˜ëŠ” ê²ƒ ê°™ë‹¤. ì¸í„°ë·°í•˜ê³  ëŠë‚€ ì ì€ ë‚´ê°€ ì›í•˜ëŠ” ë°©í–¥ì˜ ì»¤ë¦¬ì–´ëŠ” ì•„ë‹ˆë‹¤. ì¡°ê¸ˆ ë” ë°ì´í„°ë¥¼ ì‚´í´ë³´ê³  ë¶„ì„í•˜ë©´ì„œ ëª¨ë¸ì„ ë§Œë“œëŠ”, ë°ì‹¸ì ì¸ ëŠë‚Œì˜ ì»¤ë¦¬ì–´ê°€ ë‚˜ëŠ” ë” ì¢‹ì€ ê²ƒ ê°™ë‹¤. ê·¸ë¦¬ê³  ë‹¨ìˆœíˆ SIì„± ì¼ë“¤ì´ ì•„ë‹Œ, ì¢€ ë” ë†’ì€ ìˆ˜ì¤€ì˜ ë‚œì´ë„ë¥¼ ìš”êµ¬í•˜ëŠ” ì¼ì„ í•˜ê³  ì‹¶ë‹¤."
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-20-ì¡°ì–¸í•˜ëŠ”í‘œí˜„Review",
    "title": "Review day",
    "date": "2025-07-20",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "Review day"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ê±° Try to ~~ Try to cut down on drinking. Try not to skip breakfast. Make sure to ~ Make sure not to eat too much junk foods. Im worried about ~ Im worried about my love life. Im little worried about my bills. Be on the lookout for Be on the lookout for pickpockets. Be careful with ~ Be careful with your phone. Be careful not to burn yourself. Be careful not to catch a cold. Be careful not to cut yourself. By the way By the way, I was the blind date? Well the things is Well the thing is Im not really attracted to her. ê°„ë‹¨í•œ ì‘ë¬¸ (ì˜¤ëŠ˜ ë°°ìš´ê±° ì‚¬ìš©ë„ í•´ë³´ë©´ì„œ ì‘ë¬¸í•˜ëŠ” ì—°ìŠµ) Time is so fast. ì •ì‹ ì„ ì°¨ë ¤ë³´ë©´ ì‹œê°„ì€ í•œì°¸ ì§€ë‚˜ìˆë‹¤. Acheving something like, get a good english, playing guitar well, ì¢‹ì€ íšŒì‚¬ì— ì·¨ì§í•˜ëŠ” ê²ƒ ,... are needs many time in the days. If you cant focus on today looking farway and beding nervous, I never acheive somethings that needs days. You should know that today is so precious. You can do everthing, but you couldn`t do nothing if you don't move. So, make sure to focus on the day you are living. ë” ìì—°ìŠ¤ëŸ¬ìš´ ì‘ë¬¸ Time flies so fast. Before you know it, a lot of time has passed. Achieving things like getting good at English, playing guitar well, or getting a job at a good company requires many days of effort. If you can't focus on today and keep looking far ahead while getting nervous, you'll never achieve things that take time. You should know that today is precious. You can do anything, but you won't accomplish anything if you don't take action. So, make sure to focus on the day you're living. í”¼ë“œë°± ë¬¸ë²• ë° í‘œí˜„ ìˆ˜ì •ì‚¬í•­ \"Time is so fast\" â†’ \"Time flies so fast\" \"Time is fast\"ëŠ” ë¬¸ë²•ì ìœ¼ë¡œ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. \"Time flies\"ê°€ ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì…ë‹ˆë‹¤. \"Acheving\" â†’ \"Achieving\" ì² ì ì˜¤ë¥˜ì…ë‹ˆë‹¤. \"get a good english\" â†’ \"getting good at English\" \"English\"ëŠ” ëŒ€ë¬¸ìë¡œ ì‹œì‘í•´ì•¼ í•˜ë©°, \"good at English\"ê°€ ì˜¬ë°”ë¥¸ í‘œí˜„ì…ë‹ˆë‹¤. \"are needs many time\" â†’ \"requires many days of effort\" ë¬¸ë²•ì ìœ¼ë¡œ ë§ì§€ ì•ŠëŠ” êµ¬ì¡°ì…ë‹ˆë‹¤. \"require\"ë¥¼ ì‚¬ìš©í•˜ê³  \"time\" ëŒ€ì‹  \"days of effort\"ê°€ ë” ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. \"If you cant focus\" â†’ \"If you can't focus\" \"can't\"ì˜ ì¶•ì•½í˜•ì—ì„œ apostropheê°€ ë¹ ì¡ŒìŠµë‹ˆë‹¤. \"looking farway\" â†’ \"looking far ahead\" \"farway\"ëŠ” ì˜¬ë°”ë¥¸ ë‹¨ì–´ê°€ ì•„ë‹™ë‹ˆë‹¤. \"far ahead\"ê°€ ë§ìŠµë‹ˆë‹¤. \"beding nervous\" â†’ \"getting nervous\" \"beding\"ì€ \"being\"ì˜ ì˜¤íƒ€ë¡œ ë³´ì´ì§€ë§Œ, ë§¥ë½ìƒ \"getting nervous\"ê°€ ë” ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. \"I never acheive\" â†’ \"you'll never achieve\" ì£¼ì–´ë¥¼ ì¼ê´€ì„± ìˆê²Œ \"you\"ë¡œ í†µì¼í•˜ê³ , ë¯¸ë˜ ì‹œì œë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. \"somethings\" â†’ \"things\" \"something\"ì€ ì…€ ìˆ˜ ì—†ëŠ” ëª…ì‚¬ì´ë¯€ë¡œ ë³µìˆ˜í˜•ì´ ì—†ìŠµë‹ˆë‹¤. \"You can do everthing\" â†’ \"You can do anything\" \"everthing\"ì€ \"everything\"ì˜ ì˜¤íƒ€ì…ë‹ˆë‹¤. \"couldn't do nothing\" â†’ \"won't accomplish anything\" ì´ì¤‘ ë¶€ì •ì„ í”¼í•˜ê³  ë” ëª…í™•í•œ í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. \"if you don't move\" â†’ \"if you don't take action\" \"move\"ë³´ë‹¤ \"take action\"ì´ ë” êµ¬ì²´ì ì´ê³  ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ ë°°ìš´ í‘œí˜„ í™œìš©ë„ âœ… \"Make sure to\" í‘œí˜„ì„ ì˜ í™œìš©í–ˆìŠµë‹ˆë‹¤! \"Make sure to focus on the day you're living.\" ì¶”ê°€ ì œì•ˆì‚¬í•­ ë” ë‹¤ì–‘í•œ í‘œí˜„ ì‚¬ìš©: \"Try to\", \"I'm worried about\", \"Be careful with\" ë“±ì˜ í‘œí˜„ë„ í™œìš©í•´ë³´ì„¸ìš”. ë¬¸ì¥ ì—°ê²°: \"By the way\", \"Well, the thing is\" ê°™ì€ ì—°ê²°ì–´ë¥¼ ì‚¬ìš©í•˜ë©´ ë” ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”ì²´ê°€ ë©ë‹ˆë‹¤. êµ¬ì²´ì ì¸ ì˜ˆì‹œ: ì¶”ìƒì ì¸ ë‚´ìš©ë³´ë‹¤ êµ¬ì²´ì ì¸ ìƒí™©ì´ë‚˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì„¸ìš”."
  },
  {
    "id": "Causal Inference/Paper Review/review-Towards-Optimal Doubly-Robust-Estimation-of-Heterogeneous-Causal-Effects",
    "title": "[Paper Review] Towards Optimal Doubly-Robust Estimation of Heterogeneous Causal Effects",
    "date": "2025-07-20",
    "excerpt": "Doubly-Robust Estimation of Heterogeneous Causal Effects ë…¼ë¬¸ ë¦¬ë·°",
    "category": "Causal Inference",
    "tags": [
      "paper review"
    ],
    "content": "paper link Abstact"
  },
  {
    "id": "Book/ë§Œë“¤ì–´ì§„ ì‹  (The God Delusion)/ëŒ€ë‹¨íˆì¢…êµì ì¸ë¶ˆì‹ ì",
    "title": "ëŒ€ë‹¨íˆ ì¢…êµì ì¸ ë¶ˆì‹ ì",
    "date": "2025-07-20",
    "excerpt": "ë‚˜ëŠ” ì¸ê²©ì‹ ì„ ìƒìƒí•˜ë ¤ëŠ” ì‹œë„ëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤. ì‹ ì€ ìš°ë¦¬ì˜ ë¶ˆì¶©ë¶„í•œ ê°ê°ìœ¼ë¡œ ì„¸ê³„ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨ìœ¼ë¡œì¨ ê²½ì™¸ì‹¬ì„ í’ˆê²Œ í•˜ëŠ” ì •ë„ë©´ ì¶©ë¶„í•˜ë‹¤. <ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸>",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "ë§Œë“¤ì–´ì§„ ì‹ "
    ],
    "content": "ê³¼í•™ìë“¤ì´ ë§í•˜ëŠ” ì‹ ì€ ë­˜ê¹Œ? > ë‚˜ëŠ” ì¸ê²©ì‹ ì„ ìƒìƒí•˜ë ¤ëŠ” ì‹œë„ëŠ” í•˜ì§€ ì•ŠëŠ”ë‹¤. ì‹ ì€ ìš°ë¦¬ì˜ ë¶ˆì¶©ë¶„í•œ ê°ê°ìœ¼ë¡œ ì„¸ê³„ì˜ êµ¬ì¡°ë¥¼ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í•¨ìœ¼ë¡œì¨ ê²½ì™¸ì‹¬ì„ í’ˆê²Œ í•˜ëŠ” ì •ë„ë©´ ì¶©ë¶„í•˜ë‹¤. > > <ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸> ê³¼í•™ìë“¤ì´ ë§í•˜ëŠ” ì‹ ì€ ë¬´ì—‡ì¼ê¹Œ? ê°™ì€ ì‹ ì´ë¼ëŠ” ë‹¨ì–´ë¡œ í‘œí˜„ë˜ê³  ìˆì§€ë§Œ ì´ë¥¼ êµ¬ë¶„í•´ì„œ ì´í•´í•  í•„ìš”ê°€ ìˆì„ ê²ƒ ê°™ë‹¤. ì•„ì´ìŠˆíƒ€ì¸ì€ ì‹ ì„ ë§í•˜ë©° ìš°ì£¼ì˜ í•©ë¦¬ì„±ê³¼ ì§ˆì„œì— ëŒ€í•œ ê²½ì™¸ì‹¬ì„ ë“¤ì–´ë‚´ê³  ìˆë‹¤. ì˜ˆìˆ˜ë‚˜ ë¶€ì²˜ì™€ ê°™ì€ ì‹ ì„ ì˜ë¯¸í•˜ëŠ”ê²Œ ì•„ë‹Œ. > í›¨ì”¬ ë” ë¶ˆí–‰í•œ í˜¼ë€ì€ ì•„ì¸ìŠˆíƒ€ì¸ì‹ì˜ ì¢…êµì™€ ì´ˆìì—°ì ì¸ ì¢…êµë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•¨ìœ¼ë¡œì¨ ë¹šì–´ì§„ë‹¤. ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸(Albert Einstein)ì€ ì¢…ì¢… ì‹ ì´ë¼ëŠ” ëª…ì¹­ì„ ì‚¬ìš©í•¨ìœ¼ë¡œì¨(ê·¸ëŸ° ë¬´ì‹ ë¡ ì ê³¼í•™ìê°€ ê·¸ë§Œì€ ì•„ë‹ˆë‹¤), ê·¸ëŸ° ìœ ëª…í•œ ê³¼í•™ìê°€ ìì‹ ë“¤ì˜ í¸ì´ê¸°ë¥¼ ë„ˆë¬´ë‚˜ ë°”ë¼ëŠ” ì´ˆìì—°ì£¼ì˜ìë“¤ì˜ ì˜¤í•´ë¥¼ ìì´ˆí•˜ê³¤ í–ˆë‹¤. > > <ë§Œë“¤ì–´ì§„ ì‹ >, ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ - ë°€ë¦¬ì˜ ì„œì¬ ê³¼í•™ìë“¤ì´ ì‹ ì„ ì´ì•¼ê¸°í•œë‹¤ê³  ì´ë“¤ì´ ì‹ ì„ ë¯¿ëŠ”ë‹¤ê³  ìƒê°í•˜ë©´ ì•ˆë  ë“¯ í•˜ë‹¤. > ìš°ë¦¬ ì‹œëŒ€ì˜ ìœ„ëŒ€í•œ ê³¼í•™ìë“¤ì´ ì¢…êµì ì¸ ë§ì„ í•˜ëŠ” ë“¯ì´ ë³´ì—¬ë„ ê·¸ë“¤ì˜ ì‹ ë…ì„ ë” ê¹Šì´ íŒŒê³  ë“¤ì–´ê°€ë©´ ëŒ€ê°œ ê·¸ë ‡ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì´ ë“œëŸ¬ë‚œë‹¤. > > <ë§Œë“¤ì–´ì§„ ì‹ >, ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ - ë°€ë¦¬ì˜ ì„œì¬ ë‹¤ë¥¸ ì‚¬ëŒë“¤ì€ ëª°ë¼ë„ ê³¼í•™ìë“¤ ì¤‘ ì‹ ì„ ë¯¿ëŠ” ì‚¬ëŒë“¤ì€ ì™œ ìˆëŠ”ê±¸ê¹Œ? ê³ ë“±í•™êµë•Œ ì¹œêµ¬ë¥¼ ë”°ë¼ êµíšŒë¥¼ 1~2ë…„ë„ ë‹¤ë…”ë˜ ì ì´ ìˆë‹¤. ê·¸ë•Œë„ ì‹ ì´ë¼ëŠ” ì¡´ì¬ë¥¼ ì–´ë–»ê²Œ ë¯¿ì„ ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•´ ì˜ë¬¸ì´ ë“¤ì–´ ì£¼ìœ„ì— ë¬¼ì–´ë´¤ì„ ë•Œ, ë¯¿ìŒì„ ë¯¿ëŠ”ë‹¤ëŠ” ì´ì•¼ê¸°ë¥¼ ë“¤ì—ˆë‹¤. ì¦‰ ì–´ë–¤ ê·¼ê±°ê°€ ìˆì–´ì„œ ë¯¿ëŠ”ë‹¤ê¸° ë³´ë‹¤ëŠ” ë¯¿ìŒ ìì²´ë¥¼ ë¯¿ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. ê·¸ë˜ì„œ ë¯¿ì„ ìˆ˜ ì—†ë‹¤ëŠ” ì‚¬ëŒì—ê²ŒëŠ” ë¯¿ìŒì´ ë¶€ì¡±í•˜ë‹¤, ì„±ê²½ì„ ë” ê³µë¶€í•˜ë‹¤ë³´ë©´ ë¯¿ê²Œ ëœë‹¤ëŠ” ì´ì•¼ê¸°ë¥¼ í•œë‹¤. ë‚˜ëŠ” ì´ëŸ° ë¯¿ìŒì´ë¼ëŠ”ê²Œ êµ‰ì¥íˆ ìœ„í—˜í•˜ë‹¤ê³  ìƒê°í•œë‹¤. ë„·í”Œë¦­ìŠ¤ ì‹œë¦¬ì¦ˆì˜ \"ë‚˜ëŠ” ì‹ ì´ë‹¤\"ì—ì„œ ì†Œê°œë˜ëŠ” JMS ê°™ì€ ì‚¬ì´ë¹„ ì¢…êµë“¤ì˜ ë…¼ë¦¬ì™€ êµíšŒê°€ ë‹¤ë¥¼ê²Œ ë­”ê°€? ë¯¿ìŒì„ ìš°ì„ ì‹œ í•˜ëŠ” ì‚¶ì˜ íƒœë„ê°€ ë‚˜ëŠ” ë„ì €íˆ ë‚©ë“ì´ ì•ˆê°„ë‹¤. ê³¼í•™ì„ í•˜ëŠ” ì‚¬ëŒë“¤ ì¤‘ì— ì‹ ì„ ë¯¿ëŠ” ì‚¬ëŒë“¤ì´ ë§ì€ë°, ì´ë“¤ë„ ì¢…êµì— ìˆì–´ì„œëŠ” ë¯¿ìŒì„ ìš°ì„ ì‹œ í•˜ê¸° ë•Œë¬¸ì´ ì•„ë‹ê¹Œ ì‹¶ë‹¤. ì¢…êµì— ìˆì–´ì„œëŠ” ë…¼ë¦¬ì ì¸ ì´ì•¼ê¸°ê°€ í•„ìš”í•˜ì§€ ì•Šì€. [ì´ê¸°ì  ìœ ì „ì ì €ì] ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤í•œí…Œ ê¹Œì¸ ì° í’€ì–´ë“œë¦¼ : ì¢…êµëŠ” ì—†ì–´ì ¸ì•¼ í•œë‹¤ / ìµœì¬ì²œì˜ ì•„ë§ˆì¡´ ìœ íŠœë¸Œ ë§í¬ ìµœì¬ì²œ êµìˆ˜ê°€ ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ì™€ì˜ ì¸í„°ë·° ê¸°íšŒë¥¼ ë§Œë“¤ì–´ì„œ ë§Œë“¤ì–´ì§„ ì‹ ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆë˜ ì´ì•¼ê¸°. ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ê°€ ì´ì•¼ê¸°í•˜ëŠ” ëª¨íƒœì‹ ì•™. ì´ ì˜ìƒì— ìˆëŠ” ëŒ“ê¸€ ì¤‘ ê¸°ì–µì— ë‚¨ëŠ” ê²ƒë“¤ > 'ê°œì¸ì´ ë§ìƒì— ë¹ ì§€ë©´ ë¯¸ì¹œ ê²ƒì´ì§€ë§Œ ì§‘ë‹¨ì´ ë§ìƒì— ë¹ ì§€ë©´ ì¢…êµì´ë‹¤. > > '-ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ > 'ì¢…êµëŠ” ë¶€ì¡±ì˜ ìƒì¡´ì— ë„ì›€ì„ ì¤€ëŒœ. > > '- ì—ë“œì›Œë“œ ìœŒìŠ¨ > ìˆ˜ ë°±ëª…ì˜ ì‚¬ëŒì´ ëª‡ ë‹¬ë™ì•ˆ ë¯¿ìœ¼ë©´ fake newsì´ì§€ë§Œ ìˆ˜ ì²œë§Œëª…ì˜ ì‚¬ëŒë“¤ì´ ìˆ˜ ì²œë…„ ë™ì•ˆ ë¯¿ìœ¼ë©´ ì¢…êµì´ë‹¤. > > - ìœ ë°œ í•˜ë¼ë¦¬ ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ : ì°½ì¡°ë¡ ì„ ë°˜ë°•í•˜ëŠ” íŒ©íŠ¸ , ì¶©ì½”ì˜ ì² í•™ ìœ íŠœë¸Œ ë§í¬ ìœ ì „ìë“¤ ê°„ì˜ ìœ ì‚¬ì„± íŒ¨í„´. ì°½ì¡°ë¡  vs ì§„í™”ë¡ . ì‹ ì„ ë¯¿ëŠ” ì‚¬ëŒì—ê²ŒëŠ” ë…¼ìŸì„ í•  ìˆ˜ ì—†ë‹¤. ê³¼í•™ì ì¸ ê·¼ê±°ë¥¼ ë¬´ì‹œí•œì±„ ì„±ê²½ì„ ë¯¿ìœ¼ë‹ˆê¹Œ. ê³¼í•™ìì™€ ì‹ ì— ëŒ€í•œ ì •ë¦¬ (GPT ì‹œí‚´) ì™œ í•˜ë‚˜ì˜ â€˜ê³¼í•™ì ê´€ì â€™ì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ê°€ í˜„ëŒ€ ì—°êµ¬ìë“¤ì€ ê°œì¸ì  ì‹ Â·ë¹„ê°œì¸ì (ë²”-)ì‹ Â·\\\\ë¬´ì‹ (ë¶ˆê°€ì§€ë¡  í¬í•¨)\\\\ì˜ í­ë„“ì€ ìŠ¤í™íŠ¸ëŸ¼ì„ ì´ë£¨ë©°, í†µê³„ì¡°ì°¨ ì–‘ê·¹í™”ë¥¼ ë³´ì—¬ ì¤€ë‹¤. ë¯¸êµ­ ê³¼í•™ì 2,000 ì—¬ ëª…ì„ ì¡°ì‚¬í•œ Pew Research Center ë³´ê³ ì„œì— ë”°ë¥´ë©´ \\\\ê³¼í•™ìì˜ 51 %\\\\ê°€ ì–´ë–¤ í˜•íƒœë¡œë“  ì‹ ì  ì¡´ì¬ë¥¼ ì¸ì •í•˜ì§€ë§Œ, \\\\41 %\\\\ëŠ” â€œì‹ ì´ë‚˜ ì˜ì  í˜ì´ ì—†ë‹¤â€ê³  ì‘ë‹µí–ˆë‹¤. ì¼ë°˜ ëŒ€ì¤‘ê³¼ ë¹„êµí•˜ë©´ ë¬´ì‹ ë¡  ë¹„ìœ¨ì´ 10 ë°°ì— ê°€ê¹ë‹¤.([Pew Research Center][1]) --- â€œê³¼í•™ë„ ì‹ ì„ ë¯¿ëŠ”ë‹¤â€â€•ìœ ì‹ ë¡ Â·ê¸°ë…êµì  ì…ì¥ ëŒ€í‘œ ì¸ë¬¼ í•µì‹¬ ìƒê° ì£¼ìš” ì €ìˆ Â·ë°œì–¸ ------------------------ ------------------------------ ------------------------------------------------------ í”„ëœì‹œìŠ¤ ì½œë¦°ìŠ¤ (ìœ ì „í•™ì) â€œì§„í™”ëŠ” í•˜ë‚˜ë‹˜ì˜ ì°½ì¡° ë„êµ¬â€â€•â€˜ìœ ì‹ ë¡ ì  ì§„í™”â€™ The Language of God (2006)([ìœ„í‚¤ë°±ê³¼][2]) ì¡°ë¥´ì£¼ ë¥´ë©”íŠ¸ë¥´ (ë¹…ë±… ì œì•ˆìÂ·ì‚¬ì œ) â€œê³¼í•™ì€ ì–´ë–»ê²Œë¥¼, ì‹ ì•™ì€ ì™œë¥¼ ë‹¤ë£¬ë‹¤â€ NYT ì¸í„°ë·°(1933)([The Society of Catholic Scientists][3]) ê·¸ë“¤ì€ ê³¼í•™ ë°©ë²•ë¡ ì„ ì¤€ìˆ˜í•˜ë©´ì„œë„, ì°½ì¡° ëª©ì Â·ë„ë•ë²• ë“± â€˜ê¶ê·¹ì  ì˜ë¯¸â€™ëŠ” ê³¼í•™ ë°–(ì¢…êµ)ì˜ ë²”ì£¼ë¼ê³  ë³¸ë‹¤. --- â€œìš°ì£¼ê°€ ê³§ ì‹ â€â€•ë²”ì‹ ë¡ Â·ìš°ì£¼ì  ê²½ì™¸ ì•Œë² ë¥´íŠ¸ ì•„ì¸ìŠˆíƒ€ì¸: â€œë‚˜ëŠ” ê°œì¸ì  ì‹ ì„ ë¯¿ì§€ ì•ŠëŠ”ë‹¤â€ë©´ì„œë„ â€œìš°ì£¼ì˜ ì‹ ë¹„ì— ëŒ€í•œ ê²½ì™¸ë¥¼ â€˜ì¢…êµì  ê°ì •â€™ì´ë¼ ë¶€ë¥¼ ìˆ˜ ìˆë‹¤â€([Learn Religions][4]) ì¹¼ ì„¸ì´ê±´: â€œìš°ë¦¬ëŠ” \\\\ë³„ì˜ ë¨¼ì§€(star-stuff)\\\\ë¡œ ì´ë£¨ì–´ì ¸, ìš°ì£¼ê°€ ìŠ¤ìŠ¤ë¡œë¥¼ ì¸ì‹í•˜ëŠ” ë°©ì‹â€ì´ë¼ í‘œí˜„í–ˆë‹¤.([Goodreads][5]) ì´ íë¦„ì€ ë²”ì‹ ë¡ Â·ë²”ì¬ì‹ ë¡ (Spinozaì  ì‹ )ìœ¼ë¡œ ë¶„ë¥˜ë˜ë©°, ìì—°ë²•ì¹™ = ì‹ ì  ì´ì„±ê³¼ ë™ë“±ì‹œí•œë‹¤. --- â€œì‹ ì´ ì„¤ ìë¦¬ëŠ” ì—†ë‹¤â€â€•ë¬´ì‹ ë¡ Â·ìì—°ì£¼ì˜ ì¸ë¬¼ ì£¼ëœ ë…¼ì§€ ------------ --------------------------------------------------------------- ìŠ¤í‹°ë¸ í˜¸í‚¹ â€œì‹œê°„ì´ â€˜ë¹…ë±… ì´ì „â€™ì— ì¡´ì¬í•˜ì§€ ì•Šì•˜ìœ¼ë¯€ë¡œ ì°½ì¡°ì£¼ê°€ ê°œì…í•  â€˜ìˆœê°„â€™ë„ ì—†ì—ˆë‹¤â€([Live Science][6]) ë¦¬ì²˜ë“œ ë„í‚¨ìŠ¤ â€œê°œì¸ì  ì‹ ì€ â€˜ì°©ê°(delusion)â€™ì´ë©°, ì§„í™”ê°€ ì„¤ê³„ì˜ í™˜ìƒì„ ì„¤ëª…í•œë‹¤â€([ìœ„í‚¤ë°±ê³¼][7]) ë¡œëŸ°ìŠ¤ í¬ë¼ìš°ìŠ¤ â€œì–‘ìì¥ê³¼ ì¤‘ë ¥ì´ë©´ â€˜ë¬´(ç„¡)â€™ì—ì„œë„ ìš°ì£¼ê°€ ìì—° ë°œìƒí•  ìˆ˜ ìˆë‹¤â€([ìœ„í‚¤ë°±ê³¼][8]) ì´ë“¤ì€ ì¡´ì¬ë¡ ì  ìì—°ì£¼ì˜ë¥¼ ì±„íƒí•˜ë©°, ì´ˆì›”ì  ì„¤ëª…ì€ ê³¼ì‰ê°€ì„¤ë¡œ ë³¸ë‹¤. --- â€œíŒë‹¨ ìœ ë³´â€â€•ë¶ˆê°€ì§€ë¡ Â·ê³¼í•™ì  íšŒì˜ ì²œì²´ë¬¼ë¦¬í•™ì ë‹ ë“œê·¸ë˜ìŠ¤ íƒ€ì´ìŠ¨ì²˜ëŸ¼ â€œì¦ê±° ì—†ëŠ” ì£¼ì¥ì—” í™•ì‹ í•˜ì§€ ì•ŠëŠ”ë‹¤â€ëŠ” ê²½í—˜ë¡ ì  ë¶ˆê°€ì§€ë¡ ë„ í™•ì‚° ì¤‘ì´ë‹¤.([A-Z Quotes][9]) --- ë‹¤ë¦¬ë¥¼ ë†“ìœ¼ë ¤ëŠ” ì‹œë„: NOMA ì›ì¹™ ê³ (æ•…) ìŠ¤í‹°ë¸ ì œì´ êµ´ë“œì˜ ë¹„ì¤‘ì²© êµêµ¬(NOMA) ëª¨ë¸ì€ â€œê³¼í•™ì€ ì‚¬ì‹¤, ì¢…êµëŠ” ê°€ì¹˜Â·ì˜ë¯¸â€ë¼ëŠ” ì˜ì—­ ë¶„ë¦¬ë¡œ ì¶©ëŒì„ ì™„í™”í•˜ë ¤ í•œë‹¤.([ìœ„í‚¤ë°±ê³¼][10]) --- ì´ë¡ ì  ë°°ê²½ â‘  â€” ë°©ë²•ë¡ ì  ìì—°ì£¼ì˜(MN) ê³¼í•™ì€ ì‹¤í—˜ ê°€ëŠ¥í•œ ìì—° ì›ì¸ë§Œì„ íƒêµ¬í•´ì•¼ í•œë‹¤ëŠ” ê·œë²”ì´ë‹¤. ì´ˆìì—° ì›ì¸ì€ ê²€ì¦ ë¶ˆê°€í•˜ë¯€ë¡œ ì—°êµ¬ ì„¤ê³„ì—ì„œ ë°°ì œëœë‹¤. ì´ ì ì—ì„œ MNì€ â€œê³¼í•™ì˜ ì„±ê³µ ê²½í—˜â€ì´ ë‚³ì€ ì‹¤ìš©ì  ì „ëµìœ¼ë¡œ í•´ì„ëœë‹¤.([ìŠ¤í”„ë§ê±°ë§í¬][11]) > \\\\ì¡´ì¬ë¡ ì  ìì—°ì£¼ì˜(ON)\\\\ì™€ ë‹¬ë¦¬, MNì€ â€œì‹  ì¡´ì¬ ì—¬ë¶€ì— ëŒ€í•´ ì¹¨ë¬µâ€í•˜ì§€ë§Œ, ë¬´ì‹ ë¡ ìëŠ” ì¢…ì¢… MN â†’ ONì„ ì—°ì—­í•œë‹¤. ë…¼ìŸì€ ë°”ë¡œ ì—¬ê¸°ì„œ ë°œìƒí•œë‹¤. --- ì´ë¡ ì  ë°°ê²½ â‘¡ â€” ë¯¸ì„¸ ì¡°ì •Â·ë‹¤ì¤‘ìš°ì£¼Â·ì„¤ê³„ ë…¼ì¦ ë¬¼ë¦¬ ìƒìˆ˜ë“¤ì´ ìƒëª… íƒ„ìƒì— â€œì ˆë¬˜í•˜ê²Œâ€ ë§ì¶°ì ¸ ìˆë‹¤ëŠ” \\\\ë¯¸ì„¸ ì¡°ì •(fine-tuning)\\\\ì€ ë‘ ê°ˆë˜ í•´ì„ì„ ë‚³ì•˜ë‹¤. ì‹ ì  ì„¤ê³„ â€” ì½œë¦°ìŠ¤Â·í´ ë°ì´ë¹„ìŠ¤ ë“±ì€ â€œìš°ì—°ì´ë¼ ë³´ê¸°ì—” í™•ë¥ ì´ ê·¹ë¯¸â€ë¼ê³  ë³¸ë‹¤. ë‹¤ì¤‘ìš°ì£¼ / ì„ íƒ íš¨ê³¼ â€” í˜¸í‚¹Â·í¬ë¼ìš°ìŠ¤ëŠ” â€œê´€ì¸¡ ê°€ëŠ¥í•œ ìš°ì£¼ë§Œ ìƒëª… ì¹œí™”ì ì¼ ë¿â€ì´ë¼ ë°˜ë°•í•œë‹¤.([ìœ„í‚¤ë°±ê³¼][12]) --- ì—­ì‚¬ì  ë§¥ë½ í•œëˆˆì— ë³´ê¸° ì‹œëŒ€ ê³¼í•™ í˜ì‹  ì‹  ê°œë… ë³€í™” ---------- --------- -------------------------- 17C ë‰´í„´ ê¸°ê³„ì  ìš°ì£¼ê´€ â€˜ì‹œê³„ê³µ(Deus ex machina)â€™ 20C ì´ˆ ë¹…ë±…(ë¥´ë©”íŠ¸ë¥´) â€œì°½ì¡° = ì‹œì‘?â€ ë…¼ìŸ ì¬ì í™” 21C ì–‘ìì¤‘ë ¥Â·ë‹¤ì¤‘ìš°ì£¼ ë¬´ì‹ ë¡ Â·ë¶ˆê°€ì§€ë¡  ê°•í™” vs. ë¯¸ì„¸ì¡°ì • ë…¼ì¦ ê°•í™” --- ì¢…í•© Â· ì‹œì‚¬ì  ë‹¤ì–‘ì„±: ê³¼í•™ì  ì—…ì ê³¼ ì‹ ì•™ ìœ ë¬´ëŠ” ì§ê²°ë˜ì§€ ì•ŠëŠ”ë‹¤. ëŒ€í™” ëª¨ë¸: ê°ˆë“±Â·ë…ë¦½Â·ëŒ€í™”Â·ìœµí•© â€” ë„¤ ê°€ì§€ í‹€ ê°€ìš´ë° ì–´ëŠ ìª½ì„ íƒí•˜ëŠëƒì— ë”°ë¼ ì…ì¥ë„ ë¶„í™”ëœë‹¤. ì² í•™ì˜ ì—­í• : ê³¼í•™ì´ â€œì–´ë–»ê²Œâ€ë¥¼ ì„¤ëª…í• ìˆ˜ë¡, â€œì™œâ€ì— ëŒ€í•œ ì§ˆë¬¸ì€ ì² í•™Â·ì‹ í•™Â·ìœ¤ë¦¬ë¡œ ì´ë™í•œë‹¤. > ê²°ë¡ : â€œê³¼í•™ìë“¤ì€ ì‹ ì„ ë¯¿ëŠ”ê°€?â€ë¼ëŠ” ë‹¨ì¼ ì§ˆë¬¸ ëŒ€ì‹ , \\\\â€œì–´ë–¤ â€˜ì‹ â€™ ê°œë…ì„, ì–´ë–¤ ë°©ë²•ë¡ ìœ¼ë¡œ ë…¼ì˜í•˜ëŠ”ê°€?â€\\\\*ë¥¼ ë¬¼ì„ ë•Œ ë¹„ë¡œì†Œ ë…¼ìŸì˜ ê²°ì´ ì„ ëª…í•´ì§„ë‹¤. ì´ ë‹¤ì¸µì  êµ¬ë„ë¥¼ ì—¼ë‘ì— ë‘ë©´, ê³¼í•™ê³¼ ì¢…êµì˜ ê´€ê³„ëŠ” â€˜ì ëŒ€â€™ë³´ë‹¤ëŠ” ì°½ì¡°ì  ê¸´ì¥ê³¼ ìƒí˜¸ ë³´ì™„ì˜ ì¥ìœ¼ë¡œ ì¬í•´ì„ë  ìˆ˜ ìˆë‹¤. [1]: https://www.pewresearch.org/religion/2009/11/05/scientists-and-belief/ \"Scientists and Belief Pew Research Center\" [2]: https://en.wikipedia.org/wiki/The_Language_of_God \"The Language of God - Wikipedia\" [3]: https://catholicscientists.org/articles/lemaitre-follows-two-paths-to-truth-new-york-times-1933-interview-with-lemaitre/ \"\\\"LemaÃ®tre Follows Two Paths to Truth\\\" (1933 New York Times Interview with Fr. LemaÃ®tre, Father of the Big Bang Theory) - The Society of Catholic Scientists\" [4]: https://www.learnreligions.com/albert-einstein-quotations-249858 \"Albert Einstein Quotes on Science, God, and Religion\" [5]: https://www.goodreads.com/quotes/484665-the-cosmos-is-within-us-we-are-made-of-star-stuff?utm_source=chatgpt.com \"Quote by Carl Sagan: â€œThe cosmos is within us. We are made of star-st...â€ - Goodreads\" [6]: https://www.livescience.com/63854-stephen-hawking-says-no-god.html \"Stephen Hawking's Final Book Says There's 'No Possibility' of God in Our Universe Live Science\" [7]: https://en.wikipedia.org/wiki/The_God_Delusion \"The God Delusion - Wikipedia\" [8]: https://en.wikipedia.org/wiki/A_Universe_from_Nothing \"A Universe from Nothing - Wikipedia\" [9]: https://www.azquotes.com/author/14904-Neil_deGrasse_Tyson \"TOP 25 QUOTES BY NEIL DEGRASSE TYSON (of 764) A-Z Quotes\" [10]: https://en.wikipedia.org/wiki/Non-overlapping_magisteria \"Non-overlapping magisteria - Wikipedia\" [11]: https://link.springer.com/article/10.1007/s10670-024-00790-y?utm_source=chatgpt.com \"Methodological Naturalism, Analyzed Erkenntnis - Springer\" [12]: https://en.wikipedia.org/wiki/Fine-tuned_universe \"Fine-tuned universe - Wikipedia\""
  },
  {
    "id": "Current Affairs/ì¤‘êµ­ë¬¸í™”ëŒ€í˜ëª…ê³¼í™ìœ„ë³‘",
    "title": "ì¤‘êµ­ ë¬¸í™”ëŒ€í˜ëª…ê³¼ í™ìœ„ë³‘",
    "date": "2025-07-17",
    "excerpt": "ë¬¸í™”ëŒ€í˜ëª…ê³¼ í™ìœ„ë³‘ì— ëŒ€í•œ ë‚´ìš©ë“¤",
    "category": "Current Affairs",
    "tags": [
      "ì¤‘êµ­",
      "ë¬¸í™”ëŒ€í˜ëª…",
      "í™ìœ„ë³‘"
    ],
    "content": "ì°¸ê³  ìë£Œ ë¦¬ë·°ì—‰ì´ - ì‚¼ì²´ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ì‚¬ê±´ ë¬¸í™”ëŒ€í˜ëª… ë¬¸í™”ëŒ€í˜ëª…ì´ë€? ë¬¸í™”ëŒ€í˜ëª…(æ–‡åŒ–å¤§é©å‘½, Cultural Revolution)ì€ ì •ì‹ ëª…ì¹­ì´ \"ë¬´ì‚°ê³„ê¸‰ ë¬¸í™”ëŒ€í˜ëª…\"(ç„¡ç”¢éšç´šæ–‡åŒ–å¤§é©å‘½, Great Proletarian Cultural Revolution)ìœ¼ë¡œ, 1966ë…„ë¶€í„° 1976ë…„ê¹Œì§€ 10ë…„ê°„ ì¤‘êµ­ì—ì„œ ì¼ì–´ë‚œ ì‚¬íšŒì •ì¹˜ì  ìš´ë™ì´ë‹¤. ë°œìƒ ë°°ê²½ ëŒ€ì•½ì§„ìš´ë™ì˜ ì‹¤íŒ¨ (1958-1962) ë§ˆì˜¤ì©Œë‘¥ì´ ì£¼ë„í•œ ê²½ì œê°œë°œ ì •ì±…ì¸ ëŒ€ì•½ì§„ìš´ë™ì´ ì°¸í˜¹í•˜ê²Œ ì‹¤íŒ¨ 2ì²œë§Œ ëª… ì´ìƒì´ ê¸°ê·¼ìœ¼ë¡œ ì‚¬ë§ ë§ˆì˜¤ì©Œë‘¥ì˜ ê¶Œë ¥ê³¼ ì˜í–¥ë ¥ì´ í¬ê²Œ ì•½í™”ë¨ ì‹¤ìš©ì£¼ì˜ ë…¸ì„ ì˜ ë“±ì¥ ë¥˜ìƒ¤ì˜¤ì¹˜(åŠ‰å°‘å¥‡), ì €ìš°ì–¸ë¼ì´(å‘¨æ©ä¾†), ë©ìƒ¤ì˜¤í•‘(é„§å°å¹³) ë“±ì´ ì‹¤ìš©ì  ê²½ì œ ì •ì±… ë„ì… ê°œì¸ ë†ì§€ í—ˆìš© ë“± ì‹œì¥ê²½ì œ ìš”ì†Œ ë„ì…ìœ¼ë¡œ ê²½ì œ íšŒë³µ (1962-1965) ë§ˆì˜¤ì©Œë‘¥ì€ ì´ëŸ¬í•œ ì •ì±…ì´ ê³µì‚°ì£¼ì˜ ì›ì¹™ì— ìœ„ë°°ëœë‹¤ê³  íŒë‹¨ ë¬¸í™”ëŒ€í˜ëª…ì˜ ëª©í‘œ ë§ˆì˜¤ì©Œë‘¥ì´ ë¬¸í™”ëŒ€í˜ëª…ì„ ì¼ìœ¼í‚¨ ì£¼ìš” ëª©ì : ê¶Œë ¥ íšŒë³µ: ëŒ€ì•½ì§„ìš´ë™ ì‹¤íŒ¨ë¡œ ìƒì€ ì •ì¹˜ì  ì˜í–¥ë ¥ ë˜ì°¾ê¸° ì •ì  ì œê±°: ë¥˜ìƒ¤ì˜¤ì¹˜, ë©ìƒ¤ì˜¤í•‘ ë“± ì‹¤ìš©ì£¼ì˜ ë…¸ì„  ì§€ë„ìë“¤ ì¶•ì¶œ í˜ëª… ì •ì‹  ë¶€í™œ: ê´€ë£Œí™”ë˜ê³  ìˆëŠ” ì¤‘êµ­ ê³µì‚°ë‹¹ê³¼ ì‚¬íšŒì— í˜ëª… ì •ì‹  ì£¼ì… \"4êµ¬\"(å››èˆŠ) íƒ€íŒŒ: êµ¬ì‚¬ìƒ(èˆŠæ€æƒ³) - ì˜› ì‚¬ìƒ êµ¬ë¬¸í™”(èˆŠæ–‡åŒ–) - ì˜› ë¬¸í™” êµ¬í’ì†(èˆŠé¢¨ä¿—) - ì˜› í’ìŠµ êµ¬ìŠµê´€(èˆŠç¿’æ…£) - ì˜› ìŠµê´€ ë¬¸í™”ëŒ€í˜ëª…ì˜ ì „ê°œ ê³¼ì • 1ë‹¨ê³„: ë°œë°œê³¼ í˜¼ë€ (1966-1968) 1966ë…„ 5ì›”: ì‹œì‘ 5.16 í†µì§€: ì¤‘êµ­ ê³µì‚°ë‹¹ ì¤‘ì•™ìœ„ì›íšŒê°€ ë¬¸í™”ëŒ€í˜ëª… ì‹œì‘ì„ ê³µì‹ ì„ ì–¸ ë¶€ë¥´ì£¼ì•„ ë¶„ìë“¤ì´ ë‹¹, ì •ë¶€, êµ°ëŒ€, ë¬¸í™”ê³„ì— ì¹¨íˆ¬í–ˆë‹¤ê³  ì£¼ì¥ í™ìœ„ë³‘(ç´…è¡›å…µ) ì¡°ì§ ì£¼ë¡œ í•™ìƒë“¤ë¡œ êµ¬ì„±ëœ ì¤€êµ°ì‚¬ì¡°ì§ ë§ˆì˜¤ì©Œë‘¥ì–´ë¡(ëª¨íƒë™ì–´ë¡, \"ë¹¨ê°„ì±…\")ì„ ë“¤ê³  ë‹¤ë‹ˆë©° ìˆ­ë°° \"í˜ëª… ë¬´ì£„, ì¡°ë°˜ ìœ ë¦¬\"(é©å‘½ç„¡ç½ª, é€ åæœ‰ç†) - \"í˜ëª…ì€ ì£„ê°€ ì—†ê³ , ë°˜ë€ì€ ì´ì¹˜ê°€ ìˆë‹¤\" 20~30ëŒ€ í•™ìƒë“¤ì´ ì£¼ë¥¼ ì´ë¤˜ê³  ì´ë•Œì— ì—„ì²­ë‚œ ê¶Œë ¥ì„ ê°€ì§€ê³  ìˆì—ˆë‹¤ê³  í•œë‹¤. ì‚¬ë‹¨ì¥ë³´ë‹¤ ë” ë†’ì€ ê¶Œë ¥. ì‚¬ì†Œí•œ ì´ìœ ë¡œ ì‚¬ëŒë“¤ì„ ë¬´ìë¹„í•˜ê²Œ í­í–‰í•˜ê³  ì£½ì˜€ê¸° ë•Œë¬¸ì— ì‚¬ëŒë“¤ì´ í–‰ë™ì„ ì—„ì²­ ì¡°ì‹¬í–ˆì—ˆë‹¤ê³  í•œë‹¤. í­ë ¥ê³¼ íŒŒê´´ ì§€ì‹ì¸, êµì‚¬, ì •ë¶€ ê´€ë¦¬, ë¬¸í™”ì¬ ë“±ì— ëŒ€í•œ ë¬´ì°¨ë³„ ê³µê²© ê³µìë¬˜, ì²œë‹¨, ëª…ì‹­ì‚¼ë¦‰ ë“± ë¬¸í™”ìœ ì‚° íŒŒê´´ í•™êµì™€ ëŒ€í•™ íì‡„ 2ë‹¨ê³„: ê¶Œë ¥ íˆ¬ìŸê³¼ êµ° ê°œì… (1967-1969) 1967ë…„: \"ì „ë©´ì  ë‚´ì „\" í™ìœ„ë³‘ íŒŒë²Œ ê°„ ë¬´ë ¥ ì¶©ëŒ ê° ì§€ì—­ì—ì„œ \"ê¶Œë ¥ íƒˆì·¨\" ìš´ë™ ì „ê°œ ê´‘ì €ìš°, ë² ì´ì§• ë“±ì—ì„œ ëŒ€ê·œëª¨ í­ë ¥ ì‚¬íƒœ ì¸ë¯¼í•´ë°©êµ°ì˜ ê°œì… ë¦°ë±Œì˜¤(æ—å½ª)ê°€ ì´ë„ëŠ” ì¸ë¯¼í•´ë°©êµ°ì´ ì§ˆì„œ íšŒë³µì— ë‚˜ì„¬ í™ìœ„ë³‘ë“¤ì„ ë†ì´Œìœ¼ë¡œ ì¶”ë°©í•˜ëŠ” \"ìƒì‚°í•˜í–¥\"(ä¸Šå±±ä¸‹é„‰) ìš´ë™ ì‹œì‘ 3ë‹¨ê³„: ì•ˆì •í™”ì™€ ë‚´ë¶€ ê¶Œë ¥ íˆ¬ìŸ (1969-1976) ë¦°ë±Œì˜¤ ì‚¬ê±´ (1971) ë§ˆì˜¤ì©Œë‘¥ì˜ í›„ê³„ìë¡œ ì§€ëª…ëœ ë¦°ë±Œì˜¤ê°€ ì •ì¹˜ì ìœ¼ë¡œ ëª°ë½ ë§ˆì˜¤ ì•”ì‚´ ì‹œë„ í›„ ì†Œë ¨ìœ¼ë¡œ ë§ëª… ì‹œë„ ì¤‘ ëª½ê³¨ì—ì„œ ë¹„í–‰ê¸° ì¶”ë½ì‚¬ ë§ì€ ì¤‘êµ­ì¸ë“¤ì´ ê³µì‚°ë‹¹ ì§€ë„ë¶€ì— ëŒ€í•œ ì‹ ë¢° ìƒì‹¤ 4ì¸ë°©(å››äººå¹«)ì˜ ë“±ì¥ ì¥ì¹­(æ±Ÿé’, ë§ˆì˜¤ì©Œë‘¥ì˜ ë¶€ì¸), ì¥ì¶˜ì°¨ì˜¤(å¼µæ˜¥æ©‹), ì•¼ì˜¤ì›ìœ„ì•ˆ(å§šæ–‡å…ƒ), ì™•í›™ì›(ç‹æ´ªæ–‡) ë¬¸í™”ëŒ€í˜ëª… í›„ê¸°ì˜ ì‹¤ì§ˆì  ê¶Œë ¥ìë“¤ ê·¹ì¢Œ ë…¸ì„  ì¶”ì§„ ë¬¸í™”ëŒ€í˜ëª…ì˜ ì£¼ìš” ì •ì±… ìƒì‚°í•˜í–¥ ìš´ë™(ä¸Šå±±ä¸‹é„‰é‹å‹•) \"ì‚°ê³¼ ì‹œê³¨ë¡œ ê°€ë¼\"ëŠ” ì˜ë¯¸ ë„ì‹œ ì²­ë…„ 1ì²œë§Œ ëª… ì´ìƒì„ ë†ì´Œìœ¼ë¡œ ê°•ì œ ì´ì£¼ ë…¸ë™ì„ í†µí•œ \"ì¬êµìœ¡\" ëª…ëª© ì‹¤ì œë¡œëŠ” ë„ì‹œ ê³¼ë°€í™” í•´ê²°ê³¼ í™ìœ„ë³‘ í•´ì²´ ëª©ì  êµìœ¡ ì œë„ íŒŒê´´ ëŒ€í•™ ì…ì‹œ ì‹œí—˜ íì§€ (1966-1973) \"í˜ëª…ì  ìˆœìˆ˜ì„±\"ì´ í•™ë ¥ë³´ë‹¤ ì¤‘ìš”ì‹œë¨ ì§€ì‹ì¸ê³¼ ê³¼í•™ìë“¤ì´ \"êµ¬ì§€ì‹ë¶„ì\"ë¡œ íƒ„ì••ë°›ìŒ ë¬¸í™” ì˜ˆìˆ  í†µì œ ì„œêµ¬ ë¬¸í•™, ìŒì•…, ì—°ê·¹ ê¸ˆì§€ ì…°ìµìŠ¤í”¼ì–´ ì‘í’ˆ ë“± ì„œì–‘ ê³ ì „ ê¸ˆì§€ ì˜¤ì§ í˜ëª… ì£¼ì œë§Œ í—ˆìš© ë¬¸í™”ëŒ€í˜ëª…ì˜ ì¢…ë£Œ ë§ˆì˜¤ì©Œë‘¥ì˜ ì£½ìŒ (1976ë…„ 9ì›” 9ì¼) 27ë…„ê°„ ì¤‘êµ­ì„ í†µì¹˜í•œ ë§ˆì˜¤ì©Œë‘¥ ì‚¬ë§ ì „êµ­ì ìœ¼ë¡œ ì• ë„ ë¶„ìœ„ê¸° 4ì¸ë°© ì²´í¬ (1976ë…„ 10ì›” 6ì¼) í™”ê¶ˆí‘(è¯åœ‹é‹’)ì´ 4ì¸ë°©ì„ ì²´í¬ ë¬¸í™”ëŒ€í˜ëª…ì˜ ê³µì‹ì  ì¢…ë£Œë¡œ ê°„ì£¼ 4ì¸ë°©ì„ \"ë°˜í˜ëª… ë²”ì£„\" í˜ì˜ë¡œ ê¸°ì†Œ ë¬¸í™”ëŒ€í˜ëª…ì˜ ê²°ê³¼ì™€ ì˜í–¥ ëª‡ì²œë…„ì˜ ì—­ì‚¬ì™€ ë¬¸í™”ìœ ì‚°ì„ ë‹¤ ë¶€ì‰ˆë‹¤. ê³µìë‚˜ ì œê°ˆëŸ‰ê³¼ ê°™ì€ ì„ ì¡°ë“¤ì˜ ë¬´ë¤ì„ íŒŒí—¤ì¹˜ê³  ì‹œì‹ ì„ ë¶ˆíƒœì› ë‹¤. ì¸ëª… í”¼í•´ ì‚¬ë§ì: ì•½ 150ë§Œ ëª…ì—ì„œ 200ë§Œ ëª… (ì¶”ì •ì¹˜ëŠ” í•™ìë§ˆë‹¤ ë‹¤ë¦„) ë°•í•´ë°›ì€ ì¸ì›: ìˆ˜ì²œë§Œ ëª… ê´‘ì‹œì„±ì—ì„œëŠ” ì‹ì¸ í–‰ìœ„ë¥¼ í¬í•¨í•œ í•™ì‚´ ë°œìƒ ê²½ì œì  í”¼í•´ 1968ë…„ ê³µì—… ìƒì‚°ëŸ‰ì´ 1966ë…„ ëŒ€ë¹„ 12% ê°ì†Œ ì „ë°˜ì ì¸ ê²½ì œ ë°œì „ ì§€ì—° ì‚¬íšŒì  íŒŒê´´ ì „í†µ ë¬¸í™”ì™€ ì—­ì‚¬ ìœ ë¬¼ ëŒ€ëŸ‰ íŒŒê´´ ê°€ì¡±, ì¹œêµ¬ ê´€ê³„ íŒŒê´´ êµìœ¡ ì‹œìŠ¤í…œ ë¶•ê´´ë¡œ \"ìƒì–´ë²„ë¦° ì„¸ëŒ€\" ë“±ì¥ ì •ì¹˜ì  ì˜í–¥ ì¤‘êµ­ ê³µì‚°ë‹¹ì˜ ê¶Œìœ„ì™€ ì‹ ë¢°ë„ í¬ê²Œ ì‹¤ì¶” 1978ë…„ ë©ìƒ¤ì˜¤í•‘ì˜ ê°œí˜ê°œë°© ì •ì±…ìœ¼ë¡œ ì´ì–´ì§ ë¬¸í™”ëŒ€í˜ëª…ì— ëŒ€í•œ í‰ê°€ ì¤‘êµ­ ê³µì‚°ë‹¹ì˜ ê³µì‹ ì…ì¥ (1981ë…„) > \"ë¬¸í™”ëŒ€í˜ëª…ì€ ì¤‘í™”ì¸ë¯¼ê³µí™”êµ­ ê±´êµ­ ì´í›„ ë‹¹, êµ­ê°€, ì¸ë¯¼ì´ ê²ªì€ ê°€ì¥ ì‹¬ê°í•œ ì¢Œì ˆì´ì ê°€ì¥ ë¬´ê±°ìš´ ì†ì‹¤ì„ ì´ˆë˜í•œ ì‚¬ê±´ì´ë‹¤.\" í˜„ì¬ ì¤‘êµ­ì—ì„œì˜ ì·¨ê¸‰ \"10ë…„ ë™ë€\"(åå¹´å‹•äº‚) ë˜ëŠ” \"10ë…„ ì¬ë‚œ\"(åå¹´æµ©åŠ«)ìœ¼ë¡œ ë¶ˆë¦¼ ê³µê°œì  ë…¼ì˜ëŠ” ì—¬ì „íˆ ì œí•œì  ë§ˆì˜¤ì©Œë‘¥ì˜ ëª…ì„±ì„ ë³´í˜¸í•˜ê¸° ìœ„í•´ 4ì¸ë°©ì—ê²Œ ì£¼ë¡œ ì±…ì„ ì „ê°€ ë¬¸í™”ëŒ€í˜ëª…ì´ ë‚¨ê¸´ êµí›ˆ ê¶Œë ¥ ì§‘ì¤‘ì˜ ìœ„í—˜ì„±: ê°œì¸ ë…ì¬ì˜ íí•´ ëŒ€ì¤‘ ë™ì›ì˜ í•œê³„: ì¡°ì§í™”ë˜ì§€ ì•Šì€ ëŒ€ì¤‘ ìš´ë™ì˜ ìœ„í—˜ ì´ë°ì˜¬ë¡œê¸°ì˜ ë§¹ëª©ì„±: í˜„ì‹¤ì„ ë¬´ì‹œí•œ êµì¡°ì  ì‚¬ê³ ì˜ ë¬¸ì œ ì§€ì‹ê³¼ ë¬¸í™”ì˜ ì¤‘ìš”ì„±: ë°˜ì§€ì„±ì£¼ì˜ì˜ ì°¸í˜¹í•œ ê²°ê³¼ ë¬¸í™”ëŒ€í˜ëª…ì€ 20ì„¸ê¸° ì¤‘êµ­ ì—­ì‚¬ìƒ ê°€ì¥ ë¹„ê·¹ì ì¸ ì‚¬ê±´ ì¤‘ í•˜ë‚˜ë¡œ, ì¤‘êµ­ ì‚¬íšŒì— ê¹Šì€ ìƒì²˜ë¥¼ ë‚¨ê²¼ìœ¼ë©° ê·¸ ì˜í–¥ì€ ì˜¤ëŠ˜ë‚ ê¹Œì§€ë„ ì§€ì†ë˜ê³  ìˆë‹¤. ì‚¼ì²´ì™€ì˜ ì—°ê²° ì‚¼ì²´ë€? ì‚¼ì²´(ä¸‰é«”, The Three-Body Problem)ëŠ” ì¤‘êµ­ì˜ SF ì†Œì„¤ê°€ ë¥˜ì¸ ì‹ (åŠ‰æ…ˆæ¬£)ì´ 2006ë…„ì— ë°œí‘œí•œ ê³¼í•™ì†Œì„¤ë¡œ, ã€Šì§€êµ¬ì™•ì‚¬ã€‹(åœ°çƒå¾€äº‹, Remembrance of Earth's Past) ì‚¼ë¶€ì‘ì˜ ì²« ë²ˆì§¸ ì‘í’ˆì´ë‹¤. ì†Œì„¤ì˜ ì„±ì·¨ 2015ë…„ íœ´ê³ ìƒ ìˆ˜ìƒ: ì¤‘êµ­ ì†Œì„¤ ìµœì´ˆë¡œ ì„¸ê³„ ìµœê³  ê¶Œìœ„ì˜ SF ë¬¸í•™ìƒ ìˆ˜ìƒ ê¸€ë¡œë²Œ ë² ìŠ¤íŠ¸ì…€ëŸ¬: 2014ë…„ ì¼„ ë¥˜(Ken Liu)ì˜ ì˜ì–´ ë²ˆì—­ì„ í†µí•´ ì „ ì„¸ê³„ì  ì¸ê¸° íšë“ \"ì¤‘êµ­ì˜ ìŠ¤íƒ€ì›Œì¦ˆ\"ë¡œ ë¶ˆë¦¬ë©° ì¤‘êµ­ SF ë¬¸í•™ì˜ ê¸€ë¡œë²Œ ì§„ì¶œ ìƒì§• ì‚¼ì²´ì˜ ì¤„ê±°ë¦¬ ê¸°ë³¸ ì„¤ì • ì†Œì„¤ì€ ì§€êµ¬ ê·¼ì²˜ì— ìˆëŠ” ì‚¼ì„±ê³„(ä¸‰æ˜Ÿç³», Three-Body System)ì— ì‚´ê³  ìˆëŠ” ì™¸ê³„ ë¬¸ëª… ì‚¼ì²´ì¸(ä¸‰é«”äºº, Trisolarians)ê³¼ ì§€êµ¬ì˜ ì ‘ì´‰ì„ ë‹¤ë£¬ë‹¤. ì‚¼ì²´ì¸ë“¤ì€ ì„¸ ê°œì˜ íƒœì–‘ì´ ë¶ˆê·œì¹™í•˜ê²Œ ì›€ì§ì´ëŠ” í˜¼ëˆìŠ¤ëŸ¬ìš´ í™˜ê²½ì—ì„œ ì‚´ê³  ìˆì–´ ë©¸ì¢… ìœ„ê¸°ì— ì²˜í•´ ìˆë‹¤. ì£¼ìš” ì¸ë¬¼ ì˜ˆì›ì œ(è‘‰æ–‡æ½”, Ye Wenjie): ë¬¸í™”ëŒ€í˜ëª… ì‹œê¸° ë¬¼ë¦¬í•™ìì¸ ì•„ë²„ì§€ë¥¼ í™ìœ„ë³‘ì—ê²Œ ìƒì€ ì—¬ì„± ì¸ë¥˜ì— ëŒ€í•œ ì ˆë§ìœ¼ë¡œ ì™¸ê³„ ë¬¸ëª…ê³¼ì˜ ì ‘ì´‰ì„ ì‹œë„ ì´ì•¼ê¸°ì˜ í•µì‹¬ ì¸ë¬¼ì´ì ë¹„ê·¹ì  ì˜ì›… ì˜ìƒí™” ì‘í’ˆë“¤ ì¤‘êµ­íŒ ë“œë¼ë§ˆ (2023) ì œì‘ì‚¬: í…ì„¼íŠ¸(Tencent) íŠ¹ì§•: ì›ì‘ì— ì¶©ì‹¤í•˜ë˜ ë¬¸í™”ëŒ€í˜ëª… ì¥ë©´ì€ ê²€ì—´ë¡œ ì¸í•´ ì‚­ì œ í‰ê°€: ì¤‘êµ­ ë‚´ì—ì„œ ë†’ì€ ì‹œì²­ë¥  ê¸°ë¡ ë„·í”Œë¦­ìŠ¤ ë“œë¼ë§ˆ: \"3 Body Problem\" (2024) ì œì‘ì§„: ë°ì´ë¹„ë“œ ë² ë‹ˆì˜¤í”„(David Benioff) D.B. ì™€ì´ìŠ¤(D.B. Weiss) - ã€Šì™•ì¢Œì˜ ê²Œì„ã€‹ ì œì‘ì§„ ì•Œë ‰ì‚°ë” ìš°(Alexander Woo) ì œì‘ë¹„: 1ì–µ 6ì²œë§Œ ë‹¬ëŸ¬ (ë„·í”Œë¦­ìŠ¤ ì‚¬ìƒ ìµœê³ ì•¡ ë“œë¼ë§ˆ ì¤‘ í•˜ë‚˜) ì£¼ìš” ë³€í™”: ë°°ê²½ì„ ì¤‘êµ­ì—ì„œ ì˜êµ­ ëŸ°ë˜ìœ¼ë¡œ ì´ì „ ìºë¦­í„°ë“¤ì˜ ì¸ì¢… ë‹¤ì–‘í™” \"ì˜¥ìŠ¤í¼ë“œ íŒŒì´ë¸Œ\"ë¼ëŠ” ìƒˆë¡œìš´ ì¸ë¬¼ ê´€ê³„ ì°½ì¡° ë¬¸í™”ëŒ€í˜ëª…ê³¼ ì‚¼ì²´ì˜ ì—°ê²° ì†Œì„¤ ì† ë¬¸í™”ëŒ€í˜ëª… íŠ¸ë¼ìš°ë§ˆì˜ ê·¼ì›: ì˜ˆì›ì œê°€ í™ìœ„ë³‘ë“¤ì—ê²Œ ì•„ë²„ì§€ë¥¼ ìƒëŠ” ì¥ë©´ìœ¼ë¡œ ì‹œì‘ ì¸ë¥˜ì— ëŒ€í•œ ì ˆë§: ë¬¸í™”ëŒ€í˜ëª…ì˜ í­ë ¥ì„±ì´ ì˜ˆì›ì œë¡œ í•˜ì—¬ê¸ˆ ì¸ë¥˜ ì „ì²´ì— ëŒ€í•œ ì‹ ë¢°ë¥¼ ìƒê²Œ ë§Œë“¦ ì™¸ê³„ ë¬¸ëª…ê³¼ì˜ ì ‘ì´‰ ë™ê¸°: ì¸ë¥˜ë³´ë‹¤ ë‚˜ì€ ë¬¸ëª…ì— ëŒ€í•œ ê°ˆë§ ì—­ì‚¬ì  ì˜ë¯¸ ê³¼í•™ì˜ íƒ„ì••: ë¬¸í™”ëŒ€í˜ëª… ì‹œê¸° ì§€ì‹ì¸ê³¼ ê³¼í•™ìë“¤ì— ëŒ€í•œ ë°•í•´ ì´ë°ì˜¬ë¡œê¸° vs ì§„ë¦¬: ê³¼í•™ì  ì§„ì‹¤ë³´ë‹¤ ì •ì¹˜ì  ì˜¬ë°”ë¦„ì„ ìš°ì„ ì‹œí•œ ê²°ê³¼ ì§‘ë‹¨ ê´‘ê¸°: í™ìœ„ë³‘ì˜ í­ë ¥ì„±ì´ ë³´ì—¬ì£¼ëŠ” êµ°ì¤‘ ì‹¬ë¦¬ì˜ ìœ„í—˜ì„± ì‚¼ì²´ê°€ ì œê¸°í•˜ëŠ” ì² í•™ì  ì§ˆë¬¸ë“¤ ë¬¸ëª…ì˜ ì¶©ëŒ ì„œë¡œ ë‹¤ë¥¸ ë¬¸ëª…ì´ ë§Œë‚¬ì„ ë•Œì˜ í•„ì—°ì  ê°ˆë“± ìƒì¡´ì„ ìœ„í•œ ë„ë•ì  ì„ íƒì˜ ë”œë ˆë§ˆ ê³¼í•™ê³¼ ì •ì¹˜ ê³¼í•™ì˜ ì •ì¹˜ì  ì¤‘ë¦½ì„±ì— ëŒ€í•œ ì˜ë¬¸ ì§€ì‹ì¸ì˜ ì‚¬íšŒì  ì±…ì„ ì¸ë¥˜ì˜ ë¯¸ë˜ ê¸°ìˆ  ë°œì „ê³¼ ë„ë•ì  ì§„ë³´ì˜ ë¶ˆì¼ì¹˜ ë¬¸ëª…ì˜ ì§€ì†ê°€ëŠ¥ì„±ì— ëŒ€í•œ ê·¼ë³¸ì  ì˜ë¬¸ ê¸€ë¡œë²Œ ìˆ˜ìš©ê³¼ ë…¼ë€ ê¸ì •ì  í‰ê°€ ê³¼í•™ì  ìƒìƒë ¥: ì •êµí•œ ë¬¼ë¦¬í•™ ì´ë¡ ì„ ë°”íƒ•ìœ¼ë¡œ í•œ ì„¤ì • ì² í•™ì  ê¹Šì´: ë¬¸ëª…, ê³¼í•™, ë„ë•ì— ëŒ€í•œ ê¹Šì´ ìˆëŠ” ì„±ì°° ì¤‘êµ­ SFì˜ ì„¸ê³„í™”: ì„œêµ¬ ì¤‘ì‹¬ì˜ SF ì¥ë¥´ì— ë™ì–‘ì  ê´€ì  ë„ì… ë¹„íŒê³¼ ë…¼ë€ ë„·í”Œë¦­ìŠ¤ ë²„ì „ì— ëŒ€í•œ ì¤‘êµ­ ë‚´ ë°˜ë°œ: ë¬¸í™”ëŒ€í˜ëª… ì¥ë©´ì´ ì¤‘êµ­ì„ ë¹„í•˜í•œë‹¤ëŠ” ë¹„íŒ ì¤‘êµ­ì  ìš”ì†Œì˜ í¬ì„ì— ëŒ€í•œ ë¶ˆë§Œ ì •ì¹˜ì  í•´ì„: ì¤‘êµ­ êµ­ê°€ì£¼ì˜ìë“¤ì˜ \"ì„œêµ¬ ë¬¸í™” íŒ¨ê¶Œì£¼ì˜\" ë¹„íŒ ë¬¸í™”ëŒ€í˜ëª… ë¬˜ì‚¬ì— ëŒ€í•œ ê²€ì—´ ìš°ë ¤ ì‚¼ì²´ê°€ ë³´ì—¬ì£¼ëŠ” ë¬¸í™”ëŒ€í˜ëª…ì˜ ì˜í–¥ ê°œì¸ì  ì°¨ì› íŠ¸ë¼ìš°ë§ˆì˜ ì„¸ëŒ€ ì „ìŠ¹: ì˜ˆì›ì œë¥¼ í†µí•´ ë³¸ ê°œì¸ì  ìƒì²˜ ì‹ ë¢°ì˜ ë¶•ê´´: ì¸ê°„ê³¼ ì‚¬íšŒì— ëŒ€í•œ ê·¼ë³¸ì  ë¶ˆì‹  ì‚¬íšŒì  ì°¨ì› ì§€ì‹ ì²´ê³„ì˜ íŒŒê´´: ê³¼í•™ê³¼ í•™ë¬¸ì— ëŒ€í•œ íƒ„ì••ì˜ ê²°ê³¼ ë„ë•ì  í˜¼ë€: ì„ ì•…ì˜ ê¸°ì¤€ì´ ë¬´ë„ˆì§„ ì‚¬íšŒì˜ ëª¨ìŠµ ë¬¸ëª…ì‚¬ì  ì°¨ì› ì§„ë³´ì— ëŒ€í•œ íšŒì˜: ì¸ë¥˜ ë¬¸ëª…ì˜ ë°œì „ ê°€ëŠ¥ì„±ì— ëŒ€í•œ ì˜ë¬¸ ì™¸ë¶€ êµ¬ì›ì— ëŒ€í•œ í™˜ìƒ: ë‚´ë¶€ ê°œí˜ì˜ í•œê³„ì™€ ì™¸ë¶€ ê°œì… ê¸°ëŒ€ ì‚¼ì²´ëŠ” ë‹¨ìˆœí•œ SF ì†Œì„¤ì„ ë„˜ì–´ì„œ, ë¬¸í™”ëŒ€í˜ëª…ì´ë¼ëŠ” ì—­ì‚¬ì  ë¹„ê·¹ì´ ì–´ë–»ê²Œ ê°œì¸ê³¼ ë¬¸ëª… ì „ì²´ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ë³´ì—¬ì£¼ëŠ” ìš°í™”ì  ì‘í’ˆì´ë‹¤. ì˜ˆì›ì œì˜ ì„ íƒì€ ê°œì¸ì  íŠ¸ë¼ìš°ë§ˆê°€ ì–´ë–»ê²Œ ì¸ë¥˜ ì „ì²´ì˜ ìš´ëª…ì„ ë°”ê¿€ ìˆ˜ ìˆëŠ”ì§€ë¥¼ ê·¹ì ìœ¼ë¡œ ë³´ì—¬ì£¼ë©°, ë¬¸í™”ëŒ€í˜ëª…ì˜ ìƒì²˜ê°€ ì—¬ì „íˆ ì¤‘êµ­ ì‚¬íšŒì™€ ë¬¸í™”ì— ê¹Šì´ ê°ì¸ë˜ì–´ ìˆìŒì„ ì‹œì‚¬í•œë‹¤."
  },
  {
    "id": "Causal Inference/what-is-ate",
    "title": "What is ATE?",
    "date": "2025-07-17",
    "excerpt": "What is ATE?",
    "category": "Causal Inference",
    "tags": [
      "ATE"
    ],
    "content": "ATEë€? --- Average Treatment Effect (ATE)ëŠ” ëª¨ë“  ì‚¬ëŒë“¤ì´ ë°›ì€ ì²˜ì¹˜ì˜ í‰ê·  íš¨ê³¼ë¥¼ ë§í•œë‹¤. ì¦‰, ì²˜ì¹˜ë°›ì€ ê²°ê³¼ì™€ ë°›ì§€ ì•Šì€ ê²°ê³¼ì˜ ì°¨ì´ì˜ í‰ê· ì´ë¼ê³  ì´í•´í•  ìˆ˜ ìˆë‹¤. ATE ì¶”ì • --- $$ \\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) \\tag{1} $$ ì •ì˜ì— ë”°ë¼ ATEëŠ” (1)ì²˜ëŸ¼ ì •ì˜ëœë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ëª¨ë“  ì‚¬ëŒë“¤ì´ ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šê¸° ë•Œë¬¸ì— (1)ì„ ì§ì ‘ ê³„ì‚°í•  ìˆ˜ ì—†ë‹¤."
  },
  {
    "id": "Causal Inference/double-robustness-explained",
    "title": "Double Robustness: ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì™„ì „ ì •ë³µ",
    "date": "2025-07-17",
    "excerpt": "Double Robustnessì˜ ì´ë¡ ì  ê¸°ë°˜ë¶€í„° ì‹¤ìš©ì  ì‘ìš©ê¹Œì§€ ì™„ì „ í•´ë¶€",
    "category": "Causal Inference",
    "tags": [
      "Double Robustness",
      "Causal Inference",
      "Theory",
      "R-learner"
    ],
    "content": "Double Robustness: ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì™„ì „ ì •ë³µ Double Robustness(ì´ì¤‘ ê°•ê±´ì„±)ëŠ” í˜„ëŒ€ ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì¤‘ í•˜ë‚˜ë¡œ, R-learnerì™€ ê°™ì€ ìµœì‹  ë°©ë²•ë¡ ì˜ ì´ë¡ ì  ê¸°ë°˜ì´ ë©ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Double Robustnessì˜ ëª¨ë“  ì¸¡ë©´ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. Double Robustnessë€ ë¬´ì—‡ì¸ê°€? 1.1 ì§ê´€ì  ì´í•´ Double RobustnessëŠ” \"ë‘ ê°€ì§€ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ í¸í–¥ ì—†ëŠ” ì¶”ì •ì´ ê°€ëŠ¥í•œ ì„±ì§ˆ\"ì…ë‹ˆë‹¤. ê°„ë‹¨í•œ ë¹„ìœ  ìë¬¼ì‡  ë‘ ê°œ: ë¬¸ì„ ì—´ë ¤ë©´ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì—´ë©´ ë¨ ë°±ì—… ì‹œìŠ¤í…œ: ì£¼ ì‹œìŠ¤í…œì´ ì‹¤íŒ¨í•´ë„ ë³´ì¡° ì‹œìŠ¤í…œì´ ì‘ë™ ì´ì¤‘ ë³´í—˜: í•˜ë‚˜ê°€ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ í•˜ë‚˜ê°€ ë³´ì¥ 1.2 ì¸ê³¼ì¶”ë¡ ì—ì„œì˜ ì˜ë¯¸ ì¸ê³¼ì¶”ë¡ ì—ì„œ ìš°ë¦¬ê°€ ì¶”ì •í•´ì•¼ í•˜ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ìš”ì†Œ: Outcome Model (ê²°ê³¼ ëª¨ë¸): $\\mu(x, w) = \\mathbb{E}[Y X = x, W = w]$ Propensity Score Model (ì²˜ì¹˜ í™•ë¥  ëª¨ë¸): $e(x) = \\text{Pr}(W = 1 X = x)$ Double Robustness: ì´ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ ì²˜ì¹˜íš¨ê³¼ë¥¼ í¸í–¥ ì—†ì´ ì¶”ì •í•  ìˆ˜ ìˆìŒ! ìˆ˜í•™ì  ì´ë¡ : í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ 2.1 Potential Outcomes Framework ê¸°ë³¸ ì„¤ì •: ì ì¬ê²°ê³¼: $Y_i(0), Y_i(1)$ (ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œ/ë°›ì•˜ì„ ë•Œì˜ ê²°ê³¼) ê´€ì°°ê²°ê³¼: $Y_i = Y_i(W_i)$ ê°œë³„ ì²˜ì¹˜íš¨ê³¼: $\\tau_i = Y_i(1) - Y_i(0)$ í‰ê·  ì²˜ì¹˜íš¨ê³¼: $\\tau = \\mathbb{E}[Y(1) - Y(0)]$ ê·¼ë³¸ì  ë¬¸ì œ: ê°™ì€ ê°œì¸ì— ëŒ€í•´ $Y_i(0)$ê³¼ $Y_i(1)$ì„ ë™ì‹œì— ê´€ì°°í•  ìˆ˜ ì—†ìŒ! 2.2 Unconfoundedness Assumption $$\\{Y(0), Y(1)\\} \\perp \\!\\!\\! \\perp W \\mid X$$ ì˜ë¯¸: ê³µë³€ëŸ‰ $X$ë¥¼ í†µì œí•˜ë©´ ì²˜ì¹˜ í• ë‹¹ì´ ë¬´ì‘ìœ„ì™€ ê°™ìŒ ì´ ê°€ì • í•˜ì—ì„œ ë‹¤ìŒì´ ì„±ë¦½: $$\\mathbb{E}[Y(w) X] = \\mathbb{E}[Y X, W = w] = \\mu(X, w)$$ 2.3 Double Robustnessì˜ ìˆ˜í•™ì  í‘œí˜„ í•µì‹¬ í•­ë“±ì‹ Average Treatment Effect (ATE)ì— ëŒ€í•œ doubly robust ì¶”ì •ëŸ‰: $$\\hat{\\tau}_{DR} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\frac{W_i Y_i}{\\hat{e}(X_i)} - \\frac{(1-W_i) Y_i}{1-\\hat{e}(X_i)} + \\frac{W_i - \\hat{e}(X_i)}{\\hat{e}(X_i)(1-\\hat{e}(X_i))} \\left\\{ \\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0) \\right\\} \\right]$$ ë” ê°„ë‹¨í•œ í˜•íƒœ AIPW (Augmented Inverse Propensity Weighting) ì¶”ì •ëŸ‰: $$\\hat{\\tau}_{AIPW} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0) + \\frac{W_i}{\\hat{e}(X_i)} \\{Y_i - \\hat{\\mu}(X_i, 1)\\} - \\frac{1-W_i}{1-\\hat{e}(X_i)} \\{Y_i - \\hat{\\mu}(X_i, 0)\\} \\right]$$ 2.4 ì™œ \"Double Robust\"ì¸ê°€? í•µì‹¬ ì •ë¦¬ Theorem: ë‹¤ìŒ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë§Œ ë§Œì¡±í•˜ë©´ $\\mathbb{E}[\\hat{\\tau}_{AIPW}] = \\tau$: Outcome modelì´ ì •í™•: $\\hat{\\mu}(x, w) = \\mu^(x, w)$ Propensity modelì´ ì •í™•: $\\hat{e}(x) = e^(x)$ ì¦ëª…ì˜ í•µì‹¬ ì•„ì´ë””ì–´ AIPW ì¶”ì •ëŸ‰ì„ ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$\\hat{\\tau}_{AIPW} = \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\{\\hat{\\mu}(X_i, 1) - \\hat{\\mu}(X_i, 0)\\}}_{\\text{Outcome-based}} + \\underbrace{\\frac{1}{n} \\sum_{i=1}^n \\left[\\frac{W_i}{\\hat{e}(X_i)} - \\frac{1-W_i}{1-\\hat{e}(X_i)}\\right] \\{Y_i - \\hat{\\mu}(X_i, W_i)\\}}_{\\text{IPW correction}}$$ ì¼€ì´ìŠ¤ 1: Outcome modelì´ ì •í™•í•œ ê²½ìš° ì²« ë²ˆì§¸ í•­ì´ ì •í™•í•œ $\\tau$ ì œê³µ ë‘ ë²ˆì§¸ í•­ì˜ ê¸°ëŒ“ê°’ì´ 0 (ì”ì°¨ì˜ ê°€ì¤‘í‰ê· ) ì¼€ì´ìŠ¤ 2: Propensity modelì´ ì •í™•í•œ ê²½ìš° ë‘ ë²ˆì§¸ í•­ì´ IPW ì¶”ì •ëŸ‰ìœ¼ë¡œ ì •í™•í•œ $\\tau$ ì œê³µ ì²« ë²ˆì§¸ í•­ì˜ ì˜¤ì°¨ê°€ ë‘ ë²ˆì§¸ í•­ì— ì˜í•´ ìƒì‡„ ì§ê´€ì  ì´í•´: ì™œ ì‘ë™í•˜ëŠ”ê°€? 3.1 Visual Intuition `` ì§„ì‹¤í•œ ì²˜ì¹˜íš¨ê³¼: Ï„ = 5 ì‹œë‚˜ë¦¬ì˜¤ 1: Outcome Model ì •í™•, Propensity Model ë¶€ì •í™• â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Outcome Model â”‚ â”‚ Propensity Modelâ”‚ â”‚ âœ… Ï„Ì‚ = 5.0 â”‚ + â”‚ âŒ í¸í–¥ ìˆìŒ â”‚ = Ï„Ì‚ = 5.0 âœ… â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ì‹œë‚˜ë¦¬ì˜¤ 2: Outcome Model ë¶€ì •í™•, Propensity Model ì •í™• â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Outcome Model â”‚ â”‚ Propensity Modelâ”‚ â”‚ âŒ í¸í–¥ ìˆìŒ â”‚ + â”‚ âœ… ì •í™•í•¨ â”‚ = Ï„Ì‚ = 5.0 âœ… â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ì‹œë‚˜ë¦¬ì˜¤ 3: ë‘˜ ë‹¤ ì •í™• â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Outcome Model â”‚ â”‚ Propensity Modelâ”‚ â”‚ âœ… Ï„Ì‚ = 5.0 â”‚ + â”‚ âœ… ì •í™•í•¨ â”‚ = Ï„Ì‚ = 5.0 âœ… â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ` 3.2 ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ (Compensation Mechanism) Outcome Modelì´ ì •í™•í•œ ê²½ìš° `python Outcome modelì´ ì™„ë²½í•˜ë©´ mu_hat(x, 1) - mu_hat(x, 0) â‰ˆ Ï„ IPW correction termì€ E[W/e_hat(X) - (1-W)/(1-e_hat(X))] {Y - mu_hat(X,W)} â‰ˆ 0 (ì”ì°¨ì˜ ê°€ì¤‘í‰ê· ì€ 0ì— ê°€ê¹Œì›€) ` Propensity Modelì´ ì •í™•í•œ ê²½ìš° `python Propensity modelì´ ì™„ë²½í•˜ë©´ IPWê°€ ì •í™•í•œ ì¶”ì • ì œê³µ IPW_term = E[WY/e(X) - (1-W)Y/(1-e(X))] = Ï„ Outcome modelì˜ ì˜¤ì°¨ëŠ” IPW correctionì— ì˜í•´ ìƒì‡„ë¨ ` 3.3 ì‹¤ì œ ë°ì´í„° ì˜ˆì‹œ ì‹œë®¬ë ˆì´ì…˜ ì„¤ì • `python n = 1000 X = np.random.normal(0, 1, (n, 2)) e_true = expit(X[:, 0]) # true propensity W = np.random.binomial(1, e_true) True outcome model: complex nonlinear Y_0 = X[:, 0]*2 + X[:, 1] + noise Y_1 = Y_0 + 2 + X[:, 0] # treatment effect = 2 + X[:, 0] Y = W Y_1 + (1 - W) Y_0 true_ATE = 2 + np.mean(X[:, 0]) # â‰ˆ 2.0 ` ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²°ê³¼ `python Scenario 1: Good outcome model, bad propensity model mu_hat_good = fit_complex_model(Y, X, W) # RÂ² = 0.95 e_hat_bad = 0.5 # constant (wrong!) AIPW_1 = compute_AIPW(Y, W, X, mu_hat_good, e_hat_bad) Result: 2.03 (ê±°ì˜ ì •í™•! âœ…) Scenario 2: Bad outcome model, good propensity model mu_hat_bad = fit_linear_model(Y, X, W) # RÂ² = 0.3 (underfit) e_hat_good = fit_logistic_model(W, X) # very accurate AIPW_2 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_good) Result: 1.97 (ì—¬ì „íˆ ì •í™•! âœ…) Scenario 3: Both models bad AIPW_3 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_bad) Result: 1.2 (í¸í–¥ë¨ âŒ) ` Double Robustnessì˜ ì¥ì ê³¼ í•œê³„ 4.1 í•µì‹¬ ì¥ì  ê²¬ê³ ì„± (Robustness) ëª¨ë¸ ì˜¤ì§€ì •ì— ëŒ€í•œ ë³´í—˜: í•˜ë‚˜ê°€ í‹€ë ¤ë„ ì•ˆì „ ì‹¤ë¬´ì  ì•ˆì •ì„±: ì™„ë²½í•œ ëª¨ë¸ë§ì´ ì–´ë ¤ìš´ í˜„ì‹¤ì—ì„œ ìœ ìš© íš¨ìœ¨ì„± (Efficiency) ë‘˜ ë‹¤ ì •í™•í•˜ë©´ ìµœê³  íš¨ìœ¨ì„±: ìµœì†Œ ë¶„ì‚° ë‹¬ì„± Semiparametric efficiency bound ë‹¬ì„± ìœ ì—°ì„± (Flexibility) ì„œë¡œ ë‹¤ë¥¸ ë°©ë²• ì¡°í•©: ê° ëª¨ë¸ì— ìµœì í™”ëœ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ ê¸°ê³„í•™ìŠµ ë°©ë²• í™œìš©: ë³µì¡í•œ ëª¨ë¸ë„ ì‚¬ìš© ê°€ëŠ¥ 4.2 í•œê³„ì  ë‘˜ ë‹¤ í‹€ë¦¬ë©´ í¸í–¥ `python ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤ if outcome_model_wrong and propensity_model_wrong: bias = f(error_outcome, error_propensity) # í¸í–¥ ë°œìƒ ` ë¶„ì‚° ì¦ê°€ ê°€ëŠ¥ì„± IPW termì˜ ê·¹ë‹¨ê°’: propensity scoreê°€ 0 ë˜ëŠ” 1ì— ê°€ê¹Œìš°ë©´ ë¶„ì‚° ê¸‰ì¦ Overlap ì¡°ê±´ í•„ìš”: $0 < e(x) < 1$ for all $x$ ì¶”ì • ë³µì¡ì„± ë‘ ëª¨ë¸ ëª¨ë‘ ì¶”ì •: ê³„ì‚° ë¶€ë‹´ ì¦ê°€ êµì°¨ê²€ì¦ ë³µì¡ì„±: ê° ëª¨ë¸ì˜ íŠœë‹ì´ ë³µì¡ R-learnerì™€ì˜ ì—°ê²° 5.1 Robinson's Transformationê³¼ Double Robustness R-learnerëŠ” Robinson's transformationì„ ì‚¬ìš©í•©ë‹ˆë‹¤: $$Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i$$ ì´ë¥¼ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´: $$Y_i = m^(X_i) + \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i$$ Double Robust Structure R-loss í•¨ìˆ˜: $$\\hat{L}_n\\{\\tau(\\cdot)\\} = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\left\\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\right\\} - \\left\\{W_i - \\hat{e}^{(-q(i))}(X_i)\\right\\} \\tau(X_i) \\right]^2$$ í•µì‹¬: ì´ ì†ì‹¤í•¨ìˆ˜ë„ double robust ì„±ì§ˆì„ ê°€ì§‘ë‹ˆë‹¤! $\\hat{m}$ì´ ì •í™•í•˜ë©´ ì²« ë²ˆì§¸ í•­ì´ ì •í™•í•œ ì”ì°¨ ì œê³µ $\\hat{e}$ê°€ ì •í™•í•˜ë©´ ë‘ ë²ˆì§¸ í•­ì´ ì •í™•í•œ ê°€ì¤‘ì¹˜ ì œê³µ 5.2 ì™œ R-learnerê°€ Quasi-Oracleì¸ê°€? `python Oracle knows m(x) and e(x) oracle_loss = E[(Y - m(X) - {W - e(X)}Ï„(X))Â²] R-learner uses estimates r_loss = E[(Y - mÌ‚(X) - {W - Ãª(X)}Ï„Ì‚(X))Â²] Double robustness ensures: if rate(mÌ‚ - m) rate(Ãª - e) â†’ 0 faster than oracle_rate: rate(R-learner) â‰ˆ oracle_rate # ğŸ¯ ` ì‹¤ìš©ì  êµ¬í˜„ ê°€ì´ë“œ 6.1 Python Implementation `python import numpy as np from sklearn.ensemble import RandomForestRegressor from sklearn.linear_model import LogisticRegression from sklearn.model_selection import cross_val_predict def doubly_robust_ate(Y, W, X, outcome_model=None, propensity_model=None): \"\"\" Doubly Robust ATE ì¶”ì • Parameters: ----------- Y : array-like, shape (n,) Outcome variable W : array-like, shape (n,) Treatment indicator (0 or 1) X : array-like, shape (n, p) Covariates outcome_model : sklearn estimator Outcome regression model propensity_model : sklearn estimator Propensity score model Returns: -------- ate_estimate : float Doubly robust ATE estimate \"\"\" n = len(Y) # Default models if outcome_model is None: outcome_model = RandomForestRegressor(n_estimators=100, random_state=42) if propensity_model is None: propensity_model = LogisticRegression(random_state=42) # Cross-fitting for outcome model mu_1_hat = np.zeros(n) mu_0_hat = np.zeros(n) # Estimate E[Y X,W=1] idx_1 = W == 1 if np.sum(idx_1) > 0: mu_1_hat[idx_1] = cross_val_predict( outcome_model, X[idx_1], Y[idx_1], cv=5 ) outcome_model.fit(X[idx_1], Y[idx_1]) mu_1_hat[~idx_1] = outcome_model.predict(X[~idx_1]) # Estimate E[Y X,W=0] idx_0 = W == 0 if np.sum(idx_0) > 0: mu_0_hat[idx_0] = cross_val_predict( outcome_model, X[idx_0], Y[idx_0], cv=5 ) outcome_model.fit(X[idx_0], Y[idx_0]) mu_0_hat[~idx_0] = outcome_model.predict(X[~idx_0]) # Cross-fitting for propensity score e_hat = cross_val_predict( propensity_model, X, W, cv=5, method='predict_proba' )[:, 1] # Clip propensity scores to avoid extreme weights e_hat = np.clip(e_hat, 0.01, 0.99) # AIPW estimator aipw_components = ( (mu_1_hat - mu_0_hat) + # Outcome-based estimate W (Y - mu_1_hat) / e_hat - # IPW correction for treated (1 - W) (Y - mu_0_hat) / (1 - e_hat) # IPW correction for control ) ate_estimate = np.mean(aipw_components) # Standard error (using influence function) influence_func = aipw_components - ate_estimate se_estimate = np.std(influence_func) / np.sqrt(n) return { 'ate': ate_estimate, 'se': se_estimate, 'ci_lower': ate_estimate - 1.96 se_estimate, 'ci_upper': ate_estimate + 1.96 se_estimate } Usage example def simulate_data(n=1000): \"\"\"Generate simulation data\"\"\" X = np.random.normal(0, 1, (n, 3)) # True propensity score (logistic) e_true = 1 / (1 + np.exp(-(X[:, 0] + 0.5 X[:, 1]))) W = np.random.binomial(1, e_true) # True outcome model (nonlinear) Y_0 = X[:, 0]*2 + X[:, 1] + 0.5 X[:, 2] + np.random.normal(0, 0.5, n) Y_1 = Y_0 + 2 + X[:, 0] # Heterogeneous treatment effect Y = W Y_1 + (1 - W) Y_0 return Y, W, X, 2.0 # True ATE â‰ˆ 2.0 Run example Y, W, X, true_ate = simulate_data() result = doubly_robust_ate(Y, W, X) print(f\"True ATE: {true_ate:.3f}\") print(f\"DR Estimate: {result['ate']:.3f}\") print(f\"95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]\") ` 6.2 Best Practices ëª¨ë¸ ì„ íƒ ê°€ì´ë“œë¼ì¸ `python Outcome Model High flexibility for complex relationships outcome_models = { 'linear': LinearRegression(), 'rf': RandomForestRegressor(n_estimators=200), 'xgb': XGBRegressor(), 'nn': MLPRegressor(hidden_layer_sizes=(100, 50)) } Propensity Model Focus on overlap and calibration propensity_models = { 'logistic': LogisticRegression(C=1.0), 'rf': RandomForestClassifier(n_estimators=200), 'calibrated': CalibratedClassifierCV(LogisticRegression()) } ` ì§„ë‹¨ ë„êµ¬ `python def diagnose_overlap(e_hat, W): \"\"\"Check overlap assumption\"\"\" print(f\"Propensity score range: [{e_hat.min():.3f}, {e_hat.max():.3f}]\") print(f\"Extreme scores (< 0.1 or > 0.9): {np.mean((e_hat < 0.1) (e_hat > 0.9)):.1%}\") # Plot distributions import matplotlib.pyplot as plt plt.figure(figsize=(10, 4)) plt.subplot(1, 2, 1) plt.hist(e_hat[W==0], alpha=0.7, label='Control', bins=30) plt.hist(e_hat[W==1], alpha=0.7, label='Treated', bins=30) plt.xlabel('Propensity Score') plt.ylabel('Frequency') plt.legend() plt.title('Overlap Check') plt.subplot(1, 2, 2) plt.boxplot([e_hat[W==0], e_hat[W==1]], labels=['Control', 'Treated']) plt.ylabel('Propensity Score') plt.title('Distribution by Treatment') plt.tight_layout() plt.show() def check_balance(X, W, e_hat): \"\"\"Check covariate balance after weighting\"\"\" weights_1 = W / e_hat weights_0 = (1 - W) / (1 - e_hat) for j in range(X.shape[1]): mean_1 = np.average(X[W==1, j], weights=weights_1[W==1]) mean_0 = np.average(X[W==0, j], weights=weights_0[W==0]) print(f\"Variable {j}: Weighted difference = {mean_1 - mean_0:.4f}\") ` ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í™•ì¥ 7.1 Machine Learningê³¼ì˜ ê²°í•© Targeted Maximum Likelihood Estimation (TMLE) One-step correction: ì´ˆê¸° ì¶”ì •ê°’ì„ ì—…ë°ì´íŠ¸ Cross-validation: ìµœì  ëª¨ë¸ ì„ íƒ Causal Random Forests Honest splitting: í¸í–¥ ì—†ëŠ” ì¶”ì • Local centering: Double robustness ë³´ì¥ Neural Network Approaches Representation learning: ê³µí†µ íŠ¹ì„± í•™ìŠµ Domain adaptation: Treatment/control domain ì •ë ¬ 7.2 ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ í™•ì¥ Debiased Machine Learning (DML) `python Neyman orthogonality + Cross-fitting def debiased_ml_ate(Y, W, X): # Step 1: Estimate nuisance functions theta_0 = estimate_initial(Y, W, X) # Step 2: Compute orthogonal score psi = compute_orthogonal_score(Y, W, X, theta_0) # Step 3: Solve orthogonal equation theta_final = solve_orthogonal_equation(psi) return theta_final ` 7.3 ì—°ì† ì²˜ì¹˜ì™€ ë‹¤ì¤‘ ì²˜ì¹˜ Continuous Treatment `python GPS (Generalized Propensity Score) + Outcome Model def continuous_dr(Y, T, X): # Estimate treatment density: f(T X) gps_hat = estimate_gps(T, X) # Estimate dose-response: E[Y T,X] outcome_hat = estimate_outcome(Y, T, X) # Double robust estimand dr_estimate = compute_dr_continuous(Y, T, X, gps_hat, outcome_hat) return dr_estimate `` ê²°ë¡ : Double Robustnessì˜ ì˜ì˜ 8.1 ì´ë¡ ì  ê¸°ì—¬ í¸í–¥ ì—†ëŠ” ì¶”ì •ì˜ ë³´ì¥: ëª¨ë¸ ì˜¤ì§€ì •ì—ë„ robust íš¨ìœ¨ì„±: ìµœì  ì¡°ê±´ì—ì„œ semiparametric efficiency bound ë‹¬ì„± ìœ ì—°ì„±: ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ê³¼ ê²°í•© ê°€ëŠ¥ 8.2 ì‹¤ìš©ì  ê°€ì¹˜ í˜„ì‹¤ì  í•´ê²°ì±…: ì™„ë²½í•œ ëª¨ë¸ë§ì´ ì–´ë ¤ìš´ í˜„ì‹¤ì—ì„œ ì•ˆì „ë§ ì œê³µ ë°©ë²•ë¡  ë°œì „ì˜ ê¸°ë°˜: R-learner, TMLE, DML ë“±ì˜ ì´ë¡ ì  í† ëŒ€ ì‚°ì—… ì‘ìš©: A/B í…ŒìŠ¤íŠ¸, ê°œì¸í™”, ì •ì±… í‰ê°€ ë“±ì—ì„œ ë„ë¦¬ í™œìš© 8.3 ë¯¸ë˜ ì „ë§ Double RobustnessëŠ” ì•ìœ¼ë¡œë„ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡  ë°œì „ì˜ í•µì‹¬ ì›ë¦¬ë¡œ ì‘ìš©í•  ê²ƒì…ë‹ˆë‹¤: ë”¥ëŸ¬ë‹ê³¼ì˜ ê²°í•©: í‘œí˜„ í•™ìŠµ ê¸°ë°˜ double robust ë°©ë²• ì‹¤ì‹œê°„ ì¶”ë¡ : ì˜¨ë¼ì¸ í•™ìŠµì—ì„œì˜ double robustness ë³µì¡í•œ ì²˜ì¹˜ êµ¬ì¡°: ë„¤íŠ¸ì›Œí¬, ì‹œê³„ì—´ì—ì„œì˜ í™•ì¥ Double Robustnessë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ í˜„ëŒ€ ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ ë” robustí•˜ê³  íš¨ìœ¨ì ì¸ ì¸ê³¼ ì¶”ë¡  ë°©ë²•ë“¤ì´ ê³„ì† ë°œì „í•  ê²ƒì…ë‹ˆë‹¤. ğŸ¯ --- ì°¸ê³ ë¬¸í—Œ: Robins, J.M., Rotnitzky, A., & Zhao, L.P. (1994). Estimation of regression coefficients when some regressors are not always observed. Bang, H. & Robins, J.M. (2005). Doubly robust estimation in missing data and causal inference models. Chernozhukov, V., et al. (2018). Double/debiased machine learning for treatment and structural parameters. Kennedy, E.H. (2020). Towards optimal doubly robust estimation of heterogeneous causal effects."
  },
  {
    "id": "Causal Inference/Paper Review/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects",
    "title": "[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects",
    "date": "2025-07-17",
    "excerpt": "R-leaner ë°©ë²• ì†Œê°œì™€ ì´ê²Œ ê°€ì§€ëŠ” quasi-oracle propertyì— ëŒ€í•´ ì„¤ëª…",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link ë…¼ë¬¸ ë¦¬ë·° Abstract > Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì •í•˜ëŠ” ê²ƒì€ ë§ì€ ë¶„ì•¼ì—ì„œ í†µê³„ì  ë¬¸ì œì˜ í•µì‹¬ì…ë‹ˆë‹¤. ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì•Œê³  ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì•½ì²˜ë°©ì´ë‚˜ êµìœ¡ì •ì±… ê²°ì •, ìì› ë¶„ë°° ë“± ë§ì€ ë¬¸ì œì—ì„œ ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ë¥¼ ì¶”ì •í•˜ëŠ” ì¼ì€ ê½¤ë‚˜ ë³µì¡í•œ ì¼ì´ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ë…¼ë¬¸ì—ì„œëŠ” marginal effectsì™€ treatment propensityë¼ëŠ” ë‘ ê°€ì§€ nuisance componentë¥¼ ì¶”ì •í•˜ì—¬ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. > we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. ì´ëŸ¬í•œ ë°©ë²•ì€ quasi-oracle propertyë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤. <small> quasi-oracle property : marginal effectsì™€ treatment propensityì˜ ì¶”ì •ì´ ì •í™•í•˜ì§€ ì•Šë”ë¼ë„, ë§ˆì¹˜ ì´ ë‘ nuisance componentë¥¼ ë¯¸ë¦¬ ì•Œê³  ìˆëŠ” ê²ƒê³¼ ê°™ì€ ì˜¤ì°¨ ê²½ê³„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ì„±ì§ˆ</small> Introduction ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì : ë°©ë²•ë¡ ì  ë¶ˆì¼ì¹˜: ê´€ì°° ì—°êµ¬ì—ì„œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì— ì–´ë–»ê²Œ ì ìš©í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ë‹µì´ ì•„ì§ í™•ë¦½ë˜ì§€ ì•ŠìŒ ê°œë°œ ê³¼ì •ì˜ ë³µì¡ì„±: ì¸ê³¼ê´€ê³„ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ê°œë°œí•˜ëŠ” ê³¼ì •ì´ ë…¸ë™ì§‘ì•½ì ì´ë©°, ì „ë¬¸ ì—°êµ¬ìë“¤ì˜ ì°¸ì—¬ê°€ í•„ìˆ˜ì  ì´ë¡ ì  ê·¼ê±° ë¶€ì¡±: ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ë“¤ì´ ìˆ˜ì¹˜ì  ì‹¤í—˜ìœ¼ë¡œë§Œ ê²€ì¦ë˜ê³ , í˜•ì‹ì ì¸ ìˆ˜ë ´ ë³´ì¥ì´ë‚˜ ì˜¤ì°¨ ê²½ ì´ ë…¼ë¬¸ì˜ ìƒˆë¡œìš´ ì ‘ê·¼ë²•: ìë™í™”ëœ í”„ë ˆì„ì›Œí¬: ì„ì˜ì˜ ì†ì‹¤ ìµœì†Œí™” ì ˆì°¨ë¥¼ í†µí•´ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë¥¼ ì™„ì „ ìë™ìœ¼ë¡œ ëª…ì„¸í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ ì œê³µ Oracle ìˆ˜ì¤€ì˜ ì„±ëŠ¥: ë°ì´í„° ìƒì„± ë¶„í¬ì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ì•Œê³  ìˆëŠ” oracle ë°©ë²•ê³¼ ë¹„êµ ê°€ëŠ¥í•œ ì˜¤ì°¨ ê²½ê³„ ë‹¬ì„± ì´ë¡ ì  ê¸°ë°˜: ì´ì¤‘ ê°•ê±´ ì¶”ì •(double robust estimation), oracle ë¶€ë“±ì‹, êµì°¨ ê²€ì¦ì„ ê²°í•©í•˜ì—¬ ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ë„êµ¬ë¡œ ì›ë¦¬ì  í†µê³„ ì¶”ì •ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ê°œë°œ ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ ì‹¤ìš©ì  í•œê³„ì™€ ì´ë¡ ì  ë¶€ì¡±í•¨ì„ ëª¨ë‘ í•´ê²°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. A Loss Function for Treatment Effect Estimation > We formalize our problem in terms of the potential outcomes framework (Neyman, 1923; Rubin, 1974). potential outcomes frameworkë¥¼ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. ë°ì´í„° êµ¬ì¡°: ê´€ì°° ë°ì´í„°: $(X_i, Y_i, W_i)$ for $i = 1, ..., n$ $X_i \\in \\mathcal{X}$: ê°œì¸ë³„ íŠ¹ì„± (features) $Y_i \\in \\mathbb{R}$: ê´€ì°°ëœ ê²°ê³¼ (observed outcome) $W_i \\in \\{0, 1\\}$: ì²˜ì¹˜ í• ë‹¹ (treatment assignment) ì ì¬ ê²°ê³¼ (Potential Outcomes): ì ì¬ ê²°ê³¼: $\\{Y_i(0), Y_i(1)\\}$ $Y_i(0)$: ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²°ê³¼ $Y_i(1)$: ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ê²°ê³¼ ê´€ì°°ëœ ê²°ê³¼ì™€ì˜ ê´€ê³„: $Y_i = Y_i(W_i)$ ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜íš¨ê³¼ (CATE): ëª©í‘œ í•¨ìˆ˜: $\\tau^(x) = \\mathbb{E}[Y(1) - Y(0) \\mid X = x]$ íŠ¹ì„± $X = x$ì¸ ê°œì¸ë“¤ì˜ í‰ê·  ì²˜ì¹˜íš¨ê³¼ ì‹ë³„ ì¡°ê±´: Unconfoundedness: ì²˜ì¹˜ í• ë‹¹ì´ ê´€ì°°ë˜ì§€ ì•Šì€ confounding variableì— ì˜í•´ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê°€ì • ì´ ìˆ˜ì‹ë“¤ì€ ì¸ê³¼ì¶”ë¡ ì˜ í‘œì¤€ì ì¸ ì ì¬ê²°ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼ ë”°ë¥´ë©°, ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•œ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. > In order to identify $\\tau^(x)$, we assume unconfoundedness, i.e., the treatment assignment is randomized once we control for the features $X_i$ (Rosenbaum and Rubin, 1983). CATEë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ë¬´ì‘ìœ„ í• ë‹¹ì²˜ëŸ¼ unconfoundedness ê°€ì •ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ì„œ Xë¥¼ í†µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Assumption 1 : The treatment assignment $W_i$ is unconfounded, $\\{Y_i(0), Y_i(1)\\} \\perp \\!\\!\\! \\perp W_i \\mid X_i$ Treatment Propensity : $e^(x) = \\Pr(W = 1 \\mid X = x)$ Conditional Response Surfaces : $\\mu^{(w)}(x) = \\mathbb{E}[Y(w) \\mid X = x]$ for $w \\in \\{0, 1\\}$ Error Term : $\\varepsilon_i(w) := Y_i(w) - \\{\\mu^{(0)}(X_i) + w\\tau^(X_i)\\}$ ì„±ì§ˆ : unconfoundedness í•˜ì—ì„œ $\\mathbb{E}[\\varepsilon_i(W_i) \\mid X_i, W_i] = 0$ Conditional Mean Outcome : $m^(x) = \\mathbb{E}[Y \\mid X = x] = \\mu^{(0)}(x) + e^(x)\\tau^(x)$ $$ Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i \\tag{1} $$ (1)ì—ì„œ *propensity score $e^(X_i)$ë¥¼ í†µí•´ Xë¥¼ í†µì œí•©ë‹ˆë‹¤. $W_i - e^(X_i)$: ì‹¤ì œ ì²˜ì¹˜ í• ë‹¹ì—ì„œ ì˜ˆì¸¡ëœ ì²˜ì¹˜ í™•ë¥ ì„ ëº€ ê°’ ì´ëŠ” Xë¥¼ í†µì œí•œ í›„ì˜ \"ì²˜ì¹˜ í• ë‹¹ì˜ í¸ì°¨\"ë¥¼ ë‚˜íƒ€ëƒ„ ë§ˆì¹˜ Xê°€ ê°™ì€ ê·¸ë£¹ ë‚´ì—ì„œ ë¬´ì‘ìœ„ í• ë‹¹ëœ ê²ƒì²˜ëŸ¼ ë§Œë“¦ ë”°ë¼ì„œ ì´ ë“±ì‹ì€ propensity scoreë¥¼ í†µí•œ X í†µì œë¥¼ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤. > The goal of this paper is to study how we can use the Robinsonâ€™s transfomation (1) for flexible treatment effect estimation that builds on modern machine learning approaches such as boosting or deep learning. ì´ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ë¶€ìŠ¤íŒ…ì´ë‚˜ ë”¥ëŸ¬ë‹ê³¼ ê°™ì€ í˜„ëŒ€ì ì¸ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìœ ì—°í•œ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•´ Robinson's transfomation (1)ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì—°êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. > Our main result is that we can use this representation to construct a loss function that captures heterogeneous treatment effects, and that we can then accurately estimate treatment effectsâ€”both in terms of empirical performance and asymptotic guaranteesâ€”by finding regularized minimizers of this loss function. ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê²°ê³¼ëŠ” ì´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ í¬ì°©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ì†ì‹¤ í•¨ìˆ˜ì˜ ì •ê·œí™”ëœ ìµœì†Œí™” í•´ë¥¼ ì°¾ì•„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. $$ \\tau^(\\cdot) = \\arg\\min_{\\tau} \\mathbb{E}\\left[\\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\} \\tau(X_i)\\right]^2 \\tag{2} $$ $$ \\tilde{\\tau}(\\cdot) = \\arg\\min_{\\tau} \\left\\{ \\frac{1}{n} \\sum_{i=1}^n \\left( \\{ Y_i - m^(X_i) \\} - \\{ W_i - e^(X_i) \\} \\tau(X_i) \\right)^2 + \\Lambda_n[\\tau(\\cdot)] \\right\\} \\tag{3} $$ where the term $\\Lambda_n[\\tau(\\cdot)]$ is interpreted as a regularizer on the complexity of the $\\tau(\\cdot)$ function > This regularization could be explicit as in penalized regression, or implicit, e.g., as provided by a carefully designed deep neural network. ì •ê·œí™”í•­ì€ ëª¨ë¸ì˜ ì„¤ê³„ì— ë§ì¶° ì ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. > The difficulty, however, is that in practice we never know the weighted main effect function $m^(x)$ and usually donâ€™t know the treatment propensities $e^(x)$ either, and so the estimator (3) is not feasible. í•˜ì§€ë§Œ ì–´ë ¤ìš´ ì ì€ ë‹¹ì—°í•˜ê²Œë„ $m^(x)$ì™€ $e^(x)$ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ì ì…ë‹ˆë‹¤. > Given these preliminaries, we here study the following class of two-step estimators using cross-fitting (Chernozhukov et al., 2018; Schick, 1986) motivated by the above oracle procedure: Cross-fittingì„ ì´ìš©í•œ 2ë‹¨ê³„ ì¶”ì • ë°©ë²•: Step 1: ë°ì´í„° ë¶„í•  ë° Nuisance Component ì¶”ì • ë°ì´í„°ë¥¼ Qê°œ(ë³´í†µ 5 ë˜ëŠ” 10)ì˜ ê· ë“±í•œ í¬ê¸°ì˜ foldë¡œ ë¶„í•  $q(\\cdot)$: $i = 1, \\ldots, n$ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¥¼ Qê°œì˜ foldì— ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜ Cross-fittingì„ í†µí•´ $\\hat{m}$ê³¼ $\\hat{e}$ë¥¼ ìµœì  ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìœ„í•´ ì¡°ì •ëœ ë°©ë²•ìœ¼ë¡œ ì¶”ì • Step 2: Plug-in ì¶”ì • (3)ì˜ plug-in ë²„ì „ì„ í†µí•´ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • $\\hat{e}^{(-q(i))}(X_i)$ ë“±ì€ ië²ˆì§¸ í›ˆë ¨ ì˜ˆì œê°€ ì†í•œ foldë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë§Œë“  ì˜ˆì¸¡ê°’ $$ \\hat{\\tau}(\\cdot) = \\arg\\min_{\\tau} \\left\\{ \\hat{L}_n[\\tau(\\cdot)] + \\Lambda_n[\\tau(\\cdot)] \\right\\} \\tag{4} $$ $$ \\hat{L}_n[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\} - \\{W_i - \\hat{e}^{(-q(i))}(X_i)\\} \\tau(X_i) \\right]^2 \\tag{4a} $$ > In other words, the first step learns an approximation for the oracle objective, and the second step optimizes it. We refer to this approach as the R-learner in recognition of the work of Robinson (1988) and to emphasize the role of residualization. We will also refer to the squared loss $L_b^n[\\tau(\\cdot)]$ as the R-loss. 1ë‹¨ê³„: ì˜¤ë¼í´ ëª©ì í•¨ìˆ˜(ì´ë¡ ì ìœ¼ë¡œ ìµœì ì„ì„ ë³´ì¥í•˜ëŠ” í•¨ìˆ˜)ì˜ ê·¼ì‚¬ê°’ì„ í•™ìŠµ 2ë‹¨ê³„: ê·¸ ê·¼ì‚¬ ëª©ì í•¨ìˆ˜ë¥¼ ì‹¤ì œë¡œ ìµœì í™” ì´ ì „ì²´ ê³¼ì •ì„ R-learnerë¼ ë¶€ë¥´ê³ , ì†ì‹¤í•¨ìˆ˜ë¥¼ R-lossë¼ ë¶€ë¦„ ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬: ë‹¤ì–‘í•œ ë°©ë²• ì ìš© ë° ì„±ëŠ¥ R-learnerë¥¼ í˜ë„í‹° íšŒê·€, ì»¤ë„ ë¦¿ì§€ íšŒê·€, ë¶€ìŠ¤íŒ… ë“± ë‹¤ì–‘í•œ ë°©ë²•ì— ì ìš©í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„. ì´ë¡ ì  ë³´ì¥ ì»¤ë„ íšŒê·€ì˜ ê²½ìš°, ì‹¤ì œ ì¶”ì •ê¸°(plug-in estimator)ì˜ ì˜¤ì°¨ ê²½ê³„ê°€ ì˜¤ë¼í´(ì´ë¡ ì  ìµœì ) ë°©ë²•ê³¼ ê±°ì˜ ì¼ì¹˜í•¨ì„ ì¦ëª…. íŠ¹íˆ, nuisance component($m^(x)$, $e^(x)$)ì˜ ì¶”ì • ì˜¤ì°¨ê°€ ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë©´, ìµœì¢… ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ì˜¤ì§ $\\tau^(x)$ì˜ ë³µì¡ë„ì—ë§Œ ì˜ì¡´í•¨. ì‹¤ìš©ì  ì¥ì  R-learnerëŠ” ì²˜ì¹˜í™•ë¥ ê³¼ ê²°ê³¼ ì˜ˆì¸¡ì˜ ìƒê´€ê´€ê³„ë¥¼ ì†ì‹¤í•¨ìˆ˜ êµ¬ì¡°ë¡œ ë¶„ë¦¬í•˜ì—¬, ë‘ ì‘ì—…(ìƒê´€ê´€ê³„ ì œê±°, ì²˜ì¹˜íš¨ê³¼ ì¶”ì •)ì„ ëª…í™•íˆ ë¶„ë¦¬í•¨. ì´ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë„êµ¬(ì˜ˆ: glmnet, XGBoost, TensorFlow ë“±)ë¥¼ ì†ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆê³ , ì†ì‹¤í•¨ìˆ˜(R-loss)ë§Œ ì˜ ìµœì†Œí™”í•˜ë©´ ë¨. ë³µì¡í•œ êµì°¨ê²€ì¦ ì—†ì´ë„ ì†ì‹¤í•¨ìˆ˜ ê¸°ë°˜ì˜ ê°„ë‹¨í•œ íŠœë‹ì´ ê°€ëŠ¥í•¨. Related Work Regularization bias: > However, the fact that both $\\hat{\\beta}^{(0)}$ and $\\hat{\\beta}^{(1)}$ are regularized towards 0 separately may inadvertently regularize the treatment effect estimate $\\hat{\\beta}^{(1)} - \\hat{\\beta}^{(0)}$ away from 0, even when $\\tau^(x) = 0$ everywhere CATE(ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜íš¨ê³¼)ëŠ” $\\tau^(x) = \\mu^{(1)}(x) - \\mu^{(0)}(x)$ë¡œ ì“¸ ìˆ˜ ìˆìŒ. í”íˆ $\\mu^{(1)}(x)$ì™€ $\\mu^{(0)}(x)$ë¥¼ ê°ê° ë”°ë¡œ ì¶”ì •í•œ ë’¤, ê·¸ ì°¨ì´ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ êµ¬í•˜ëŠ”ë°, ì´ë•Œ ë‘ í•¨ìˆ˜ë¥¼ ë³„ë„ë¡œ ì •ê·œí™”(regularization)í•˜ë©´ regularization biasê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ. ì˜ˆë¥¼ ë“¤ì–´, ë¼ì˜(lasso) íšŒê·€ë¥¼ ê°ê°ì˜ ì§‘ë‹¨(ì²˜ì¹˜/ë¹„ì²˜ì¹˜)ì— ë”°ë¡œ ì ìš©í•˜ë©´, ë‘ ì¶”ì •ì¹˜ ëª¨ë‘ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡ ì •ê·œí™”ë˜ì–´ ì‹¤ì œë¡œëŠ” ì²˜ì¹˜íš¨ê³¼ê°€ ì—†ì„ ë•Œë„ $\\hat{\\tau}(x) = \\hat{\\mu}^{(1)}(x) - \\hat{\\mu}^{(0)}(x)$ê°€ 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆìŒ. íŠ¹íˆ, ì²˜ì¹˜êµ°ê³¼ ëŒ€ì¡°êµ°ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ë‹¤ë¥¼ ë•Œ ì´ í˜„ìƒì´ ë” ì‹¬í•´ì§. $$ \\begin{align} \\hat{\\beta}^{(w)} = \\arg\\min_{\\beta^{(w)}} \\left\\{ \\sum_{i:W_i=w} \\left( Y_i - X_i^\\top \\beta^{(w)} \\right)^2 + \\lambda^{(w)} \\ \\beta^{(w)}\\ _1 \\right\\} \\tag{5} \\end{align} $$ ê°„ë‹¨í•˜ê²Œ ëŒ€ì¡°êµ°ê³¼ í†µì œêµ°ì„ ë”°ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ê²°ê³¼ë¥¼ ë¹„êµí•´ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì–»ìœ¼ë©´ ë˜ëŠ”ê±° ì•„ë‹ˆì•¼? ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ regularization biasë¼ê³  í•˜ëŠ”, ê³¼í•˜ê²Œ ì •ê·œí™”ë˜ì–´ ì²˜ì¹˜íš¨ê³¼ê°€ 0ì¼ ë•Œë„ 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤. Regularization Biasë¥¼ í”¼í•˜ëŠ” ìµœê·¼ ë°©ë²•ë“¤: ìµœê·¼ ì—°êµ¬ë“¤ì€ regularization bias ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ êµ¬ì¡°ì (machine learning êµ¬ì¡° ìì²´ì˜) ê°œì„  ë°©ë²•ì„ ì œì•ˆí•¨. ì˜ˆë¥¼ ë“¤ì–´, Imai & Ratkovic (2013)ì€ ì‹ (5)ì²˜ëŸ¼ ì²˜ì¹˜êµ°/ëŒ€ì¡°êµ°ì„ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , ì•„ë˜ì™€ ê°™ì´ í•˜ë‚˜ì˜ ë¼ì˜(lasso) íšŒê·€ë¡œ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨: $$ \\begin{align} \\hat{b}, \\hat{\\delta} = \\arg\\min_{b, \\delta} \\left\\{ \\sum_{i=1}^n \\left( Y_i - X_i^\\top b + (W_i - 0.5) X_i^\\top \\delta \\right)^2 + \\lambda_b \\ b\\ _1 + \\lambda_\\delta \\ \\delta\\ _1 \\right\\} \\tag{6} \\end{align} $$ ì—¬ê¸°ì„œ ìµœì¢… ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì€ $\\hat{\\tau}(x) = x^\\top \\hat{\\delta}$ë¡œ ê³„ì‚°í•¨. ì´ ë°©ë²•ì€ $\\delta$ì—ë§Œ í¬ì†Œì„±(sparsity)ì„ ê°•ì œí•˜ì—¬, ì²˜ì¹˜íš¨ê³¼ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ë” ì˜ ë°˜ì˜í•  ìˆ˜ ìˆìŒ. ì´ì™¸ì—ë„ ì‹ ê²½ë§ ë“± ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì—ì„œ ì²˜ì¹˜íš¨ê³¼ ì´ì§ˆì„±(heterogeneity)ì„ ì˜ ì¶”ì •í•˜ë„ë¡ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ê³  ìˆìŒ(ì˜ˆ: Shalit et al., 2017). Loss Function(ì†ì‹¤í•¨ìˆ˜) ë³€ê²½ì„ í†µí•œ ì ‘ê·¼: > Here, instead of trying to modify the algorithms underlying different machine learning tools to improve their performance as treatment effect estimators, we focus on modifying the loss function used to training generic machine learning methods. ìµœê·¼ ì—°êµ¬ë“¤ì€ ëª¨ë¸ êµ¬ì¡° ìì²´ë¥¼ ë°”ê¾¸ëŠ” ëŒ€ì‹ , ì†ì‹¤í•¨ìˆ˜(R-loss)ë¥¼ ë°”ê¿”ì„œ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•ì— ì£¼ëª©í•˜ê³  ìˆìŒ. ì´ ì ‘ê·¼ì€ van der Laanê³¼ Dudoit(2003) ë“±ì—ì„œ ì‹œì‘ëœ ì—°êµ¬ íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ë“¤ì€ doubly robust objective(ì´ì¤‘ ê°•ê±´ ëª©ì í•¨ìˆ˜)ì— ëŒ€í•œ êµì°¨ê²€ì¦ì„ í†µí•´ ìµœì ì˜ í†µê³„ì  ê·œì¹™ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨. Luedtke & van der Laan(2016) ë“±ì€ ì´ëŸ¬í•œ ëª©ì í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê°œë³„í™” ì²˜ì¹˜ê·œì¹™(individualized treatment rules)ì´ë‚˜ ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼(heterogeneous treatment effects)ë¥¼ í•™ìŠµí•˜ëŠ” ë‹¤ì–‘í•œ ìœ íš¨í•œ ëª©ì í•¨ìˆ˜(oracle loss ë“±)ì˜ ì„±ì§ˆì„ ë¶„ì„í•¨. ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬ëŠ”, R-lossë¥¼ í™œìš©í•´ ë²”ìš© ê¸°ê³„í•™ìŠµ(generic machine learning)ìœ¼ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³ , ì»¤ë„ íë²„íŠ¸ ê³µê°„ì—ì„œì˜ ì •ê·œí™” íšŒê·€ ë“± ë„ë¦¬ ì“°ì´ëŠ” ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì— ëŒ€í•´ ê°•í•œ ì´ë¡ ì  ì˜¤ì°¨ ê²½ê³„(ìˆ˜ë ´ë¥  ë³´ì¥)ë¥¼ ì œê³µí•œ ê²ƒì„. meta-learning ë°©ë²•ë“¤: meta learner ê´€ë ¨ ì„¤ëª…ìë£Œ X-learner (KÃ¼nzel et al., 2019): ë¨¼ì € $\\hat{\\mu}^{(w)}(x)$ë¥¼ ë¹„ëª¨ìˆ˜ íšŒê·€ ë°©ë²•ìœ¼ë¡œ ì¶”ì • ì²˜ì¹˜ ê´€ì°°ê°’ì— ëŒ€í•´ pseudo-effects $D_i = Y_i - \\hat{\\mu}^{(-i)}{(0)}(X_i)$ë¥¼ ì •ì˜í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ $\\hat{\\tau}^{(1)}(X_i)$ë¥¼ ë¹„ëª¨ìˆ˜ íšŒê·€ë¡œ ì¶”ì • ëŒ€ì¡°êµ°ì— ëŒ€í•´ì„œë„ ìœ ì‚¬í•˜ê²Œ $\\hat{\\tau}^{(0)}(X_i)$ë¥¼ êµ¬í•˜ê³ , ë‘ ì¶”ì •ê¸°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê²°í•©: $$ \\hat{\\tau}(x) = \\{1 - \\hat{e}(x)\\} \\hat{\\tau}^{(1)}(x) + \\hat{e}(x) \\hat{\\tau}^{(0)}(x) \\tag{7} $$ U-learner (KÃ¼nzel et al., 2019): $U_i = \\frac{Y_i - m^(X_i)}{W_i - e^(X_i)}$ì— ëŒ€í•´ $\\mathbb{E}[U_i \\mid X_i = x] = \\tau(x)$ì„ì„ ì´ìš© $U_i$ë¥¼ $X_i$ì— ëŒ€í•´ ë²”ìš© ê¸°ê³„í•™ìŠµ ë°©ë²•ìœ¼ë¡œ íšŒê·€ Propensity Score ê°€ì¤‘ ë°©ë²•ë“¤: Athey & Imbens (2016), Tian et al. (2014) ë“±ì´ ì œì•ˆ ê²°ê³¼ë‚˜ ê³µë³€ëŸ‰ì„ propensity scoreë¡œ ê°€ì¤‘í•˜ì—¬ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • ì˜ˆ: $Y_i\\{W_i - e^(X_i)\\}/\\{e^(X_i)(1-e^(X_i))\\}$ë¥¼ $X_i$ì— ëŒ€í•´ íšŒê·€ ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬: R-learner ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë‹¤ì–‘í•œ ì„¤ì •ì—ì„œ baselineë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ê°œì„ ì„ ì œê³µ Quasi-oracle ì˜¤ì°¨ ê²½ê³„ë¥¼ ì œê³µí•˜ì—¬ $\\hat{\\tau}$ì˜ ì˜¤ì°¨ê°€ $\\hat{e}$ë‚˜ $\\hat{m}$ì˜ ì˜¤ì°¨ë³´ë‹¤ ë¹ ë¥´ê²Œ ê°ì†Œí•  ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¥ ê´€ë ¨ ì—°êµ¬ì™€ ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì : ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ ì—°êµ¬: Zhao, Small, and Ertefaie (2017): Robinson ë³€í™˜ê³¼ ë¼ì˜(lasso)ë¥¼ ê²°í•©í•´ ê³ ì°¨ì› ì„ í˜•ëª¨í˜•ì—ì„œ íš¨ê³¼ ìˆ˜ì •(effect modification)ì— ëŒ€í•œ ìœ íš¨í•œ ì‚¬í›„ì„ íƒ ì¶”ë¡ (post-selection inference)ì„ ì œê³µ í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ë§¥ë½ì—ì„œ ì†ì‹¤í•¨ìˆ˜ë¡œ Robinson ë³€í™˜ì„ í™œìš©í•œ ê²ƒì€ ë³¸ ë…¼ë¬¸ì´ ì²˜ìŒ ì´ë¡ ì  ê¸°ë°˜: ë³¸ ë…¼ë¬¸ì˜ ì´ë¡ ì  ê²°ê³¼ëŠ” Robinson(1988) ë“±ì—ì„œ ë°œì „ëœ ì¤€ëª¨ìˆ˜ì  íš¨ìœ¨ì„±(semiparametric efficiency)ê³¼ ì§êµ ëª¨ë©˜íŠ¸(orthogonal moments) ì´ë¡ ì— ê¸°ë°˜ ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œëŠ” Targeted Maximum Likelihood Estimation(TMIE)ì™€ ìœ ì‚¬: nuisance componentë¥¼ ë¹„ëª¨ìˆ˜ì ìœ¼ë¡œ ì¶”ì • ì´ë¥¼ í™œìš©í•´ likelihood(ë˜ëŠ” ì†ì‹¤í•¨ìˆ˜)ë¥¼ ìµœì í™” Cross-fitting(í™€ë“œì•„ì›ƒ ì˜ˆì¸¡)ì€ ìµœê·¼ ì¤€ëª¨ìˆ˜ì  ì¶”ì •ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ë°©ë²• ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì : ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì£¼ë¡œ ë‹¨ì¼(ë˜ëŠ” ì €ì°¨ì›) íŒŒë¼ë¯¸í„° ì¶”ì •ì— ì´ˆì  ë³¸ ë…¼ë¬¸ì€ ë³µì¡í•œ í•¨ìˆ˜ì  ê°ì²´(ì¦‰, $\\tau^(\\cdot)$ ì „ì²´ í•¨ìˆ˜)ë¥¼ ì¶”ì •í•˜ëŠ” ë° ì´ˆì  ìµœì  ì²˜ì¹˜ í• ë‹¹ ê·œì¹™(optimal treatment allocation rule) ì¶”ì •ê³¼ë„ ê´€ë ¨ ìˆì§€ë§Œ, ëª©ì í•¨ìˆ˜(ì†ì‹¤)ê°€ ë‹¤ë¦„ ì¶”ê°€ ë…¼ì˜: ë³¸ ë…¼ë¬¸ì€ ëª¨ì§‘ë‹¨ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¶œëœ ìƒ˜í”Œì„ ê°€ì • ì—„ê²©í•œ ë¬´ì‘ìœ„í™” ì¶”ë¡ (randomization inference) í•˜ì—ì„œì˜ ë¹„ëª¨ìˆ˜ì  ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë„ í¥ë¯¸ë¡œìš´ ì£¼ì œì„ The R-Learner in Action 4.1 Application to a Voting Study íˆ¬í‘œì— ê´€í•œ ì—°êµ¬ì— ì´ë¥¼ ì ìš©í•´ë³¸ ì‚¬ë¡€ë¥¼ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—°êµ¬ ë°°ê²½ ì›ë³¸ ì—°êµ¬: Arceneaux, Gerber, and Green (2006) ì—°êµ¬ ì£¼ì œ: ìœ ë£Œ íˆ¬í‘œ ë…ë ¤ ì „í™”ê°€ íˆ¬í‘œìœ¨ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼ ì‹¤ì œ ê²°ê³¼: íˆ¬í‘œ ë…ë ¤ ì „í™”ì˜ íš¨ê³¼ê°€ ê±°ì˜ ì—†ìŒ (1% ë¯¸ë§Œ) ì‹¤í—˜ ì„¤ê³„ ë°ì´í„° êµ¬ì„± ì „ì²´ ìƒ˜í”Œ: 1,895,468ê°œ ê´€ì¸¡ì¹˜ ì²˜ì¹˜ ê·¸ë£¹:59,264ê°œ (ì „í™” ë°›ì€ ì‚¬ëŒ) ë¶„ì„ ìƒ˜í”Œ: 148160ê°œ (ëª¨ë“  ì²˜ì¹˜ ê·¸ë£¹ + ëœë¤ ëŒ€ì¡° ê·¸ë£¹) ì²˜ì¹˜ ë¹„ìœ¨: 2/5 (40 ë°ì´í„° ë¶„í•  í›ˆë ¨ ì„¸íŠ¸: 1000ê°œ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸: 25,0ê°œ í™€ë“œì•„ì›ƒ ì„¸íŠ¸: ë‚˜ë¨¸ì§€ ì¸ê³µì  ì²˜ì¹˜íš¨ê³¼ ì¶”ê°€ ì‹¤ì œë¡œëŠ” ì²˜ì¹˜íš¨ê³¼ê°€ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ, ì¸ê³µì ìœ¼ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ê°€: $$ \\tau^(X_i) = -\\frac{\\text{VOTE00}_i}{2 + \\frac{100}{\\text{AGE}_i}} $$ VOTE00_i: 200íˆ¬í‘œ ì—¬ë¶€ AGE_i: ë‚˜ì´ ëª©ì : ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•œ ì˜ë¯¸ìˆëŠ” ì‘ì—… ìƒì„± R-learner ì ìš© ê³¼ì • Step 1: Nuisance Components ì¶”ì • ë¶€ìŠ¤íŒ…ê³¼ ë¼ì˜ ë‘ ë°©ë²•ìœ¼ë¡œ $\\hat{e}(\\cdot)$ì™€ $\\hat{m}(\\cdot)$ ì¶”ì • êµì°¨ê²€ì¦ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ ê²°ê³¼: ë¶€ìŠ¤íŒ…ì´ ë‘ nuisance components ëª¨ë‘ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥ Step2 R-loss ìµœì í™” ë¼ì˜: R-loss = 00.1816 (í›ˆë ¨), 0.1781 (í™€ë“œì•„ì›ƒ) ë¶€ìŠ¤íŒ…: R-loss = 00.1818 (í›ˆë ¨), 0.1783í™€ë“œì•„ì›ƒ) ìµœì¢… ì„ íƒ: ë¼ì˜ ê¸°ë°˜ $\\hat{\\tau}(\\cdot)$ ì„±ëŠ¥ ë¹„êµ Oracle Test Set MSE (Mean Squared Error) ë°©ë²• MSE ------ ----- R-learner (ë¼ì˜) 00.47 Ã—10â»Â³ R-learner (ë¶€ìŠ¤íŒ…) 1.2310 Single Lasso 00.61 Ã—10â»Â³ BART 40.05 Ã—10â»Â³ ì„±ëŠ¥ ë¶„ì„ R-learner (ë¼ì˜)ê°€ ìµœê³  ì„±ëŠ¥2. ë¼ì˜ vs ë¶€ìŠ¤íŒ…: ë¼ì˜: í¸í–¥ ìˆì§€ë§Œ ë¶„ì‚° ë‚®ìŒ ë¶€ìŠ¤íŒ…: ë…¸ì´ì¦ˆ ë§ì§€ë§Œ í¸í–¥ ì ìŒ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„ì—ì„œ ë¼ì˜ê°€ ìœ ë¦¬ í•µì‹¬ ë°œê²¬ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì˜ ìš°ìˆ˜ì„± Nuisance components: ë¹„ëª¨ìˆ˜ì  ë°©ë²• (ë¶€ìŠ¤íŒ…) ì‚¬ìš© ì²˜ì¹˜íš¨ê³¼ ì¶”ì •: ê°„ë‹¨í•œ ë°©ë²• (ë¼ì˜) ì‚¬ìš© ì´ìœ : ê° ë‹¨ê³„ì— ìµœì í™”ëœ ë°©ë²• ì„ íƒ ê¸°ì¡´ ë°©ë²•ë“¤ì˜ í•œê³„ Single Lasso: ëª¨ë“  ê³³ì—ì„œ ì„ í˜• ëª¨ë¸ë§ â†’ ëª¨ë¸ ì˜¤ì§€ì • ê°€ëŠ¥ì„± BART: ëª¨ë“  ê³³ì—ì„œ ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ë§ â†’ ë¶ˆì•ˆì •í•œ $\\tau(\\cdot)$ ì¶”ì • R-learnerì˜ ì¥ì  ìœ ì—°ì„±: ê° ë‹¨ê³„ì— ì í•©í•œ ë°©ë²• ì„ íƒ ê°€ëŠ¥ ì•ˆì •ì„±: êµì°¨ê²€ì¦ì„ í†µí•œ ì•ˆì •ì ì¸ ëª¨ë¸ ì„ íƒ ì„±ëŠ¥: ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì¶”ì • ì •í™•ë„ ì‹¤ìš©ì  êµí›ˆ ë‹¨ê³„ë³„ ìµœì í™”: ê° ë‹¨ê³„ì— ê°€ì¥ ì í•©í•œ ë°©ë²• ì„ íƒ êµì°¨ê²€ì¦ì˜ ì¤‘ìš”ì„±: ê³¼ì í•© ë°©ì§€ì™€ ëª¨ë¸ ì„ íƒ í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„: ë°ì´í„° í¬ê¸°ì™€ ë³µì¡ì„± ê³ ë ¤ ì‹¤ì œ ë°ì´í„° ì ìš©: ì´ë¡ ì  ë°©ë²•ì˜ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦ 4.2 Model Averaging with the R-Learner ì—¬ëŸ¬ ëª¨ë¸ì„ í•©ì¹˜ëŠ” stacking ë°©ë²•ì„ ì ìš©í•œ ì‹œë„ë¥¼ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë°°ê²½ê³¼ ë™ê¸° ì•ì„  ì„¹ì…˜ì—ì„œëŠ” R-learnerì˜ ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ì¶”ì • ì „ëµì„ ì„¸ì‹¬í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë¶„ì„ì˜ ì‹œì‘ì ìœ¼ë¡œ ê¸°ì„±í’ˆ(off-the-shelf) ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì´ ì„¹ì…˜ì—ì„œëŠ” stackingì˜ ë³€í˜•ì„ í†µí•´ í•©ì˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ êµ¬ì¶•í•˜ëŠ” R-learning ì ‘ê·¼ë²•ì„ ë…¼ì˜í•©ë‹ˆë‹¤. Stacking ê¸°ë°˜ í•©ì˜ ì¶”ì • ê¸°ë³¸ ì„¤ì • $k = 1, \\ldots, K$ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸° $\\hat{\\tau}_k$ ë³´ìœ  í›ˆë ¨ ì„¸íŠ¸ì—ì„œ out-of-fold ì¶”ì •ì¹˜ $\\hat{\\tau}_k^{(-i)}(X_i)$ ì ‘ê·¼ ê°€ëŠ¥ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” out-of-fold ì¶”ì •ì¹˜ $\\hat{e}^{(-i)}(X_i)$ì™€ $\\hat{m}^{(-i)}(X_i)$ ë³´ìœ  í•©ì˜ ì¶”ì • ë°©ë²• R-lossì— ë”°ë¼ $\\hat{\\tau}_k(\\cdot)$ì˜ ìµœì  ì–‘ì˜ ì„ í˜• ê²°í•©ì„ ì·¨í•˜ì—¬ í•©ì˜ ì¶”ì • $\\hat{\\tau}(\\cdot)$ë¥¼ êµ¬ì¶•: $$ \\hat{\\tau}(x) = \\hat{c} + \\sum_{k=1}^K \\alpha_k \\hat{\\tau}_k(x) \\tag{8a} $$ $$ (\\hat{b}, \\hat{c}, \\hat{\\alpha}) = \\arg\\min_{b, c, \\alpha} \\left\\{ \\sum_{i=1}^n \\left[ \\left(Y_i - \\hat{m}^{(-i)}(X_i)\\right) - b - \\left(c + \\sum_{k=1}^K \\alpha_k \\hat{\\tau}_k^{(-i)}(X_i)\\right) \\left(W_i - \\hat{e}^{(-i)}(X_i)\\right) \\right]^2 : \\alpha \\geq 0 \\right\\} \\tag{8b} $$ ìˆ˜ì‹ì˜ êµ¬ì„± ìš”ì†Œ $\\hat{c}$: ìƒìˆ˜ ì²˜ì¹˜íš¨ê³¼ í•­ (ìœ ì—°ì„±ì„ ìœ„í•´ ììœ ë¡­ê²Œ ì¡°ì •) $\\hat{b}$: ì ˆí¸ ($\\hat{m}$ì˜ ì ì¬ì  í¸í–¥ í¡ìˆ˜) $\\alpha_k$: ê° ì¶”ì •ê¸°ì˜ ê°€ì¤‘ì¹˜ (ë¹„ìŒ ì œì•½) ì‹¤í—˜ ì„¤ì • ë°ì´í„° ìƒì„± ê³¼ì • ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„° ìƒì„± ë¶„í¬ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤: $$ X_i \\sim N(0, I_{d \\times d}), \\quad W_i \\sim \\mathrm{Bernoulli}(0.5) \\tag{9a} $$ $$ Y_i \\mid X_i, W_i \\sim N\\left(\\frac{3}{1 + e^{X_{i3} - X_{i2}}} + (W_i - 0.5) \\tau^(X_i), \\sigma^2\\right) \\tag{9b} $$ ì‹¤í—˜ ì¡°ê±´: ìƒ˜í”Œ í¬ê¸°: $n = 10,000$ (ë¬´ì‘ìœ„ ì—°êµ¬ ì„¤ê³„) ì°¨ì›: $d = 10$ ë‹¤ì–‘í•œ $\\tau^(\\cdot)$ì™€ $\\sigma$ ì„ íƒ ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ ë‘ ê°€ì§€ ìœ í˜•ì˜ ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ë¥¼ ê³ ë ¤: ì—°ì† í•¨ìˆ˜: $\\tau^(X_i) = \\frac{1}{1 + e^{X_{i1} - X_{i2}}}$ ë¶ˆì—°ì† í•¨ìˆ˜: $\\tau^(X_i) = \\frac{\\mathbf{1}\\{X_{i1} > 0\\}}{1 + e^{-X_{i2}}}$ ë¹„êµ ë°©ë²• ì¶”ì • ë°©ë²•ë“¤: BART (Bayesian Additive Regression Trees) Causal Forests (ì¸ê³¼ê´€ê³„ ìˆ²) Stacking: ì‹ (8)ì„ ì´ìš©í•œ ë‘ ë°©ë²•ì˜ ê²°í•© ì‹¤í—˜ ì¡°ê±´: ë°ì´í„°ê°€ ë¬´ì‘ìœ„í™”ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ìê°€ ì•Œê³  ìˆë‹¤ê³  ê°€ì • í•„ìš”í•œ ê³³ì—ì„œëŠ” $\\hat{e}(x) = 0.5$ ì‚¬ìš© Stackingìš© $\\hat{m}(\\cdot)$ëŠ” random forestë¡œ ì¶”ì • ì‹¤í—˜ ê²°ê³¼ (Figure 2) ì—°ì† ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ì˜ ê²½ìš° BART: Causal Forestsë³´ë‹¤ ì•½ê°„ ìš°ìˆ˜í•œ ì„±ëŠ¥ Stacking: ë…¸ì´ì¦ˆ ìˆ˜ì¤€ $\\sigma$ê°€ ë§¤ìš° í´ ë•Œê¹Œì§€ ê°œë³„ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜ ê³ ë…¸ì´ì¦ˆ í™˜ê²½: ëª¨ë“  ë°©ë²•ì´ ìƒìˆ˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€ ë¶ˆì—°ì† ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ì˜ ê²½ìš° Causal Forests: ë‚®ì€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì—ì„œ íŠ¹íˆ ìœ ë¦¬ Stacking: ë” ì •í™•í•œ ê¸°ë³¸ í•™ìŠµìì˜ ì„±ëŠ¥ì„ ìë™ìœ¼ë¡œ ë§¤ì¹­ í•µì‹¬ ë°œê²¬ Stackingì˜ ìë™ ì ì‘ì„± ê° ì„¤ì •ì—ì„œ ë” ë‚˜ì€ ê¸°ë³¸ í•™ìŠµìë¥¼ ìë™ìœ¼ë¡œ ì‹ë³„ ìˆ˜ë™ ì„ íƒ ì—†ì´ë„ ìµœì  ì„±ëŠ¥ì— ê·¼ì ‘ ë°©ë²•ë¡ ì  ìœ ì—°ì„± ê¸°ì¡´ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë“¤ì„ ê·¸ëŒ€ë¡œ í™œìš© ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì • ì—†ì´ ì„±ëŠ¥ ê°œì„  ì‹¤ìš©ì  ì¥ì  ìë™í™”: ë³µì¡í•œ ëª¨ë¸ ì„ íƒ ê³¼ì • ë‹¨ìˆœí™” ê²¬ê³ ì„±: ë‹¤ì–‘í•œ ë°ì´í„° íŠ¹ì„±ì— ëŒ€í•œ ì ì‘ë ¥ íš¨ìœ¨ì„±: ê¸°ì¡´ ë„êµ¬ë“¤ì˜ ê°•ì ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•© A Quasi-Oracle Error Bound ì´ ì„¹ì…˜ì—ì„œëŠ” R-learnerì˜ í•µì‹¬ ì´ë¡ ì  ê¸°ì—¬ì¸ quasi-oracle propertyì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. 5.1 Oracleê³¼ Quasi-Oracleì˜ ê°œë… Oracle Estimator ì´ìƒì ì¸ oracleì€ ë‹¤ìŒì„ ë¯¸ë¦¬ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤: True marginal effect: $m^(x) = \\mathbb{E}[Y \\mid X = x]$ True propensity score: $e^(x) = \\Pr(W = 1 \\mid X = x)$ Oracleì´ ì´ ì •ë³´ë¥¼ ì•Œê³  ìˆë‹¤ë©´, ë‹¤ìŒ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì§ì ‘ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$ \\mathcal{L}_n^{\\text{oracle}}[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\} \\tau(X_i) \\right]^2 $$ R-learner (Quasi-Oracle) ì‹¤ì œë¡œëŠ” $m^(x)$ì™€ $e^(x)$ë¥¼ ëª¨ë¥´ë¯€ë¡œ, ì¶”ì •ê°’ $\\hat{m}(\\cdot)$ê³¼ $\\hat{e}(\\cdot)$ì„ ì‚¬ìš©: $$ \\hat{\\mathcal{L}}_n[\\tau(\\cdot)] = \\frac{1}{n} \\sum_{i=1}^n \\left[ \\{Y_i - \\hat{m}^{(-q(i))}(X_i)\\} - \\{W_i - \\hat{e}^{(-q(i))}(X_i)\\} \\tau(X_i) \\right]^2 $$ 5.2 Main Theoretical Result ë…¼ë¬¸ì˜ í•µì‹¬ ì´ë¡ ì  ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: Theorem (Informal) ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ, R-learner $\\hat{\\tau}$ëŠ” ë‹¤ìŒ ì„±ì§ˆì„ ë§Œì¡±í•©ë‹ˆë‹¤: $$ \\mathbb{E}\\left[\\ \\hat{\\tau} - \\tau^\\ ^2\\right] \\lesssim \\text{Oracle Rate} + \\text{Nuisance Rate} $$ ì—¬ê¸°ì„œ: Oracle Rate: oracleì´ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìµœì  ìˆ˜ë ´ë¥  Nuisance Rate: nuisance components ì¶”ì • ì˜¤ì°¨ì˜ ê³± Quasi-Oracle Property ë§Œì•½ nuisance componentsì˜ ì¶”ì •ì´ ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•œë‹¤ë©´: $$ \\text{Nuisance Rate} = \\ \\hat{m} - m^\\ \\cdot \\ \\hat{e} - e^\\ \\to 0 $$ ì´ ê²½ìš° R-learnerëŠ” oracle rateë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤: $$ \\mathbb{E}\\left[\\ \\hat{\\tau} - \\tau^\\ ^2\\right] \\lesssim \\text{Oracle Rate} $$ 5.3 Why Does This Work? Cross-fittingì˜ ì—­í•  Overfitting ë°©ì§€: ê°™ì€ ë°ì´í„°ë¡œ ì¶”ì •ê³¼ ì˜ˆì¸¡ì„ í•˜ì§€ ì•ŠìŒ Bias ê°ì†Œ: $\\mathbb{E}[\\hat{m}^{(-i)}(X_i) - m^(X_i)] \\approx 0$ Doubly Robust Structure R-lossëŠ” doubly robust êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤: $$ \\mathbb{E}\\left[\\left\\{Y_i - m^(X_i)\\right\\} - \\left\\{W_i - e^(X_i)\\right\\} \\tau^(X_i) \\mid X_i\\right] = 0 $$ ì´ëŠ” ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤: $m^(x)$ ë˜ëŠ” $e^(x)$ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ í¸í–¥ì´ ì—†ìŒ ë‘ ì¶”ì •ì˜ ì˜¤ì°¨ê°€ ê³±ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¨ Product Form of Error ìµœì¢… ì˜¤ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´ë©ë‹ˆë‹¤: $$ \\text{Total Error} \\approx \\text{Oracle Error} + \\ \\hat{m} - m^\\ \\times \\ \\hat{e} - e^\\ $$ ë”°ë¼ì„œ ë‘ nuisance componentsê°€ ëª¨ë‘ $o(1)$ ì†ë„ë¡œ ìˆ˜ë ´í•˜ë©´, ê³±ì€ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤. 5.4 Practical Implications Flexible Method Choice ê° ë‹¨ê³„ì—ì„œ ì„œë¡œ ë‹¤ë¥¸ ê¸°ê³„í•™ìŠµ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥ $\\hat{m}$ì—ëŠ” random forests, $\\hat{e}$ì—ëŠ” logistic regression ë“± Robustness í•œ nuisance componentê°€ ë¶€ì •í™•í•´ë„ ë‹¤ë¥¸ ê²ƒì´ ë³´ì™„ ì™„ë²½í•œ ì¶”ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ Rate Optimality ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ minimax optimal ë‹¬ì„± ì´ë¡ ì ìœ¼ë¡œ ìµœì„ ì˜ ì„±ëŠ¥ ë³´ì¥ ë…¼ë¬¸ì—ì„œ ê¶ê¸ˆí•œ ì ë“¤ ì •ë¦¬ ê·¸ë˜ì„œ quasi-oracle propertyëŠ” ì–´ë–»ê²Œ ë³´ì¥ë˜ëŠ”ê±´ë°? í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜ Robinson's Transformationì˜ ë§ˆë²• Robinson's transformation (1988)ì´ í•µì‹¬ì…ë‹ˆë‹¤: $$ Y_i - m^(X_i) = \\{W_i - e^(X_i)\\} \\tau^(X_i) + \\varepsilon_i $$ ì´ ë³€í™˜ì˜ íŠ¹ë³„í•œ ì„±ì§ˆ: Orthogonality: $\\mathbb{E}[\\varepsilon_i \\mid X_i, W_i] = 0$ Causal Isolation: ì²˜ì¹˜íš¨ê³¼ë§Œ ë¶„ë¦¬ë¨ Cross-fittingì˜ í•µì‹¬ ì—­í•  `` Step 1: ë°ì´í„°ë¥¼ Kê°œ foldë¡œ ë¶„í•  Step 2: kë²ˆì§¸ foldë¥¼ ì œì™¸í•˜ê³  mÌ‚, Ãª ì¶”ì • Step 3: kë²ˆì§¸ foldì—ì„œ Ï„Ì‚ ì¶”ì • Step 4: ëª¨ë“  foldì— ëŒ€í•´ ë°˜ë³µ ` ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€? Independence: ì¶”ì •ì— ì‚¬ìš©ëœ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„°ê°€ ë…ë¦½ Bias Reduction: $\\mathbb{E}[\\hat{m}^{(-i)}(X_i) \\mid X_i] \\approx m^(X_i)$ Doubly Robustì˜ ìˆ˜í•™ì  êµ¬ì¡° í•µì‹¬ì€ ë‹¤ìŒ decompositionì…ë‹ˆë‹¤: $$ \\hat{\\mathcal{L}}_n - \\mathcal{L}_n^{\\text{oracle}} = \\text{Bias Term} + \\text{Variance Term} $$ Bias Term: $$ \\frac{2}{n} \\sum_{i=1}^n \\left[\\{Y_i - m^(X_i)\\} - \\{W_i - e^(X_i)\\}\\tau^(X_i)\\right] \\times \\left[\\{\\hat{m}^{(-i)} - m^\\}(X_i) - \\{\\hat{e}^{(-i)} - e^\\}(X_i)\\tau^(X_i)\\right] $$ Cross-fittingì— ì˜í•´ ì´ termì˜ ê¸°ëŒ“ê°’ì´ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤. Variance Term: $$ \\left\\ \\{\\hat{m} - m^\\} - \\{\\hat{e} - e^\\}\\tau^\\right\\ ^2 $$ ì´ëŠ” $\\ \\hat{m} - m^\\ \\times \\ \\hat{e} - e^\\ $ì˜ orderì…ë‹ˆë‹¤. ìˆ˜ë ´ë¥ ì˜ ê³±ì…ˆ êµ¬ì¡° ë§Œì•½: $\\ \\hat{m} - m^\\ = O_p(r_m)$ $\\ \\hat{e} - e^\\ = O_p(r_e)$ ê·¸ëŸ¬ë©´: $$ \\ \\hat{\\tau} - \\tau^\\ = O_p(\\max\\{r_{\\text{oracle}}, r_m \\times r_e\\}) $$ ì˜ˆì‹œ: $r_m = r_e = n^{-1/4}$ (ëŠë¦° ìˆ˜ë ´) $r_m \\times r_e = n^{-1/2}$ (ë¹ ë¥¸ ìˆ˜ë ´!) Oracle rate $r_{\\text{oracle}} = n^{-1/3}$ì´ë©´ ìµœì¢… rate = $\\max\\{n^{-1/3}, n^{-1/2}\\} = n^{-1/3}$ âœ¨ ì‹¤ì œ ë™ì‘ ì˜ˆì‹œ ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ë²½í•œ Oracle `python Oracle knows true m(x) and e(x) oracle_loss = R_loss(Y, W, X, tau, m_true, e_true) oracle_rate = n^(-1/3) # optimal rate for Ï„ complexity ` ì‹œë‚˜ë¦¬ì˜¤ 2: R-learner with Imperfect Estimates `python Step 1: Estimate nuisances with cross-fitting m_hat = estimate_m_with_crossfit(Y, X) # rate: n^(-1/4) e_hat = estimate_e_with_crossfit(W, X) # rate: n^(-1/4) Step 2: Optimize R-loss tau_hat = optimize_R_loss(Y, W, X, m_hat, e_hat) Achieved rate: max(n^(-1/3), n^(-1/4) Ã— n^(-1/4)) = n^(-1/3) Same as oracle! ğŸ‰ `` ì™œ ì´ê²Œ \"Quasi-Oracle\"ì¸ê°€? Oracleê³¼ ê°™ì€ ìˆ˜ë ´ë¥ : $n^{-1/3}$ ë‹¬ì„± í•˜ì§€ë§Œ Oracleì´ ì•„ë‹˜: $m^, e^*$ë¥¼ ì‹¤ì œë¡œëŠ” ëª¨ë¦„ Automatic: ì¶”ê°€ì ì¸ íŠœë‹ ì—†ì´ ìë™ìœ¼ë¡œ ë‹¬ì„± Robust: nuisance ì¶”ì •ì´ ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ì‘ë™ ì´ê²ƒì´ ë°”ë¡œ \"quasi-oracle property\"ì˜ í•µì‹¬ì…ë‹ˆë‹¤! ğŸ¯"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/PropensityScoreNotebook",
    "title": "Propensity Score Notebook",
    "date": "2025-07-17",
    "excerpt": "Propensity Score Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "--- title: \"Propensity Score Notebook\" date: \"2025-07-17\" excerpt: \"Propensity Score Notebook\" category: \"Causal Inference\" tags: [\"Causal Inference\"] --- Propensity Score # The Psychology of Growth ì¹˜ë£Œ(treatment)ì™€ ê²°ê³¼(outcome) ë³€ìˆ˜ ì™¸ì—ë„, ì´ ì—°êµ¬ì—ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë‹¤ë¥¸ íŠ¹ì„±ë“¤ë„ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤: schoolid: í•™ìƒì˜ í•™êµ ì‹ë³„ì; success_expect: ë¯¸ë˜ ì„±ê³µì— ëŒ€í•œ ìê°€ ë³´ê³  ê¸°ëŒ€ì¹˜, ì‚¬ì „ ì„±ì·¨ë„ì˜ ëŒ€ìš© ì§€í‘œë¡œ, ë¬´ì‘ìœ„ ë°°ì • ì „ì— ì¸¡ì •ë¨; gender: í•™ìƒì´ ì‹ë³„í•œ ì„±ë³„ì˜ ë²”ì£¼í˜• ë³€ìˆ˜; frst_in_family: í•™ìƒì˜ 1ì„¸ëŒ€ ëŒ€í•™ìƒ ì§€ìœ„ì— ëŒ€í•œ ë²”ì£¼í˜• ë³€ìˆ˜, ì¦‰ ê°€ì¡± ì¤‘ ëŒ€í•™ì— ì§„í•™í•œ ì²« ë²ˆì§¸ ì‚¬ëŒ; school_urbanicity: í•™êµì˜ ë„ì‹œí™” ì •ë„ì— ëŒ€í•œ í•™êµ ìˆ˜ì¤€ì˜ ë²”ì£¼í˜• ë³€ìˆ˜, ì¦‰ ë†ì´Œ, êµì™¸ ë“±; school_mindset: í•™ìƒë“¤ì˜ ê³ ì •ëœ ì‚¬ê³ ë°©ì‹ì˜ í•™êµ ìˆ˜ì¤€ í‰ê· , ë¬´ì‘ìœ„ ë°°ì • ì „ì— ë³´ê³ ë¨, í‘œì¤€í™”ë¨; school_achievement: í•™êµ ì„±ì·¨ ìˆ˜ì¤€, ì´ì „ 4ê°œ í•™ë…„ í•™ìƒë“¤ì˜ ì‹œí—˜ ì ìˆ˜ì™€ ëŒ€í•™ ì¤€ë¹„ë„ë¥¼ ì¸¡ì •í•œ ê²ƒìœ¼ë¡œ, í‘œì¤€í™”ë¨; school_ethnic_minority: í•™êµ ì¸ì¢…/ë¯¼ì¡± ì†Œìˆ˜ì êµ¬ì„±, ì¦‰ í‘ì¸, ë¼í‹°ë…¸, ë˜ëŠ” ì•„ë©”ë¦¬ì¹´ ì›ì£¼ë¯¼ì¸ í•™ìƒ ë¹„ìœ¨, í‘œì¤€í™”ë¨; school_poverty: í•™êµ ë¹ˆê³¤ ì§‘ì¤‘ë„, ì¦‰ ì—°ë°© ë¹ˆê³¤ì„  ì´í•˜ ì†Œë“ì„ ê°€ì§„ ê°€ì • ì¶œì‹  í•™ìƒë“¤ì˜ ë¹„ìœ¨, í‘œì¤€í™”ë¨; school_size: í•™êµì˜ ëª¨ë“  4ê°œ í•™ë…„ í•™ìƒë“¤ì˜ ì´ ìˆ˜, í‘œì¤€í™”ë¨. import pandas as pd pd.set_option(\"display.max_columns\", None) data = pd.read_csv(\"./data/learning_mindset.csv\") data.sample(5, random_state=5) > Although the study was randomised, it doesnâ€™t seem to be the case that this data is free from confounding > So, although the opportunity to participate was random, participation itself is not. We are dealing with a case of non-compliance here. ë§ˆì¸ë“œì…‹ì„ ë°”ê¾¸ëŠ” í”„ë¡œê·¸ë¨ ì°¸ì—¬ ì—¬ë¶€ëŠ” ëœë¤í•˜ê²Œ ì£¼ì–´ì¡Œì§€ë§Œ, ì‹¤ì œë¡œ ì°¸ì„í•˜ëŠ”ë°ì—ëŠ” í•™ìƒë“¤ì˜ ê°œë³„ì ì¸ ìš”ì¸ì˜ ì˜í–¥ì´ ìˆìœ¼ë‹ˆê¹Œ data.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) > One evidence of this is how the studentâ€™s success expectation is correlated with the participation in the seminar. > Students with higher self-reported success expectation are more likely to have joined the growth mindset seminar. ìŠ¤ìŠ¤ë¡œ ë” ë†’ì€ ê¸°ëŒ€ê°€ ìˆëŠ” í•™ìƒë“¤ì´ ì‹¤ì œë¡œ ë” ë§ì´ ì„¸ë¯¸ë‚˜ì— ì°¸ì„í•˜ë”ë¼ -> confounding biasê°€ ì¡´ì¬í•œë‹¤. > Still, letâ€™s see what the difference in means looks like. This will be a useful baseline to compare against. $$ \\hat{ATE} = \\frac{1}{N} \\sum_{i=1}^{N} (Y_i(1) - Y_i(0)) = E[Y_i(1)] - E[Y_i(0)] $$ import statsmodels.formula.api as smf smf.ols(\"achievement_score ~ intervention\", data=data).fit().summary().tables[1] ì²˜ì¹˜ê°€ 0ë˜ëŠ” 1ì´ê¸° ë•Œë¬¸ì— ì²˜ì¹˜ë§Œì„ ê°€ì§€ê³  ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” íšŒê·€ì‹ì„ í†µí•´ $E[Y_i(1)]$ê³¼ $E[Y_i(0)]$ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì²˜ì¹˜ í˜¹ì€ ê°œì…ì´ ìˆëŠ” ê²½ìš° ì—†ëŠ” ê²½ìš°ë³´ë‹¤ í‰ê· ì ìœ¼ë¡œ 0.4723ì •ë„ ë†’ë‹¤. ê·¼ë° ì´ì •ë„ ì°¨ì´ë©´ í°ê±´ê°€? > But is this big or small? I know that interpreting standardized outcomes can be challenging, but bear with me for a moment. > I think it is worth going through this because it wonâ€™t be the last time you will encounter standardized scores. scoreê°€ ì •ê·œí™”ë˜ì—ˆê¸° ë•Œë¬¸ì— ì´ ë‹¨ìœ„ëŠ” stdë‹¤. ë”°ë¼ì„œ 0.4723ì´ë€ ì˜ë¯¸ëŠ” ì•½ 0.5std ì •ë„ ì°¨ì´ê°€ ìˆë‹¤ëŠ” ì˜ë¯¸. > Since the treated group has an average standardized score of about 0.5, this means that they fall above 70% in terms of individual achievement. > Or, in other words, they are in the top 30% who achieve more. Here is what this looks like in a picture. import matplotlib.pyplot as plt plt.hist(data[\"achievement_score\"], bins=20, alpha=0.3, label=\"All\") plt.hist(data.query(\"intervention==0\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C2\") plt.hist(data.query(\"intervention==1\")[\"achievement_score\"], bins=20, alpha=0.3, color=\"C3\") plt.vlines(-0.1538, 0, 300, label=\"Untreated\", color=\"C2\") plt.vlines(-0.1538+0.4723, 0, 300, label=\"Treated\", color=\"C3\") plt.legend() > Of course, we still think this result is biased. > The difference between treated and untreated is probably smaller than this, because we think the bias is positive ê³¼ì—° ì´ì •ë„ ì°¨ì´ ë‚˜ëŠ”ê²Œ ë§ì„ê¹Œ? ì•ì„œ í™•ì¸í•œ ê²ƒì²˜ëŸ¼ í¸í–¥ì´ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒì„ í™•ì¸í–ˆë‹¤. ë¹„êµêµ° ì‚¬ì´ì— ìŠ¤ìŠ¤ë¡œ ê¸°ëŒ€í•˜ëŠ” ì •ë„ (success_expect)ê°€ ì°¨ì´ê°€ ìˆì—ˆê³ , ì´ëŠ” ì„±ê³µí™•ë¥ ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ìš”ì¸ì´ ë  ìˆ˜ ìˆë‹¤. # Propensity Score > Propensity score comes from the realisation that you donâ€™t need to directly control for confounders X to achieve conditional independence $(Y_1, Y_0) \\perp T X$ > Instead, it is sufficient to control for a balancing score $E(T X)$ í¸í–¥ì„ ì œê±°í•˜ê³  ì¡°ê±´ë¶€ ë…ë¦½ì„±ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œëŠ” Xë¥¼ í†µì œí•´ì•¼í•˜ëŠ”ë°, ì´ê²Œ ì–´ë ¤ìš°ë‹ˆê¹Œ ì´ë¥¼ ëŒ€ì‹ í•´ì„œ Xì¼ë•Œ Tì¼ í™•ë¥ ($e(x)$, propensity score)ì„ í†µì œí•˜ìëŠ” ì»¨ì…‰ $$(Y_1, Y_0) \\perp T e(x)$$ import graphviz as gr g = gr.Digraph() g.edge(\"T\", \"Y\") g.edge(\"X\", \"Y\") g.edge(\"X\", \"e(x)\") g.edge(\"e(x)\", \"T\") g > If I know what e(x) is, X alone tells me nothing more that can help me learn what T would be. > Which means that controlling for e(x) acts the same way as controlling for X directly ë°ì´í„°ì˜ í¸í–¥ì„ ìƒê°í•´ë³´ë©´, success expectê°€ ë†’ì€ í•™ìƒì€ ë³¸ì¸ ì¼ë„ ì—´ì‹¬íˆí•˜ê³  ì„¸ë¯¸ë‚˜ì— ì°¸ì„í•  í™•ë¥ ë„ ë†’ì„ ê²ƒì´ë‹¤. ë”°ë¼ì„œ ëœë¤ìœ¼ë¡œ ê¸°íšŒë¥¼ ì¤¬ì§€ë§Œ ì‹¤ì œë¡œëŠ” ì›ë˜ ì„±ê³µí•  í™•ë¥ ì´ ë†’ì€ í•™ìƒì´ ì„¸ë¯¸ë‚˜ì— ì°¸ì„í•  í™•ë¥ ì´ ë” ë†’ìŒ. > Think about it. If they have the exact same probability of receiving the treatment, the only reason one of them received it and the other did not is pure chance. > Holding the propensity score constant acts in a way of making the data look as good as random. ì°¸ì„í•  í™•ë¥ ì„ ê°™ê²Œ ë§Œë“ ë‹¤ë©´, ì›ë˜ ëª©ì ì²˜ëŸ¼ ì°¸ì„í•  í™•ë¥ ì´ ëœë¤í•˜ê²Œ ëœë‹¤. # Propensity Weighting $$ E[Y X, T = 1] - E[Y X, T = 0] = E[\\frac{Y}{e(x)} X, T = 1]P(T) - E\\frac{Y}{(1 - e(x))} X, T = 0) $$ ì²˜ì¹˜ë¥¼ ë°›ì€ ê²ƒ ì¤‘ì— ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì„ ê²ƒ ê°™ì€ ìƒ˜í”Œ ($e(x)$ê°€ ë‚®ì€ ê²½ìš°)ëŠ” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ê³ , ë°˜ëŒ€ë¡œ ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì€ ê²ƒ ì¤‘ì— ì²˜ì¹˜ë¥¼ ë°›ì„ ê²ƒ ê°™ì€ ìƒ˜í”Œ ($e(x)$ê°€ ë†’ì€ ê²½ìš°)ëŠ” í° ê°€ì¤‘ì¹˜ë¥¼ ì£¼ëŠ”. ì´ëŸ¬í•œ ê°€ì¤‘ì¹˜ëŠ” ì§ê´€ì ì¸ ìƒê°ê³¼ ì¼ì¹˜í•œë‹¤. ì™œëƒí•˜ë©´ ì²˜ì¹˜ë¥¼ ë°›ì€ ê²ƒ ì¤‘ì— ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì´ ë†’ì€ ê±´ ëœë¤ì— ê°€ê¹ê²Œ í•˜ë ¤ë©´ ë‚®ì¶°ì•¼ í•˜ê³ , ì²˜ì¹˜ë¥¼ ë°›ì„ í™•ë¥ ì´ ë‚®ì€ ê±´ ëœë¤ì— ê°€ê¹ê²Œ í•˜ë ¤ë©´ ë†’ì—¬ì•¼í•˜ê¸° ë•Œë¬¸ì´ë‹¤. > This estimator is called the Inverse Probability of Treatment Weighting (IPTW), since it scales each unit by the inverse probability of the treatment it received. !iptw ì ìš©í•œ ì˜ˆì œ"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/PropensityScore",
    "title": "Propensity Score",
    "date": "2025-07-17",
    "excerpt": "Propensity Score",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "Propensity Score ì´ ì±…ì˜ ì €ìê°€ ì°¸ ê¸€ì„ ì˜ì“´ë‹¤. ë˜ê²Œ ë§¤ë ¥ì ì¸ ì£¼ì œë¡œ ì´ ë‚´ìš©ì„ í’€ì–´ê°„ë‹¤. Growth Mindset vs Fixed Mindset ì‹¬ë¦¬í•™ìë“¤ì€ ë§ˆì¸ë“œì…‹ì´ ì¸ìƒì— í° ì˜í–¥ì„ ë¯¸ì¹œë‹¤. ë‚´ ëŠ¥ë ¥ì´ ê³ ì •ë˜ì–´ìˆì§€ ì•Šê³  ì¶©ë¶„íˆ ë³€í™”í•  ìˆ˜ ìˆë‹¤ê³  ë¯¿ëŠ” ì‚¬ëŒë“¤ì€, ë‚´ ëŠ¥ë ¥ì´ ê³ ì •ë˜ì–´ìˆë‹¤ê³  ë¯¿ëŠ” ì‚¬ëŒë³´ë‹¤ ë” ì„±ê³µí•œë‹¤. ë‚˜ëŠ” ì´ ë¶€ë¶„ì— í¬ê²Œ ê³µê°í•˜ê³ , ì„±ì¥í˜• ë§ˆì¸ë“œì…‹ì„ ê°€ì§€ë ¤ê³  í•œë‹¤. ë‹¤ë§Œ ì¸ê³¼ì¶”ë¡ ì„ ë‹¤ë£¨ëŠ” ì…ì¥ì—ì„œëŠ” ì´ì— ë‹¤ìŒê³¼ ê°™ì€ ì˜ë¬¸ì„ ë˜ì ¸ë³¼ ìˆ˜ ìˆë‹¤. > Is it that a growth mindset causes people to achieve more? Or is simply the case that people who achieve more are prone to develop a growth mindset as a result of their success? ì§„ì§œ ì„±ì¥í˜• ì‚¬ê³ ë°©ì‹ì´ ì‚¬ëŒë“¤ì„ ë” ì„±ê³µí•˜ê²Œ í–ˆë‚˜? ì•„ë‹ˆë©´ ì„±ê³µí•œ ì‚¬ëŒë“¤ì´ ê·¸ëŸ° ë§ˆì¸ë“œì…‹ì„ ê°€ì§€ê³  ìˆì„ í™•ë¥ ì´ ë†’ì€ê±´ê°€? ë˜ê²Œ ì¬ë°ŒëŠ” ì§ˆë¬¸ì´ë‹¤. ì´ ë¶€ë¶„ì„ ìœ„í•œ ì‹¤í—˜ì´ ì§„í–‰ëœê²Œ ìˆêµ¬ë‚˜. > To settle things, researchers designed the The National Study of Learning Mindsets. It is a randomised study conducted in U.S. public high schools which aims at finding the impact of a growth mindset. > The real data on this study is not publicly available in order to preserve studentsâ€™ privacy. However, we have a simulated dataset with the same statistical properties provided by Athey and Wager, so we will use that instead. ë…¸íŠ¸ë¶ ì°¸ê³ : Propensity Score Notebook"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimationNotebook",
    "title": "Doubly Robust Estimation Notebook",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation Notebook",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "--- title: \"Doubly Robust Estimation Notebook\" date: \"2025-07-17\" excerpt: \"Doubly Robust Estimation Notebook\" category: \"Causal Inference\" tags: [\"Causal Inference\"] --- Doubly Robust Estimation import warnings warnings.filterwarnings('ignore') import pandas as pd import numpy as np from matplotlib import style from matplotlib import pyplot as plt import seaborn as sns from sklearn.linear_model import LogisticRegression, LinearRegression %matplotlib inline style.use(\"fivethirtyeight\") pd.set_option(\"display.max_columns\", None) # í•™ìƒë“¤ì˜ ë§ˆì¸ë“œì…‹ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•œ ì„¸ë¯¸ë‚˜ë¥¼ ì§„í–‰í–ˆëŠ”ë°, ì´ê²Œ ì–´ëŠì •ë„ ì˜í–¥ì´ ìˆì—ˆëŠ”ì§€ ê°€ìƒì˜ ë°ì´í„° ì„¸ë¯¸ë‚˜ ì°¸ì„ ì—¬ë¶€ -> intervention ì–¼ë§ˆë‚˜ ì„±ê³µí–ˆëŠ”ì§€ í˜¹ì€ ì„±ê³µí•  í™•ë¥  -> success_expect data = pd.read_csv(\"./data/learning_mindset.csv\") data.sample(5, random_state=5) # ì„±ê³µí•œ ì •ë„ (success expect)ê°€ ë†’ì„ìˆ˜ë¡ ì„¸ë¯¸ë‚˜ ì°¸ì„ ì—¬ë¶€(intervention)ê°€ ë†’ì€ì§€ data.groupby(\"success_expect\")[\"intervention\"].agg([\"mean\", \"count\"]) í•™ìƒì˜ ìš”ì¸ì— ë”°ë¼ ì°¸ì„ ì—¬ë¶€ê°€ ê²°ì •ë˜ê³ , ë˜ ì´ ìš”ì¸ì´ ì„±ê³µí™•ë¥ ì„ ê²°ì •í•œë‹¤. -> confounding variable. ì¦‰, ë‹¨ìˆœíˆ ì°¸ì„ ì—¬ë¶€ì— ë”°ë¼ ì„±ê³µí™•ë¥ ì„ ë¹„êµí•´ ì„¸ë¯¸ë‚˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸°ì—” ì–´ë µë‹¤. í•™ìƒì˜ ë‹¤ë¥¸ ìš”ì¸ë“¤ì´ ì˜í–¥ì„ ë¯¸ì¹˜ê³  ìˆìœ¼ë‹ˆê¹Œ # regressionì„ í†µí•´ í¸í–¥ì„ ì œê±°í•˜ê³  ì„¸ë¯¸ë‚˜ì˜ íš¨ê³¼ ë³´ê¸° ì´ë¥¼ ìœ„í•´ categorical featuresë¥¼ ë”ë¯¸ ë³€ìˆ˜ë¡œ ë³€í™˜ categ = [\"ethnicity\", \"gender\", \"school_urbanicity\"] cont = [\"school_mindset\", \"school_achievement\", \"school_ethnic_minority\", \"school_poverty\", \"school_size\"] data_with_categ = pd.concat([ data.drop(columns=categ), # dataset without the categorical features pd.get_dummies(data[categ], columns=categ, drop_first=False) # categorical features converted to dummies ], axis=1) print(data_with_categ.shape)"
  },
  {
    "id": "Causal Inference/Causal Inference for The Brave and True/DoublyRobustEstimation",
    "title": "Doubly Robust Estimation",
    "date": "2025-07-17",
    "excerpt": "Doubly Robust Estimation",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "Doubly Robust Estimation"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-16-ê·¸ë‚˜ì €ë‚˜ì†Œê°œíŒ…ì–´ë• ì–´",
    "title": "ê·¸ë‚˜ì €ë‚˜ ì†Œê°œíŒ… ì–´ë• ì–´?",
    "date": "2025-07-16",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ê³ ë¯¼ ë° ì¡°ì–¸í•˜ëŠ” í‘œí–”"
    ],
    "content": "ë°°ìš´ ê²ƒ How was it? ê·¸ë‚˜ì €ë‚˜, ê·¸ëŸ°ë° By the way, how was the blind date? BTW (By The Way) ì¹œêµ¬ ì‚¬ì´ì—ì„œ ê°€ë³ê²Œ ì“°ëŠ” ë§ By the way, how was the dinner yesterday? Blind date ì†Œê°œíŒ… Well, the thing is he is a vegeterian. ê¸€ì„, ë¬¸ì œëŠ” out of budget to make it somewhere : ì–´ë””ë“  ê°€ë‹¤ ê°„ë‹¨í•œ ì‘ë¬¸. Simple writing or essay Daily talk to myself as usual. By the way, How was your day? Did you do your best? Well, the thing is condition. Today, I was so tired. So I can fully concentrate. So I think I should have a sleep early. ìˆ˜ì •ëœ ë²„ì „ (Improved Version) Just having my usual daily conversation with myself. By the way, how was your day? Did you do your best? Well, the thing is my condition wasn't great. Today, I was so tired that I couldn't fully concentrate. So I think I should go to sleep early tonight. í”¼ë“œë°± (Feedback) ì£¼ìš” ìˆ˜ì • ì‚¬í•­: \"Daily talk to myself\" â†’ \"Having my usual daily conversation with myself\" ë” ì™„ì „í•œ ë¬¸ì¥ êµ¬ì¡°ë¡œ ìˆ˜ì • \"Well, the thing is condition\" â†’ \"Well, the thing is my condition wasn't great\" ë¶ˆì™„ì „í•œ ë¬¸ì¥ì„ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ìˆ˜ì • ì†Œìœ ê²© \"my\" ì¶”ê°€ ë° êµ¬ì²´ì ì¸ ì„¤ëª… ì¶”ê°€ \"So I can fully concentrate\" â†’ \"So I couldn't fully concentrate\" ë§¥ë½ìƒ ë¶€ì •ë¬¸ì´ ë§ìŒ (í”¼ê³¤í•´ì„œ ì§‘ì¤‘í•  ìˆ˜ ì—†ì—ˆë‹¤ëŠ” ì˜ë¯¸) \"I should have a sleep early\" â†’ \"I should go to sleep early tonight\" \"go to sleep\"ì´ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ \"tonight\" ì¶”ê°€ë¡œ ì‹œê°„ ëª…ì‹œ ë¬¸ë²• í¬ì¸íŠ¸: \"so + í˜•ìš©ì‚¬/ë¶€ì‚¬ + that + ì£¼ì–´ + ë™ì‚¬\" êµ¬ë¬¸ ì‚¬ìš© ì‹œì œ ì¼ì¹˜ (ê³¼ê±° ìƒí™©ì„ ì„¤ëª…í•  ë•ŒëŠ” ê³¼ê±°í˜• ì‚¬ìš©) ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ í‘œí˜„ ì‚¬ìš©"
  },
  {
    "id": "Causal Inference/tikhonov-regularization-and-convex-optimization",
    "title": "Tikhonov Regularizationê³¼ Convex Optimization: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½",
    "date": "2025-07-16",
    "excerpt": "Ill-posed problemì„ í•´ê²°í•˜ëŠ” Tikhonov regularizationê³¼ convex optimizationì˜ ì›ë¦¬",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Optimization",
      "Regularization"
    ],
    "content": "ê°œìš” --- ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ R-learnerë¥¼ ì ìš©í•  ë•Œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ Tikhonov regularizationê³¼ convex optimization ì´ë¡ ì´ í•µì‹¬ì ì¸ ì—­í• ì„ í•©ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ëŸ¬í•œ ìˆ˜í•™ì  ë„êµ¬ë“¤ì˜ ì›ë¦¬ì™€ ì‘ìš©ì„ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. Ill-posed Problemê³¼ Non-identification --- 1.1 Well-posed vs Ill-posed Problem Well-posed problem (ì˜ ì •ì˜ëœ ë¬¸ì œ) ì˜ 3ê°€ì§€ ì¡°ê±´: ì¡´ì¬ì„± (Existence): í•´ê°€ ì¡´ì¬í•œë‹¤ ìœ ì¼ì„± (Uniqueness): í•´ê°€ ìœ ì¼í•˜ë‹¤ ì•ˆì •ì„± (Stability): ì…ë ¥ì˜ ì‘ì€ ë³€í™”ê°€ í•´ì˜ ì‘ì€ ë³€í™”ë¥¼ ê°€ì ¸ì˜¨ë‹¤ Ill-posed problemì€ ì´ ì¤‘ í•˜ë‚˜ ì´ìƒì˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ì§€ ì•ŠëŠ” ë¬¸ì œì…ë‹ˆë‹¤. 1.2 ì—°ì†í˜• ì²˜ì¹˜ì—ì„œì˜ Non-identification ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œëŠ” ìœ ì¼ì„± ì¡°ê±´ì´ ìœ„ë°˜ë©ë‹ˆë‹¤: $$L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right]$$ ì´ ì†ì‹¤í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” í•´ì§‘í•©ì€: $$S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\}$$ > ë¬¸ì œ: ë¬´í•œíˆ ë§ì€ í•´ê°€ ì¡´ì¬í•˜ì—¬ ìœ ì¼í•œ CATE í•¨ìˆ˜ $\\tau(x, t)$ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ìŒ Tikhonov Regularization --- 2.1 ê¸°ë³¸ ì›ë¦¬ Tikhonov regularizationì€ ill-posed problemì„ well-posed problemìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì›ë˜ ë¬¸ì œ: $$\\min_{h} L(h)$$ ì •ê·œí™”ëœ ë¬¸ì œ: $$\\min_{h} L(h) + \\rho \\ h\\ ^2$$ ì—¬ê¸°ì„œ: $\\rho > 0$: ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ $\\ h\\ ^2$: ì •ê·œí™” í•­ (ì¼ë°˜ì ìœ¼ë¡œ $L_2$ norm) 2.2 ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œì˜ ì ìš© $$ \\begin{equation} \\tau_\\rho = \\arg \\min_{h \\in L_2^P(X,T)} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right] \\end{equation} $$ ì •ê·œí™” í•­ì˜ ì—­í• : $\\ h\\ _{L_2^P}^2 = E\\{h^2(X, T)\\}$ í•¨ìˆ˜ì˜ \"í¬ê¸°\"ë¥¼ ì œí•œí•˜ì—¬ ê³¼ì í•© ë°©ì§€ ë¬´í•œíˆ ë§ë˜ í•´ë¥¼ ìœ ì¼í•œ í•´ë¡œ ìˆ˜ë ´ì‹œí‚´ 2.3 Tikhonov ì •ë¦¬ ì •ë¦¬: ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ $\\rho > 0$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, ì •ê·œí™”ëœ ë¬¸ì œëŠ” ìœ ì¼í•œ í•´ $\\tau_\\rho$ë¥¼ ê°€ì§„ë‹¤. ì¦ëª… ì•„ì´ë””ì–´: $L_c(h)$ëŠ” convexí•˜ì§€ë§Œ strictly convexí•˜ì§€ ì•ŠìŒ $\\rho \\ h\\ _{L_2^P}^2$ëŠ” strictly convexí•¨ ë‘˜ì˜ í•©ì€ strictly convexê°€ ë˜ì–´ ìœ ì¼í•œ ìµœì†Ÿê°’ì„ ê°€ì§ Convex Optimization --- 3.1 Convex Functionì˜ ì •ì˜ í•¨ìˆ˜ $f: \\mathbb{R}^n \\to \\mathbb{R}$ê°€ convexí•˜ë‹¤ëŠ” ê²ƒì€: $$f(\\lambda x + (1-\\lambda) y) \\leq \\lambda f(x) + (1-\\lambda) f(y)$$ ëª¨ë“  $x, y \\in \\text{dom}(f)$ì™€ $\\lambda \\in [0, 1]$ì— ëŒ€í•´ ì„±ë¦½í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 3.2 Strictly Convex Function í•¨ìˆ˜ $f$ê°€ strictly convexí•˜ë‹¤ëŠ” ê²ƒì€: $$f(\\lambda x + (1-\\lambda) y) < \\lambda f(x) + (1-\\lambda) f(y)$$ $x \\neq y$ì´ê³  $\\lambda \\in (0, 1)$ì¼ ë•Œ ë¶€ë“±í˜¸ê°€ ì—„ê²©í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 3.3 Convex Functionì˜ ì¤‘ìš”í•œ ì„±ì§ˆ ì„±ì§ˆ 1: ìœ ì¼í•œ ìµœì†Ÿê°’ Convex function: ì§€ì—­ ìµœì†Ÿê°’ = ì „ì—­ ìµœì†Ÿê°’ Strictly convex function: ìµœì†Ÿê°’ì´ ìœ ì¼í•¨ ì„±ì§ˆ 2: ìµœì í™”ì˜ ìš©ì´ì„± ê²½ì‚¬í•˜ê°•ë²•ì´ ì „ì—­ ìµœì†Ÿê°’ìœ¼ë¡œ ìˆ˜ë ´ ë³´ì¥ ìˆ˜ì¹˜ì  ì•ˆì •ì„± ì„±ì§ˆ 3: Jensen's Inequality $$E[f(X)] \\geq f(E[X])$$ 3.4 R-learnerì—ì„œì˜ Convexity ì›ë˜ ì†ì‹¤í•¨ìˆ˜ $L_c(h)$: Convexí•˜ì§€ë§Œ strictly convexí•˜ì§€ ì•ŠìŒ ë”°ë¼ì„œ ìµœì†Ÿê°’ì´ ìœ ì¼í•˜ì§€ ì•ŠìŒ ì •ê·œí™”ëœ ì†ì‹¤í•¨ìˆ˜ $L_c(h) + \\rho \\ h\\ ^2$: $\\rho \\ h\\ ^2$ê°€ strictly convexí•¨ ì „ì²´ í•¨ìˆ˜ê°€ strictly convexê°€ ë¨ ìœ ì¼í•œ ìµœì†Ÿê°’ ë³´ì¥ ìˆ˜í•™ì  ì¦ëª… --- 4.1 $\\ h\\ _{L_2^P}^2$ê°€ Strictly Convexì„ì„ ì¦ëª… ì •ë¦¬: $\\ h\\ _{L_2^P}^2 = E\\{h^2(X, T)\\}$ëŠ” strictly convexí•˜ë‹¤. ì¦ëª…: ì„ì˜ì˜ $h_1 \\neq h_2$ì™€ $\\lambda \\in (0, 1)$ì— ëŒ€í•´: $$ \\begin{align} \\ \\lambda h_1 + (1-\\lambda) h_2\\ _{L_2^P}^2 &= E\\{[\\lambda h_1(X,T) + (1-\\lambda) h_2(X,T)]^2\\} \\\\ &= E\\{\\lambda^2 h_1^2(X,T) + 2\\lambda(1-\\lambda) h_1(X,T)h_2(X,T) + (1-\\lambda)^2 h_2^2(X,T)\\} \\\\ &= \\lambda^2 E\\{h_1^2(X,T)\\} + 2\\lambda(1-\\lambda) E\\{h_1(X,T)h_2(X,T)\\} + (1-\\lambda)^2 E\\{h_2^2(X,T)\\} \\end{align} $$ Cauchy-Schwarz ë¶€ë“±ì‹ì— ì˜í•´: $$E\\{h_1(X,T)h_2(X,T)\\} \\leq \\sqrt{E\\{h_1^2(X,T)\\}} \\sqrt{E\\{h_2^2(X,T)\\}}$$ $h_1 \\neq h_2$ì¼ ë•Œ ë¶€ë“±í˜¸ê°€ strictí•˜ë¯€ë¡œ: $$\\ \\lambda h_1 + (1-\\lambda) h_2\\ _{L_2^P}^2 < \\lambda \\ h_1\\ _{L_2^P}^2 + (1-\\lambda) \\ h_2\\ _{L_2^P}^2$$ 4.2 ì •ê·œí™”ëœ ì†ì‹¤í•¨ìˆ˜ì˜ Strict Convexity ê²°ë¡ : $L_{c,\\ell_2}(h \\mid \\rho) = L_c(h) + \\rho \\ h\\ _{L_2^P}^2$ëŠ” $\\rho > 0$ì¼ ë•Œ strictly convexí•˜ë‹¤. ì´ìœ : $L_c(h)$ëŠ” convex $\\rho \\ h\\ _{L_2^P}^2$ëŠ” strictly convex Convex + Strictly convex = Strictly convex ì‹¤ì œ ì‘ìš©ê³¼ ì˜ë¯¸ --- 5.1 Two-step Procedure Step 1: Tikhonov regularizationìœ¼ë¡œ ì¤‘ê°„ í•¨ìˆ˜ ì¶”ì • $$\\tilde{\\tau}_\\rho = \\arg \\min_{h} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right]$$ Step 2: Zero-constraining operatorë¡œ ìµœì¢… CATE ì¶”ì • $$\\tau(x, t) = \\text{ZeroConstraint}(\\tilde{\\tau}_\\rho(x, t))$$ 5.2 ì •ê·œí™” ë§¤ê°œë³€ìˆ˜ $\\rho$ì˜ ì—­í•  $\\rho$ê°€ ì‘ì„ ë•Œ: ì›ë˜ ë¬¸ì œì— ê°€ê¹Œì›€ BiasëŠ” ì‘ì§€ë§Œ varianceê°€ í´ ìˆ˜ ìˆìŒ $\\rho$ê°€ í´ ë•Œ: ì •ê·œí™” íš¨ê³¼ê°€ ê°•í•¨ VarianceëŠ” ì‘ì§€ë§Œ biasê°€ í´ ìˆ˜ ìˆìŒ ìµœì  $\\rho$ ì„ íƒ: Cross-validation Information criteria (AIC, BIC) Theoretical convergence rate 5.3 ìˆ˜ë ´ ì†ë„ì™€ ì ê·¼ì  ì„±ì§ˆ ì •ê·œí™”ëœ ì¶”ì •ëŸ‰ $\\hat{\\tau}_\\rho$ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ìš°ìˆ˜í•œ ì„±ì§ˆì„ ê°€ì§‘ë‹ˆë‹¤: Consistency: $\\hat{\\tau}_\\rho \\to \\tau$ as $n \\to \\infty$ Convergence rate: $O_p(n^{-r})$ for some $r > 0$ Asymptotic normality: $\\sqrt{n}(\\hat{\\tau}_\\rho - \\tau) \\xrightarrow{d} N(0, \\Sigma)$ ë‹¤ë¥¸ ì •ê·œí™” ë°©ë²•ê³¼ì˜ ë¹„êµ --- 6.1 Ridge vs Lasso vs Tikhonov ë°©ë²• ì •ê·œí™” í•­ íŠ¹ì§• ------ ----------- ------ Ridge $\\lambda \\ \\beta\\ _2^2$ ê³„ìˆ˜ ì¶•ì†Œ, ë³€ìˆ˜ ì„ íƒ X Lasso $\\lambda \\ \\beta\\ _1$ ë³€ìˆ˜ ì„ íƒ, í¬ì†Œì„± Tikhonov $\\rho \\ h\\ _{L_2^P}^2$ í•¨ìˆ˜ ê³µê°„ì—ì„œì˜ ì •ê·œí™” 6.2 ë‹¤ë¥¸ í•´ê²° ë°©ë²•ë“¤ Alternative 1: Truncated SVD íŠ¹ì´ê°’ ë¶„í•´ í›„ ì‘ì€ íŠ¹ì´ê°’ ì œê±° ìˆ˜ì¹˜ì  ë°©ë²• Alternative 2: Iterative methods Landweber iteration Conjugate gradient methods Alternative 3: Bayesian approach Prior distribution ì„¤ì • Posterior inference ê²°ë¡  --- Tikhonov regularizationì˜ í•µì‹¬ ì¥ì : ì´ë¡ ì  ë³´ì¥: Strictly convex optimizationìœ¼ë¡œ ìœ ì¼í•œ í•´ ë³´ì¥ ìˆ˜ì¹˜ì  ì•ˆì •ì„±: Well-conditioned ë¬¸ì œë¡œ ë³€í™˜ ì¼ë°˜ì„±: ë‹¤ì–‘í•œ ill-posed problemì— ì ìš© ê°€ëŠ¥ êµ¬í˜„ ìš©ì´ì„±: ê¸°ì¡´ ìµœì í™” ì•Œê³ ë¦¬ì¦˜ í™œìš© ê°€ëŠ¥ ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œì˜ ì˜ë¯¸: Non-identification ë¬¸ì œì˜ ê·¼ë³¸ì  í•´ê²° ì‹¤ìš©ì ì¸ CATE ì¶”ì • ë°©ë²• ì œê³µ ì´ë¡ ì  ë¶„ì„ê³¼ ì‹¤ì œ êµ¬í˜„ì˜ êµëŸ‰ ì—­í•  ì´ëŸ¬í•œ ìˆ˜í•™ì  ë„êµ¬ë“¤ì€ ë‹¨ìˆœíˆ ê¸°ìˆ ì  ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ, ì¸ê³¼ì¶”ë¡ ì˜ ê·¼ë³¸ì ì¸ ì–´ë ¤ì›€ì„ ì´í•´í•˜ê³  ê·¹ë³µí•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤. --- ì°¸ê³  ë¬¸í—Œ: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization Boyd, S., & Vandenberghe, L. (2004). Convex optimization Wahba, G. (1990). Spline models for observational data Engl, H. W., Hanke, M., & Neubauer, A. (2000). Regularization of inverse problems"
  },
  {
    "id": "Book/ë§Œë“¤ì–´ì§„ ì‹  (The God Delusion)/ì±…ì„ì½ê¸°ì‹œì‘í•˜ë©´ì„œ",
    "title": "ì±…ì„ ì½ê¸° ì‹œì‘í•˜ë©´ì„œ",
    "date": "2025-07-16",
    "excerpt": "ì±…ì„ ì½ê¸° ì „ì— ì–´ë–¤ ìƒê°ì„ ê°€ì§€ê³  ìˆëŠ”ì§€ ì •ë¦¬",
    "category": "Book",
    "tags": [
      "The God Delusion",
      "ë§Œë“¤ì–´ì§„ ì‹ "
    ],
    "content": "ë‚˜ëŠ” í™•ì‹¤í•œ ë¬´ì‹ ë¡ ìë‹¤. ë¶€ëª¨ë‹˜ì˜ ì˜í–¥ë„ ìˆê² ì§€ë§Œ, ì¹œêµ¬ë”°ë¼ êµíšŒì— ë‹¤ë…”ë˜ ê²½í—˜, ë‹¤ì–‘í•œ ë§¤ì²´ì—ì„œ ì ‘í•œ í˜„ìƒë“¤ ë“±ë“±ì´ ì§€ê¸ˆ ë‚˜ì˜ ìƒê°ì„ ë§Œë“¤ì—ˆë‹¤. ì‹ ì´ ìˆëŠ”ê²Œ ë§ì´ ë˜ë‚˜? ë‚˜ì˜ ì‚¬ê³ ë°©ì‹ì€ ì§ì—…ì ì¸ ì˜í–¥ (ê°œë°œì or ë°ì´í„°ê³¼í•™ì) ë•Œë¬¸ì¸ì§€ ëª°ë¼ë„ ê·¼ê±°ì— ì§€ë°°ì ì´ë‹¤. ê·¼ê±°ê°€ ëª…í™•í•˜ë‹¤ë©´ ì§€ê¸ˆë„ ë‹¹ì¥ ë°”ë€” ìˆ˜ ìˆê³ , ê·¼ê±°ê°€ ì—†ë‹¤ë©´ ë°±ë‚  ì„¤ë“ì„ í•˜ë ¤í•´ë„ ì „í˜€ ì†Œìš©ì´ ì—†ì„ ê²ƒì´ë‹¤. ë‚´ê°€ ì´ ì±…ì„ ì½ê²Œ ëœ ì´ìœ ëŠ” ì‹ ì´ ì—†ëŠ” ì´ìœ ë¥¼ ì•Œê³  ì‹¶ë‹¤ê¸° ë³´ë‹¤ëŠ”, ì‚¬ëŒë“¤ì´ ì™œ ì‹ ì„ ë¯¿ëŠ”ì§€ê°€ ë” ê¶ê¸ˆí•œê²Œ í¬ë‹¤. ì–´ì¨Œë“  ì±…ì—ì„œëŠ” ì‚¬ëŒë“¤ì´ ì‹ ì„ ë¯¿ëŠ” ì´ìœ ë¥¼ ì–¸ê¸‰í•˜ê³  ì´ë¥¼ ë°˜ë°•í• í…Œë‹ˆê¹Œ. ì´ ì±…ì˜ ì˜ì–´ ì œëª©ì€ ë˜ê²Œ íŒŒê²©ì ì´ë‹¤. í—›ì›ƒìŒì´ ë‚˜ì˜¬ì •ë„ë¡œ. Delusion, ë§ìƒì´ë¼ëŠ” ë‹¨ì–´ë¥¼ ì“°ë‹¤ë‹ˆ... > ëˆ„êµ°ê°€ ë§ìƒì— ì‹œë‹¬ë¦¬ë©´ ì •ì‹  ì´ìƒì´ë¼ê³  í•œë‹¤. > ë‹¤ìˆ˜ê°€ ë§ìƒì— ì‹œë‹¬ë¦¬ë©´ ì¢…êµë¼ê³  í•œë‹¤. > -> ë¡œë²„íŠ¸ í¼ì‹œê·¸ ì‚¬ì‹¤ ë‚˜ë„ ë¹„ìŠ·í•œ ìƒê°ì´ê¸´ í•˜ë‹¤. ë‚´ê°€ ìƒê°í•˜ëŠ” ì‹ ì€ ê³µí†µì ìœ¼ë¡œ ê·¸ë¦¬ê³  ìˆëŠ” í—ˆìƒì´ë¼ê³  ìƒê°í•œë‹¤. ì˜ˆì „ì— ëª¨ì‹œë˜ í˜¸ë‘ì´ ì‹ ê³¼ ì˜ˆìˆ˜ëŠ” ì‚¬ì‹¤ ê°™ë‹¤ê³  ë³´ì—¬ì§„ë‹¤. ì‚¬ëŒë“¤ì˜ í¸ì˜ë¥¼ ìœ„í•´ ì–´ë–¤ í—ˆìƒì˜ ê°œë…ì„ ë§Œë“¤ê³ , ì´ë¥¼ í•¨ê»˜ ë¯¿ëŠ” ê²ƒ. ì£¼ìœ„ì— ì¢…êµë¥¼ ê°€ì§„ ì¹œêµ¬ë“¤ì´ ë§ì€ë° ê·¸ë“¤ì„ ë¬´ì‹œí•˜ëŠ” ìƒê°ì€ ì§„ì§œ ì „í˜€ì—†ë‹¤. ê·¸ëƒ¥ ë‚˜ì˜ ìƒê°ì´ ì´ëŸ´ ë¿. ë˜ í—ˆìƒì´ë©´ ì–´ë–»ë‚˜ ì‚¶ì— ë„ì›€ì´ ë˜ë©´ ê·¸ê²Œ ì¢‹ì€ê±°ì§€ ë¼ëŠ” ìƒê°ë„ í•œë‹¤. ì–´ì¨Œë“  ì±…ì„ ì½ì–´ë‚˜ê°€ë©´ì„œ ë‚˜ì˜ ìƒê°ì€ ì–´ë–»ê²Œ ë°”ë€Œê³ , ë˜ ì–´ë–¤ ê´€ì ì´ ì—´ë¦´ì§€ ê¸°ëŒ€ê°€ ëœë‹¤. ì­‰ ì½ì–´ë´ì•¼ê² ë‹¤."
  },
  {
    "id": "Recommendation/user-based-collaborative-filtering",
    "title": "User-Based Collaborative Filtering",
    "date": "2025-07-15",
    "excerpt": "User-Based Collaborative Filtering ì•Œê³ ë¦¬ì¦˜ì— ëŒ€í•œ ì„¤ëª…",
    "category": "Recommendation",
    "tags": [
      "ì¶”ì²œì‹œìŠ¤í…œ"
    ],
    "content": "User-Based Collaborative Filtering (ì‚¬ìš©ì ê¸°ë°˜ í˜‘ì—… í•„í„°ë§) ê°œìš” User-Based Collaborative Filteringì€ ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ê°€ì¥ ì˜¤ë˜ë˜ê³  ì§ê´€ì ì¸ ì´ì›ƒ(neighborhood) ê¸°ë°˜ ë°©ë²•ì…ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” \"ë¹„ìŠ·í•œ ì·¨í–¥ì„ ê°€ì§„ ì‚¬ìš©ìë“¤ì´ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì¢‹ì•„í•  ê²ƒì´ë‹¤\" ë¼ëŠ” ê°€ì •ì— ê¸°ë°˜í•©ë‹ˆë‹¤. ê¸°ë³¸ ê°œë… ì‚¬ìš©ì-ì•„ì´í…œ í‰ì  í–‰ë ¬ $R$: í–‰ì€ ì‚¬ìš©ì, ì—´ì€ ì•„ì´í…œ ì›ì†Œ $r_{u,i}$: ì‚¬ìš©ì $u$ê°€ ì•„ì´í…œ $i$ì— ë§¤ê¸´ í‰ì  (ë˜ëŠ” ì•”ì‹œì  í–‰ë™) ëª©í‘œ: ì‚¬ìš©ì $u$ì—ê²Œ ì•„ì´í…œ $i$ë¥¼ ì¶”ì²œí• ì§€ ê²°ì • ì•Œê³ ë¦¬ì¦˜ ì ˆì°¨ ìœ ì‚¬ë„(Similarity) ê³„ì‚° ì‚¬ìš©ì ê°„ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ë“¤: Cosine Similarity $$\\text{sim}(u,v) = \\frac{\\mathbf{r}_u \\cdot \\mathbf{r}_v}{\\ \\mathbf{r}_u\\ \\times \\ \\mathbf{r}_v\\ }$$ Pearson Correlation $$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)(r_{v,i} - \\bar{r}_v)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_u)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_v)^2}}$$ ì—¬ê¸°ì„œ $I_{uv}$ëŠ” ë‘ ì‚¬ìš©ìê°€ ëª¨ë‘ í‰ê°€í•œ ì•„ì´í…œ ì§‘í•© Adjusted Cosine Similarity $$\\text{sim}(u,v) = \\frac{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)(r_{v,i} - \\bar{r}_i)}{\\sqrt{\\sum_{i \\in I_{uv}} (r_{u,i} - \\bar{r}_i)^2} \\times \\sqrt{\\sum_{i \\in I_{uv}} (r_{v,i} - \\bar{r}_i)^2}}$$ ì—¬ê¸°ì„œ $\\bar{r}_i$ëŠ” ì•„ì´í…œ $i$ì˜ í‰ê·  í‰ì  Top-K ì´ì›ƒ ì„ íƒ ì‚¬ìš©ì $u$ì™€ ìœ ì‚¬ë„ê°€ ê°€ì¥ ë†’ì€ $K$ëª…ì˜ ì‚¬ìš©ì ì´ì›ƒ ì§‘í•© $N_u$ ì„ íƒ í‰ì  ì˜ˆì¸¡ / ì¶”ì²œ ì ìˆ˜ ê³„ì‚° ê°€ì¤‘ í‰ê·  ë°©ì‹ $$\\hat{r}_{u,i} = \\bar{r}_u + \\frac{\\sum_{v \\in N_u} \\text{sim}(u,v) \\times (r_{v,i} - \\bar{r}_v)}{\\sum_{v \\in N_u} \\text{sim}(u,v) }$$ ë‹¨ìˆœ ê°€ì¤‘í•© ë°©ì‹ $$\\hat{r}_{u,i} = \\frac{\\sum_{v \\in N_u} \\text{sim}(u,v) \\times r_{v,i}}{\\sum_{v \\in N_u} \\text{sim}(u,v) }$$ ì¶”ì²œ ë¦¬ìŠ¤íŠ¸ ìƒì„± ì•„ì§ ì†Œë¹„í•˜ì§€ ì•Šì€ ì•„ì´í…œ ì¤‘ $\\hat{r}_{u,i}$ê°€ ë†’ì€ ìˆœìœ¼ë¡œ $N$ê°œ ë°˜í™˜ ì¥ì  ì§ê´€ì„±: êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³  ì´í•´í•˜ê¸° ì‰¬ì›€ New Item Cold-Start ëŒ€ì‘: ìƒˆë¡œìš´ ì•„ì´í…œì´ ë“¤ì–´ì™€ë„ ë¹„ìŠ·í•œ ì‚¬ìš©ìê°€ í‰ê°€í–ˆë‹¤ë©´ ê³§ë°”ë¡œ ì¶”ì²œ ê°€ëŠ¥ ë¯¸ë¬˜í•œ íŒ¨í„´ í¬ì°©: ì‚¬ìš©ì ì„ í˜¸ì˜ ì„¸ë°€í•œ íŒ¨í„´ì„ ì´ì›ƒ ê¸°ë°˜ìœ¼ë¡œ í¬ì°© ê°€ëŠ¥ í•´ì„ ê°€ëŠ¥ì„±: ì™œ ì´ ì•„ì´í…œì„ ì¶”ì²œí–ˆëŠ”ì§€ ì„¤ëª… ê°€ëŠ¥ ë‹¨ì  í¬ì†Œì„±(Sparsity): í‰ì  í–‰ë ¬ì´ í¬ì†Œí•˜ë©´ ê³µí†µ í‰ê°€ ì•„ì´í…œì´ ì ì–´ ìœ ì‚¬ë„ ê³„ì‚°ì´ ë¶ˆì•ˆì • í™•ì¥ì„±(Scalability): ì‚¬ìš©ì ìˆ˜ê°€ ëŠ˜ì–´ë‚ ìˆ˜ë¡ ì „ì²´ ìœ ì‚¬ë„ ê³„ì‚° ë¹„ìš© $O( U ^2)$ ì‚¬ìš©ì Cold-Start: ìƒˆ ì‚¬ìš©ìëŠ” í‰ì ì´ ì—†ì–´ ì¶”ì²œ ë¶ˆê°€ Popularity Bias: ë§ì€ í‰ì ì„ ë‚¨ê¸´ í—¤ë¹„ ìœ ì €ì—ê²Œ ìœ ì‚¬ë„ê°€ ì§‘ì¤‘ë  ìˆ˜ ìˆìŒ Gray Sheep ë¬¸ì œ: ë…íŠ¹í•œ ì·¨í–¥ì„ ê°€ì§„ ì‚¬ìš©ìëŠ” ìœ ì‚¬í•œ ì´ì›ƒì„ ì°¾ê¸° ì–´ë ¤ì›€ ê°œì„  ê¸°ë²• Significance Weighting ê³µí†µ í‰ê°€ ì•„ì´í…œ ìˆ˜ê°€ ì ìœ¼ë©´ ìœ ì‚¬ë„ì— íŒ¨ë„í‹° ë¶€ì—¬ Neighborhood Size ìµœì í™” $K$ê°’ íŠœë‹ ìœ ì‚¬ë„ threshold ì„¤ì • ì •ê·œí™” ë° ë³´ì • Normalization (í‰ê· Â·í‘œì¤€í¸ì°¨ ì œê±°) Shrinkage Baseline ë³´ì • (ê¸€ë¡œë²Œ í‰ê·  + user bias + item bias) Hybrid ì ‘ê·¼ë²• ì‚¬ìš©ì ê¸°ë°˜ CF + ì•„ì´í…œ ê¸°ë°˜ CF ê²°í•© ì½˜í…ì¸  ê¸°ë°˜ íŠ¹ì§•ê³¼ ê²°í•© ëª¨ë¸ ê¸°ë°˜(í–‰ë ¬ ë¶„í•´)ê³¼ ìŠ¤ìœ„ì¹­ ì•„ì´í…œ ê¸°ë°˜ CFì™€ ë¹„êµ êµ¬ë¶„ User-Based CF Item-Based CF ------ --------------- --------------- ê°€ì • ë¹„ìŠ·í•œ ì‚¬ìš©ìê°€ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì¢‹ì•„í•¨ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì†Œë¹„í•œ ì‚¬ìš©ìëŠ” í–¥í›„ ë¹„ìŠ·í•œ ì•„ì´í…œì„ ì†Œë¹„ ì í•©í•œ ìƒí™© ì‚¬ìš©ì í–‰ë™ì´ í’ë¶€í•˜ê³  ë§ì¶¤í˜• ê´€ê³„ íŒŒì•… í•„ìš” ëŒ€ëŸ‰ ì‚¬ìš©ì, ìƒëŒ€ì ìœ¼ë¡œ ì ì€ ì•„ì´í…œ í™•ì¥ì„± ì‚¬ìš©ì ìˆ˜ì— ë”°ë¼ ì„±ëŠ¥ ì €í•˜ ì•„ì´í…œ ìˆ˜ì— ë”°ë¼ ì„±ëŠ¥ ì €í•˜ Cold-Start New Itemì— ê°•í•¨, New Userì— ì•½í•¨ New Userì— ê°•í•¨, New Itemì— ì•½í•¨ êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­ ë¼ì´ë¸ŒëŸ¬ë¦¬ í™œìš© Scikit-surprise: ë‹¤ì–‘í•œ CF ì•Œê³ ë¦¬ì¦˜ ì§€ì› Implicit: ì•”ì‹œì  í”¼ë“œë°± ê¸°ë°˜ CF LightFM: í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë¸ ì„±ëŠ¥ ìµœì í™” ì˜¤í”„ë¼ì¸ ê³„ì‚°: ìœ ì‚¬ë„ í–‰ë ¬ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬ ìºì‹œ ìƒ˜í”Œë§: ëŒ€ê·œëª¨ ë°ì´í„°ì—ì„œ ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ê³„ì‚°ëŸ‰ ê°ì†Œ Locality Sensitive Hashing: ê·¼ì‚¬ ìœ ì‚¬ë„ ê³„ì‚°ìœ¼ë¡œ ì†ë„ í–¥ìƒ ì‹¤ì‹œê°„ ì²˜ë¦¬ ì£¼ê¸°ì  ì¬ê³„ì‚° + ìºì‹œë¡œ ëŒ€ì‘ ì¦ë¶„ ì—…ë°ì´íŠ¸ ë°©ì‹ ê³ ë ¤ A/B í…ŒìŠ¤íŠ¸ $K$ê°’, ìœ ì‚¬ë„ ì¢…ë¥˜, ì •ê·œí™” ë°©ì‹ íŠœë‹ ì¶”ì²œ í’ˆì§ˆê³¼ ì„±ëŠ¥ ê°„ ê· í˜•ì  ì°¾ê¸° Python êµ¬í˜„ ì˜ˆì‹œ ``python import numpy as np from scipy.spatial.distance import cosine from scipy.stats import pearsonr class UserBasedCF: def __init__(self, k=10): self.k = k self.user_similarities = None def fit(self, ratings_matrix): \"\"\"ì‚¬ìš©ì ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°\"\"\" n_users = ratings_matrix.shape[0] self.user_similarities = np.zeros((n_users, n_users)) for i in range(n_users): for j in range(i+1, n_users): # ê³µí†µ í‰ê°€ ì•„ì´í…œ ì°¾ê¸° common_items = np.where((ratings_matrix[i] != 0) & (ratings_matrix[j] != 0))[0] if len(common_items) < 2: similarity = 0 else: # Pearson correlation ê³„ì‚° user_i_ratings = ratings_matrix[i, common_items] user_j_ratings = ratings_matrix[j, common_items] try: similarity, _ = pearsonr(user_i_ratings, user_j_ratings) if np.isnan(similarity): similarity = 0 except: similarity = 0 self.user_similarities[i, j] = similarity self.user_similarities[j, i] = similarity def predict(self, user_id, item_id, ratings_matrix): \"\"\"íŠ¹ì • ì‚¬ìš©ìì˜ íŠ¹ì • ì•„ì´í…œ í‰ì  ì˜ˆì¸¡\"\"\" if self.user_similarities is None: raise ValueError(\"Model must be fitted first\") # ì‚¬ìš©ìì˜ í‰ê·  í‰ì  user_ratings = ratings_matrix[user_id] user_mean = np.mean(user_ratings[user_ratings != 0]) # ì´ì›ƒ ì‚¬ìš©ìë“¤ ì°¾ê¸° similarities = self.user_similarities[user_id] neighbors = np.argsort(similarities)[::-1][1:self.k+1] numerator = 0 denominator = 0 for neighbor in neighbors: if similarities[neighbor] <= 0: continue neighbor_rating = ratings_matrix[neighbor, item_id] if neighbor_rating == 0: continue neighbor_mean = np.mean(ratings_matrix[neighbor][ratings_matrix[neighbor] != 0]) numerator += similarities[neighbor] * (neighbor_rating - neighbor_mean) denominator += abs(similarities[neighbor]) if denominator == 0: return user_mean predicted_rating = user_mean + (numerator / denominator) return max(1, min(5, predicted_rating)) # í‰ì  ë²”ìœ„ ì œí•œ `` ê²°ë¡  User-Based CFëŠ” ì¶”ì²œ ì‹œìŠ¤í…œì˜ ê¸°ë³¸ì´ ë˜ëŠ” ì§ê´€ì ì´ê³  íš¨ê³¼ì ì¸ ë°©ë²•ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ë°ì´í„° í¬ì†Œì„±ê³¼ í™•ì¥ì„± í•œê³„ë¡œ ì¸í•´, ëŒ€ê·œëª¨ ì„œë¹„ìŠ¤ì—ì„œëŠ” ë‹¤ë¥¸ ê¸°ë²•(ì•„ì´í…œ ê¸°ë°˜, í–‰ë ¬ ë¶„í•´, ë”¥ëŸ¬ë‹)ê³¼ ê²°í•©í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì‹¤ì œ ì ìš© ì‹œì—ëŠ” ë¹„ì¦ˆë‹ˆìŠ¤ ë„ë©”ì¸, ë°ì´í„° íŠ¹ì„±, ì„±ëŠ¥ ìš”êµ¬ì‚¬í•­ì„ ì¢…í•©ì ìœ¼ë¡œ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. --- ì°¸ê³  ìë£Œ: Collaborative Filtering for Implicit Feedback Datasets Item-based collaborative filtering recommendation algorithms Matrix Factorization Techniques for Recommender Systems"
  },
  {
    "id": "English Study/TOEFL/DefinitionOfArt",
    "title": "Definition of Art",
    "date": "2025-07-15",
    "excerpt": "TOEFL Reading ë¬¸ì œ ê³µë¶€ - ì˜ˆìˆ ì˜ ì •ì˜",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›",
      "OMSCS"
    ],
    "content": "Definition of Art ì˜ˆìˆ ì˜ ì •ì˜ Paragraph 1 <button class=\"toggle-button translations\" data-type=\"paragraph1\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph1-content\"> > How art is defined has varied throughout history, leading to a wide range of sometimes-contradictory meanings. <span class=\"translation\">ì˜ˆìˆ ì´ ì–´ë–»ê²Œ ì •ì˜ë˜ëŠ”ì§€ëŠ” ì—­ì‚¬ë¥¼ í†µí‹€ì–´ ë³€í™”í•´ì™”ìœ¼ë©°, ë•Œë¡œëŠ” ìƒë°˜ë˜ëŠ” ì˜ë¯¸ë“¤ì˜ ê´‘ë²”ìœ„í•œ ìŠ¤í™íŠ¸ëŸ¼ì„ ë§Œë“¤ì–´ëƒˆë‹¤.</span> > In the ancient world, the term was used in a broad sense that was not strictly related to visual arts. <span class=\"translation\">ê³ ëŒ€ ì„¸ê³„ì—ì„œ ì´ ìš©ì–´ëŠ” ì‹œê° ì˜ˆìˆ ì— ì—„ê²©í•˜ê²Œ êµ­í•œë˜ì§€ ì•ŠëŠ” ê´‘ë²”ìœ„í•œ ì˜ë¯¸ë¡œ ì‚¬ìš©ë˜ì—ˆë‹¤.</span> > The application of the word \"art\" exclusively to works in the form of paintings, sculptures, and prints came about only in the Renaissance, and later, in the eighteenth century, the use of the term expanded to include music and poetry. <span class=\"translation\">\"ì˜ˆìˆ \"ì´ë¼ëŠ” ë‹¨ì–´ë¥¼ íšŒí™”, ì¡°ê°, íŒí™” í˜•íƒœì˜ ì‘í’ˆë“¤ì—ë§Œ ë…ì ì ìœ¼ë¡œ ì ìš©í•˜ëŠ” ê²ƒì€ ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ì— ë¹„ë¡œì†Œ ë‚˜íƒ€ë‚¬ìœ¼ë©°, ì´í›„ 18ì„¸ê¸°ì—ëŠ” ì´ ìš©ì–´ì˜ ì‚¬ìš©ì´ ìŒì•…ê³¼ ì‹œë¥¼ í¬í•¨í•˜ë„ë¡ í™•ì¥ë˜ì—ˆë‹¤.</span> > Subsequently, in modern times, a new meaning came into fashion, obscuring the distinction between what is art and what is not. <span class=\"translation\">ê·¸ í›„ í˜„ëŒ€ì— ë“¤ì–´ì„œëŠ” ìƒˆë¡œìš´ ì˜ë¯¸ê°€ ìœ í–‰í•˜ê²Œ ë˜ì—ˆê³ , ë¬´ì—‡ì´ ì˜ˆìˆ ì´ê³  ë¬´ì—‡ì´ ì˜ˆìˆ ì´ ì•„ë‹Œì§€ì˜ êµ¬ë³„ì„ ëª¨í˜¸í•˜ê²Œ ë§Œë“¤ì—ˆë‹¤.</span> </div> Paragraph 2 <button class=\"toggle-button translations\" data-type=\"paragraph2\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph2-content\"> > In the ancient world, there was no clear difference between art and crafts. <span class=\"translation\">ê³ ëŒ€ ì„¸ê³„ì—ì„œëŠ” ì˜ˆìˆ ê³¼ ê³µì˜ˆ ì‚¬ì´ì— ëª…í™•í•œ ì°¨ì´ê°€ ì—†ì—ˆë‹¤.</span> > In Greek and Latin, the terms techne and ars both translated as \"art,\" and they stood for any activity that had rules and required expertise. <span class=\"translation\">ê·¸ë¦¬ìŠ¤ì–´ì™€ ë¼í‹´ì–´ì—ì„œ í…Œí¬ë„¤(techne)ì™€ ì•„ë¥´ìŠ¤(ars)ë¼ëŠ” ìš©ì–´ë“¤ì€ ë‘˜ ë‹¤ \"ì˜ˆìˆ \"ë¡œ ë²ˆì—­ë˜ì—ˆìœ¼ë©°, ê·œì¹™ì„ ê°€ì§€ê³  ì „ë¬¸ì„±ì„ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í™œë™ì„ ì˜ë¯¸í–ˆë‹¤.</span> > They were thus used in reference to a wide range of fields, from medicine and law to blacksmithing. <span class=\"translation\">ë”°ë¼ì„œ ì´ ìš©ì–´ë“¤ì€ ì˜í•™ê³¼ ë²•ë¥ ì—ì„œë¶€í„° ëŒ€ì¥ì¥ì´ ì¼ê¹Œì§€ ê´‘ë²”ìœ„í•œ ë¶„ì•¼ë“¤ì„ ì§€ì¹­í•˜ëŠ” ë° ì‚¬ìš©ë˜ì—ˆë‹¤.</span> > Those engaged in the production of art were viewed as craftsmen, more similar to manual laborers than artists. <span class=\"translation\">ì˜ˆìˆ  ì œì‘ì— ì¢…ì‚¬í•˜ëŠ” ì‚¬ëŒë“¤ì€ ì¥ì¸ìœ¼ë¡œ ì—¬ê²¨ì¡Œìœ¼ë©°, ì˜ˆìˆ ê°€ë³´ë‹¤ëŠ” ìœ¡ì²´ ë…¸ë™ìì— ë” ê°€ê¹Œì› ë‹¤.</span> > To the ancients, art was a pursuit necessitating skills that were learned according to rules, but that were believed to require no unique inspiration. <span class=\"translation\">ê³ ëŒ€ì¸ë“¤ì—ê²Œ ì˜ˆìˆ ì€ ê·œì¹™ì— ë”°ë¼ ìŠµë“í•˜ëŠ” ê¸°ìˆ ì„ í•„ìš”ë¡œ í•˜ëŠ” ì¶”êµ¬ì˜€ì§€ë§Œ, ë…íŠ¹í•œ ì˜ê°ì„ ìš”êµ¬í•˜ì§€ëŠ” ì•ŠëŠ”ë‹¤ê³  ì—¬ê²¨ì¡Œë‹¤.</span> > Painting and sculpture had no special place in higher learning, as only music and poetry were taught as academic subjects. <span class=\"translation\">íšŒí™”ì™€ ì¡°ê°ì€ ê³ ë“± í•™ë¬¸ì—ì„œ íŠ¹ë³„í•œ ìë¦¬ë¥¼ ì°¨ì§€í•˜ì§€ ëª»í–ˆëŠ”ë°, ìŒì•…ê³¼ ì‹œë§Œì´ í•™ë¬¸ ê³¼ëª©ìœ¼ë¡œ ê°€ë¥´ì³ì¡Œê¸° ë•Œë¬¸ì´ë‹¤.</span> > The visual arts were viewed simply as manual crafts, and because the ancients looked down on manual labor, artisans occupied the bottom of the social hierarchy. <span class=\"translation\">ì‹œê° ì˜ˆìˆ ì€ ë‹¨ìˆœíˆ ìˆ˜ê³µ ê¸°ìˆ ë¡œ ì—¬ê²¨ì¡Œìœ¼ë©°, ê³ ëŒ€ì¸ë“¤ì´ ìœ¡ì²´ ë…¸ë™ì„ ì—…ì‹ ì—¬ê²¼ê¸° ë•Œë¬¸ì— ì¥ì¸ë“¤ì€ ì‚¬íšŒ ê³„ì¸µì˜ ë§¨ ì•„ë˜ë¥¼ ì°¨ì§€í–ˆë‹¤.</span> </div> Paragraph 3 <button class=\"toggle-button translations\" data-type=\"paragraph3\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph3-content\"> > The medieval world inherited this view of art, but things began to change during the Renaissance. <span class=\"translation\">ì¤‘ì„¸ ì„¸ê³„ëŠ” ì´ëŸ¬í•œ ì˜ˆìˆ ê´€ì„ ë¬¼ë ¤ë°›ì•˜ì§€ë§Œ, ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ì— ìƒí™©ì´ ë³€í™”í•˜ê¸° ì‹œì‘í–ˆë‹¤.</span> > At the forefront of this transformation was the elevated position of painters, sculptors, and architects. <span class=\"translation\">ì´ ë³€í˜ì˜ ìµœì „ì„ ì—ëŠ” í™”ê°€, ì¡°ê°ê°€, ê±´ì¶•ê°€ë“¤ì˜ ì§€ìœ„ í–¥ìƒì´ ìˆì—ˆë‹¤.</span> > Renaissance artists increasingly employed scientific methods, and mathematics became central to their trade. <span class=\"translation\">ë¥´ë„¤ìƒìŠ¤ ì˜ˆìˆ ê°€ë“¤ì€ ì ì  ë” ê³¼í•™ì  ë°©ë²•ì„ ì‚¬ìš©í–ˆìœ¼ë©°, ìˆ˜í•™ì´ ê·¸ë“¤ ê¸°ì˜ˆì˜ ì¤‘ì‹¬ì´ ë˜ì—ˆë‹¤.</span> > Because their work was not simply mechanical but also intellectual, artists argued that they were superior to mere craftsmen, and their call for respect began to sway public opinion. <span class=\"translation\">ê·¸ë“¤ì˜ ì‘ì—…ì´ ë‹¨ìˆœíˆ ê¸°ê³„ì ì¸ ê²ƒì´ ì•„ë‹ˆë¼ ì§€ì ì´ê¸°ë„ í–ˆê¸° ë•Œë¬¸ì—, ì˜ˆìˆ ê°€ë“¤ì€ ìì‹ ë“¤ì´ ë‹¨ìˆœí•œ ì¥ì¸ë“¤ë³´ë‹¤ ìš°ì›”í•˜ë‹¤ê³  ì£¼ì¥í–ˆìœ¼ë©°, ì¡´ê²½ì„ ìš”êµ¬í•˜ëŠ” ê·¸ë“¤ì˜ ì™¸ì¹¨ì´ ì—¬ë¡ ì„ ì›€ì§ì´ê¸° ì‹œì‘í–ˆë‹¤.</span> > Artists gradually gained acceptance as creative geniuses, and their reputation rose to be on par with that of poets. <span class=\"translation\">ì˜ˆìˆ ê°€ë“¤ì€ ì ì°¨ ì°½ì¡°ì  ì²œì¬ë¡œ ì¸ì •ë°›ê²Œ ë˜ì—ˆìœ¼ë©°, ê·¸ë“¤ì˜ ëª…ì„±ì€ ì‹œì¸ë“¤ê³¼ ë™ë“±í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì˜¬ë¼ê°”ë‹¤.</span> > Painting, sculpture, and architecture were given their own place in academies, and art became synonymous with the label \"classical art.\" <span class=\"translation\">íšŒí™”, ì¡°ê°, ê±´ì¶•ì€ í•™ì›ì—ì„œ ìì‹ ë“¤ë§Œì˜ ìë¦¬ë¥¼ ë¶€ì—¬ë°›ì•˜ìœ¼ë©°, ì˜ˆìˆ ì€ \"ê³ ì „ ì˜ˆìˆ \"ì´ë¼ëŠ” ë¼ë²¨ê³¼ ë™ì˜ì–´ê°€ ë˜ì—ˆë‹¤.</span> > During the 1700s, the definition was slightly expanded as the concept of fine art emerged. <span class=\"translation\">1700ë…„ëŒ€ì—ëŠ” ìˆœìˆ˜ ì˜ˆìˆ ì˜ ê°œë…ì´ ë“±ì¥í•˜ë©´ì„œ ì •ì˜ê°€ ì•½ê°„ í™•ì¥ë˜ì—ˆë‹¤.</span> > Collectively, the classical arts, along with music and poetry, came to represent the fine arts. <span class=\"translation\">ì¢…í•©ì ìœ¼ë¡œ, ê³ ì „ ì˜ˆìˆ ë“¤ì€ ìŒì•…ê³¼ ì‹œì™€ í•¨ê»˜ ìˆœìˆ˜ ì˜ˆìˆ ì„ ëŒ€í‘œí•˜ê²Œ ë˜ì—ˆë‹¤.</span> </div> Paragraph 4 <button class=\"toggle-button translations\" data-type=\"paragraph4\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph4-content\"> > It was also in the Renaissance that the term \"artisan\" first appeared. <span class=\"translation\">\"ì¥ì¸\"ì´ë¼ëŠ” ìš©ì–´ê°€ ì²˜ìŒ ë“±ì¥í•œ ê²ƒë„ ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ì˜€ë‹¤.</span> > The word was used to clearly distinguish between arts and crafts. <span class=\"translation\">ì´ ë‹¨ì–´ëŠ” ì˜ˆìˆ ê³¼ ê³µì˜ˆë¥¼ ëª…í™•íˆ êµ¬ë³„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ì—ˆë‹¤.</span> > Art required academic training, and its goal was to create perfect and idealized representations of reality that followed established principles of beauty. <span class=\"translation\">ì˜ˆìˆ ì€ í•™ë¬¸ì  í›ˆë ¨ì„ ìš”êµ¬í–ˆìœ¼ë©°, ê·¸ ëª©í‘œëŠ” í™•ë¦½ëœ ì•„ë¦„ë‹¤ì›€ì˜ ì›ì¹™ì„ ë”°ë¥´ëŠ” í˜„ì‹¤ì˜ ì™„ë²½í•˜ê³  ì´ìƒí™”ëœ í‘œí˜„ì„ ì°½ì¡°í•˜ëŠ” ê²ƒì´ì—ˆë‹¤.</span> > This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes. <span class=\"translation\">ì´ëŠ” êµ¬ìŠ¬ ì„¸ê³µì´ë‚˜ ë°”êµ¬ë‹ˆ ë§Œë“¤ê¸°ì™€ ê°™ì€ ê³µì˜ˆì™€ ëŒ€ì¡°ë˜ëŠ”ë°, ì´ë“¤ì€ ì£¼ë¡œ ì¼ìƒì ì¸ ëª©ì ì„ ìœ„í•´ ì œì‘ë˜ì—ˆë‹¤.</span> > Craftsmen whose production consisted of such practical, or merely decorative, items were called artisans. <span class=\"translation\">ê·¸ëŸ¬í•œ ì‹¤ìš©ì ì´ê±°ë‚˜ ë‹¨ìˆœíˆ ì¥ì‹ì ì¸ ë¬¼í’ˆë“¤ì„ ìƒì‚°í•˜ëŠ” ì¥ì¸ë“¤ì€ ì•„í‹°ì”ì´ë¼ê³  ë¶ˆë ¸ë‹¤.</span> > They often learned their trade from master craftsmen, but they did not study art in the academies. <span class=\"translation\">ê·¸ë“¤ì€ ì¢…ì¢… ìˆ™ë ¨ëœ ì¥ì¸ë“¤ë¡œë¶€í„° ê¸°ìˆ ì„ ë°°ì› ì§€ë§Œ, í•™ì›ì—ì„œ ì˜ˆìˆ ì„ ê³µë¶€í•˜ì§€ëŠ” ì•Šì•˜ë‹¤.</span> > The clear division between arts and crafts saw its peak in the eighteenth century. <span class=\"translation\">ì˜ˆìˆ ê³¼ ê³µì˜ˆ ì‚¬ì´ì˜ ëª…í™•í•œ êµ¬ë¶„ì€ 18ì„¸ê¸°ì— ì ˆì •ì— ë‹¬í–ˆë‹¤.</span> </div> Paragraph 5 <button class=\"toggle-button translations\" data-type=\"paragraph5\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph5-content\"> > In the nineteenth century, modernists sought to promote the idea of \"art for art's sake\" and rejected classical definitions. <span class=\"translation\">19ì„¸ê¸°ì— ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì€ \"ì˜ˆìˆ ì„ ìœ„í•œ ì˜ˆìˆ \"ì´ë¼ëŠ” ì‚¬ìƒì„ ì´‰ì§„í•˜ë ¤ í–ˆìœ¼ë©° ê³ ì „ì  ì •ì˜ë“¤ì„ ê±°ë¶€í–ˆë‹¤.</span> > Artists felt hindered by the conventions of classicism and began to ignore form in favor of stylistic expression. <span class=\"translation\">ì˜ˆìˆ ê°€ë“¤ì€ ê³ ì „ì£¼ì˜ì˜ ê´€ìŠµì— ì˜í•´ ë°©í•´ë°›ëŠ”ë‹¤ê³  ëŠê¼ˆìœ¼ë©° í˜•íƒœë¥¼ ë¬´ì‹œí•˜ê³  ì–‘ì‹ì  í‘œí˜„ì„ ì„ í˜¸í•˜ê¸° ì‹œì‘í–ˆë‹¤.</span> > At the same time, they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society. <span class=\"translation\">ë™ì‹œì—, ê·¸ë“¤ì€ ì˜ˆìˆ ì´ ê·¼ë³¸ì ìœ¼ë¡œ ì‹¤ìš©ì ì´ê³  ì‚¬íšŒì— ìœ ìµí•´ì•¼ í•œë‹¤ê³  ì£¼ì¥í•˜ëŠ” ì¼ë¶€ ë™ì‹œëŒ€ì¸ë“¤ì˜ ì‹ ë…ì„ ê±°ë¶€í–ˆë‹¤.</span> > Modernists denied that purpose, whether moral or political, was an important criterion for art to have merit. <span class=\"translation\">ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì€ ë„ë•ì ì´ë“  ì •ì¹˜ì ì´ë“  ëª©ì ì´ ì˜ˆìˆ ì´ ê°€ì¹˜ë¥¼ ê°–ê¸° ìœ„í•œ ì¤‘ìš”í•œ ê¸°ì¤€ì´ë¼ëŠ” ê²ƒì„ ë¶€ì¸í–ˆë‹¤.</span> > Accordingly, they believed all art had an intrinsic value of its own. <span class=\"translation\">ë”°ë¼ì„œ, ê·¸ë“¤ì€ ëª¨ë“  ì˜ˆìˆ ì´ ê·¸ ìì²´ë¡œ ë‚´ì¬ì  ê°€ì¹˜ë¥¼ ê°€ì§„ë‹¤ê³  ë¯¿ì—ˆë‹¤.</span> > In addition, artists experimented with new techniques and subject matter. <span class=\"translation\">ë˜í•œ, ì˜ˆìˆ ê°€ë“¤ì€ ìƒˆë¡œìš´ ê¸°ë²•ê³¼ ì£¼ì œë¥¼ ì‹¤í—˜í–ˆë‹¤.</span> > They began to include more impressionistic and abstract designs, emphasizing experimental over learned techniques. <span class=\"translation\">ê·¸ë“¤ì€ ë” ì¸ìƒì£¼ì˜ì ì´ê³  ì¶”ìƒì ì¸ ë””ìì¸ì„ í¬í•¨í•˜ê¸° ì‹œì‘í–ˆìœ¼ë©°, ë°°ìš´ ê¸°ë²•ë³´ë‹¤ ì‹¤í—˜ì ì¸ ê²ƒì„ ê°•ì¡°í–ˆë‹¤.</span> </div> Paragraph 6 <button class=\"toggle-button translations\" data-type=\"paragraph6\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph6-content\"> > This new perception of art as an end in itself eventually erased the fragmentation between artist and artisan. <span class=\"translation\">ê·¸ ìì²´ê°€ ëª©ì ì¸ ì˜ˆìˆ ì— ëŒ€í•œ ì´ ìƒˆë¡œìš´ ì¸ì‹ì€ ê²°êµ­ ì˜ˆìˆ ê°€ì™€ ì¥ì¸ ì‚¬ì´ì˜ ë¶„ì—´ì„ ì§€ì›Œë²„ë ¸ë‹¤.</span> > In a clear mockery of the classical tradition, the notion of art was extended to include anything a person creates. <span class=\"translation\">ê³ ì „ì  ì „í†µì— ëŒ€í•œ ëª…ë°±í•œ ì¡°ë¡±ìœ¼ë¡œ, ì˜ˆìˆ ì˜ ê°œë…ì€ ì‚¬ëŒì´ ì°½ì¡°í•˜ëŠ” ëª¨ë“  ê²ƒì„ í¬í•¨í•˜ë„ë¡ í™•ì¥ë˜ì—ˆë‹¤.</span> > Therefore, it effectively eliminated the distinction between crafts and the so-called fine arts. <span class=\"translation\">ë”°ë¼ì„œ, ê·¸ê²ƒì€ ê³µì˜ˆì™€ ì†Œìœ„ ìˆœìˆ˜ ì˜ˆìˆ  ì‚¬ì´ì˜ êµ¬ë³„ì„ íš¨ê³¼ì ìœ¼ë¡œ ì œê±°í–ˆë‹¤.</span> > A person who produced a basket and a person who painted a canvas in classical style were no different; they were both using their skills to create something. <span class=\"translation\">ë°”êµ¬ë‹ˆë¥¼ ë§Œë“  ì‚¬ëŒê³¼ ê³ ì „ì  ì–‘ì‹ìœ¼ë¡œ ìº”ë²„ìŠ¤ì— ê·¸ë¦¼ì„ ê·¸ë¦° ì‚¬ëŒì€ ë‹¤ë¥´ì§€ ì•Šì•˜ë‹¤; ê·¸ë“¤ì€ ë‘˜ ë‹¤ ë¬´ì–¸ê°€ë¥¼ ì°½ì¡°í•˜ê¸° ìœ„í•´ ìì‹ ì˜ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ê³  ìˆì—ˆë‹¤.</span> > This meant that any creative work could be viewed as genuine and prestigious. <span class=\"translation\">ì´ê²ƒì€ ëª¨ë“  ì°½ì¡°ì  ì‘ì—…ì´ ì§„ì •í•˜ê³  ê¶Œìœ„ ìˆëŠ” ê²ƒìœ¼ë¡œ ì—¬ê²¨ì§ˆ ìˆ˜ ìˆìŒì„ ì˜ë¯¸í–ˆë‹¤.</span> > Further, art moved beyond the realm of productive creativity. <span class=\"translation\">ë” ë‚˜ì•„ê°€, ì˜ˆìˆ ì€ ìƒì‚°ì  ì°½ì¡°ì„±ì˜ ì˜ì—­ì„ ë„˜ì–´ì„°ë‹¤.</span> > Some artists randomly collected everyday objects and displayed them, completely unmodified, as \"creative sculpture.\" <span class=\"translation\">ì¼ë¶€ ì˜ˆìˆ ê°€ë“¤ì€ ì¼ìƒì ì¸ ë¬¼ê±´ë“¤ì„ ë¬´ì‘ìœ„ë¡œ ìˆ˜ì§‘í•˜ì—¬ ì™„ì „íˆ ìˆ˜ì •í•˜ì§€ ì•Šì€ ì±„ë¡œ \"ì°½ì¡°ì  ì¡°ê°\"ìœ¼ë¡œ ì „ì‹œí–ˆë‹¤.</span> </div> Paragraph 7 <button class=\"toggle-button translations\" data-type=\"paragraph7\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph7-content\"> > Conceptions of art have changed remarkably and created much confusion. <span class=\"translation\">ì˜ˆìˆ ì— ëŒ€í•œ ê°œë…ë“¤ì€ í˜„ì €íˆ ë³€í™”í–ˆìœ¼ë©° ë§ì€ í˜¼ë€ì„ ë§Œë“¤ì–´ëƒˆë‹¤.</span> > Today there is no consensus on the meaning of the term, and it is applied very subjectively. <span class=\"translation\">ì˜¤ëŠ˜ë‚  ê·¸ ìš©ì–´ì˜ ì˜ë¯¸ì— ëŒ€í•œ í•©ì˜ê°€ ì—†ìœ¼ë©°, ê·¸ê²ƒì€ ë§¤ìš° ì£¼ê´€ì ìœ¼ë¡œ ì ìš©ëœë‹¤.</span> > Evidence can be seen in art museums and galleries devoted to everything from primitive artifacts to video productions to commercial graphics. <span class=\"translation\">ì¦ê±°ëŠ” ì›ì‹œ ìœ ë¬¼ì—ì„œë¶€í„° ë¹„ë””ì˜¤ ì œì‘ë¬¼, ìƒì—…ì  ê·¸ë˜í”½ì— ì´ë¥´ê¸°ê¹Œì§€ ëª¨ë“  ê²ƒì„ ë‹¤ë£¨ëŠ” ë¯¸ìˆ ê´€ê³¼ ê°¤ëŸ¬ë¦¬ì—ì„œ ë³¼ ìˆ˜ ìˆë‹¤.</span> > Certainly, the question of what constitutes art has become largely subjective. <span class=\"translation\">í™•ì‹¤íˆ, ë¬´ì—‡ì´ ì˜ˆìˆ ì„ êµ¬ì„±í•˜ëŠ”ê°€ë¼ëŠ” ì§ˆë¬¸ì€ ëŒ€ì²´ë¡œ ì£¼ê´€ì ì¸ ê²ƒì´ ë˜ì—ˆë‹¤.</span> </div> --- ğŸ“š í•µì‹¬ ë‹¨ì–´ ì‚¬ì „ <button class=\"toggle-button vocabulary\" data-type=\"vocabulary\">ğŸ” ë‹¨ì–´ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"vocabulary-content\"> ğŸ¨ ì˜ˆìˆ ì‚¬ ê´€ë ¨ ìš©ì–´ contradictory <span class=\"meaning\">(ìƒë°˜ë˜ëŠ”, ëª¨ìˆœì ì¸) - ì„œë¡œ ë°˜ëŒ€ë˜ëŠ”</span> exclusively <span class=\"meaning\">(ë…ì ì ìœ¼ë¡œ, ì˜¤ì§) - ë‹¤ë¥¸ ê²ƒì„ ë°°ì œí•˜ê³ </span> Renaissance <span class=\"meaning\">(ë¥´ë„¤ìƒìŠ¤) - 14-16ì„¸ê¸° ìœ ëŸ½ ë¬¸ì˜ˆë¶€í¥ì‹œëŒ€</span> obscure <span class=\"meaning\">(ëª¨í˜¸í•˜ê²Œ í•˜ë‹¤) - ë¶ˆë¶„ëª…í•˜ê²Œ ë§Œë“¤ë‹¤</span> techne <span class=\"meaning\">(í…Œí¬ë„¤) - ê·¸ë¦¬ìŠ¤ì–´ë¡œ \"ê¸°ìˆ , ê¸°ì˜ˆ\"</span> ars <span class=\"meaning\">(ì•„ë¥´ìŠ¤) - ë¼í‹´ì–´ë¡œ \"ê¸°ìˆ , ì˜ˆìˆ \"</span> expertise <span class=\"meaning\">(ì „ë¬¸ì„±) - ì „ë¬¸ì  ê¸°ìˆ ì´ë‚˜ ì§€ì‹</span> blacksmithing <span class=\"meaning\">(ëŒ€ì¥ì¥ì´ ì¼) - ì² ì„ ë‹¤ë£¨ëŠ” ê¸°ìˆ </span> craftsmen <span class=\"meaning\">(ì¥ì¸) - ìˆ˜ê³µì—…ì— ì¢…ì‚¬í•˜ëŠ” ì‚¬ëŒ</span> manual laborers <span class=\"meaning\">(ìœ¡ì²´ ë…¸ë™ì) - ì†ìœ¼ë¡œ ì¼í•˜ëŠ” ì‚¬ëŒë“¤</span> ğŸ›ï¸ í•™ë¬¸/êµìœ¡ ê´€ë ¨ ìš©ì–´ academic subjects <span class=\"meaning\">(í•™ë¬¸ ê³¼ëª©) - í•™êµì—ì„œ ê°€ë¥´ì¹˜ëŠ” ì •ê·œ ê³¼ëª©</span> higher learning <span class=\"meaning\">(ê³ ë“± í•™ë¬¸) - ëŒ€í•™ ìˆ˜ì¤€ì˜ êµìœ¡</span> social hierarchy <span class=\"meaning\">(ì‚¬íšŒ ê³„ì¸µ) - ì‚¬íšŒì˜ ë“±ê¸‰ êµ¬ì¡°</span> academies <span class=\"meaning\">(í•™ì›, ì•„ì¹´ë°ë¯¸) - ì˜ˆìˆ ì´ë‚˜ í•™ë¬¸ì„ ê°€ë¥´ì¹˜ëŠ” ê¸°ê´€</span> classical art <span class=\"meaning\">(ê³ ì „ ì˜ˆìˆ ) - ì „í†µì ì¸ ì–‘ì‹ì„ ë”°ë¥´ëŠ” ì˜ˆìˆ </span> fine arts <span class=\"meaning\">(ìˆœìˆ˜ ì˜ˆìˆ ) - ì‹¤ìš©ì„±ë³´ë‹¤ ë¯¸ì  ê°€ì¹˜ë¥¼ ì¶”êµ¬í•˜ëŠ” ì˜ˆìˆ </span> ğŸ”§ ê¸°ìˆ /ê³µì˜ˆ ê´€ë ¨ ìš©ì–´ artisan <span class=\"meaning\">(ì•„í‹°ì”, ìˆ˜ê³µì—…ì) - ìˆ™ë ¨ëœ ìˆ˜ê³µì—… ì¢…ì‚¬ì</span> beadwork <span class=\"meaning\">(êµ¬ìŠ¬ ì„¸ê³µ) - êµ¬ìŠ¬ì„ ì´ìš©í•œ ê³µì˜ˆ</span> basket making <span class=\"meaning\">(ë°”êµ¬ë‹ˆ ë§Œë“¤ê¸°) - ë°”êµ¬ë‹ˆ ì œì‘ ê¸°ìˆ </span> decorative <span class=\"meaning\">(ì¥ì‹ì ì¸) - ê¾¸ë¯¸ëŠ” ëª©ì ì˜</span> master craftsmen <span class=\"meaning\">(ìˆ™ë ¨ ì¥ì¸) - ê¸°ìˆ ì„ ì™„ì „íˆ ìµíŒ ì¥ì¸</span> ğŸ’­ ì² í•™/ì‚¬ìƒ ê´€ë ¨ ìš©ì–´ modernists <span class=\"meaning\">(ëª¨ë”ë‹ˆìŠ¤íŠ¸) - í˜„ëŒ€ì£¼ì˜ìë“¤</span> art for art's sake <span class=\"meaning\">(ì˜ˆìˆ ì„ ìœ„í•œ ì˜ˆìˆ ) - ì˜ˆìˆ ì˜ ììœ¨ì„±ì„ ì£¼ì¥í•˜ëŠ” ì‚¬ìƒ</span> conventions <span class=\"meaning\">(ê´€ìŠµ) - ì „í†µì ìœ¼ë¡œ ë”°ë¥´ëŠ” ë°©ì‹</span> classicism <span class=\"meaning\">(ê³ ì „ì£¼ì˜) - ê³ ì „ì  ì–‘ì‹ì„ ë”°ë¥´ëŠ” ì˜ˆìˆ  ì‚¬ì¡°</span> stylistic expression <span class=\"meaning\">(ì–‘ì‹ì  í‘œí˜„) - ê°œì¸ì  ìŠ¤íƒ€ì¼ì˜ í‘œí˜„</span> utilitarian <span class=\"meaning\">(ì‹¤ìš©ì£¼ì˜ì ì¸) - ì‹¤ìš©ì„±ì„ ì¤‘ì‹œí•˜ëŠ”</span> intrinsic value <span class=\"meaning\">(ë‚´ì¬ì  ê°€ì¹˜) - ê·¸ ìì²´ê°€ ê°€ì§„ ê°€ì¹˜</span> impressionistic <span class=\"meaning\">(ì¸ìƒì£¼ì˜ì ì¸) - ì¸ìƒì£¼ì˜ ì–‘ì‹ì˜</span> abstract <span class=\"meaning\">(ì¶”ìƒì ì¸) - êµ¬ì²´ì ì´ì§€ ì•Šì€</span> ğŸŒ í˜„ëŒ€ ì˜ˆìˆ  ê´€ë ¨ ìš©ì–´ fragmentation <span class=\"meaning\">(ë¶„ì—´, íŒŒí¸í™”) - ë‚˜ë‰˜ì–´ì§„ ìƒíƒœ</span> mockery <span class=\"meaning\">(ì¡°ë¡±) - ë¹„ì›ƒìŒ</span> genuine <span class=\"meaning\">(ì§„ì •í•œ) - ê°€ì§œê°€ ì•„ë‹Œ</span> prestigious <span class=\"meaning\">(ê¶Œìœ„ ìˆëŠ”) - ëª…ì„±ì´ ë†’ì€</span> realm <span class=\"meaning\">(ì˜ì—­) - ë¶„ì•¼ë‚˜ ë²”ìœ„</span> unmodified <span class=\"meaning\">(ìˆ˜ì •ë˜ì§€ ì•Šì€) - ë³€í™”ì‹œí‚¤ì§€ ì•Šì€</span> consensus <span class=\"meaning\">(í•©ì˜) - ì˜ê²¬ì˜ ì¼ì¹˜</span> subjectively <span class=\"meaning\">(ì£¼ê´€ì ìœ¼ë¡œ) - ê°œì¸ì  ê²¬í•´ì— ë”°ë¼</span> primitive artifacts <span class=\"meaning\">(ì›ì‹œ ìœ ë¬¼) - ê³ ëŒ€ì˜ ì¸ê³µ ì œì‘ë¬¼</span> constitutes <span class=\"meaning\">(êµ¬ì„±í•˜ë‹¤) - ì´ë£¨ë‹¤, í˜•ì„±í•˜ë‹¤</span> </div> --- ğŸ’¬ ì¤‘ìš”í•œ ì˜ì–´ í‘œí˜„ë“¤ <button class=\"toggle-button expressions\" data-type=\"expressions\">ğŸ” í‘œí˜„ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"expressions-content\"> ğŸ“Š ë³€í™”/ë°œì „ í‘œí˜„ has varied throughout history <span class=\"meaning\">(ì—­ì‚¬ë¥¼ í†µí‹€ì–´ ë³€í™”í•´ì™”ë‹¤) - \"How art is defined has varied throughout history\"</span> leading to <span class=\"meaning\">(~ë¡œ ì´ì–´ì§€ëŠ”) - \"leading to a wide range of meanings\"</span> came about <span class=\"meaning\">(ìƒê²¨ë‚˜ë‹¤, ë°œìƒí•˜ë‹¤) - \"came about only in the Renaissance\"</span> came into fashion <span class=\"meaning\">(ìœ í–‰í•˜ê²Œ ë˜ë‹¤) - \"a new meaning came into fashion\"</span> began to change <span class=\"meaning\">(ë³€í™”í•˜ê¸° ì‹œì‘í–ˆë‹¤) - \"things began to change during the Renaissance\"</span> at the forefront of <span class=\"meaning\">(~ì˜ ìµœì „ì„ ì—) - \"At the forefront of this transformation\"</span> ğŸ¯ ëŒ€ì¡°/ë¹„êµ í‘œí˜„ in contrast to <span class=\"meaning\">(~ì™€ ëŒ€ì¡°ì ìœ¼ë¡œ) - \"This is in contrast to crafts\"</span> more similar to <span class=\"meaning\">(~ì— ë” ê°€ê¹Œìš´) - \"more similar to manual laborers than artists\"</span> on par with <span class=\"meaning\">(~ì™€ ë™ë“±í•œ) - \"on par with that of poets\"</span> as opposed to <span class=\"meaning\">(~ì™€ëŠ” ë°˜ëŒ€ë¡œ) - implied in contrasts</span> in a clear mockery of <span class=\"meaning\">(~ì— ëŒ€í•œ ëª…ë°±í•œ ì¡°ë¡±ìœ¼ë¡œ) - \"In a clear mockery of the classical tradition\"</span> â° ì‹œê°„/ìˆœì„œ í‘œí˜„ subsequently <span class=\"meaning\">(ê·¸ í›„ì—) - \"Subsequently, in modern times\"</span> gradually <span class=\"meaning\">(ì ì°¨ì ìœ¼ë¡œ) - \"Artists gradually gained acceptance\"</span> increasingly <span class=\"meaning\">(ì ì  ë”) - \"increasingly employed scientific methods\"</span> eventually <span class=\"meaning\">(ê²°êµ­) - \"eventually erased the fragmentation\"</span> saw its peak <span class=\"meaning\">(ì ˆì •ì— ë‹¬í–ˆë‹¤) - \"saw its peak in the eighteenth century\"</span> ğŸ¨ ì˜ˆìˆ  ê´€ë ¨ í‘œí˜„ applied exclusively to <span class=\"meaning\">(~ì—ë§Œ ë…ì ì ìœ¼ë¡œ ì ìš©ë˜ë‹¤) - \"applied exclusively to works\"</span> became synonymous with <span class=\"meaning\">(~ì™€ ë™ì˜ì–´ê°€ ë˜ë‹¤) - \"became synonymous with classical art\"</span> came to represent <span class=\"meaning\">(~ë¥¼ ëŒ€í‘œí•˜ê²Œ ë˜ë‹¤) - \"came to represent the fine arts\"</span> in favor of <span class=\"meaning\">(~ì„ ì„ í˜¸í•˜ì—¬) - \"in favor of stylistic expression\"</span> beyond the realm of <span class=\"meaning\">(~ì˜ ì˜ì—­ì„ ë„˜ì–´ì„œ) - \"beyond the realm of productive creativity\"</span> ğŸ“ í•™ìˆ ì  í‘œí˜„ stood for <span class=\"meaning\">(~ë¥¼ ì˜ë¯¸í–ˆë‹¤) - \"they stood for any activity\"</span> were viewed as <span class=\"meaning\">(~ë¡œ ì—¬ê²¨ì¡Œë‹¤) - \"were viewed as craftsmen\"</span> necessitating <span class=\"meaning\">(~ë¥¼ í•„ìš”ë¡œ í•˜ëŠ”) - \"necessitating skills\"</span> occupied the bottom of <span class=\"meaning\">(~ì˜ ë§¨ ì•„ë˜ë¥¼ ì°¨ì§€í–ˆë‹¤) - \"occupied the bottom of the social hierarchy\"</span> inherited this view <span class=\"meaning\">(ì´ ê²¬í•´ë¥¼ ë¬¼ë ¤ë°›ì•˜ë‹¤) - \"The medieval world inherited this view\"</span> ğŸ” ê°•ì¡°/ì„¤ëª… í‘œí˜„ it was also... that <span class=\"meaning\">(~í•œ ê²ƒë„ ë°”ë¡œ ...ì˜€ë‹¤) - \"It was also in the Renaissance that...\"</span> accordingly <span class=\"meaning\">(ë”°ë¼ì„œ) - \"Accordingly, they believed\"</span> in addition <span class=\"meaning\">(ë˜í•œ) - \"In addition, artists experimented\"</span> therefore <span class=\"meaning\">(ë”°ë¼ì„œ) - \"Therefore, it effectively eliminated\"</span> certainly <span class=\"meaning\">(í™•ì‹¤íˆ) - \"Certainly, the question has become\"</span> ğŸ’­ ì˜ê²¬/íŒë‹¨ í‘œí˜„ argued that <span class=\"meaning\">(~ë¼ê³  ì£¼ì¥í–ˆë‹¤) - \"artists argued that they were superior\"</span> believed to require <span class=\"meaning\">(~ë¥¼ ìš”êµ¬í•œë‹¤ê³  ì—¬ê²¨ì¡Œë‹¤) - \"believed to require no unique inspiration\"</span> denied that <span class=\"meaning\">(~ë¼ëŠ” ê²ƒì„ ë¶€ì¸í–ˆë‹¤) - \"Modernists denied that purpose\"</span> sought to promote <span class=\"meaning\">(~ë¥¼ ì´‰ì§„í•˜ë ¤ í–ˆë‹¤) - \"sought to promote the idea\"</span> felt hindered by <span class=\"meaning\">(~ì— ì˜í•´ ë°©í•´ë°›ëŠ”ë‹¤ê³  ëŠê¼ˆë‹¤) - \"felt hindered by the conventions\"</span> </div> --- ë¬¸ì œí’€ì´ TOEFL Reading Comprehension Questions Question 12 Which of the following is consistent with the ancient view of art as described in paragraph 2? (A) Visual art was not an undertaking worthy of esteem. (B) Art was an activity that required special inspiration. (C) Visual art occupied a special place in higher learning. (D) Art applied to any pursuit involving formal education. <button class=\"toggle-button answer\" data-type=\"answer12\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer12-content\"> ì •ë‹µ: (A) Visual art was not an undertaking worthy of esteem. í•´ì„¤: 2ë‹¨ë½ì—ì„œ ê³ ëŒ€ì¸ë“¤ì˜ ì˜ˆìˆ ê´€: í•µì‹¬ ê·¼ê±°: \"The visual arts were viewed simply as manual crafts\" \"because the ancients looked down on manual labor, artisans occupied the bottom of the social hierarchy\" \"Those engaged in the production of art were viewed as craftsmen, more similar to manual laborers than artists\" ê³ ëŒ€ì¸ë“¤ì€ ì‹œê° ì˜ˆìˆ ì„ ë‹¨ìˆœí•œ ìˆ˜ê³µ ê¸°ìˆ ë¡œ ì—¬ê²¼ê³ , ìœ¡ì²´ ë…¸ë™ì„ ì—…ì‹ ì—¬ê²¼ê¸° ë•Œë¬¸ì— ì‹œê° ì˜ˆìˆ ì€ ì¡´ê²½ë°›ì„ ë§Œí•œ ì¼ì´ ì•„ë‹ˆì—ˆìŠµë‹ˆë‹¤. ì˜¤ë‹µ ë¶„ì„: (B) \"believed to require no unique inspiration\" - íŠ¹ë³„í•œ ì˜ê°ì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ê³  ì—¬ê¹€ (C) \"Painting and sculpture had no special place in higher learning\" - ê³ ë“± í•™ë¬¸ì—ì„œ íŠ¹ë³„í•œ ìë¦¬ ì—†ìŒ (D) ê³µì‹ êµìœ¡ì´ ì•„ë‹Œ \"rules and required expertise\"ë¥¼ ìš”êµ¬í•˜ëŠ” ëª¨ë“  í™œë™ì— ì ìš© </div> Question 13 Which of the following is NOT listed as an example of fine art in paragraph 3? (A) Sculpture (B) Architecture (C) Mathematics (D) Music <button class=\"toggle-button answer\" data-type=\"answer13\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer13-content\"> ì •ë‹µ: (C) Mathematics í•´ì„¤: 3ë‹¨ë½ì—ì„œ ì–¸ê¸‰ëœ ìˆœìˆ˜ ì˜ˆìˆ (fine arts)ë“¤: ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰ëœ ê²ƒë“¤: \"Painting, sculpture, and architecture were given their own place in academies\" \"Collectively, the classical arts, along with music and poetry, came to represent the fine arts\" ìˆ˜í•™ì˜ ì—­í• : \"mathematics became central to their trade\"ë¡œ ì–¸ê¸‰ë˜ì§€ë§Œ, ì´ëŠ” ì˜ˆìˆ ê°€ë“¤ì´ ì‚¬ìš©í•˜ëŠ” ë„êµ¬ë¡œì„œì˜ ìˆ˜í•™ì´ì§€, ìˆ˜í•™ ìì²´ê°€ ìˆœìˆ˜ ì˜ˆìˆ ë¡œ ë¶„ë¥˜ëœ ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ìˆœìˆ˜ ì˜ˆìˆ ë¡œ ì–¸ê¸‰ëœ ê²ƒë“¤: (A) Sculpture âœ“ - ëª…ì‹œì  ì–¸ê¸‰ (B) Architecture âœ“ - ëª…ì‹œì  ì–¸ê¸‰ (D) Music âœ“ - \"along with music and poetry\" ì–¸ê¸‰ </div> Question 14 Which of the following is a distinguishing feature of crafts according to paragraph 4? (A) They were made for decoration. (B) They were produced in large quantities. (C) They were produced by artisans in the academies. (D) They were made to closely follow aesthetic principles. <button class=\"toggle-button answer\" data-type=\"answer14\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer14-content\"> ì •ë‹µ: (A) They were made for decoration. í•´ì„¤: 4ë‹¨ë½ì—ì„œ ê³µì˜ˆì˜ íŠ¹ì§•: í•µì‹¬ ê·¼ê±°: \"This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes\" \"Craftsmen whose production consisted of such practical, or merely decorative, items were called artisans\" ê³µì˜ˆëŠ” ì‹¤ìš©ì (practical)ì´ê±°ë‚˜ ë‹¨ìˆœíˆ ì¥ì‹ì (decorative)ì¸ ëª©ì ìœ¼ë¡œ ë§Œë“¤ì–´ì¡Œë‹¤ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì˜ˆìˆ ê³¼ì˜ ëŒ€ì¡°: ì˜ˆìˆ : \"create perfect and idealized representations of reality that followed established principles of beauty\" ê³µì˜ˆ: \"fashioned largely for everyday purposes\" + \"merely decorative\" ì˜¤ë‹µ ë¶„ì„: (B) ëŒ€ëŸ‰ ìƒì‚°ì— ëŒ€í•œ ì–¸ê¸‰ ì—†ìŒ (C) \"they did not study art in the academies\" - í•™ì›ì—ì„œ ê³µë¶€í•˜ì§€ ì•ŠìŒ (D) ë¯¸ì  ì›ì¹™ì„ ë”°ë¥´ëŠ” ê²ƒì€ ì˜ˆìˆ ì˜ íŠ¹ì§• </div> Question 15 The word \"convictions\" in the passage is closest in meaning to (A) intense oppositions (B) strong beliefs (C) wrong assumptions (D) feeble suggestions <button class=\"toggle-button answer\" data-type=\"answer15\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer15-content\"> ì •ë‹µ: (B) strong beliefs í•´ì„¤: 5ë‹¨ë½ì˜ ë¬¸ë§¥ì—ì„œ \"convictions\" ì‚¬ìš©: ì›ë¬¸: \"they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society\" ë¬¸ë§¥ ë¶„ì„: ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ ê±°ë¶€í•œ ê²ƒì€ ë™ì‹œëŒ€ì¸ë“¤ì˜ ì‹ ë…/í™•ì‹ ì´ì—ˆìŠµë‹ˆë‹¤. ì´ë“¤ì€ ì˜ˆìˆ ì´ ì‹¤ìš©ì ì´ê³  ì‚¬íšŒì— ìœ ìµí•´ì•¼ í•œë‹¤ê³  ê°•í•˜ê²Œ ë¯¿ì—ˆë˜ ì‚¬ëŒë“¤ì´ì—ˆìŠµë‹ˆë‹¤. ì–´íœ˜ ë¶„ì„: conviction = í™•ì‹ , ì‹ ë… (ê°•í•œ ë¯¿ìŒ) (A) intense oppositions: ê°•í•œ ë°˜ëŒ€ - ë¬¸ë§¥ì— ë§ì§€ ì•ŠìŒ (B) strong beliefs: ê°•í•œ ë¯¿ìŒ âœ“ (C) wrong assumptions: ì˜ëª»ëœ ê°€ì • - ê°€ì¹˜ íŒë‹¨ì  (D) feeble suggestions: ì•½í•œ ì œì•ˆ - convictionì˜ ê°•í•œ ì˜ë¯¸ì™€ ë°˜ëŒ€ </div> Question 16 What can be inferred from paragraph 5 about artists in the nineteenth century? (A) Some of them thought that the primary value of art was in its social purpose. (B) Most of them approached art as an academic discipline that required learning. (C) They placed a greater emphasis on aesthetics than did classical artists. (D) They viewed moral and political art as having no intrinsic value. <button class=\"toggle-button answer\" data-type=\"answer16\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer16-content\"> ì •ë‹µ: (A) Some of them thought that the primary value of art was in its social purpose. í•´ì„¤: 5ë‹¨ë½ì—ì„œ 19ì„¸ê¸° ì˜ˆìˆ ê°€ë“¤ì˜ ìƒí™©: í•µì‹¬ ê·¼ê±°: \"At the same time, they rejected the convictions of some of their contemporaries, who claimed that art should be fundamentally utilitarian and beneficial to society\" ì´ ë¬¸ì¥ì€ ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ ê±°ë¶€í•œ ë™ì‹œëŒ€ì¸ë“¤ì˜ ê²¬í•´ë¥¼ ì„¤ëª…í•©ë‹ˆë‹¤. ì¦‰, 19ì„¸ê¸°ì—ëŠ” ì¼ë¶€ ì˜ˆìˆ ê°€ë“¤ì´ ì˜ˆìˆ ì˜ ì£¼ëœ ê°€ì¹˜ê°€ ì‚¬íšŒì  ëª©ì ì— ìˆë‹¤ê³  ìƒê°í–ˆë‹¤ëŠ” ê²ƒì„ ì¶”ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¦¬ êµ¬ì¡°: ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ \"art for art's sake\"ë¥¼ ì£¼ì¥ ê·¸ë“¤ì´ ê±°ë¶€í•œ ê²ƒ: ì˜ˆìˆ ì´ ì‹¤ìš©ì ì´ê³  ì‚¬íšŒì— ìœ ìµí•´ì•¼ í•œë‹¤ëŠ” ê²¬í•´ â†’ ë”°ë¼ì„œ ê·¸ëŸ° ê²¬í•´ë¥¼ ê°€ì§„ ì‚¬ëŒë“¤ì´ ì¡´ì¬í–ˆìŒì„ ì˜ë¯¸ ì˜¤ë‹µ ë¶„ì„: (B) ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì€ ì˜¤íˆë ¤ ì „í†µì  í•™ë¬¸ì  ì ‘ê·¼ì„ ê±°ë¶€ (C) í˜•íƒœë³´ë‹¤ ì–‘ì‹ì  í‘œí˜„ì„ ê°•ì¡° - ìˆœìˆ˜ ë¯¸í•™ì  ì ‘ê·¼ì€ ì•„ë‹˜ (D) ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì´ ê±°ë¶€í•œ ê²ƒì´ì§€, ëª¨ë“  ì˜ˆìˆ ê°€ê°€ ê·¸ëŸ° ê²ƒì€ ì•„ë‹˜ </div> Question 17 What is the purpose of paragraph 6 in the passage? (A) To introduce the reasons for the changes outlined in the preceding paragraphs (B) To describe the resulting outcome of the ideas described in the previous paragraph (C) To offer supporting evidence for the ideas introduced in the previous paragraph (D) To provide a transition between the preceding paragraphs and the final paragraph <button class=\"toggle-button answer\" data-type=\"answer17\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer17-content\"> ì •ë‹µ: (B) To describe the resulting outcome of the ideas described in the previous paragraph í•´ì„¤: ë‹¨ë½ë“¤ ê°„ì˜ ë…¼ë¦¬ì  ì—°ê²°: 5ë‹¨ë½ (ì›ì¸): ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì˜ ì‚¬ìƒ \"art for art's sake\" ì˜ˆìˆ ì˜ ë‚´ì¬ì  ê°€ì¹˜ ì‹¤í—˜ì  ê¸°ë²• ì¶”êµ¬ 6ë‹¨ë½ (ê²°ê³¼): ì´ëŸ¬í•œ ì‚¬ìƒì˜ ê²°ê³¼ \"This new perception of art as an end in itself eventually erased the fragmentation between artist and artisan\" \"effectively eliminated the distinction between crafts and the so-called fine arts\" ì˜ˆìˆ  ê°œë…ì˜ ê·¹ë‹¨ì  í™•ì¥ ë…¼ë¦¬ì  êµ¬ì¡°: 5ë‹¨ë½: ëª¨ë”ë‹ˆìŠ¤íŠ¸ ì‚¬ìƒ ì†Œê°œ (ì›ì¸) 6ë‹¨ë½: \"This new perception\" â†’ ê·¸ ê²°ê³¼ ì„¤ëª… ì—°ê²°ì–´: \"This\" = ì• ë‹¨ë½ì˜ ë‚´ìš©ì„ ë°›ìŒ ì˜¤ë‹µ ë¶„ì„: (A) ë³€í™”ì˜ ì´ìœ  ì†Œê°œ - 5ë‹¨ë½ì—ì„œ ì´ë¯¸ ë‹¤ë£¸ (C) ì§€ì§€ ì¦ê±° ì œê³µ - ìƒˆë¡œìš´ ê²°ê³¼ë¥¼ ì„¤ëª…í•˜ëŠ” ê²ƒ (D) ì „í™˜ ì—­í•  - 6ë‹¨ë½ ìì²´ê°€ ì‹¤ì§ˆì  ë‚´ìš©ì„ ë‹´ê³  ìˆìŒ </div> Question 18 The word \"prestigious\" in the passage is closest in meaning to (A) obscure (B) ubiquitous (C) reputable (D) credulous <button class=\"toggle-button answer\" data-type=\"answer18\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer18-content\"> ì •ë‹µ: (C) reputable í•´ì„¤: 6ë‹¨ë½ì˜ ë¬¸ë§¥ì—ì„œ \"prestigious\" ì‚¬ìš©: ì›ë¬¸: \"This meant that any creative work could be viewed as genuine and prestigious\" ë¬¸ë§¥ ë¶„ì„: ëª¨ë“  ì°½ì¡°ì  ì‘ì—…ì´ ì§„ì •í•˜ê³ (genuine) ê¶Œìœ„ ìˆëŠ”(prestigious) ê²ƒìœ¼ë¡œ ì—¬ê²¨ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ \"prestigious\"ëŠ” ë†’ì€ ì§€ìœ„ë‚˜ ëª…ì„±ì„ ê°€ì§„ì´ë¼ëŠ” ëœ»ì…ë‹ˆë‹¤. ì–´íœ˜ ë¶„ì„: prestigious = ê¶Œìœ„ ìˆëŠ”, ëª…ì„±ì´ ë†’ì€ (A) obscure: ëª¨í˜¸í•œ, ì˜ ì•Œë ¤ì§€ì§€ ì•Šì€ - ì •ë°˜ëŒ€ ì˜ë¯¸ (B) ubiquitous: ì–´ë””ì—ë‚˜ ìˆëŠ” - ê´€ë ¨ ì—†ìŒ (C) reputable: í‰íŒì´ ì¢‹ì€, ëª…ì„± ìˆëŠ” âœ“ (D) credulous: ì˜ ë¯¿ëŠ”, ìˆœì§„í•œ - ê´€ë ¨ ì—†ìŒ ë™ì˜ì–´ ê´€ê³„: prestigious â‰ˆ reputable â‰ˆ respected â‰ˆ esteemed </div> Question 19 Look at the four squares [â– ] that indicate where the following sentence could be added to the passage. Other examples of crafts included studio pottery and metalworking. Where would the sentence best fit? <button class=\"toggle-button answer\" data-type=\"answer19\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer19-content\"> ì ì ˆí•œ ìœ„ì¹˜: 4ë‹¨ë½ì˜ ì„¸ ë²ˆì§¸ [â– ] ë’¤ í•´ì„¤: ì‚½ì…í•  ë¬¸ì¥: \"Other examples of crafts included studio pottery and metalworking.\" (ê³µì˜ˆì˜ ë‹¤ë¥¸ ì˜ˆì‹œë“¤ë¡œëŠ” ìŠ¤íŠœë””ì˜¤ ë„ìê¸°ì™€ ê¸ˆì† ê³µì˜ˆê°€ í¬í•¨ë˜ì—ˆë‹¤.) 4ë‹¨ë½ì˜ êµ¬ì¡°: \"artisan\" ìš©ì–´ì˜ ë“±ì¥ ì˜ˆìˆ ê³¼ ê³µì˜ˆì˜ êµ¬ë³„ [â–  ì—¬ê¸°] \"This is in contrast to crafts, like beadwork or basket making...\" ì•„í‹°ì”ì˜ ì •ì˜ ì•„í‹°ì”ì˜ êµìœ¡ ë°©ì‹ ì˜ˆìˆ ê³¼ ê³µì˜ˆ êµ¬ë¶„ì˜ ì ˆì • ê°€ì¥ ì ì ˆí•œ ìœ„ì¹˜ ë¶„ì„: ì„¸ ë²ˆì§¸ [â– ] ë’¤ê°€ ê°€ì¥ ì ì ˆ: ì• ë¬¸ì¥: \"This is in contrast to crafts, like beadwork or basket making, which were fashioned largely for everyday purposes\" ì‚½ì… ë¬¸ì¥: \"Other examples of crafts included studio pottery and metalworking\" ë’¤ ë¬¸ì¥: \"Craftsmen whose production consisted of such practical...\" ë…¼ë¦¬ì  ì—°ê²°ì„±: ê³µì˜ˆì˜ ì²« ë²ˆì§¸ ì˜ˆì‹œ ì œì‹œ (beadwork, basket making) â†’ ê³µì˜ˆì˜ ì¶”ê°€ ì˜ˆì‹œ ì œê³µ (pottery, metalworking) â†’ ì´ëŸ¬í•œ ê³µì˜ˆí’ˆì„ ë§Œë“œëŠ” ì‚¬ëŒë“¤ì˜ ëª…ì¹­ ì„¤ëª… \"Other examples\"ë¼ëŠ” í‘œí˜„ì´ ì•ì„œ ì œì‹œëœ ì˜ˆì‹œë“¤ì„ ë°›ì•„ì„œ ì¶”ê°€ ì •ë³´ë¥¼ ì œê³µí•˜ëŠ” ì—­í•  </div> Question 20 Directions: An introductory sentence for a brief summary of the passage is provided below. Complete the summary by selecting the THREE answer choices that express the most important ideas in the passage. Some sentences do not belong in the summary because they express ideas that are not presented in the passage or are minor ideas in the passage. This question is worth 2 points. The term \"art\" has changed so much that its meaning has become rather subjective. Answer Choices: (A) In the Latin and Greek worlds, art was looked down upon as a form of manual labor. (B) Artists of the modern period began to experiment by combining classical and abstract techniques in their art. (C) After the medieval period, the number of artists attending academies rose due to increased prestige in the arts. (D) The ancients applied the term in a broad manner to any rule-based activity that required skill. (E) Modernists, hoping to revolutionize the way art was conceived, completely redefined the term. (F) As artists began to gain status, the distinction between arts and crafts became more apparent. <button class=\"toggle-button answer\" data-type=\"answer20\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer20-content\"> ì •ë‹µ: (D), (E), (F) í•´ì„¤: ì„ íƒëœ ë‹µì•ˆë“¤: (D) The ancients applied the term in a broad manner to any rule-based activity that required skill. âœ“ 2ë‹¨ë½: \"they stood for any activity that had rules and required expertise\" \"from medicine and law to blacksmithing\" ê³ ëŒ€ì˜ ê´‘ë²”ìœ„í•œ ì˜ˆìˆ  ê°œë…ì´ ì£¼ìš” ì£¼ì œ (E) Modernists, hoping to revolutionize the way art was conceived, completely redefined the term. âœ“ 5-6ë‹¨ë½: ëª¨ë”ë‹ˆìŠ¤íŠ¸ë“¤ì˜ í˜ëª…ì  ë³€í™” \"art for art's sake\", ì˜ˆìˆ ê³¼ ê³µì˜ˆì˜ êµ¬ë¶„ ì œê±° í˜„ëŒ€ì  ì˜ˆìˆ  ê°œë…ì˜ í•µì‹¬ ë³€í™” (F) As artists began to gain status, the distinction between arts and crafts became more apparent. âœ“ 3-4ë‹¨ë½: ë¥´ë„¤ìƒìŠ¤ ì‹œëŒ€ ì˜ˆìˆ ê°€ ì§€ìœ„ í–¥ìƒ \"artisan\" ìš©ì–´ ë“±ì¥, ì˜ˆìˆ ê³¼ ê³µì˜ˆì˜ ëª…í™•í•œ êµ¬ë¶„ ì—­ì‚¬ì  ë°œì „ ê³¼ì •ì˜ ì¤‘ìš”í•œ ë‹¨ê³„ ì œì™¸ëœ ë‹µì•ˆë“¤: (A) ê³ ëŒ€ì—ì„œ ì˜ˆìˆ ì´ ì—…ì‹ ì—¬ê²¨ì§„ ê²ƒì€ ë§ì§€ë§Œ, í•µì‹¬ì€ \"ê´‘ë²”ìœ„í•œ ì ìš©\" (B) ê³ ì „ê³¼ ì¶”ìƒ ê¸°ë²•ì˜ ê²°í•©ì€ ì–¸ê¸‰ë˜ì§€ ì•ŠìŒ (C) í•™ì› ì¶œì„ì ìˆ˜ ì¦ê°€ëŠ” ì„¸ë¶€ ì‚¬í•­, ì£¼ìš” ì•„ì´ë””ì–´ ì•„ë‹˜ ìš”ì•½ì˜ ë…¼ë¦¬ì  íë¦„: ê³ ëŒ€: ê´‘ë²”ìœ„í•œ ì ìš© (D) ë¥´ë„¤ìƒìŠ¤: ì§€ìœ„ í–¥ìƒê³¼ êµ¬ë¶„ ëª…í™•í™” (F) í˜„ëŒ€: ëª¨ë”ë‹ˆìŠ¤íŠ¸ì˜ ì¬ì •ì˜ (E) </div>"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-15-ì •ì‹ ë˜‘ë°”ë¡œì°¨ë¦¬ê³ ìˆì–´",
    "title": "ì •ì‹  ë˜‘ë°”ë¡œ ì°¨ë¦¬ê³  ìˆì–´",
    "date": "2025-07-15",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ê³ ë¯¼ ë° ì¡°ì–¸í•˜ëŠ” í‘œí–”"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Be on the lookout : ì •ì‹  ë˜‘ë°”ë¡œ ì°¨ë ¤ Be on the lookout for pickpockets Be on the lookout for voicefishing Be careful with Be careful with your bag on the subway. in + ìë™ì°¨ (íƒ€ì„œ ì•‰ìœ¼ë©´ ë) on + ë¹„í–‰ê¸°, ë°°, ì§€í•˜ì²  passport catch a cold Be careful not to catch a cold Be careful not to burn yourself Be careful not to hurt yourself During this outbreak? outbreak: ì§ˆë³‘ ë°œìƒê¸°ê°„ scammers : ì‚¬ê¸°ê¾¼ Make sure not to go ê°„ë‹¨í•œ ì‘ë¬¸ As usaul, I talk to myself. Hey MK, be on the lookout for ë‚˜íƒœí•¨. Be careful with your habit that do work later (ë¯¸ë£¨ëŠ” ìŠµê´€) Nice work today, just keep this pace. But don`t forget happy is around you. Feel happy with ì‚¬ì†Œí•œ thing around you, and victories you made in today. ìˆ˜ì •ëœ ì‘ë¬¸ (Improved Version) As usual, time for my daily self-talk. Hey MK, don't get lazy on me! Stop putting things off - let's break this procrastination habit once and for all. *procrastination: ê¾¸ë¬¼ê±°ë¦¬ë‹¤ ì˜ˆë¬¸: \"Stop procrastinating and start studying!\" (ë¯¸ë£¨ì§€ ë§ê³  ê³µë¶€ ì‹œì‘í•´!) \"Procrastination is the thief of time.\" (ë¯¸ë£¨ê¸°ëŠ” ì‹œê°„ ë„ë‘‘ì´ë‹¤) \"I need to overcome my procrastination habit.\" (ë¯¸ë£¨ëŠ” ìŠµê´€ì„ ê³ ì³ì•¼ í•´) You crushed it today, keep this energy going! And hey, don't forget to enjoy the little things around you. Take a moment to appreciate the small wins and celebrate what you accomplished today. í”¼ë“œë°± (Feedback) ê°œì„ ëœ ì ë“¤: ë¬¸ë²•ì  ìˆ˜ì • \"As usaul\" â†’ \"As usual\" (ì² ì ì˜¤ë¥˜ ìˆ˜ì •) \"do work later\" â†’ \"Stop putting things off\" (ë” ì§ì ‘ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"happy is around you\" â†’ \"enjoy the little things around you\" (ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ì¥ êµ¬ì¡°) ì–´íœ˜ ì„ íƒ ê°œì„  (ë” ìºì£¼ì–¼í•˜ê³  í˜„ëŒ€ì ìœ¼ë¡œ) \"ë‚˜íƒœí•¨\" â†’ \"don't get lazy on me!\" (ì¹œê·¼í•˜ê³  ì—ë„ˆì§€ ë„˜ì¹˜ëŠ” í‘œí˜„) \"Nice work\" â†’ \"You crushed it\" (í˜„ëŒ€ì ì´ê³  ì„íŒ©íŠ¸ ìˆëŠ” ì¹­ì°¬) \"keep this pace\" â†’ \"keep this energy going\" (ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"ì‚¬ì†Œí•œ thing\" â†’ \"little things\", \"small wins\" (ì¼ìƒì ì¸ í‘œí˜„) ë¬¸ì¥ êµ¬ì¡° ê°œì„  ë„ˆë¬´ ê¸´ ë¬¸ì¥ì„ ì§§ê²Œ ë‚˜ëˆ„ì–´ ì½ê¸° ì‰½ê²Œ ë§Œë“¦ ëª…ë ¹í˜•ì„ ì‚¬ìš©í•´ ë” ì§ì ‘ì ì´ê³  ë™ê¸°ë¶€ì—¬ì ìœ¼ë¡œ í‘œí˜„ ì‹¤ì œ ìê¸° ëŒ€í™”ì—ì„œ ì“¸ ë²•í•œ ìì—°ìŠ¤ëŸ¬ìš´ í†¤ìœ¼ë¡œ ë³€ê²½ ì „ì²´ì ì¸ í†¤ ê°œì„  Formalí•œ í‘œí˜„ì„ ìºì£¼ì–¼í•˜ê³  ì¹œê·¼í•œ í‘œí˜„ìœ¼ë¡œ ë³€ê²½ ê°íƒ„ì‚¬(\"hey\") ì¶”ê°€ë¡œ ë” ìƒë™ê° ìˆëŠ” ëŒ€í™”ì²´ êµ¬í˜„ ì§§ê³  ì„íŒ©íŠ¸ ìˆëŠ” ë¬¸ì¥ìœ¼ë¡œ ë™ê¸°ë¶€ì—¬ íš¨ê³¼ ê·¹ëŒ€í™” í•µì‹¬ í•™ìŠµ í¬ì¸íŠ¸: ìê¸° ëŒ€í™”ëŠ” ìºì£¼ì–¼í•˜ê³  ì§ì ‘ì ì¼ìˆ˜ë¡ ìì—°ìŠ¤ëŸ½ë‹¤ í˜„ëŒ€ì ì¸ ìŠ¬ë­ í‘œí˜„(\"crushed it\")ë„ ì ì ˆíˆ í™œìš©í•˜ê¸° ë„ˆë¬´ ê¸´ ë¬¸ì¥ë³´ë‹¤ëŠ” ì§§ê³  ëª…í™•í•œ ë¬¸ì¥ì´ íš¨ê³¼ì  ê°ì •ì„ ë‹´ì€ í‘œí˜„(\"don't get lazy on me!\")ì´ ë” ë™ê¸°ë¶€ì—¬ê°€ ëœë‹¤ ì•ìœ¼ë¡œ ì—°ìŠµí•  ì : í•œêµ­ì–´ì™€ ì˜ì–´ê°€ ì„ì¸ ë¶€ë¶„ì„ í”¼í•˜ê³  ì™„ì „í•œ ì˜ì–´ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ê¸° í˜„ëŒ€ì ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ì–´ì²´ í‘œí˜„ ë” í•™ìŠµí•˜ê¸° ê°ì •ì„ ë‹´ì€ ìƒë™ê° ìˆëŠ” í‘œí˜„ ì—°ìŠµí•˜ê¸°"
  },
  {
    "id": "Causal Inference/tilde-tau-explained",
    "title": "Why Do We Introduce the Centered Function tilde_tau(x,t)?",
    "date": "2025-07-15",
    "excerpt": "The role of the intermediate, mean-zero CATE in the continuous-treatment R-learner.",
    "category": "Causal Inference",
    "tags": [
      "Concept",
      "R-learner",
      "Continuous Treatment"
    ],
    "content": "The problem in one sentence R-lossë¥¼ ê·¸ëŒ€ë¡œ ìµœì†Œí™”í•˜ë©´ $$ h(x,t) = \\tau(x,t) + s(x) $$ í˜•íƒœë¡œ x ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë§ë¶™ì„ í•¨ìˆ˜ $s(x)$ ê°€ ìˆì–´ë„ ì†ì‹¤ê°’ì´ ë³€í•˜ì§€ ì•ŠëŠ”ë‹¤. ê·¸ë˜ì„œ ìµœì†Œê°’ì´ ë¬´í•œíˆ ë§ì•„ì ¸ í•´ê°€ ìœ ì¼í•˜ì§€ ì•Šë‹¤. --- Where does s(x) come from? R-lossì— $\\mathbb{E}[ h(X,T) \\mid X ]$ í•­ì´ ë“¤ì–´ê°€ë¯€ë¡œ $h(x,t) + s(x)$ ë¥¼ ë„£ì–´ë„ $s(x)$ ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ìƒì‡„ëœë‹¤. ê·¸ ê²°ê³¼ ì†ì‹¤ê°’ì´ ë™ì¼í•˜ë‹ˆ $s(x)$ ë¥¼ ì–¼ë§ˆë“ ì§€ ë¶™ì¼ ìˆ˜ ìˆë‹¤. ì´ê²ƒì´ non-identification(ì‹ë³„ ë¶ˆëŠ¥) ë¬¸ì œì˜ ê·¼ì›ì´ë‹¤. --- Idea: â€œcenterâ€ the function ì¤‘ê°„ í•¨ìˆ˜ $\\tilde{\\tau}(x,t)$ ë¥¼ ë‹¤ìŒì²˜ëŸ¼ ì •ì˜í•œë‹¤. $$ \\tilde{\\tau}(x,t) = \\tau(x,t) - \\mathbb{E}[ \\tau(X,T) \\mid X = x ] $$ ì¦‰, ê° x ë§ˆë‹¤ t ì „ì—­ í‰ê· ì´ 0 ì´ ë˜ë„ë¡ ì¤‘ì‹¬í™”í•œë‹¤. mean-zero ì„±ì§ˆ ë•Œë¬¸ì— $h(x,t)$ ì™€ $h(x,t)+s(x)$ ì˜ ì°¨ì´ì ì´ R-loss ì•ˆì—ì„œëŠ” ì§€ì›Œì§€ì§€ ì•ŠëŠ”ë‹¤. ì‹¤ì œë¡œ $h(X,T)+s(X) - \\mathbb{E}[h(X,T)+s(X)\\mid X] = h(X,T)-\\mathbb{E}[h(X,T)\\mid X]$ ì´ë¯€ë¡œ, R-loss ê°’ì€ ê·¸ëŒ€ë¡œë‹¤. ë”°ë¼ì„œ ì¤‘ì‹¬í™”ë§Œìœ¼ë¡œëŠ” ì‹ë³„ ë¬¸ì œê°€ ì™„ì „íˆ ì‚¬ë¼ì§€ì§€ ì•ŠëŠ”ë‹¤. ê²°ì •ì ì¸ ì—´ì‡ ëŠ” ë‹¤ìŒ ë‹¨ê³„ì˜ L2 ì •ê·œí™”ë‹¤. L2 íŒ¨ë„í‹° $\\rho\\,\\ h\\ _2^2$ ê°€ ì¶”ê°€ë˜ë©´, $s(x)$ ë¥¼ ë¶™ì´ëŠ” ìˆœê°„ ë…¸ë¦„ì´ ì»¤ì ¸ì„œ ëª©ì í•¨ìˆ˜ê°€ ì¦ê°€í•œë‹¤. ìµœì í™”ëŠ” $s(x)=0$ ì„ ì„ íƒí•´ ìœ ì¼í•´ë¥¼ ë§Œë“ ë‹¤. ì¦‰ $\\tilde{\\tau}$ ëŠ” \"mean-zero\" ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” í›„ë³´ ì¤‘ í•˜ë‚˜ì´ë©°, ì‹ë³„ì„ ì™„ì„±í•˜ëŠ” ê²ƒì€ ì •ê·œí™”ë¼ëŠ” ì ì„ ê¸°ì–µí•´ì•¼ í•œë‹¤. --- How regularisation fits in Step-1 (Tikhonov) ì •ê·œí™”ëœ ìµœì í™” ë¬¸ì œë¥¼ í‘¼ë‹¤. $$ \\tau_{\\rho} = \\operatorname{arg\\,min}_{h} \\Bigl\\{ \\text{R-loss}(h) + \\rho \\, \\ h \\ _2^2 \\Bigr\\} $$ L2 íŒ¨ë„í‹°ê°€ ë“¤ì–´ê°€ë©´ ë¬¸ì œê°€ ì—„ë°€íˆ convex í•´ì ¸ì„œ ìœ ì¼í•œ í•´ê°€ ë³´ì¥ëœë‹¤. Step-2 (Scaling) ì´ë¡ ì ìœ¼ë¡œ $\\tilde{\\tau} = (1 + \\rho)\\, \\tau_{\\rho}$ ì„ì´ ì¦ëª…ëœë‹¤(Theorem 1). Step-3 (Zero-constraining operator) ë§ˆì§€ë§‰ìœ¼ë¡œ $$ \\mathcal{C}(f)(x,t) = f(x,t) - f(x,0) $$ ë¥¼ ì ìš©í•˜ë©´ ì‹¤ì œ ê´€ì‹¬ í•¨ìˆ˜ $\\tau(x,t)$ ë¥¼ ë³µì›í•œë‹¤. --- Practical take-aways ëª¨ë¸ë§ â€“ ì‹ ê²½ë§ ê°™ì€ ìœ ì—°í•œ í•¨ìˆ˜ ê·¼ì‚¬ê¸°ë¥¼ ì“¸ ë•Œë„ $\\tilde{\\tau}$ ë¥¼ í•™ìŠµí•˜ë©´ ì‹ë³„ ë¬¸ì œê°€ ìë™ìœ¼ë¡œ í•´ê²°ëœë‹¤. ìˆ˜ì¹˜ ì•ˆì •ì„± â€“ L2 ì •ê·œí™”(Ï>0)ê°€ ì—†ìœ¼ë©´ ì—¬ì „íˆ ill-posed; Ï ê°’ì€ í¬ì§€ ì•Šì•„ë„ \"ìœ ì¼ ìµœì†Œ\" í™•ë³´ì— ì¶©ë¶„í•˜ë‹¤. í•´ì„ â€“ $\\tilde{\\tau}$ ëŠ” \"x ì¡°ê±´ë¶€ì—ì„œ t ë³€í™”ë§Œ ë‹´ì€ ìˆœìˆ˜ íš¨ê³¼\"ë¼ì„œ $s(x)$(x ì „ìš© í¸í–¥)ì™€ ê¹”ë”íˆ ë¶„ë¦¬ëœë‹¤. --- Mini-checklist for implementation ë‘ nuisance í•¨ìˆ˜ m(x) ì™€ varpi(t x) ì¶”ì • R-loss + L2 íŒ¨ë„í‹°ë¡œ $\\tau_{\\rho}$ ê³„ì‚° $$\\tilde{\\tau} = (1 + \\rho)\\, \\tau_{\\rho}$$ $$\\hat{\\tau}(x,t) = \\tilde{\\tau}(x,t) - \\tilde{\\tau}(x,0)$$ --- Related reading Robinson (1988) â€“ partially linear models Nie & Wager (2021) â€“ Quasi-Oracle R-learner Kennedy et al. (2017) â€“ continuous-treatment GPS Tikhonov (1963) â€“ regularisation for ill-posed problems --- 3.5 Why do we still center? L2 ì •ê·œí™”ê°€ ìµœì¢…ì ìœ¼ë¡œ $s(x)$ ë¥¼ ì œê±°í•˜ë”ë¼ë„, ì¤‘ì‹¬í™”ê°€ ì„ í–‰ë¼ì•¼ í•˜ëŠ” ì‹¤ìš©ì  ì´ìœ ê°€ ìˆìŠµë‹ˆë‹¤. Canonical representative â€“ ê°™ì€ R-loss ê°’ì„ ê°–ëŠ” ë™ì¹˜ë¥˜ $\\{h+s(x)\\}$ ì¤‘ì—ì„œ $\\mathbb{E}[h\\mid X]=0$ ì¸ í•¨ìˆ˜ê°€ ê°€ì¥ ì‘ì€ $L_2$ ë…¸ë¦„ì„ ê°€ì§€ë¯€ë¡œ, ì •ê·œí™”ê°€ ìì—°ìŠ¤ëŸ½ê²Œ ê·¸ í•¨ìˆ˜ë¥¼ ì„ íƒí•˜ê²Œ ëœë‹¤. ë‹«íŒ í˜•íƒœ í•´ â€“ Theorem 1 ì˜ $(1+\\rho)^{-1}$ ìŠ¤ì¼€ì¼ ê´€ê³„ëŠ” $\\mathbb{E}[h\\mid X]=0$ ì¡°ê±´ì´ ìˆì–´ì•¼ ì„±ë¦½í•œë‹¤. Zero-constraining operator â€“ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ $\\mathcal{C}(f)(x,t)=f(x,t)-f(x,0)$ ë¥¼ ì ìš©í•  ë•Œ, ì¤‘ì‹¬í™”ë¼ ìˆì–´ì•¼ ì •í™•íˆ $\\tau(x,t)$ ë¡œ ë³µì›ëœë‹¤. ---"
  },
  {
    "id": "English Study/TOEFL/ì¤€ë¹„ì „ëµì„œì¹˜",
    "title": "ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼í• ì§€ ì„œì¹˜í•˜ê³  ì •ë¦¬",
    "date": "2025-07-14",
    "excerpt": "OMSCS ì§€ì›ì„ ìœ„í•œ í† í”Œ ê³µë¶€",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›",
      "OMSCS"
    ],
    "content": ""
  },
  {
    "id": "English Study/TOEFL/Fact and Negative Fact",
    "title": "Factì™€ Negative Fact ì°¨ì´",
    "date": "2025-07-14",
    "excerpt": "Factì™€ Negative Fact ìœ í˜• í’€ì´ë²•",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›",
      "OMSCS"
    ],
    "content": ""
  },
  {
    "id": "English Study/TOEFL/BirdDialects",
    "title": "Bird Dialects",
    "date": "2025-07-14",
    "excerpt": "Diagnostic Test ë¬¸ì œ ê³µë¶€",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›",
      "OMSCS"
    ],
    "content": "Bird Dialects ìƒˆ ë‹¤ì´ëŸ¬ë ‰ì¸ ? ìƒˆì˜ ë°©ì–¸ Paragraph 1 <button class=\"toggle-button translations\" data-type=\"paragraph1\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph1-content\"> > Birds learn acoustic information from their parents and environment from an early age. <span class=\"translation\">ìƒˆë“¤ì€ ì–´ë¦° ë‚˜ì´ë¶€í„° ë¶€ëª¨ì™€ í™˜ê²½ìœ¼ë¡œë¶€í„° ìŒí–¥ ì •ë³´ë¥¼ ë°°ìš´ë‹¤.</span> > The most complex form of avian vocalization is the birdsong, an often melodic series of communicative sounds produced primarily during breeding. <span class=\"translation\">ì¡°ë¥˜ì˜ ë°œì„± ì¤‘ ê°€ì¥ ë³µì¡í•œ í˜•íƒœëŠ” ìƒˆì˜ ë…¸ë˜ë¡œ, ë²ˆì‹ê¸°ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ” ì¢…ì¢… ì„ ìœ¨ì ì¸ ì˜ì‚¬ì†Œí†µ ì†Œë¦¬ì˜ ì—°ì†ì´ë‹¤.</span> > In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects. <span class=\"translation\">ê°œì²´êµ°ì´ ì§€ë¦¬ì  ê²½ê³„ë¥¼ ê°€ì§„ ê²½ìš°, ë…¸ë˜ì—ì„œ ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ ì§€ë¦¬ì  í•œê³„ê°€ ëšœë ·í•˜ë©´, í•œ ê°œì²´êµ° ë‚´ì—ì„œ ê³µìœ ë˜ëŠ” ë…¸ë˜ ìœ í˜•ì„ ë°©ì–¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤.</span> > Ornithologists are keenly interested in how these dialects form and to what extent genetic factors contribute to dialectical variation. <span class=\"translation\">ì¡°ë¥˜í•™ìë“¤ì€ ì´ëŸ¬í•œ ë°©ì–¸ì´ ì–´ë–»ê²Œ í˜•ì„±ë˜ëŠ”ì§€ì™€ ìœ ì „ì  ìš”ì¸ì´ ë°©ì–¸ì  ë³€í™”ì— ì–´ëŠ ì •ë„ ê¸°ì—¬í•˜ëŠ”ì§€ì— ê¹Šì€ ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆë‹¤.</span> > One of the best-studied songbirds in this regard is the white-crowned sparrow, Zonotrichia leucophrys, a species with several subspecies and many dialects. <span class=\"translation\">ì´ì™€ ê´€ë ¨í•˜ì—¬ ê°€ì¥ ì˜ ì—°êµ¬ëœ ëª…ê¸ˆ ì¤‘ í•˜ë‚˜ëŠ” í°ê´€ì°°ì°Œìƒˆ(Zonotrichia leucophrys)ë¡œ, ì—¬ëŸ¬ ì•„ì¢…ê³¼ ë§ì€ ë°©ì–¸ì„ ê°€ì§„ ì¢…ì´ë‹¤.</span> </div> Paragraph 2 <button class=\"toggle-button translations\" data-type=\"paragraph2\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph2-content\"> > In Zonotrichia leucophrys, males demonstrate the greatest dialectical diversity. Therefore, studying male birds offers the best opportunity for understanding dialect formation. <span class=\"translation\">í°ê´€ì°°ì°Œìƒˆ(Zonotrichia leucophrys)ì—ì„œ ìˆ˜ì»·ì´ ê°€ì¥ í° ë°©ì–¸ì  ë‹¤ì–‘ì„±ì„ ë³´ì¸ë‹¤. ë”°ë¼ì„œ ìˆ˜ì»· ìƒˆë¥¼ ì—°êµ¬í•˜ëŠ” ê²ƒì´ ë°©ì–¸ í˜•ì„±ì„ ì´í•´í•˜ëŠ” ë° ê°€ì¥ ì¢‹ì€ ê¸°íšŒë¥¼ ì œê³µí•œë‹¤.</span> > In order to make sense of birdsongs, scientists break them into smaller units. <span class=\"translation\">ìƒˆì˜ ë…¸ë˜ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ê³¼í•™ìë“¤ì€ ê·¸ê²ƒë“¤ì„ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤.</span> > A phrase, for example, is a large category below the level of song, and specific sound types like whistles, pulses, and buzzes reflect the individual speech parts, or syllables, of a phrase. <span class=\"translation\">ì˜ˆë¥¼ ë“¤ì–´, êµ¬ë¬¸ì€ ë…¸ë˜ ìˆ˜ì¤€ ì•„ë˜ì˜ í° ë²”ì£¼ì´ë©°, íœ˜íŒŒëŒ, í„ìŠ¤, ìœ™ìœ™ê±°ë¦¬ëŠ” ì†Œë¦¬ì™€ ê°™ì€ íŠ¹ì • ì†Œë¦¬ ìœ í˜•ì€ êµ¬ë¬¸ì˜ ê°œë³„ ë°œí™” ë¶€ë¶„, ì¦‰ ìŒì ˆì„ ë°˜ì˜í•œë‹¤.</span> > The phrases are ordered according to their position in the song so that specific parts can be analyzed separately and compared across different individuals. <span class=\"translation\">êµ¬ë¬¸ë“¤ì€ ë…¸ë˜ì—ì„œì˜ ìœ„ì¹˜ì— ë”°ë¼ ìˆœì„œê°€ ì •í•´ì ¸ì„œ, íŠ¹ì • ë¶€ë¶„ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ë¶„ì„í•˜ê³  ì„œë¡œ ë‹¤ë¥¸ ê°œì²´ë“¤ ê°„ì— ë¹„êµí•  ìˆ˜ ìˆë‹¤.</span> </div> Paragraph 3 <button class=\"toggle-button translations\" data-type=\"paragraph3\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph3-content\"> > Some phrases show more variability than others, and once scientists detect variability in a specific category, they can focus on that section of the songs in future studies. <span class=\"translation\">ì¼ë¶€ êµ¬ë¬¸ë“¤ì€ ë‹¤ë¥¸ êµ¬ë¬¸ë“¤ë³´ë‹¤ ë” ë§ì€ ë³€ì´ì„±ì„ ë³´ì´ë©°, ê³¼í•™ìë“¤ì´ íŠ¹ì • ë²”ì£¼ì—ì„œ ë³€ì´ì„±ì„ ë°œê²¬í•˜ë©´, í–¥í›„ ì—°êµ¬ì—ì„œ ë…¸ë˜ì˜ ê·¸ ë¶€ë¶„ì— ì§‘ì¤‘í•  ìˆ˜ ìˆë‹¤.</span> > In other words, if the terminal phrase shows the greatest inconsistency, recordings can be edited to single it out for comparisons. <span class=\"translation\">ë‹¤ì‹œ ë§í•´ì„œ, ë§Œì•½ ë§ˆì§€ë§‰ êµ¬ë¬¸ì´ ê°€ì¥ í° ë¶ˆì¼ì¹˜ì„±ì„ ë³´ì¸ë‹¤ë©´, ë…¹ìŒì„ í¸ì§‘í•˜ì—¬ ë¹„êµë¥¼ ìœ„í•´ ê·¸ê²ƒë§Œ ë”°ë¡œ ë¶„ë¦¬í•  ìˆ˜ ìˆë‹¤.</span> > Most white-crowned sparrow songs begin similarly with one or two whistles; consequently, the initial sequence is generally the most consistent component across subgroups. <span class=\"translation\">ëŒ€ë¶€ë¶„ì˜ í°ê´€ì°°ì°Œìƒˆ ë…¸ë˜ëŠ” í•˜ë‚˜ ë˜ëŠ” ë‘ ê°œì˜ íœ˜íŒŒëŒìœ¼ë¡œ ë¹„ìŠ·í•˜ê²Œ ì‹œì‘í•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ, ì´ˆê¸° ìˆœì„œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ì•„ì¢…ë“¤ ê°„ì— ê°€ì¥ ì¼ê´€ëœ êµ¬ì„± ìš”ì†Œì´ë‹¤.</span> > In addition, a convenient feature of male white-crowned sparrows is that individual adults generally sing a single song, making analysis across groups a relatively simple task in comparison to other bird species that routinely learn multiple songs, such as song sparrows (Melospiza melodia). <span class=\"translation\">ë˜í•œ, ìˆ˜ì»· í°ê´€ì°°ì°Œìƒˆì˜ í¸ë¦¬í•œ íŠ¹ì§• ì¤‘ í•˜ë‚˜ëŠ” ê°œë³„ ì„±ì²´ê°€ ì¼ë°˜ì ìœ¼ë¡œ í•˜ë‚˜ì˜ ë…¸ë˜ë§Œ ë¶€ë¥¸ë‹¤ëŠ” ê²ƒìœ¼ë¡œ, ì´ëŠ” ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ë°°ìš°ëŠ” ë…¸ë˜ì°Œìƒˆ(Melospiza melodia)ì™€ ê°™ì€ ë‹¤ë¥¸ ì¡°ë¥˜ ì¢…ì— ë¹„í•´ ì§‘ë‹¨ ê°„ ë¶„ì„ì„ ìƒëŒ€ì ìœ¼ë¡œ ê°„ë‹¨í•œ ì‘ì—…ìœ¼ë¡œ ë§Œë“ ë‹¤.</span> </div> Paragraph 4 <button class=\"toggle-button translations\" data-type=\"paragraph4\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph4-content\"> > Variations present among groups led some scientists to wonder whether there might be a genetic basis for dialectical uniqueness. <span class=\"translation\">ì§‘ë‹¨ë“¤ ê°„ì— ì¡´ì¬í•˜ëŠ” ë³€ì´ë“¤ì€ ì¼ë¶€ ê³¼í•™ìë“¤ì´ ë°©ì–¸ì  ê³ ìœ ì„±ì— ìœ ì „ìì ì¸ ê¸°ì €ê°€ ìˆëŠ”ì§€ ê¶ê¸ˆí•˜ê²Œ í–ˆë‹¤.</span> > Perhaps through the evolutionary isolation of some populations (for example, female preference for males of their own dialect), dispersal of the gene pool has been slowed to the point that differences in song reflect expressions of varying genetic compositions. <span class=\"translation\">ì•„ë§ˆë„ ì¼ë¶€ ê°œì²´êµ°ì˜ ì§„í™”ì  ê²©ë¦¬ë¥¼ í†µí•´ (ì˜ˆë¥¼ ë“¤ì–´, ì•”ì»·ì´ ìì‹ ì˜ ë°©ì–¸ì„ ê°€ì§„ ìˆ˜ì»·ì„ ì„ í˜¸í•˜ëŠ” ê²ƒ), ìœ ì „ì í’€ì˜ ë¶„ì‚°ì´ ëŠë ¤ì ¸ì„œ ë…¸ë˜ì˜ ì°¨ì´ê°€ ë‹¤ì–‘í•œ ìœ ì „ì  êµ¬ì„±ì˜ í‘œí˜„ì„ ë°˜ì˜í•˜ê²Œ ë˜ì—ˆì„ ê²ƒì´ë‹¤.</span> > Though a genetic explanation seems plausible, laboratory experiments have found that white-crowned sparrow chicks of particular dialect groups can learn the songs of other groups in their own subspecies in addition to dialects of other subspecies. <span class=\"translation\">ìœ ì „ìì  í•´ì„ì´ ë§ì´ í”ŒëŸ¬ì„œë¸” í•˜ë‹¤ê³  ë³¼ ìˆ˜ë„ ìˆì§€ë§Œ, ì‹¤í—˜ì—ì„œ í•˜ì–€ êµ°ì¤‘ ê¹Œë§ˆê·€ëŠ” ë‹¤ë¥¸ ì•„ì¢…ì˜ ë…¸ë˜ë‚˜ ë°©ì–¸ì„ ë°°ìš¸ ìˆ˜ ìˆë‹¤.</span> > Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects, although the maximum number is approximately seven. <span class=\"translation\">ì„±ì²´ì˜ ë…¹ìŒì„ ì‚¬ìš©í•˜ì—¬, ê³¼í•™ìë“¤ì€ ì–´ë¦° ìƒˆë“¤ì´ ì—¬ëŸ¬ ë°©ì–¸ì„ ì¬í˜„í•˜ë„ë¡ í›ˆë ¨ì‹œí‚¬ ìˆ˜ ìˆì—ˆëŠ”ë°, ìµœëŒ€ ê°œìˆ˜ëŠ” ì•½ 7ê°œì˜€ë‹¤.</span> > In addition, comparisons of genetic material taken from blood sample DNA have revealed unsubstantial differences. <span class=\"translation\">ë˜í•œ, í˜ˆì•¡ ìƒ˜í”Œ DNAì—ì„œ ì¶”ì¶œí•œ ìœ ì „ ë¬¼ì§ˆì˜ ë¹„êµëŠ” ì‹¤ì§ˆì ì´ì§€ ì•Šì€ ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤.</span> > In some instances, genetic variation among individuals of a dialect group was found to be greater than genetic variation among larger groups, such as subspecies. <span class=\"translation\">ì–´ë–¤ ê²½ìš°ì—ëŠ”, ë°©ì–¸ ì§‘ë‹¨ ë‚´ ê°œì²´ë“¤ ê°„ì˜ ìœ ì „ì  ë³€ì´ê°€ ì•„ì¢…ê³¼ ê°™ì€ ë” í° ì§‘ë‹¨ë“¤ ê°„ì˜ ìœ ì „ì  ë³€ì´ë³´ë‹¤ ë” í° ê²ƒìœ¼ë¡œ ë°í˜€ì¡Œë‹¤.</span> </div> Paragraph 5 <button class=\"toggle-button translations\" data-type=\"paragraph5\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph5-content\"> > If genetic differences are insignificant in comparison to cultural influences, why do sparrows with overlapping territories sing only one song in the field? <span class=\"translation\">ë§Œì•½ ìœ ì „ì  ì°¨ì´ê°€ ë¬¸í™”ì  ì˜í–¥ì— ë¹„í•´ ì¤‘ìš”í•˜ì§€ ì•Šë‹¤ë©´, ì™œ ê²¹ì¹˜ëŠ” ì˜ì—­ì„ ê°€ì§„ ì°¸ìƒˆë“¤ì€ ì•¼ìƒì—ì„œ ë‹¨ í•˜ë‚˜ì˜ ë…¸ë˜ë§Œ ë¶€ë¥´ëŠ”ê°€?</span> > Some, such as those of the sedentary subspecies known as Nuttall's sparrow, do encounter other dialect groups. <span class=\"translation\">ì¼ë¶€ëŠ”, ë„ˆíƒˆ ì°¸ìƒˆë¡œ ì•Œë ¤ì§„ ì •ì£¼ì„± ì•„ì¢… ê°™ì€ ê²½ìš°, ë‹¤ë¥¸ ë°©ì–¸ ê·¸ë£¹ê³¼ ë§ˆì£¼ì¹œë‹¤.</span> > Gambell's sparrow, a migratory subspecies, overwinters in the coastal California area that Nuttall's sparrow inhabits. <span class=\"translation\">ê°ë²¨ ì°¸ìƒˆëŠ”, ì´ë™ì„± ì•„ì¢…ìœ¼ë¡œ, ë„ˆíƒˆ ì°¸ìƒˆê°€ ì„œì‹í•˜ëŠ” ìº˜ë¦¬í¬ë‹ˆì•„ ì—°ì•ˆ ì§€ì—­ì—ì„œ ì›”ë™í•œë‹¤.</span> > Interestingly, despite contacts between October and March with Gambell's sparrow, male Nuttall's sparrows typically sing their own distinct songs. <span class=\"translation\">í¥ë¯¸ë¡­ê²Œë„, 10ì›”ê³¼ 3ì›” ì‚¬ì´ì— ê°ë²¨ ì°¸ìƒˆì™€ ì ‘ì´‰í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³ , ìˆ˜ì»· ë„ˆíƒˆ ì°¸ìƒˆë“¤ì€ ì¼ë°˜ì ìœ¼ë¡œ ìì‹ ë“¤ë§Œì˜ ë…íŠ¹í•œ ë…¸ë˜ë¥¼ ë¶€ë¥¸ë‹¤.</span> > This is because the encounters between the two subspecies occur outside the primary song crystallization period, which occurs during the first two months of a bird's life. <span class=\"translation\">ì´ê²ƒì€ ë‘ ì•„ì¢… ê°„ì˜ ë§Œë‚¨ì´ ì£¼ìš” ë…¸ë˜ ê²°ì •í™” ê¸°ê°„ ë°–ì—ì„œ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì¸ë°, ì´ ê¸°ê°„ì€ ìƒˆì˜ ìƒí›„ ì²« ë‘ ë‹¬ ë™ì•ˆ ë°œìƒí•œë‹¤.</span> </div> Paragraph 6 <button class=\"toggle-button translations\" data-type=\"paragraph6\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph6-content\"> > Detailed field research led to the discovery that some exceptional members of the species do master multiple songs, but their knowledge acquisition is contingent on contact with other groups in one of two ways. <span class=\"translation\">ìƒì„¸í•œ í˜„ì¥ ì—°êµ¬ëŠ” ì´ ì¢…ì˜ ì¼ë¶€ ì˜ˆì™¸ì ì¸ êµ¬ì„±ì›ë“¤ì´ ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ìˆ™ë‹¬í•œë‹¤ëŠ” ê²ƒì„ ë°œê²¬í–ˆì§€ë§Œ, ê·¸ë“¤ì˜ ì§€ì‹ ìŠµë“ì€ ë‘ ê°€ì§€ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ë‹¤ë¥¸ ì§‘ë‹¨ê³¼ì˜ ì ‘ì´‰ì— ë‹¬ë ¤ìˆë‹¤.</span> > Either young birds come in contact with migratory subspecies during the formative period of language learning or they live on the margins of dialect borders. <span class=\"translation\">ì–´ë¦° ìƒˆë“¤ì´ ì–¸ì–´ í•™ìŠµì˜ í˜•ì„± ê¸°ê°„ ë™ì•ˆ ì´ë™ì„± ì•„ì¢…ê³¼ ì ‘ì´‰í•˜ê±°ë‚˜, ë°©ì–¸ ê²½ê³„ì˜ ê°€ì¥ìë¦¬ì— ì‚´ê³  ìˆë‹¤.</span> > Nuttall's sparrows hatching early or late in the season are more likely to be exposed to Gambell's sparrows, but broods hatching after March or before October will mostly only interact with other members of their own kind. <span class=\"translation\">ê³„ì ˆì— ì¼ì° ë˜ëŠ” ëŠ¦ê²Œ ë¶€í™”í•˜ëŠ” Nuttall's sparrowë“¤ì€ Gambell's sparrowë“¤ì— ë…¸ì¶œë  ê°€ëŠ¥ì„±ì´ ë” ë†’ì§€ë§Œ, 3ì›” ì´í›„ ë˜ëŠ” 10ì›” ì´ì „ì— ë¶€í™”í•˜ëŠ” ìƒˆë¼ë“¤ì€ ëŒ€ë¶€ë¶„ ìì‹ ì˜ ì¢…ë¥˜ì˜ ë‹¤ë¥¸ êµ¬ì„±ì›ë“¤ê³¼ë§Œ ìƒí˜¸ì‘ìš©í•  ê²ƒì´ë‹¤.</span> > In like fashion, individuals living near the peripheries of their dialectic range may regularly hear the songs of an adjacent group and develop multilingual abilities. <span class=\"translation\">ê°™ì€ ë°©ì‹ìœ¼ë¡œ, ë°©ì–¸ ë²”ìœ„ì˜ ì£¼ë³€ì— ì‚¬ëŠ” ê°œì²´ë“¤ì€ ì¸ì ‘í•œ ì§‘ë‹¨ì˜ ë…¸ë˜ë¥¼ ì •ê¸°ì ìœ¼ë¡œ ë“£ê³  ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì„ ê°œë°œí•  ìˆ˜ ìˆë‹¤.</span> </div> Paragraph 7 <button class=\"toggle-button translations\" data-type=\"paragraph7\">ğŸ” AI í•´ì„ ê°€ë¦¬ê¸°</button> <div class=\"paragraph7-content\"> > In light of these findings, most scientists have concluded that geographic differences may mildly restrict gene flow, but that genetic isolation has not been strong enough to create a rigid linguistic barrier. <span class=\"translation\">ì´ëŸ¬í•œ ë°œê²¬ì„ ë°”íƒ•ìœ¼ë¡œ, ëŒ€ë¶€ë¶„ì˜ ê³¼í•™ìë“¤ì€ ì§€ë¦¬ì  ì°¨ì´ê°€ ìœ ì „ì íë¦„ì„ ì•½ê°„ ì œí•œí•  ìˆ˜ ìˆì§€ë§Œ, ìœ ì „ì  ê²©ë¦¬ê°€ ì—„ê²©í•œ ì–¸ì–´ì  ì¥ë²½ì„ ë§Œë“¤ ë§Œí¼ ì¶©ë¶„íˆ ê°•í•˜ì§€ ì•Šì•˜ë‹¤ê³  ê²°ë¡ ì§€ì—ˆë‹¤.</span> > Dialectical variety in birds can thus be viewed as primarily a cultural phenomenon. <span class=\"translation\">ë”°ë¼ì„œ ìƒˆë“¤ì˜ ë°©ì–¸ì  ë‹¤ì–‘ì„±ì€ ì£¼ë¡œ ë¬¸í™”ì  í˜„ìƒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆë‹¤.</span> </div> --- ğŸ“š í•µì‹¬ ë‹¨ì–´ ì‚¬ì „ <button class=\"toggle-button vocabulary\" data-type=\"vocabulary\">ğŸ” ë‹¨ì–´ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"vocabulary-content\"> ğŸ¦ ì¡°ë¥˜í•™ ê´€ë ¨ ìš©ì–´ avian vocalization <span class=\"meaning\">(ì¡°ë¥˜ì˜ ë°œì„±) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬</span> birdsong <span class=\"meaning\">(ìƒˆì˜ ë…¸ë˜) - ìƒˆê°€ ë¶€ë¥´ëŠ” ë³µì¡í•œ ì†Œë¦¬</span> dialect <span class=\"meaning\">(ë°©ì–¸) - ì§€ë¦¬ì  ê²½ê³„ì— ë”°ë¥¸ ë…¸ë˜ì˜ ì°¨ì´</span> phrase <span class=\"meaning\">(êµ¬ë¬¸) - ë…¸ë˜ì˜ êµ¬ì„± ë‹¨ìœ„</span> syllable <span class=\"meaning\">(ìŒì ˆ) - êµ¬ë¬¸ì˜ ê°œë³„ ë°œí™” ë¶€ë¶„</span> whistle <span class=\"meaning\">(íœ˜íŒŒëŒ) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬ ìœ í˜•</span> pulse <span class=\"meaning\">(í„ìŠ¤) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬ ìœ í˜•</span> buzz <span class=\"meaning\">(ìœ™ìœ™ê±°ë¦¬ëŠ” ì†Œë¦¬) - ìƒˆê°€ ë‚´ëŠ” ì†Œë¦¬ ìœ í˜•</span> ğŸ§¬ ìœ ì „í•™ ê´€ë ¨ ìš©ì–´ gene pool <span class=\"meaning\">(ìœ ì „ì í’€) - ì§‘ë‹¨ì˜ ì „ì²´ ìœ ì „ì  ë‹¤ì–‘ì„±</span> genetic isolation <span class=\"meaning\">(ìœ ì „ì  ê²©ë¦¬) - ìœ ì „ì êµí™˜ì˜ ì°¨ë‹¨</span> subspecies <span class=\"meaning\">(ì•„ì¢…) - ì¢…ì˜ í•˜ìœ„ ë¶„ë¥˜</span> genetic variation <span class=\"meaning\">(ìœ ì „ì  ë³€ì´) - ìœ ì „ì  ì°¨ì´</span> gene flow <span class=\"meaning\">(ìœ ì „ì íë¦„) - ì§‘ë‹¨ ê°„ ìœ ì „ì êµí™˜</span> ğŸ• ì‹œê°„ ê´€ë ¨ ìš©ì–´ formative period <span class=\"meaning\">(í˜•ì„± ê¸°ê°„) - í•™ìŠµì´ ì´ë£¨ì–´ì§€ëŠ” ì¤‘ìš”í•œ ì‹œê¸°</span> song crystallization period <span class=\"meaning\">(ë…¸ë˜ ê²°ì •í™” ê¸°ê°„) - ìƒí›„ 2ê°œì›”ì˜ í•µì‹¬ í•™ìŠµ ì‹œê¸°</span> overwinter <span class=\"meaning\">(ì›”ë™í•˜ë‹¤) - ê²¨ìš¸ì„ ë‚˜ë‹¤</span> ğŸ—ºï¸ ì§€ë¦¬ ê´€ë ¨ ìš©ì–´ geographical boundaries <span class=\"meaning\">(ì§€ë¦¬ì  ê²½ê³„) - ì§€ì—­ì  í•œê³„</span> overlapping territories <span class=\"meaning\">(ê²¹ì¹˜ëŠ” ì˜ì—­) - ì„œë¡œ ê²¹ì¹˜ëŠ” ì„œì‹ì§€</span> sedentary <span class=\"meaning\">(ì •ì£¼ì„±) - í•œ ê³³ì— ë¨¸ë¬´ë¥´ëŠ”</span> migratory <span class=\"meaning\">(ì´ë™ì„±) - ê³„ì ˆì— ë”°ë¼ ì´ë™í•˜ëŠ”</span> peripheries <span class=\"meaning\">(ì£¼ë³€ë¶€) - ê°€ì¥ìë¦¬ ì§€ì—­</span> </div> --- ğŸ’¬ ìœ ìš©í•œ ì˜ì–´ í‘œí˜„ë“¤ <button class=\"toggle-button expressions\" data-type=\"expressions\">ğŸ” í‘œí˜„ ëœ» ê°€ë¦¬ê¸°</button> <div class=\"expressions-content\"> ğŸ”¬ ì—°êµ¬/ì‹¤í—˜ ê´€ë ¨ í‘œí˜„ in this regard <span class=\"meaning\">(ì´ì™€ ê´€ë ¨í•˜ì—¬) - \"One of the best-studied songbirds in this regard is...\"</span> led to the discovery <span class=\"meaning\">(ë°œê²¬ìœ¼ë¡œ ì´ì–´ì¡Œë‹¤) - \"Detailed field research led to the discovery that...\"</span> have been able to <span class=\"meaning\">(í•  ìˆ˜ ìˆì—ˆë‹¤) - \"Scientists have been able to train fledglings...\"</span> have found that <span class=\"meaning\">(ë°œê²¬í–ˆë‹¤) - \"Laboratory experiments have found that...\"</span> have revealed <span class=\"meaning\">(ë³´ì—¬ì£¼ì—ˆë‹¤) - \"Comparisons have revealed unsubstantial differences.\"</span> ğŸ“Š ë¹„êµ/ëŒ€ì¡° í‘œí˜„ in comparison to <span class=\"meaning\">(ë¹„êµí–ˆì„ ë•Œ) - \"If genetic differences are insignificant in comparison to cultural influences...\"</span> in addition <span class=\"meaning\">(ë˜í•œ) - \"In addition, a convenient feature...\"</span> though <span class=\"meaning\">(ë¹„ë¡...í•˜ì§€ë§Œ) - \"Though a genetic explanation seems plausible...\"</span> despite <span class=\"meaning\">(ì—ë„ ë¶ˆêµ¬í•˜ê³ ) - \"Despite contacts between October and March...\"</span> in light of <span class=\"meaning\">(ì„ ë°”íƒ•ìœ¼ë¡œ) - \"In light of these findings...\"</span> â° ì‹œê°„/ìˆœì„œ í‘œí˜„ from an early age <span class=\"meaning\">(ì–´ë¦° ë‚˜ì´ë¶€í„°) - \"Birds learn from an early age.\"</span> during <span class=\"meaning\">(ë™ì•ˆ) - \"Produced primarily during breeding.\"</span> between <span class=\"meaning\">(ì‚¬ì´ì—) - \"Between October and March.\"</span> after/before <span class=\"meaning\">(ì´í›„/ì´ì „) - \"After March or before October.\"</span> once <span class=\"meaning\">(ì¼ë‹¨...í•˜ë©´) - \"Once scientists detect variability...\"</span> ğŸ¯ ê°•ì¡°/ì„¤ëª… í‘œí˜„ in other words <span class=\"meaning\">(ë‹¤ì‹œ ë§í•´ì„œ) - \"In other words, if the terminal phrase...\"</span> for example <span class=\"meaning\">(ì˜ˆë¥¼ ë“¤ì–´) - \"For example, female preference for males...\"</span> such as <span class=\"meaning\">(ê°™ì€) - \"Such as song sparrows.\"</span> in like fashion <span class=\"meaning\">(ê°™ì€ ë°©ì‹ìœ¼ë¡œ) - \"In like fashion, individuals living near...\"</span> thus <span class=\"meaning\">(ë”°ë¼ì„œ) - \"Dialectical variety can thus be viewed as...\"</span> ğŸ” ì¡°ê±´/ê°€ì • í‘œí˜„ in cases where <span class=\"meaning\">(ì¸ ê²½ìš°ì—) - \"In cases where populations have geographical boundaries...\"</span> if <span class=\"meaning\">(ë§Œì•½) - \"If these geographical limits are distinct...\"</span> whether <span class=\"meaning\">(ì¸ì§€ ì•„ë‹Œì§€) - \"Whether there might be a genetic basis...\"</span> perhaps <span class=\"meaning\">(ì•„ë§ˆë„) - \"Perhaps through the evolutionary isolation...\"</span> may <span class=\"meaning\">(í•  ìˆ˜ ìˆë‹¤) - \"Geographic differences may mildly restrict gene flow.\"</span> ğŸ“ í•™ìˆ ì  í‘œí˜„ referred to as <span class=\"meaning\">(ë¼ê³  ë¶ˆë¦°ë‹¤) - \"Are referred to as dialects.\"</span> keenly interested in <span class=\"meaning\">(ì— ê¹Šì€ ê´€ì‹¬ì„ ê°€ì§€ê³  ìˆë‹¤) - \"Ornithologists are keenly interested in...\"</span> make sense of <span class=\"meaning\">(ì´í•´í•˜ë‹¤) - \"In order to make sense of birdsongs...\"</span> contingent on <span class=\"meaning\">(ì— ë‹¬ë ¤ìˆë‹¤) - \"Knowledge acquisition is contingent on contact...\"</span> concluded that <span class=\"meaning\">(ë¼ê³  ê²°ë¡ ì§€ì—ˆë‹¤) - \"Scientists have concluded that...\"</span> </div> ë¬¸ì œí’€ì´ TOEFL Reading Comprehension Questions Question 1 The word \"primarily\" in the passage is closest in meaning to (A) abundantly (B) mainly (C) loudly (D) commonly <button class=\"toggle-button answer\" data-type=\"answer1\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer1-content\"> ì •ë‹µ: (B) mainly í•´ì„¤: \"primarily\"ëŠ” \"ì£¼ë¡œ, ì£¼ëœ\"ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì§€ë¬¸ì—ì„œ \"produced primarily during breeding\"ì€ \"ë²ˆì‹ê¸°ì— ì£¼ë¡œ ë°œìƒí•˜ëŠ”\"ì´ë¼ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (A) abundantly: í’ë¶€í•˜ê²Œ - ì–‘ì  ì˜ë¯¸ (B) mainly: ì£¼ë¡œ - ì£¼ëœ ëª©ì ì´ë‚˜ ì‹œê¸°ë¥¼ ë‚˜íƒ€ëƒ„ (C) loudly: í¬ê²Œ - ì†Œë¦¬ì˜ í¬ê¸° (D) commonly: í”íˆ - ë¹ˆë„ì˜ ì˜ë¯¸ </div> Question 2 Which of the sentences below best expresses the essential information in the highlighted sentence in the passage? Incorrect choices change the meaning in important ways or leave out essential information. ì›ë¬¸: \"In cases where populations have geographical boundaries, differences in song may appear, and if these geographical limits are distinct, the shared song types within a population are referred to as dialects.\" (A) Dialects are those song types within a population that are shared by the group. (B) Shared song types within a population with clear geographical boundaries are called dialects. (C) Geographical boundaries may result in differences in song between distinct populations. (D) Populations having clear dialects result from the shared song types within the group. <button class=\"toggle-button answer\" data-type=\"answer2\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer2-content\"> ì •ë‹µ: (B) Shared song types within a population with clear geographical boundaries are called dialects. í•´ì„¤: ì›ë¬¸ì˜ í•µì‹¬ ì •ë³´ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: ì§€ë¦¬ì  ê²½ê³„ê°€ ìˆëŠ” ê°œì²´êµ°ì—ì„œëŠ” ë…¸ë˜ì˜ ì°¨ì´ê°€ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤ ì´ëŸ¬í•œ ì§€ë¦¬ì  í•œê³„ê°€ ëšœë ·í•˜ë©´, ê°œì²´êµ° ë‚´ì—ì„œ ê³µìœ ë˜ëŠ” ë…¸ë˜ ìœ í˜•ì„ ë°©ì–¸ì´ë¼ê³  ë¶€ë¥¸ë‹¤ (A) ë°©ì–¸ì˜ ì •ì˜ë§Œ í¬í•¨, ì§€ë¦¬ì  ê²½ê³„ ì¡°ê±´ ëˆ„ë½ (B) ì§€ë¦¬ì  ê²½ê³„ì™€ ê³µìœ ëœ ë…¸ë˜ ìœ í˜•ì´ ë°©ì–¸ì´ë¼ëŠ” í•µì‹¬ ì •ë³´ ëª¨ë‘ í¬í•¨ (C) ì§€ë¦¬ì  ê²½ê³„ê°€ ì°¨ì´ë¥¼ ë§Œë“ ë‹¤ëŠ” ë¶€ë¶„ë§Œ, ë°©ì–¸ì˜ ì •ì˜ ëˆ„ë½ (D) ì¸ê³¼ê´€ê³„ê°€ ì˜ëª»ë¨ (ë°©ì–¸ì´ ê°œì²´êµ°ì„ ë§Œë“œëŠ” ê²ƒì´ ì•„ë‹˜) </div> Question 3 According to paragraph 2, scientists attempt to understand birdsongs by (A) dividing them into smaller categories (B) analyzing them as a single unit (C) listening to phrases in reverse order (D) labeling them as male or female <button class=\"toggle-button answer\" data-type=\"answer3\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer3-content\"> ì •ë‹µ: (A) dividing them into smaller categories í•´ì„¤: 2ë‹¨ë½ì—ì„œ \"In order to make sense of birdsongs, scientists break them into smaller units\"ë¼ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³¼í•™ìë“¤ì€ ìƒˆì˜ ë…¸ë˜ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆˆë‹¤ê³  ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ: ë…¸ë˜(song) â†’ êµ¬ë¬¸(phrase) â†’ ìŒì ˆ(syllables)ë¡œ ì„¸ë¶„í™” íœ˜íŒŒëŒ, í„ìŠ¤, ìœ™ìœ™ê±°ë¦¬ëŠ” ì†Œë¦¬ ë“± íŠ¹ì • ì†Œë¦¬ ìœ í˜•ìœ¼ë¡œ ë¶„ë¥˜ ì´ëŠ” \"ë” ì‘ì€ ë²”ì£¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒ\"ê³¼ ì •í™•íˆ ì¼ì¹˜í•©ë‹ˆë‹¤. </div> Question 4 What can be inferred from paragraph 4 about white-crowned sparrow chicks? (A) The simplest dialects are the ones they learn first. (B) The number of songs they can be trained to learn is limited. (C) They learn songs of their own subspecies faster than those of other subspecies. (D) They learn more easily from laboratory recordings than in the wild. <button class=\"toggle-button answer\" data-type=\"answer4\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer4-content\"> ì •ë‹µ: (B) The number of songs they can be trained to learn is limited. í•´ì„¤: 4ë‹¨ë½ì—ì„œ \"Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects, although the maximum number is approximately seven\"ë¼ê³  ëª…ì‹œë˜ì–´ ìˆìŠµë‹ˆë‹¤. ê³¼í•™ìë“¤ì´ ì–´ë¦° ìƒˆë“¤ì„ í›ˆë ¨ì‹œì¼œ ì—¬ëŸ¬ ë°©ì–¸ì„ ì¬í˜„í•˜ê²Œ í•  ìˆ˜ ìˆì—ˆì§€ë§Œ, ìµœëŒ€ ê°œìˆ˜ëŠ” ì•½ 7ê°œë¼ê³  í•œì •ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” í•™ìŠµí•  ìˆ˜ ìˆëŠ” ë…¸ë˜ì˜ ìˆ˜ê°€ ì œí•œì ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. (A) ê°€ì¥ ê°„ë‹¨í•œ ë°©ì–¸ì„ ë¨¼ì € ë°°ìš´ë‹¤ëŠ” ì–¸ê¸‰ ì—†ìŒ (C) ìì‹ ì˜ ì•„ì¢… ë…¸ë˜ë¥¼ ë” ë¹¨ë¦¬ ë°°ìš´ë‹¤ëŠ” ì–¸ê¸‰ ì—†ìŒ (D) ì‹¤í—˜ì‹¤ ë…¹ìŒìœ¼ë¡œ ë” ì‰½ê²Œ ë°°ìš´ë‹¤ëŠ” ë¹„êµ ì—†ìŒ </div> Question 5 According to paragraph 5, which of the following is true of Nuttall's sparrows? (A) They migrate to the same overwintering locations that Gambell's sparrows do. (B) They do not often encounter Gambell's sparrows during the initial period of song formation. (C) They only have contact with Gambell's sparrows during the breeding season. (D) They do not have the same inherent vocalization abilities that Gambell's sparrows do. <button class=\"toggle-button answer\" data-type=\"answer5\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer5-content\"> ì •ë‹µ: (B) They do not often encounter Gambell's sparrows during the initial period of song formation. í•´ì„¤: 5ë‹¨ë½ì—ì„œ í•µì‹¬ ì •ë³´: Nuttall's sparrowëŠ” ì •ì£¼ì„±(sedentary) ì•„ì¢… Gambell's sparrowëŠ” ì´ë™ì„±(migratory) ì•„ì¢…ìœ¼ë¡œ 10ì›”~3ì›”ì— Nuttall's ì„œì‹ì§€ì—ì„œ ì›”ë™ \"This is because the encounters between the two subspecies occur outside the primary song crystallization period, which occurs during the first two months of a bird's life\" ì¦‰, ë‘ ì•„ì¢…ì˜ ë§Œë‚¨ì€ ì£¼ìš” ë…¸ë˜ ê²°ì •í™” ê¸°ê°„(ìƒí›„ ì²« 2ê°œì›”) ë°–ì—ì„œ ì¼ì–´ë‚˜ê¸° ë•Œë¬¸ì—, Nuttall's sparrowëŠ” ë…¸ë˜ í˜•ì„± ì´ˆê¸°ì— Gambell's sparrowì™€ ìì£¼ ë§ˆì£¼ì¹˜ì§€ ì•ŠìŠµë‹ˆë‹¤. (A) Nuttall'sëŠ” ì´ë™í•˜ì§€ ì•ŠìŒ (sedentary) (C) 10ì›”~3ì›”ì— ì ‘ì´‰ (ë²ˆì‹ê¸°ê°€ ì•„ë‹˜) (D) ë°œì„± ëŠ¥ë ¥ì˜ ì°¨ì´ì— ëŒ€í•œ ì–¸ê¸‰ ì—†ìŒ </div> Question 6 The word \"exceptional\" in the passage is closest in meaning to (A) extraordinary (B) perceptual (C) intelligent (D) capable <button class=\"toggle-button answer\" data-type=\"answer6\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer6-content\"> ì •ë‹µ: (A) extraordinary í•´ì„¤: 6ë‹¨ë½ì—ì„œ \"some exceptional members of the species do master multiple songs\"ë¼ê³  ì‚¬ìš©ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ë§¥ìƒ ëŒ€ë¶€ë¶„ì˜ ê°œì²´ë“¤ì€ í•˜ë‚˜ì˜ ë…¸ë˜ë§Œ ë¶€ë¥´ëŠ”ë°, ì¼ë¶€ ì˜ˆì™¸ì ì¸ êµ¬ì„±ì›ë“¤ì€ ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ìˆ™ë‹¬í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. (A) extraordinary: íŠ¹ë³„í•œ, ì˜ˆì™¸ì ì¸ (B) perceptual: ì§€ê°ì˜, ì¸ì‹ì˜ (C) intelligent: ì§€ëŠ¥ì ì¸ (D) capable: ëŠ¥ë ¥ìˆëŠ” \"exceptional\"ì€ \"ì¼ë°˜ì ì´ì§€ ì•Šì€, íŠ¹ë³„í•œ\"ì´ë¼ëŠ” ëœ»ìœ¼ë¡œ \"extraordinary\"ì™€ ê°€ì¥ ì˜ë¯¸ê°€ ê°€ê¹ìŠµë‹ˆë‹¤. </div> Question 7 According to paragraph 6, which of the following does NOT contribute to multilingualism in Nuttall's sparrows? (A) Living on the edges of dialectical boundaries (B) Interacting with other subspecies (C) Hatching early or late in the season (D) Mating between March and October âœ“ <button class=\"toggle-button answer\" data-type=\"answer7\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer7-content\"> ì •ë‹µ: (D) Mating between March and October í•´ì„¤: 6ë‹¨ë½ì—ì„œ ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì— ê¸°ì—¬í•˜ëŠ” ìš”ì¸ë“¤: ê¸°ì—¬í•˜ëŠ” ê²ƒë“¤: (A) ë°©ì–¸ ê²½ê³„ì˜ ê°€ì¥ìë¦¬ì— ê±°ì£¼: \"individuals living near the peripheries of their dialectic range may regularly hear the songs of an adjacent group\" (B) ë‹¤ë¥¸ ì•„ì¢…ê³¼ì˜ ìƒí˜¸ì‘ìš©: \"young birds come in contact with migratory subspecies during the formative period\" (C) ê³„ì ˆ ì´ˆê¸°ë‚˜ ë§ê¸°ì— ë¶€í™”: \"Nuttall's sparrows hatching early or late in the season are more likely to be exposed to Gambell's sparrows\" ê¸°ì—¬í•˜ì§€ ì•ŠëŠ” ê²ƒ: (D) 3ì›”~10ì›” ì‚¬ì´ì˜ ì§ì§“ê¸°: ì§€ë¬¸ì—ì„œ ì–¸ê¸‰ë˜ì§€ ì•Šì•˜ìœ¼ë©°, ì˜¤íˆë ¤ \"broods hatching after March or before October will mostly only interact with other members of their own kind\"ë¼ê³  í•˜ì—¬ ì´ ì‹œê¸°ì— ë¶€í™”í•˜ëŠ” ìƒˆë¼ë“¤ì€ ìì‹ ì˜ ì¢…ë¥˜ì™€ë§Œ ìƒí˜¸ì‘ìš©í•œë‹¤ê³  ëª…ì‹œ </div> Question 8 Why does the author include paragraph 7 in the passage? (A) To argue that genetic factors have nothing to do with linguistic ability in birds (B) To offer a concluding argument for why birds have a variety of dialects (C) To introduce a new issue that has emerged in research on bird dialects (D) To suggest that language acquisition in birds is still poorly understood <button class=\"toggle-button answer\" data-type=\"answer8\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer8-content\"> ì •ë‹µ: (B) To offer a concluding argument for why birds have a variety of dialects í•´ì„¤: 7ë‹¨ë½ì€ ì „ì²´ ì§€ë¬¸ì˜ ê²°ë¡  ë‹¨ë½ì…ë‹ˆë‹¤: \"In light of these findings, most scientists have concluded that geographic differences may mildly restrict gene flow, but that genetic isolation has not been strong enough to create a rigid linguistic barrier. Dialectical variety in birds can thus be viewed as primarily a cultural phenomenon.\" ì´ ë‹¨ë½ì—ì„œ ì €ìëŠ”: ì•ì„œ ì œì‹œëœ ì—°êµ¬ ê²°ê³¼ë“¤ì„ ì¢…í•© ìœ ì „ì  ê²©ë¦¬ë³´ë‹¤ëŠ” ë¬¸í™”ì  ìš”ì¸ì´ ë” ì¤‘ìš”í•˜ë‹¤ê³  ê²°ë¡  ìƒˆë“¤ì˜ ë°©ì–¸ì  ë‹¤ì–‘ì„±ì´ ì£¼ë¡œ ë¬¸í™”ì  í˜„ìƒì´ë¼ëŠ” ìµœì¢… ë‹µì„ ì œì‹œ ì´ëŠ” ìƒˆë“¤ì´ ì™œ ë‹¤ì–‘í•œ ë°©ì–¸ì„ ê°€ì§€ê²Œ ë˜ëŠ”ì§€ì— ëŒ€í•œ ê²°ë¡ ì  ì£¼ì¥ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. (A) ìœ ì „ì  ìš”ì¸ì´ ì „í˜€ ê´€ë ¨ì—†ë‹¤ê³  ì£¼ì¥í•˜ì§€ ì•ŠìŒ (mildly restrictë¼ê³  ì¸ì •) (C) ìƒˆë¡œìš´ ì´ìŠˆë¥¼ ì†Œê°œí•˜ëŠ” ê²ƒì´ ì•„ë‹˜ (D) ì–¸ì–´ ìŠµë“ì´ ì˜ ì´í•´ë˜ì§€ ì•ŠëŠ”ë‹¤ê³  ì œì‹œí•˜ì§€ ì•ŠìŒ </div> Question 9 Look at the four squares [â– ] that indicate where the following sentence could be added to the passage. Single adult males of this species have been recorded singing eleven different songs. Where would the sentence best fit? <button class=\"toggle-button answer\" data-type=\"answer9\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer9-content\"> ì ì ˆí•œ ìœ„ì¹˜: Paragraph 6ì˜ ì²« ë²ˆì§¸ ë¬¸ì¥ ë’¤ í•´ì„¤: ì‚½ì…í•  ë¬¸ì¥: \"Single adult males of this species have been recorded singing eleven different songs.\" (ì´ ì¢…ì˜ ì„±ì²´ ìˆ˜ì»· ê°œì²´ë“¤ì´ 11ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ë…¸ë˜ë¥¼ ë¶€ë¥´ëŠ” ê²ƒì´ ê¸°ë¡ë˜ì—ˆë‹¤.) ê°€ì¥ ì ì ˆí•œ ìœ„ì¹˜ëŠ” 6ë‹¨ë½ ì²« ë¬¸ì¥ ë’¤ì…ë‹ˆë‹¤: \"Detailed field research led to the discovery that some exceptional members of the species do master multiple songs, but their knowledge acquisition is contingent on contact with other groups in one of two ways. [â–  ì—¬ê¸°] Either young birds come in contact with migratory subspecies...\" ì´ìœ : ë…¼ë¦¬ì  ì—°ê²°: ì• ë¬¸ì¥ì—ì„œ \"ì¼ë¶€ ì˜ˆì™¸ì ì¸ êµ¬ì„±ì›ë“¤ì´ ì—¬ëŸ¬ ë…¸ë˜ë¥¼ ìˆ™ë‹¬í•œë‹¤\"ê³  í•˜ê³ , ì‚½ì… ë¬¸ì¥ì€ ê·¸ êµ¬ì²´ì ì¸ ì˜ˆì‹œ(11ê°œ ë…¸ë˜)ë¥¼ ì œê³µ êµ¬ì²´ì  ì¦ê±°: \"some exceptional members\"ì˜ êµ¬ì²´ì ì¸ ì‚¬ë¡€ ë¬¸ë§¥ì˜ ì¼ê´€ì„±: ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì— ëŒ€í•œ ë…¼ì˜ê°€ ì´ì–´ì§€ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ìœ„ì¹˜ ë‹¤ë¥¸ ìœ„ì¹˜ë“¤ì€ ë¬¸ë§¥ìƒ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. </div> Question 10 Directions: An introductory sentence for a brief summary of the passage is provided below. Complete the summary by selecting the THREE answer choices that express the most important ideas in the passage. Some sentences do not belong in the summary because they express ideas that are not presented in the passage or are minor ideas in the passage. This question is worth 2 points. In some bird populations, there are regional dialects in the birds' songs. Answer Choices: (A) White-crowned sparrows are useful subjects for studying bird dialects because they usually only learn a single song and have many dialects. (B) Using recordings in a laboratory setting, scientists have demonstrated that white-crowned sparrow chicks are capable of learning multiple dialects. (C) Research conducted on Nuttall's sparrows and Gambell's sparrows suggests that the two subspecies have developed different dialects because one is sedentary while the other is migratory. (D) Scientists thought that genetic factors may have contributed to dialectical diversity, but research has shown that genetic factors are limited. (E) Field studies of two Zonotrichia leucophrys subspecies revealed that multilingualism is dependent upon the timing and nature of interactions between dialectical group members. (F) One thing that is clear about the various subspecies of Zonotrichia leucophrys is that they all seem to begin learning songs for the first time after two months of age. <button class=\"toggle-button answer\" data-type=\"answer10\">ğŸ” ì •ë‹µ ë° í•´ì„¤ ê°€ë¦¬ê¸°</button> <div class=\"answer10-content\"> ì •ë‹µ: (A), (B), (D) í•´ì„¤: ì„ íƒëœ ë‹µì•ˆë“¤: (A) White-crowned sparrows are useful subjects for studying bird dialects because they usually only learn a single song and have many dialects. âœ“ 1ë‹¨ë½: \"One of the best-studied songbirds... is the white-crowned sparrow\" 3ë‹¨ë½: \"individual adults generally sing a single song\" ì—°êµ¬ ëŒ€ìƒìœ¼ë¡œì„œì˜ ìœ ìš©ì„±ì´ ì£¼ìš” ì£¼ì œ (B) Using recordings in a laboratory setting, scientists have demonstrated that white-crowned sparrow chicks are capable of learning multiple dialects. âœ“ 4ë‹¨ë½: \"Using recordings of adults, scientists have been able to train fledglings to reproduce multiple dialects\" ì‹¤í—˜ì‹¤ ì—°êµ¬ì˜ ì¤‘ìš”í•œ ë°œê²¬ (D) Scientists thought that genetic factors may have contributed to dialectical diversity, but research has shown that genetic factors are limited. âœ“ 4ë‹¨ë½: ìœ ì „ì  ì„¤ëª…ì´ ê·¸ëŸ´ë“¯í•´ ë³´ì˜€ì§€ë§Œ 7ë‹¨ë½: \"genetic isolation has not been strong enough... primarily a cultural phenomenon\" ì§€ë¬¸ì˜ í•µì‹¬ ê²°ë¡  ì œì™¸ëœ ë‹µì•ˆë“¤: (C) ì •ì£¼ì„±/ì´ë™ì„±ì˜ ì°¨ì´ê°€ ë°©ì–¸ ë°œë‹¬ì˜ ì›ì¸ì´ë¼ê³  ì œì‹œí•˜ì§€ ì•ŠìŒ (E) ë‹¤ì–¸ì–´ ëŠ¥ë ¥ì€ ë¶€ì°¨ì  ì£¼ì œ (F) 2ê°œì›” í›„ì— í•™ìŠµì„ ì‹œì‘í•œë‹¤ëŠ” ë‚´ìš© ì—†ìŒ (ì˜¤íˆë ¤ ì²« 2ê°œì›”ì´ ì¤‘ìš”í•œ ì‹œê¸°) </div>"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-14-ê±±ì •í•˜ëŠ”í‘œí˜„",
    "title": "ê±±ì •í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-14",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ worried love life : ì—°ì•  ì‚¬ì—… I'm worried about my love life. worry about : (í•­ìƒ) ~ë¥¼ ê±±ì •í•´ I worry about my future. 'm worried about : (ìš”ì¦˜) ê±±ì • ë¼ I'm worried about my love life. I'm worried about my job interview. I'm worried about the traffic. ì°¨ê°€ ë§‰íê¹Œë°”ã…— I'm worried about my presentation tommorow. ì¡°ê¸ˆ ê±±ì •ëœë‹¤ I'm a little worried about ì—„ì²­ ê±±ì •ëœë‹¤ I'm really worried about ì‹ ê²½ì“°ì¸ë‹¤ I'm concerned about the new flu virus. I'm concerned about the news lately. travel ban What are some things that are on your mind? as well , ë‚˜ë„ ê°„ë‹¨í•œ ì‘ë¬¸ Nowadays, I'm worried about my future of career. ë‚´ê°€ ì‹œì¥ ê°€ì¹˜ê°€ ìˆì„ì§€? ë‹¤ë¥¸ íšŒì‚¬ì— ê°ˆ ìˆ˜ ìˆì„ì§€?ì— ëŒ€í•´ ê³ ë¯¼ì„ í•˜ê³  ìˆì–´. I'm preparing the job transition, but if fails in the resume screening. So I can believe my self. ì•ìœ¼ë¡œ ë” ë‚˜ì•„ì§ˆì§€. My ì—°ë´‰ can be better than now? I have a some concerns, but what I have to do or I can to is just focus on the today, just step ahead. So í˜¸í¡ì„ ê°€ë‹¤ë“¬ê³ , í˜„ì¬ì— ì§‘ì¤‘í•˜ë ¤ê³  ë…¸ë ¥í•œë‹¤. ìˆ˜ì •ëœ ì˜ì–´ ê¸€ Nowadays, I'm worried about my career path. I keep wondering if I'm competitive enough in the job market to move to another company. I'm preparing for a job transition, but I'm afraid of failing at the resume screening stage. It makes me doubt myself and wonder if I can actually improve from here. I also question if I can earn a better salary than I do now. Even with these concerns, I know that all I can really do is focus on today and take things one step at a time. So, I'm trying to take a deep breath and concentrate on the present. í”¼ë“œë°± ë° í•´ì„¤ my future of career â†’ my career path í”¼ë“œë°±: careerëŠ” ì´ë¯¸ ë¯¸ë˜ì˜ ê¸¸(path)ì´ë¼ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ í¬í•¨í•˜ê³  ìˆì–´ìš”. my career path (ë‚˜ì˜ ì»¤ë¦¬ì–´ ê²½ë¡œ) ë˜ëŠ” my future career (ë‚˜ì˜ ë¯¸ë˜ ì»¤ë¦¬ì–´)ë¼ê³  í‘œí˜„í•˜ëŠ” ê²ƒì´ í›¨ì”¬ ìì—°ìŠ¤ëŸ¬ì›Œìš”. ì˜ˆì‹œ: \"I need some advice on my career path.\" (ì œ ì»¤ë¦¬ì–´ ê²½ë¡œì— ëŒ€í•´ ì¡°ì–¸ì´ í•„ìš”í•´ìš”.) if fails in the resume screening â†’ I'm afraid of failing at the resume screening stage í”¼ë“œë°±: ì›ë˜ ë¬¸ì¥ì€ \"ë§Œì•½ ì„œë¥˜ ì‹¬ì‚¬ì—ì„œ ë–¨ì–´ì§€ë©´\"ì´ë¼ëŠ” ì¡°ê±´ì ˆë§Œ ìˆì–´ì„œ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ì „í•˜ì§€ ì•Šì•„ìš”. \"ì„œë¥˜ ì‹¬ì‚¬ ë‹¨ê³„ì—ì„œ ë–¨ì–´ì§€ëŠ” ê²ƒì´ ë‘ë µë‹¤\"ëŠ” ì˜ë¯¸ë¡œ I'm afraid of failing...ê³¼ ê°™ì´ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ë§Œë“¤ì–´ì£¼ì—ˆì–´ìš”. íŒ: afraid of + (ë™)ëª…ì‚¬ëŠ” \"~í•˜ëŠ” ê²ƒì´ ë‘ë µë‹¤\"ëŠ” í‘œí˜„ìœ¼ë¡œ ìì£¼ ì“°ì—¬ìš”. So I can believe my self â†’ It makes me doubt myself í”¼ë“œë°±: ë¬¸ë§¥ìƒ \"ë‚˜ ìì‹ ì„ ë¯¿ì„ ìˆ˜ ìˆì„ê¹Œ?\"ë¼ë©° ìŠ¤ìŠ¤ë¡œë¥¼ ì˜ì‹¬í•˜ëŠ” ì˜ë¯¸ë¡œ í•´ì„ë¼ìš”. doubt myself (ë‚˜ ìì‹ ì„ ì˜ì‹¬í•˜ë‹¤)ë¼ëŠ” í‘œí˜„ì´ ë” ëª…í™•í•˜ê²Œ ì˜ë¯¸ë¥¼ ì „ë‹¬í•´ìš”. ë˜í•œ, myselfëŠ” í•­ìƒ í•œ ë‹¨ì–´ë¡œ ë¶™ì—¬ ì¨ì•¼ í•©ë‹ˆë‹¤. ì˜ˆì‹œ: \"Don't doubt yourself. You can do it!\" (ìŠ¤ìŠ¤ë¡œë¥¼ ì˜ì‹¬í•˜ì§€ ë§ˆ. ë„Œ í•  ìˆ˜ ìˆì–´!) My ì—°ë´‰ can be better than now? â†’ if I can earn a better salary than I do now í”¼ë“œë°±: \"ë‚´ ì—°ë´‰ì´ ì§€ê¸ˆë³´ë‹¤ ë‚˜ì•„ì§ˆ ìˆ˜ ìˆì„ê¹Œ?\"ë¼ëŠ” ì§ˆë¬¸ì„ if (~ì¸ì§€ ì•„ë‹Œì§€)ë¥¼ ì‚¬ìš©í•´ \"ë” ë‚˜ì€ ì—°ë´‰ì„ ë°›ì„ ìˆ˜ ìˆì„ì§€ ê¶ê¸ˆí•˜ë‹¤/ì˜ë¬¸ì´ë‹¤\"ëŠ” ì˜ë¯¸ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ë¬¸ì¥ì— ë…¹ì—¬ëƒˆì–´ìš”. ì˜ˆì‹œ: \"I wonder if I can finish this on time.\" (ì´ê±¸ ì œì‹œê°„ì— ëë‚¼ ìˆ˜ ìˆì„ì§€ ëª¨ë¥´ê² ì–´.) I have a some concerns â†’ Even with these concerns í”¼ë“œë°±: aì™€ someì€ í•¨ê»˜ ì“¸ ìˆ˜ ì—†ì–´ìš”. I have some concerns (ëª‡ ê°€ì§€ ê±±ì •ì´ ìˆë‹¤)ê°€ ë§ìŠµë‹ˆë‹¤. ë¬¸ë§¥ì˜ íë¦„ì„ ë” ìì—°ìŠ¤ëŸ½ê²Œ í•˜ê¸° ìœ„í•´ \"ì´ëŸ° ê±±ì •ì—ë„ ë¶ˆêµ¬í•˜ê³ \"ë¼ëŠ” ì˜ë¯¸ì˜ Even with these concernsë¡œ ìˆ˜ì •í–ˆì–´ìš”. what I have to do or I can to is... â†’ all I can really do is... í”¼ë“œë°±: ì›ë˜ ë¬¸ì¥ì˜ I can toëŠ” ë¬¸ë²•ì ìœ¼ë¡œ í‹€ë¦° í‘œí˜„ì´ì—ìš”. I can doê°€ ë§ìŠµë‹ˆë‹¤. ì˜ë¯¸ë¥¼ ì‚´ë ¤ì„œ \"ë‚´ê°€ ì •ë§ë¡œ í•  ìˆ˜ ìˆëŠ” ëª¨ë“  ê²ƒì€\"ì´ë¼ëŠ” all I can really do is...ë¡œ ë‹¤ë“¬ì—ˆì–´ìš”. focus on the today, just step ahead â†’ focus on today and take things one step at a time í”¼ë“œë°±: today, yesterday, tomorrow ì•ì—ëŠ” ë³´í†µ theë¥¼ ë¶™ì´ì§€ ì•Šì•„ìš”. just step aheadë„ ì¢‹ì€ í‘œí˜„ì´ì§€ë§Œ, \"ì°¨ê·¼ì°¨ê·¼ ë‚˜ì•„ê°€ë‹¤\"ë¼ëŠ” ì˜ë¯¸ë¥¼ ë” ì˜ ì‚´ë¦¬ëŠ” ê´€ìš©êµ¬ì¸ take things one step at a timeì´ ì´ëŸ° ìƒí™©ì— ì•„ì£¼ ì˜ ì–´ìš¸ë ¤ìš”."
  },
  {
    "id": "Data Science/dependency-injection-pattern",
    "title": "ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒí•˜ê¸°",
    "date": "2025-07-14",
    "excerpt": "ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì„ í†µí•´ ì½”ë“œì˜ ê²°í•©ë„ë¥¼ ë‚®ì¶”ê³  í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ ë†’ì´ëŠ” ë°©ë²•ì„ ì‹¤ì œ ì˜ˆì œì™€ í•¨ê»˜ ì„¤ëª…í•©ë‹ˆë‹¤.",
    "category": "Data Science",
    "tags": [
      "python",
      "design-pattern",
      "dependency-injection",
      "software-architecture",
      "clean-code",
      "testing"
    ],
    "content": "ë°°ê²½ ìµœê·¼ì— ëŒ€í™”í˜• AI ì±—ë´‡ í”„ë¡œì íŠ¸ë¥¼ ê°œë°œí•˜ë©´ì„œ ê²ªì—ˆë˜ ì½”ë“œ êµ¬ì¡°ì˜ ë¬¸ì œì ë“¤ê³¼, ì´ë¥¼ ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ê°œì„ í•œ ê²½í—˜ì„ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ ìƒí™© FastAPI ê¸°ë°˜ ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ OpenAI GPT ëª¨ë¸ì„ í™œìš©í•œ ëŒ€í™”í˜• ê²€ìƒ‰ ì‹œìŠ¤í…œ ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ê²€ìƒ‰ ì‚¬ìš©ìë³„ ëŒ€í™” ì„¸ì…˜ ê´€ë¦¬ ê²ªì—ˆë˜ ë¬¸ì œì ë“¤ ê°•í•œ ê²°í•© (Tight Coupling) í´ë˜ìŠ¤ë“¤ì´ ì„œë¡œ ë„ˆë¬´ ë°€ì ‘í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆì–´ì„œ, í•˜ë‚˜ë¥¼ ìˆ˜ì •í•˜ë©´ ë‹¤ë¥¸ ê²ƒë“¤ë„ í•¨ê»˜ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ìƒí™©ì´ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í•˜ë“œì½”ë”©ëœ ì„¤ì • ëª¨ë¸ëª…, ë°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ, API í‚¤ ë“±ì´ ì½”ë“œì— ì§ì ‘ ì‘ì„±ë˜ì–´ ìˆì–´ì„œ í™˜ê²½ë³„ë¡œ ì„¤ì •ì„ ë°”ê¾¸ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. í…ŒìŠ¤íŠ¸ ì–´ë ¤ì›€ ê° ì»´í¬ë„ŒíŠ¸ë¥¼ ë…ë¦½ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸í•˜ê¸° ì–´ë ¤ì›Œì„œ, ì „ì²´ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•´ì•¼ë§Œ í…ŒìŠ¤íŠ¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ê´€ë ¨ ê°œë…ë“¤ ì •ë¦¬ ê°•í•œ ê²°í•© (Tight Coupling) - ì‰½ê²Œ ì´í•´í•˜ê¸° ğŸ  ì§‘ ì§“ê¸° ë¹„ìœ  ê°•í•œ ê²°í•©ì˜ ì˜ˆì‹œ: ``python ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ - ê°•í•œ ê²°í•© class ì§‘: def __init__(self): # ì§‘ì´ ì§ì ‘ ë²½ëŒê³µì¥ì„ ë§Œë“¤ì–´ì„œ ë²½ëŒì„ ê°€ì ¸ì˜´ self.ë²½ëŒê³µì¥ = ë²½ëŒê³µì¥() self.ë²½ëŒ = self.ë²½ëŒê³µì¥.ë²½ëŒë§Œë“¤ê¸°() # ì§‘ì´ ì§ì ‘ ëª©ìˆ˜ê³µì¥ì„ ë§Œë“¤ì–´ì„œ ë¬¸ì„ ê°€ì ¸ì˜´ self.ëª©ìˆ˜ê³µì¥ = ëª©ìˆ˜ê³µì¥() self.ë¬¸ = self.ëª©ìˆ˜ê³µì¥.ë¬¸ë§Œë“¤ê¸°() ` ë¬¸ì œì : ì§‘ì´ ë²½ëŒê³µì¥ê³¼ ëª©ìˆ˜ê³µì¥ì„ ì§ì ‘ ì•Œê³  ìˆì–´ì•¼ í•¨ ë‹¤ë¥¸ ì¬ë£Œ(ì½˜í¬ë¦¬íŠ¸, ì•Œë£¨ë¯¸ëŠ„)ë¡œ ë°”ê¾¸ë ¤ë©´ ì§‘ í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•´ì•¼ í•¨ í…ŒìŠ¤íŠ¸í•  ë•Œ ì§„ì§œ ê³µì¥ ëŒ€ì‹  ê°€ì§œ ê³µì¥ì„ ì‚¬ìš©í•˜ê¸° ì–´ë ¤ì›€ í•´ê²°ì±… - ëŠìŠ¨í•œ ê²°í•©: `python ê°œì„ ëœ ì½”ë“œ - ëŠìŠ¨í•œ ê²°í•© class ì§‘: def __init__(self, ë²½ëŒì œê³µì, ë¬¸ì œê³µì): # ì™¸ë¶€ì—ì„œ ë²½ëŒê³¼ ë¬¸ì„ ë°›ì•„ì˜´ self.ë²½ëŒ = ë²½ëŒì œê³µì.ë²½ëŒë§Œë“¤ê¸°() self.ë¬¸ = ë¬¸ì œê³µì.ë¬¸ë§Œë“¤ê¸°() ì‚¬ìš©í•  ë•Œ ì§‘1 = ì§‘(ë²½ëŒê³µì¥(), ëª©ìˆ˜ê³µì¥()) ì§‘2 = ì§‘(ì½˜í¬ë¦¬íŠ¸ê³µì¥(), ì•Œë£¨ë¯¸ëŠ„ê³µì¥()) # ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŒ! ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python ê°•í•œ ê²°í•© - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class ChatBot: def __init__(self): # ì§ì ‘ ì˜ì¡´ì„±ì„ ìƒì„± self.llm = OpenAI(model=\"gpt-4\") # OpenAIì— ê°•í•˜ê²Œ ê²°í•© self.database = ChromaDB(path=\"./db\") # ChromaDBì— ê°•í•˜ê²Œ ê²°í•© self.scheduler = APScheduler() # APSchedulerì— ê°•í•˜ê²Œ ê²°í•© ëŠìŠ¨í•œ ê²°í•© - ê°œì„ ëœ ì½”ë“œ class ChatBot: def __init__(self, llm_provider, database, scheduler): # ì™¸ë¶€ì—ì„œ ì˜ì¡´ì„±ì„ ì£¼ì…ë°›ìŒ self.llm = llm_provider self.database = database self.scheduler = scheduler ` í•˜ë“œì½”ë”©ëœ ì„¤ì • - ì™œ ë¬¸ì œì¸ê°€? ğŸ• í”¼ì ì£¼ë¬¸ ë¹„ìœ  í•˜ë“œì½”ë”©ëœ ì„¤ì •ì˜ ë¬¸ì œ: `python ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class í”¼ìì§‘: def __init__(self): self.í† í•‘ = \"í˜í¼ë¡œë‹ˆ\" # í•˜ë“œì½”ë”©! self.í¬ê¸° = \"ë¼ì§€\" # í•˜ë“œì½”ë”©! self.ê°€ê²© = 20000 # í•˜ë“œì½”ë”©! ëª¨ë“  í”¼ìê°€ ë˜‘ê°™ì´ ë‚˜ì˜´ ğŸ˜ í”¼ì1 = í”¼ìì§‘() # í•­ìƒ í˜í¼ë¡œë‹ˆ ë¼ì§€ 20000ì› í”¼ì2 = í”¼ìì§‘() # í•­ìƒ í˜í¼ë¡œë‹ˆ ë¼ì§€ 20000ì› ` ê°œì„ ëœ ì½”ë“œ: `python ì„¤ì • íŒŒì¼ ì‚¬ìš© class í”¼ìì„¤ì •: def __init__(self): self.ê¸°ë³¸í† í•‘ = \"í˜í¼ë¡œë‹ˆ\" self.ê¸°ë³¸í¬ê¸° = \"ë¼ì§€\" self.ê¸°ë³¸ê°€ê²© = 20000 class í”¼ìì§‘: def __init__(self, ì„¤ì •): self.í† í•‘ = ì„¤ì •.ê¸°ë³¸í† í•‘ self.í¬ê¸° = ì„¤ì •.ê¸°ë³¸í¬ê¸° self.ê°€ê²© = ì„¤ì •.ê¸°ë³¸ê°€ê²© í™˜ê²½ë³„ë¡œ ë‹¤ë¥¸ ì„¤ì • ì‚¬ìš© ê°€ëŠ¥ ê°œë°œì„¤ì • = í”¼ìì„¤ì •() ê°œë°œì„¤ì •.ê¸°ë³¸í† í•‘ = \"ì¹˜ì¦ˆ\" # ê°œë°œí™˜ê²½ì€ ì¹˜ì¦ˆ ìš´ì˜ì„¤ì • = í”¼ìì„¤ì •() ìš´ì˜ì„¤ì •.ê¸°ë³¸í† í•‘ = \"í˜í¼ë¡œë‹ˆ\" # ìš´ì˜í™˜ê²½ì€ í˜í¼ë¡œë‹ˆ ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python í•˜ë“œì½”ë”©ëœ ì„¤ì • - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class AIì±—ë´‡: def __init__(self): self.model = \"gpt-4o-mini-2024-07-18\" # í•˜ë“œì½”ë”©! self.api_key = \"sk-1234567890abcdef\" # í•˜ë“œì½”ë”©! self.db_path = \"./db/chroma_db\" # í•˜ë“œì½”ë”©! ê°œì„ ëœ ì½”ë“œ - ì„¤ì • íŒŒì¼ ì‚¬ìš© import os from pydantic import BaseSettings class Settings(BaseSettings): model: str = \"gpt-4o-mini-2024-07-18\" api_key: str = \"\" db_path: str = \"./db/chroma_db\" class Config: env_file = \".env\" # í™˜ê²½ë³€ìˆ˜ íŒŒì¼ì—ì„œ ì½ì–´ì˜´ class AIì±—ë´‡: def __init__(self, settings: Settings): self.model = settings.model self.api_key = settings.api_key self.db_path = settings.db_path ì‚¬ìš© settings = Settings() ì±—ë´‡ = AIì±—ë´‡(settings) ` ì˜ì¡´ì„± ì£¼ì… (Dependency Injection) - í•µì‹¬ ê°œë… ğŸ¯ ì˜ì¡´ì„± ì£¼ì…ì´ë€? ì˜ì¡´ì„±(Dependency): ê°ì²´ê°€ ë™ì‘í•˜ê¸° ìœ„í•´ í•„ìš”í•œ ë‹¤ë¥¸ ê°ì²´ë“¤ ì£¼ì…(Injection): ì™¸ë¶€ì—ì„œ ê·¸ ê°ì²´ë“¤ì„ ì œê³µí•´ì£¼ëŠ” ê²ƒ ğŸš— ìë™ì°¨ ë¹„ìœ  `python ì˜ì¡´ì„± ì£¼ì… ì—†ì´ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class ìë™ì°¨: def __init__(self): # ìë™ì°¨ê°€ ì§ì ‘ ì—”ì§„ì„ ë§Œë“¦ self.ì—”ì§„ = ê°€ì†”ë¦°ì—”ì§„() # ê°€ì†”ë¦°ì—”ì§„ì— ê°•í•˜ê²Œ ê²°í•© self.íƒ€ì´ì–´ = ë¯¸ì‰ë¦°íƒ€ì´ì–´() # ë¯¸ì‰ë¦°íƒ€ì´ì–´ì— ê°•í•˜ê²Œ ê²°í•© ë¬¸ì œì : ì „ê¸°ì°¨ë¡œ ë°”ê¾¸ë ¤ë©´ ìë™ì°¨ í´ë˜ìŠ¤ë¥¼ ìˆ˜ì •í•´ì•¼ í•¨ í…ŒìŠ¤íŠ¸í•  ë•Œ ì§„ì§œ ì—”ì§„ ëŒ€ì‹  ê°€ì§œ ì—”ì§„ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŒ ë‹¤ë¥¸ ë¸Œëœë“œ íƒ€ì´ì–´ë¡œ ë°”ê¾¸ê¸° ì–´ë ¤ì›€ ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš© - ê°œì„ ëœ ì½”ë“œ class ìë™ì°¨: def __init__(self, ì—”ì§„, íƒ€ì´ì–´): # ì™¸ë¶€ì—ì„œ ì—”ì§„ê³¼ íƒ€ì´ì–´ë¥¼ ë°›ì•„ì˜´ self.ì—”ì§„ = ì—”ì§„ self.íƒ€ì´ì–´ = íƒ€ì´ì–´ ì‚¬ìš©í•  ë•Œ ê°€ì†”ë¦°ì°¨ = ìë™ì°¨(ê°€ì†”ë¦°ì—”ì§„(), ë¯¸ì‰ë¦°íƒ€ì´ì–´()) ì „ê¸°ì°¨ = ìë™ì°¨(ì „ê¸°ì—”ì§„(), í•œêµ­íƒ€ì´ì–´()) # ì‰½ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŒ! í…ŒìŠ¤íŠ¸í•  ë•Œ í…ŒìŠ¤íŠ¸ì°¨ = ìë™ì°¨(ê°€ì§œì—”ì§„(), ê°€ì§œíƒ€ì´ì–´()) # ê°€ì§œ ë¶€í’ˆìœ¼ë¡œ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥! ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python ì˜ì¡´ì„± ì£¼ì… ì—†ì´ class ChatBot: def __init__(self): self.llm = OpenAI(model=\"gpt-4\") # ì§ì ‘ ìƒì„± self.db = ChromaDB(path=\"./db\") # ì§ì ‘ ìƒì„± ì˜ì¡´ì„± ì£¼ì… ì‚¬ìš© class ChatBot: def __init__(self, llm_provider, database): self.llm = llm_provider # ì™¸ë¶€ì—ì„œ ì£¼ì… self.database = database # ì™¸ë¶€ì—ì„œ ì£¼ì… ì‚¬ìš©í•  ë•Œ ì±—ë´‡1 = ChatBot(OpenAI(model=\"gpt-4\"), ChromaDB(path=\"./db\")) ì±—ë´‡2 = ChatBot(Anthropic(model=\"claude\"), Pinecone(index=\"my-index\")) í…ŒìŠ¤íŠ¸í•  ë•Œ í…ŒìŠ¤íŠ¸ì±—ë´‡ = ChatBot(MockLLM(), MockDatabase()) ` íŒ©í† ë¦¬ íŒ¨í„´ (Factory Pattern) - ê°ì²´ ìƒì‚° ê³µì¥ ğŸ­ ê³µì¥ ë¹„ìœ  íŒ©í† ë¦¬ íŒ¨í„´ì´ë€? ê°ì²´ë¥¼ ë§Œë“œëŠ” ì „ìš© ê³µì¥ì„ ë”°ë¡œ ë‘ëŠ” íŒ¨í„´ `python íŒ©í† ë¦¬ ì—†ì´ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class í”¼ìì§‘: def __init__(self): # í”¼ìì§‘ì´ ì§ì ‘ ì¬ë£Œë¥¼ ì¤€ë¹„ self.ë„ìš° = ë°€ê°€ë£¨ë„ìš°() self.ì†ŒìŠ¤ = í† ë§ˆí† ì†ŒìŠ¤() self.ì¹˜ì¦ˆ = ëª¨ì§œë ë¼ì¹˜ì¦ˆ() ë¬¸ì œì : í”¼ìì§‘ì´ ì¬ë£Œ ì¤€ë¹„ ë°©ë²•ì„ ëª¨ë‘ ì•Œì•„ì•¼ í•¨ íŒ©í† ë¦¬ íŒ¨í„´ ì‚¬ìš© - ê°œì„ ëœ ì½”ë“œ class ì¬ë£Œê³µì¥: def ë„ìš°ë§Œë“¤ê¸°(self): return ë°€ê°€ë£¨ë„ìš°() def ì†ŒìŠ¤ë§Œë“¤ê¸°(self): return í† ë§ˆí† ì†ŒìŠ¤() def ì¹˜ì¦ˆë§Œë“¤ê¸°(self): return ëª¨ì§œë ë¼ì¹˜ì¦ˆ() class í”¼ìì§‘: def __init__(self, ì¬ë£Œê³µì¥): self.ê³µì¥ = ì¬ë£Œê³µì¥ self.ë„ìš° = self.ê³µì¥.ë„ìš°ë§Œë“¤ê¸°() self.ì†ŒìŠ¤ = self.ê³µì¥.ì†ŒìŠ¤ë§Œë“¤ê¸°() self.ì¹˜ì¦ˆ = self.ê³µì¥.ì¹˜ì¦ˆë§Œë“¤ê¸°() ì‚¬ìš© ê³µì¥ = ì¬ë£Œê³µì¥() í”¼ìì§‘ = í”¼ìì§‘(ê³µì¥) ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python LLM Factory class LLMFactory: def __init__(self, config): self.config = config def create_refine_llm(self): return OpenAI( model=self.config.get(\"refine_model\", \"gpt-4o-mini\"), api_key=self.config.get(\"api_key\") ) def create_answer_llm(self): return OpenAI( model=self.config.get(\"answer_model\", \"gpt-4o\"), api_key=self.config.get(\"api_key\") ) ì‚¬ìš© config = {\"refine_model\": \"gpt-4o-mini\", \"answer_model\": \"gpt-4o\"} factory = LLMFactory(config) refine_llm = factory.create_refine_llm() answer_llm = factory.create_answer_llm() ` ì‹±ê¸€í†¤ íŒ¨í„´ (Singleton Pattern) - í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ëŠ” ê°ì²´ ğŸ‘‘ ì™• ë¹„ìœ  ì‹±ê¸€í†¤ íŒ¨í„´ì´ë€? í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ë„ë¡ ë³´ì¥í•˜ëŠ” íŒ¨í„´ `python ì¼ë°˜ì ì¸ í´ë˜ìŠ¤ - ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ê°€ëŠ¥ class ì¼ë°˜ì‚¬ëŒ: def __init__(self, ì´ë¦„): self.ì´ë¦„ = ì´ë¦„ ì‚¬ëŒ1 = ì¼ë°˜ì‚¬ëŒ(\"ê¹€ì² ìˆ˜\") ì‚¬ëŒ2 = ì¼ë°˜ì‚¬ëŒ(\"ì´ì˜í¬\") print(ì‚¬ëŒ1 is ì‚¬ëŒ2) # False - ë‹¤ë¥¸ ê°ì²´ ì‹±ê¸€í†¤ íŒ¨í„´ - í•˜ë‚˜ë§Œ ì¡´ì¬ class ì™•: _instance = None # í´ë˜ìŠ¤ ë³€ìˆ˜ë¡œ ì¸ìŠ¤í„´ìŠ¤ ì €ì¥ def __new__(cls): if cls._instance is None: cls._instance = super().__new__(cls) cls._instance.ì´ë¦„ = \"í˜„ì¬ì™•\" return cls._instance ì™•1 = ì™•() ì™•2 = ì™•() print(ì™•1 is ì™•2) # True - ê°™ì€ ê°ì²´! ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° - ì‹±ê¸€í†¤ íŒ¨í„´ class DatabaseConnection: _instance = None def __new__(cls): if cls._instance is None: cls._instance = super().__new__(cls) cls._instance.connection = create_database_connection() return cls._instance def get_connection(self): return self.connection ì‚¬ìš© db1 = DatabaseConnection() db2 = DatabaseConnection() print(db1 is db2) # True - ê°™ì€ ì—°ê²° ê°ì²´ ` í”„ë¡œí† ì½œ (Protocol) - ì¸í„°í˜ì´ìŠ¤ ì •ì˜ ğŸ“‹ ê³„ì•½ì„œ ë¹„ìœ  í”„ë¡œí† ì½œì´ë€? ê°ì²´ê°€ ì–´ë–¤ ë©”ì„œë“œë¥¼ ê°€ì ¸ì•¼ í•˜ëŠ”ì§€ ì •ì˜í•˜ëŠ” \"ê³„ì•½ì„œ\" `python í”„ë¡œí† ì½œ ì—†ì´ - ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ class ChatBot: def __init__(self, llm): self.llm = llm def ëŒ€í™”í•˜ê¸°(self, ë©”ì‹œì§€): # llmì´ generate ë©”ì„œë“œë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  ê°€ì • return self.llm.generate(ë©”ì‹œì§€) # ëŸ°íƒ€ì„ì— ì—ëŸ¬ ê°€ëŠ¥! ë¬¸ì œì : llm ê°ì²´ê°€ generate ë©”ì„œë“œê°€ ì—†ìœ¼ë©´ ì—ëŸ¬ ë°œìƒ í”„ë¡œí† ì½œ ì‚¬ìš© - ê°œì„ ëœ ì½”ë“œ from typing import Protocol class LLMProvider(Protocol): \"\"\"LLM ì œê³µìëŠ” ë°˜ë“œì‹œ generate ë©”ì„œë“œë¥¼ ê°€ì ¸ì•¼ í•¨\"\"\" def generate(self, message: str) -> str: ... class ChatBot: def __init__(self, llm: LLMProvider): # íƒ€ì… íŒíŠ¸ë¡œ ê²€ì¦ self.llm = llm def ëŒ€í™”í•˜ê¸°(self, ë©”ì‹œì§€): return self.llm.generate(ë©”ì‹œì§€) # ì•ˆì „! ì‚¬ìš© class OpenAI: def generate(self, message: str) -> str: return f\"OpenAI: {message}\" class Anthropic: def generate(self, message: str) -> str: return f\"Anthropic: {message}\" ë‘˜ ë‹¤ LLMProvider í”„ë¡œí† ì½œì„ ë§Œì¡±í•˜ë¯€ë¡œ ì‚¬ìš© ê°€ëŠ¥ ì±—ë´‡1 = ChatBot(OpenAI()) ì±—ë´‡2 = ChatBot(Anthropic()) ` ì‹¤ì œ ì½”ë“œ ì˜ˆì‹œ `python from typing import Protocol, List, Dict, Any class Retriever(Protocol): \"\"\"ê²€ìƒ‰ê¸° ì¸í„°í˜ì´ìŠ¤\"\"\" def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: \"\"\"ì¿¼ë¦¬ë¡œ ë¬¸ì„œë¥¼ ê²€ìƒ‰\"\"\" ... class ChromaRetriever: def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: # ChromaDBë¡œ ê²€ìƒ‰í•˜ëŠ” ì‹¤ì œ êµ¬í˜„ return [{\"content\": \"ê²€ìƒ‰ëœ ë¬¸ì„œ\", \"score\": 0.9}] class PineconeRetriever: def search(self, query: str, max_results: int) -> List[Dict[str, Any]]: # Pineconeìœ¼ë¡œ ê²€ìƒ‰í•˜ëŠ” ì‹¤ì œ êµ¬í˜„ return [{\"content\": \"ê²€ìƒ‰ëœ ë¬¸ì„œ\", \"score\": 0.8}] ë‘˜ ë‹¤ Retriever í”„ë¡œí† ì½œì„ ë§Œì¡±í•˜ë¯€ë¡œ ê°™ì€ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš© ê°€ëŠ¥ def search_documents(retriever: Retriever, query: str): return retriever.search(query, max_results=5) ì‚¬ìš© chroma_results = search_documents(ChromaRetriever(), \"ê²€ìƒ‰ì–´\") pinecone_results = search_documents(PineconeRetriever(), \"ê²€ìƒ‰ì–´\") ` ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ìœ¼ë¡œ ì½”ë“œ í’ˆì§ˆ í–¥ìƒí•˜ê¸° ê°œìš” ì˜ì¡´ì„± ì£¼ì…(Dependency Injection, DI)ì€ ê°ì²´ê°€ í•„ìš”ë¡œ í•˜ëŠ” ì˜ì¡´ì„±ì„ ì™¸ë¶€ì—ì„œ ì œê³µë°›ëŠ” ë””ìì¸ íŒ¨í„´ì…ë‹ˆë‹¤. ì´ íŒ¨í„´ì„ í†µí•´ ì½”ë“œì˜ ê²°í•©ë„ë¥¼ ë‚®ì¶”ê³ , í…ŒìŠ¤íŠ¸ ìš©ì´ì„±ì„ ë†’ì´ë©°, ìœ ì§€ë³´ìˆ˜ì„±ì„ ê°œì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í˜„ì¬ ì½”ë“œì˜ ë¬¸ì œì  ê°•í•œ ê²°í•© (Tight Coupling) `python ë¬¸ì œê°€ ìˆëŠ” ì½”ë“œ ì˜ˆì‹œ class MyApp: def __init__(self): self.app = FastAPI() # ì§ì ‘ ì˜ì¡´ì„± ìƒì„± - ê°•í•œ ê²°í•© self.retriever = ChromadbRetriever(\"./db/chroma_db\") self.scheduler = BackgroundScheduler(timezone=\"Asia/Seoul\") self.llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\") ` ë¬¸ì œì : ëª¨ë“  ì˜ì¡´ì„±ì„ í´ë˜ìŠ¤ ë‚´ë¶€ì—ì„œ ì§ì ‘ ìƒì„± ë‹¤ë¥¸ êµ¬í˜„ì²´ë¡œ êµì²´í•˜ê¸° ì–´ë ¤ì›€ í…ŒìŠ¤íŠ¸ ì‹œ Mock ê°ì²´ ì£¼ì… ë¶ˆê°€ëŠ¥ í•˜ë“œì½”ë”©ëœ ì„¤ì • `python class ConversationalRetrievalChain: def __init__(self, retriever: ChromadbRetriever): # í•˜ë“œì½”ë”©ëœ ëª¨ë¸ëª…ë“¤ self.refine_llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\", system_message=\"\") self.answer_llm = LLM_openai(model=\"gpt-4o-2024-05-13\", system_message=\"\") self.lang_detect_llm = LLM_openai(model=\"gpt-4o-mini-2024-07-18\", system_message=\"\") ` ë¬¸ì œì : ëª¨ë¸ëª…ì´ ì½”ë“œì— í•˜ë“œì½”ë”©ë¨ í™˜ê²½ë³„ ì„¤ì • ë³€ê²½ ì–´ë ¤ì›€ ì„¤ì • ê´€ë¦¬ì˜ ì¤‘ì•™í™” ë¶€ì¡± ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ ì ìš© ì¸í„°í˜ì´ìŠ¤ ì •ì˜ (Protocol ì‚¬ìš©) ë¨¼ì € ì˜ì¡´ì„±ë“¤ì˜ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì •ì˜í•©ë‹ˆë‹¤: `python interfaces/llm_provider.py from typing import Protocol, List, Dict, Any class LLMProvider(Protocol): \"\"\"LLM ì œê³µì ì¸í„°í˜ì´ìŠ¤\"\"\" def generate(self, user_message: str) -> str: \"\"\"ë‹¨ì¼ ë©”ì‹œì§€ ìƒì„±\"\"\" ... def generate_with_chat_history(self, history: List, user_message: str) -> str: \"\"\"ì±„íŒ… íˆìŠ¤í† ë¦¬ì™€ í•¨ê»˜ ë©”ì‹œì§€ ìƒì„±\"\"\" ... class Retriever(Protocol): \"\"\"ê²€ìƒ‰ê¸° ì¸í„°í˜ì´ìŠ¤\"\"\" def get_docs_with_decomposition(self, query: str, max_k: int, threshold: float, lang: str) -> Dict[str, Any]: \"\"\"ì¿¼ë¦¬ ë¶„í•´ë¥¼ í†µí•œ ë¬¸ì„œ ê²€ìƒ‰\"\"\" ... class Scheduler(Protocol): \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì¸í„°í˜ì´ìŠ¤\"\"\" def add_job(self, func, trigger, kwargs): \"\"\"ì‘ì—… ì¶”ê°€\"\"\" ... def get_job(self, job_id: str): \"\"\"ì‘ì—… ì¡°íšŒ\"\"\" ... def start(self): \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì‹œì‘\"\"\" ... ` Factory í´ë˜ìŠ¤ë“¤ ìƒì„± ì˜ì¡´ì„± ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” Factory í´ë˜ìŠ¤ë“¤ì„ ë§Œë“­ë‹ˆë‹¤: `python factories/llm_factory.py from typing import Dict from utils.llm_providers import LLM_openai from interfaces.llm_provider import LLMProvider class LLMFactory: \"\"\"LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” Factory\"\"\" def __init__(self, config: Dict[str, str]): self.config = config def create_refine_llm(self) -> LLMProvider: \"\"\"ì •ì œìš© LLM ìƒì„±\"\"\" return LLM_openai( model=self.config.get(\"refine_model\", \"gpt-4o-mini-2024-07-18\"), system_message=\"\" ) def create_answer_llm(self) -> LLMProvider: \"\"\"ë‹µë³€ìš© LLM ìƒì„±\"\"\" return LLM_openai( model=self.config.get(\"answer_model\", \"gpt-4o-2024-05-13\"), system_message=\"\" ) def create_lang_detect_llm(self) -> LLMProvider: \"\"\"ì–¸ì–´ ê°ì§€ìš© LLM ìƒì„±\"\"\" return LLM_openai( model=self.config.get(\"lang_detect_model\", \"gpt-4o-mini-2024-07-18\"), system_message=get_lang_detect_message(type=\"system\") ) factories/retriever_factory.py from chains.custom_retriever import ChromadbRetriever from interfaces.retriever import Retriever class RetrieverFactory: \"\"\"ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ ìƒì„±ì„ ë‹´ë‹¹í•˜ëŠ” Factory\"\"\" def __init__(self, db_path: str): self.db_path = db_path def create_retriever(self) -> Retriever: \"\"\"ê²€ìƒ‰ê¸° ìƒì„±\"\"\" return ChromadbRetriever(self.db_path) ` ì„¤ì • ê´€ë¦¬ ê°œì„  ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ë¥¼ ìœ„í•´ Pydanticì„ ì‚¬ìš©í•©ë‹ˆë‹¤: `python config/settings.py from pydantic import BaseSettings from typing import Dict class Settings(BaseSettings): \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì„¤ì •\"\"\" # LLM ì„¤ì • refine_model: str = \"gpt-4o-mini-2024-07-18\" answer_model: str = \"gpt-4o-2024-05-13\" lang_detect_model: str = \"gpt-4o-mini-2024-07-18\" # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • chroma_db_path: str = \"./db/chroma_db\" # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • timezone: str = \"Asia/Seoul\" # API ì„¤ì • api_host: str = \"0.0.0.0\" api_port: int = 8000 class Config: env_file = \".env\" case_sensitive = False ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ settings = Settings() ` ì˜ì¡´ì„± ì»¨í…Œì´ë„ˆ ìƒì„± ì˜ì¡´ì„±ë“¤ì„ ê´€ë¦¬í•˜ëŠ” ì»¨í…Œì´ë„ˆë¥¼ ë§Œë“­ë‹ˆë‹¤: `python di/container.py from typing import Dict from fastapi import FastAPI from apscheduler.schedulers.background import BackgroundScheduler from factories.llm_factory import LLMFactory from factories.retriever_factory import RetrieverFactory from config.settings import settings class DependencyContainer: \"\"\"ì˜ì¡´ì„± ì£¼ì… ì»¨í…Œì´ë„ˆ\"\"\" def __init__(self): self._services: Dict[str, object] = {} self._factories: Dict[str, object] = {} self._setup_factories() def _setup_factories(self): \"\"\"Factory ì´ˆê¸°í™”\"\"\" self._factories[\"llm\"] = LLMFactory({ \"refine_model\": settings.refine_model, \"answer_model\": settings.answer_model, \"lang_detect_model\": settings.lang_detect_model }) self._factories[\"retriever\"] = RetrieverFactory(settings.chroma_db_path) def get_retriever(self): \"\"\"ê²€ìƒ‰ê¸° ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)\"\"\" if \"retriever\" not in self._services: self._services[\"retriever\"] = self._factories[\"retriever\"].create_retriever() return self._services[\"retriever\"] def get_scheduler(self): \"\"\"ìŠ¤ì¼€ì¤„ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜ (ì‹±ê¸€í†¤ íŒ¨í„´)\"\"\" if \"scheduler\" not in self._services: scheduler = BackgroundScheduler(timezone=settings.timezone) scheduler.start() self._services[\"scheduler\"] = scheduler return self._services[\"scheduler\"] def get_llm_factory(self): \"\"\"LLM Factory ë°˜í™˜\"\"\" return self._factories[\"llm\"] def reset(self): \"\"\"í…ŒìŠ¤íŠ¸ìš© ë¦¬ì…‹ ë©”ì„œë“œ\"\"\" self._services.clear() ì „ì—­ ì»¨í…Œì´ë„ˆ ì¸ìŠ¤í„´ìŠ¤ container = DependencyContainer() ` ë¦¬íŒ©í† ë§ëœ MyApp í´ë˜ìŠ¤ ì˜ì¡´ì„± ì£¼ì…ì„ ì ìš©í•œ MyApp í´ë˜ìŠ¤: `python app.py (ë¦¬íŒ©í† ë§ í›„) from fastapi import FastAPI from di.container import container from interfaces.retriever import Retriever from interfaces.scheduler import Scheduler class MyApp: \"\"\"ì˜ì¡´ì„± ì£¼ì…ì´ ì ìš©ëœ ë©”ì¸ ì• í”Œë¦¬ì¼€ì´ì…˜ í´ë˜ìŠ¤\"\"\" def __init__(self, retriever: Retriever = None, scheduler: Scheduler = None): self.app = FastAPI() # ì˜ì¡´ì„± ì£¼ì… self.retriever = retriever or container.get_retriever() self.scheduler = scheduler or container.get_scheduler() # ì‚¬ìš©ìë³„ ì„¸ì…˜ ê´€ë¦¬ self.user_conversations: Dict[str, ConversationalRetrievalChain] = {} # êµ¬ê¸€ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ ì—°ë™ self.sheet = None self.setup_events() def setup_events(self): \"\"\"ì´ë²¤íŠ¸ ì„¤ì •\"\"\" select_sheet(self) self.scheduler.add_job( select_sheet, \"cron\", hour=0, minute=0, args=[self], id=\"create new sheet\" ) def get_conversation_chain(self, user_id: str) -> ConversationalRetrievalChain: \"\"\"ì‚¬ìš©ìë³„ ëŒ€í™” ì²´ì¸ ë°˜í™˜\"\"\" if user_id not in self.user_conversations: self.user_conversations[user_id] = ConversationalRetrievalChain( retriever=self.retriever ) return self.user_conversations[user_id] í…ŒìŠ¤íŠ¸ìš© íŒ©í† ë¦¬ í•¨ìˆ˜ def create_app(retriever=None, scheduler=None) -> MyApp: \"\"\"ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)\"\"\" return MyApp(retriever=retriever, scheduler=scheduler) ì• í”Œë¦¬ì¼€ì´ì…˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„± myapp = create_app() ` ë¦¬íŒ©í† ë§ëœ ConversationalRetrievalChain í´ë˜ìŠ¤ ì˜ì¡´ì„± ì£¼ì…ì„ ì ìš©í•œ ì²´ì¸ í´ë˜ìŠ¤: `python chains/main_chain.py (ë¦¬íŒ©í† ë§ í›„) from interfaces.retriever import Retriever from factories.llm_factory import LLMFactory from di.container import container class ConversationalRetrievalChain: \"\"\"ì˜ì¡´ì„± ì£¼ì…ì´ ì ìš©ëœ ëŒ€í™”í˜• ê²€ìƒ‰ ì²´ì¸\"\"\" def __init__(self, retriever: Retriever, llm_factory: LLMFactory = None): self.retriever = retriever # Factoryë¥¼ í†µí•œ ì˜ì¡´ì„± ìƒì„± factory = llm_factory or container.get_llm_factory() self.refine_llm = factory.create_refine_llm() self.answer_llm = factory.create_answer_llm() self.lang_detect_llm = factory.create_lang_detect_llm() # ì²´ì»¤ ì²´ì¸ë“¤ self.checker_runnable_executor = get_combined_checker_chain( model_name=settings.lang_detect_model ) self.checker_reservation_runnable_executor = get_combined_checker_chain_reservation( model_name=settings.lang_detect_model ) self.chat_history = [] def process_query(self, user_message: str, user_id: str) -> str: \"\"\"ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬\"\"\" # ì–¸ì–´ ê°ì§€ detected_lang = self.detect_language(user_message) # ë¬¸ì„œ ê²€ìƒ‰ docs = self.retriever.get_docs_with_decomposition( query=user_message, max_k=5, threshold=0.7, lang=detected_lang ) # ë‹µë³€ ìƒì„± response = self.generate_response(user_message, docs) # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸ self.update_chat_history(user_message, response) return response ` í…ŒìŠ¤íŠ¸ ìš©ì´ì„± í–¥ìƒ Mock ê°ì²´ë¥¼ ì‚¬ìš©í•œ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ `python tests/test_myapp.py import pytest from unittest.mock import Mock, MagicMock from app import create_app class TestMyApp: \"\"\"MyApp í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸\"\"\" def test_app_initialization_with_mocks(self): \"\"\"Mock ê°ì²´ë¥¼ ì‚¬ìš©í•œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸\"\"\" # Mock ê°ì²´ ìƒì„± mock_retriever = Mock() mock_scheduler = Mock() # ì˜ì¡´ì„± ì£¼ì…ìœ¼ë¡œ ì•± ìƒì„± app = create_app(retriever=mock_retriever, scheduler=mock_scheduler) # ê²€ì¦ assert app.retriever == mock_retriever assert app.scheduler == mock_scheduler assert isinstance(app.app, FastAPI) def test_conversation_chain_creation(self): \"\"\"ëŒ€í™” ì²´ì¸ ìƒì„± í…ŒìŠ¤íŠ¸\"\"\" mock_retriever = Mock() app = create_app(retriever=mock_retriever) # ì‚¬ìš©ìë³„ ì²´ì¸ ìƒì„± chain = app.get_conversation_chain(\"user123\") assert chain is not None assert chain.retriever == mock_retriever tests/test_conversational_chain.py class TestConversationalRetrievalChain: \"\"\"ConversationalRetrievalChain í´ë˜ìŠ¤ í…ŒìŠ¤íŠ¸\"\"\" def test_chain_initialization_with_mock_factory(self): \"\"\"Mock Factoryë¥¼ ì‚¬ìš©í•œ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸\"\"\" mock_retriever = Mock() mock_factory = Mock() # Mock LLM ì¸ìŠ¤í„´ìŠ¤ë“¤ mock_refine_llm = Mock() mock_answer_llm = Mock() mock_lang_detect_llm = Mock() mock_factory.create_refine_llm.return_value = mock_refine_llm mock_factory.create_answer_llm.return_value = mock_answer_llm mock_factory.create_lang_detect_llm.return_value = mock_lang_detect_llm # ì²´ì¸ ìƒì„± chain = ConversationalRetrievalChain( retriever=mock_retriever, llm_factory=mock_factory ) # ê²€ì¦ assert chain.retriever == mock_retriever assert chain.refine_llm == mock_refine_llm assert chain.answer_llm == mock_answer_llm assert chain.lang_detect_llm == mock_lang_detect_llm ` í†µí•© í…ŒìŠ¤íŠ¸ `python tests/test_integration.py import pytest from di.container import container class TestIntegration: \"\"\"í†µí•© í…ŒìŠ¤íŠ¸\"\"\" def setup_method(self): \"\"\"í…ŒìŠ¤íŠ¸ ì „ ì»¨í…Œì´ë„ˆ ë¦¬ì…‹\"\"\" container.reset() def test_container_dependency_resolution(self): \"\"\"ì»¨í…Œì´ë„ˆ ì˜ì¡´ì„± í•´ê²° í…ŒìŠ¤íŠ¸\"\"\" retriever = container.get_retriever() scheduler = container.get_scheduler() llm_factory = container.get_llm_factory() assert retriever is not None assert scheduler is not None assert llm_factory is not None # ì‹±ê¸€í†¤ íŒ¨í„´ ê²€ì¦ retriever2 = container.get_retriever() assert retriever is retriever2 ` í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • `bash .env.development REFINE_MODEL=gpt-4o-mini-2024-07-18 ANSWER_MODEL=gpt-4o-2024-05-13 LANG_DETECT_MODEL=gpt-4o-mini-2024-07-18 CHROMA_DB_PATH=./db/chroma_db_dev TIMEZONE=Asia/Seoul API_HOST=0.0.0.0 API_PORT=8000 ` `bash .env.production REFINE_MODEL=gpt-4o-2024-05-13 ANSWER_MODEL=gpt-4o-2024-05-13 LANG_DETECT_MODEL=gpt-4o-mini-2024-07-18 CHROMA_DB_PATH=/data/chroma_db_prod TIMEZONE=Asia/Seoul API_HOST=0.0.0.0 API_PORT=80 ` ì„¤ì • ë¡œë”© ê°œì„  `python config/settings.py (ê°œì„ ëœ ë²„ì „) import os from pydantic import BaseSettings from typing import Dict class Settings(BaseSettings): \"\"\"í™˜ê²½ë³„ ì„¤ì • ê´€ë¦¬\"\"\" # í™˜ê²½ ì„¤ì • environment: str = \"development\" # LLM ì„¤ì • refine_model: str = \"gpt-4o-mini-2024-07-18\" answer_model: str = \"gpt-4o-2024-05-13\" lang_detect_model: str = \"gpt-4o-mini-2024-07-18\" # ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • chroma_db_path: str = \"./db/chroma_db\" # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì • timezone: str = \"Asia/Seoul\" # API ì„¤ì • api_host: str = \"0.0.0.0\" api_port: int = 8000 class Config: env_file = f\".env.{os.getenv('ENVIRONMENT', 'development')}\" case_sensitive = False settings = Settings() `` ì¥ì ê³¼ íš¨ê³¼ í…ŒìŠ¤íŠ¸ ìš©ì´ì„± Mock ê°ì²´ë¥¼ ì‰½ê²Œ ì£¼ì…í•  ìˆ˜ ìˆìŒ ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ì™€ í†µí•© í…ŒìŠ¤íŠ¸ ë¶„ë¦¬ ê°€ëŠ¥ í…ŒìŠ¤íŠ¸ ê²©ë¦¬ ë³´ì¥ ìœ ì—°ì„± ëŸ°íƒ€ì„ì— ë‹¤ë¥¸ êµ¬í˜„ì²´ë¡œ êµì²´ ê°€ëŠ¥ ìƒˆë¡œìš´ LLM ì œê³µì ì¶”ê°€ ìš©ì´ ì„¤ì • ë³€ê²½ ì‹œ ì½”ë“œ ìˆ˜ì • ë¶ˆí•„ìš” ìœ ì§€ë³´ìˆ˜ì„± ì¤‘ì•™í™”ëœ ì„¤ì • ê´€ë¦¬ ëª…í™•í•œ ì˜ì¡´ì„± ê´€ê³„ ë‹¨ì¼ ì±…ì„ ì›ì¹™ ì¤€ìˆ˜ í™•ì¥ì„± ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ì˜í–¥ ìµœì†Œí™” í”ŒëŸ¬ê·¸ì¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ ê°€ëŠ¥ ë§ˆì´í¬ë¡œì„œë¹„ìŠ¤ ì „í™˜ ìš©ì´ ê²°ë¡  ì˜ì¡´ì„± ì£¼ì… íŒ¨í„´ì„ ì ìš©í•¨ìœ¼ë¡œì¨ ì½”ë“œì˜ í’ˆì§ˆì„ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ: ê²°í•©ë„ ê°ì†Œ: í´ë˜ìŠ¤ ê°„ì˜ ì˜ì¡´ì„±ì´ ëª…ì‹œì ìœ¼ë¡œ ê´€ë¦¬ë¨ í…ŒìŠ¤íŠ¸ ìš©ì´ì„±: Mock ê°ì²´ ì£¼ì…ìœ¼ë¡œ ê²©ë¦¬ëœ í…ŒìŠ¤íŠ¸ ê°€ëŠ¥ ì„¤ì • ê´€ë¦¬: ì¤‘ì•™í™”ëœ ì„¤ì •ìœ¼ë¡œ í™˜ê²½ë³„ ê´€ë¦¬ ìš©ì´ í™•ì¥ì„±**: ìƒˆë¡œìš´ ê¸°ëŠ¥ ì¶”ê°€ ì‹œ ê¸°ì¡´ ì½”ë“œ ì˜í–¥ ìµœì†Œí™” ì´ëŸ¬í•œ íŒ¨í„´ì€ ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸ì—ì„œ íŠ¹íˆ ìœ ìš©í•˜ë©°, ì½”ë“œì˜ ì¥ê¸°ì ì¸ ìœ ì§€ë³´ìˆ˜ì„±ì„ ë³´ì¥í•©ë‹ˆë‹¤. ì°¸ê³  ìë£Œ Python Protocol Classes Pydantic Settings Management FastAPI Dependency Injection Clean Architecture by Robert C. Martin"
  },
  {
    "id": "Causal Inference/Industry Application/why-we-need-causal-inference",
    "title": "ì¸ê³¼ ì¶”ë¡  ë„ì…ë°°ê²½",
    "date": "2025-07-14",
    "excerpt": "ì¸ê³¼ì¶”ë¡ ì„ ì™œ ì‚¬ìš©í•´ì•¼í•˜ëŠ”ì§€ ì„¤ëª…í•˜ê³  ì´ë¥¼ ë„ì…í•˜ê¸° ìœ„í•´ ì„¤ë“í•˜ëŠ” ê¸€",
    "category": "Causal Inference",
    "tags": [
      "ICL-Lenze-sizing"
    ],
    "content": "Abstract - ë°°ê²½, ìš”ì•½ --- í™˜ìì—ê²Œ ë Œì¦ˆ ì‚½ì…ìˆ ì„ ìœ„í•œ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ”, ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œëŠ” ì˜ˆì¸¡ ëª¨ë¸ì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì–´ì™”ë‹¤. í™˜ìì˜ ëˆˆì˜ ì¡°ê±´ê³¼ ì‚½ì…í•œ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥ ë°›ì•„ ìˆ˜ìˆ ê²°ê³¼(vaulting, ë Œì¦ˆ í›„ë©´ê³¼ ìˆ˜ì •ì²´ ì „ë©´ê¹Œì§€ì˜ ê±°ë¦¬)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ê°œë°œí•˜ê³ , ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ì„ í†µí•´ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•˜ëŠ” ë°©ì‹ì´ë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì€ í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ì´ë²ˆ ë³´ê³ ì„œë¥¼ í†µí•´ ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì ì„ ë¶„ì„í•˜ê³  ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì ì„ ìš”ì•½í•˜ë©´ ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ê²°ê³¼ì— ì˜í–¥ì„ ì£¼ëŠ” ì¸ê³¼ì ì¸ ì˜í–¥ë„ë¥¼ ì¶”ì •í•˜ì§€ ëª»í•˜ë©°, ë‹¨ìˆœíˆ ì˜ˆì¸¡ í‰ê°€ì§€í‘œë§Œì„ ê³ ë ¤í•˜ê³  ìˆì–´ ì¶”ì²œì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•˜ì§€ ëª»í•œë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŸ¬í•œ í•œê³„ì ì„ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ ë„ì…í•  ê²ƒì„ ì œì•ˆí•œë‹¤. ì´ë•Œ ê¸°ëŒ€ë˜ëŠ” íš¨ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆê³  ì´ë¥¼ í†µí•´ ê°œì¸í™”ëœ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ì¶”ì²œì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•  ìˆ˜ ìˆë‹¤. Introduction - ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì  ë¶„ì„ --- ë Œì¦ˆ ì‚½ì…ìˆ ì„ ìœ„í•œ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì°¾ëŠ” ì¼ì€ ì˜ì‚¬ì˜ ë†’ì€ ìˆ˜ì¤€ì˜ ê²½í—˜ê³¼ ë…¸í•˜ìš°ì— ì˜ì¡´í•˜ëŠ” ì¼ì´ë‹¤. ì´ë¥¼ ë„ì™€ì£¼ê¸° ìœ„í•´ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ” ëª¨ë¸ì´ ê°œë°œë˜ì–´ì™”ë‹¤. ê¸°ì¡´ì˜ ìµœì ì˜ ì‚¬ì´ì¦ˆë¥¼ ì°¾ê¸° ìœ„í•œ ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œëŠ” ì˜ˆì¸¡ ëª¨ë¸ì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì–´ì™”ë‹¤. í™˜ìì˜ ëˆˆì˜ ì¡°ê±´ê³¼ ì‚½ì…í•œ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥ ë°›ì•„ ìˆ˜ìˆ ê²°ê³¼(vaulting, ë Œì¦ˆ í›„ë©´ê³¼ ìˆ˜ì •ì²´ ì „ë©´ê¹Œì§€ì˜ ê±°ë¦¬)ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë°©í–¥ì„ ì§„í–‰ë˜ì—ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ MAEì™€ ê°™ì´ ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ë¥¼ í‰ê°€í•˜ëŠ” ì˜ˆì¸¡í‰ê°€ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ì´ë¥¼ ê°œì„ í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì—°êµ¬ê°€ ì§„í–‰ë˜ì—ˆë‹¤. MAE: Mean Absolute Error, ì‹¤ì œ ê°’ê³¼ ì˜ˆì¸¡ê°’ì˜ ì°¨ì´ì˜ ì ˆëŒ€ê°’ì˜ í‰ê·  í•˜ì§€ë§Œ ì˜ˆì¸¡ ëª¨ë¸ ê¸°ë°˜ì˜ ë Œì¦ˆ ì‚¬ì´ì§• ëª¨ë¸ì¸ ê¸°ì¡´ ì—°êµ¬ëŠ” ë¶„ëª…í•œ í•œê³„ì ì´ ì¡´ì¬í•œë‹¤. ë¨¼ì € ì´ë¥¼ ì´í•´í•˜ê¸° ìœ„í•´ì„œëŠ” ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒì˜ ë¬¸ì œë¥¼ ì´í•´í•˜ê³  êµ¬ë¶„í•´ì•¼ í•œë‹¤. 1.1 ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒì˜ ë¬¸ì œ ì°¨ì´ --- ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒì˜ ë¬¸ì œëŠ” ê·¼ë³¸ì ìœ¼ë¡œ ë‹¤ë¥¸ ì„±ê²©ì„ ê°€ì§„ë‹¤. ë¨¼ì € ì˜ˆì¸¡ ë¬¸ì œëŠ” ì£¼ì–´ì§„ ì…ë ¥ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \"í™˜ì Aê°€ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ìœ¼ë©´ vaultingì´ ì–¼ë§ˆê°€ ë ê¹Œ?\"ì™€ ê°™ì€ ë¬¸ì œê°€ ì˜ˆì¸¡ ë¬¸ì œì— í•´ë‹¹í•œë‹¤. ì˜ˆì¸¡ ë¬¸ì œì˜ ëª©í‘œëŠ” ì •í™•í•œ ì˜ˆì¸¡ì´ë‹¤. ì˜ˆì¸¡ ë¬¸ì œì˜ ì„±ëŠ¥ ì§€í‘œëŠ” MAE, RMSE, RÂ² ë“±ì´ ìˆë‹¤. ì„ íƒì˜ ë¬¸ì œëŠ” ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ ìµœì ì˜ ì„ íƒì„ í•˜ëŠ” ë¬¸ì œë¥¼ ë§í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ \"í™˜ì Aì—ê²Œ 12.1mm, 12.6mm, 13.2mm ì¤‘ ì–´ë–¤ ë Œì¦ˆë¥¼ ì¶”ì²œí• ê¹Œ?\"ì™€ ê°™ì€ ë¬¸ì œê°€ ì„ íƒì˜ ë¬¸ì œì— í•´ë‹¹í•œë‹¤. ì„ íƒ ë¬¸ì œì˜ ëª©í‘œëŠ” ìµœì ì˜ ì„ íƒì´ë‹¤. ì„ íƒ ë¬¸ì œì˜ ì„±ëŠ¥ ì§€í‘œëŠ” ì²˜ì¹˜ íš¨ê³¼, ê°œì¸í™”ëœ ì¶”ì²œ ì •í™•ë„ ë“±ì´ ìˆë‹¤. ì˜ˆì¸¡ ë¬¸ì œì™€ ì„ íƒ ë¬¸ì œì˜ ê°€ì¥ í° ì°¨ì´ì ì€ í†µì œ ë³€ìˆ˜(Control Variable)ì˜ ì¡´ì¬ ì—¬ë¶€ì´ë‹¤. í†µì œ ë³€ìˆ˜ëŠ” ì—°êµ¬ìê°€ ì§ì ‘ ì¡°ì‘í•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ë¡œ, ì‹¤í—˜ ì„¤ê³„ì—ì„œ í•µì‹¬ì ì¸ ì—­í• ì„ í•œë‹¤. ì´ëŠ” ì¸ê³¼ì¶”ë¡ ì—ì„œ ì²˜ì¹˜(Treatment)ì™€ ë™ì¼í•œ ê°œë…ìœ¼ë¡œ, ì—°êµ¬ìê°€ ì˜ë„ì ìœ¼ë¡œ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë…ë¦½ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œëŠ” ë Œì¦ˆì˜ ì‚¬ì´ì¦ˆê°€ ì²˜ì¹˜ì— í•´ë‹¹í•˜ë©° ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì²˜ì¹˜íš¨ê³¼ë¼ê³  í•œë‹¤. ì˜ˆì¸¡ ë¬¸ì œì—ì„œëŠ” ëª¨ë“  ì…ë ¥ ë³€ìˆ˜ê°€ ì£¼ì–´ì§„ ê°’ìœ¼ë¡œ ê³ ì •ë˜ì–´ ìˆë‹¤. ì¦‰, ëª¨ë¸ì˜ ëª©í‘œëŠ” ì£¼ì–´ì§„ ì¡°ê±´ í•˜ì—ì„œ ê²°ê³¼ë¥¼ ì •í™•íˆ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, \"í™˜ì Aê°€ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ìœ¼ë©´ vaultingì´ ì–¼ë§ˆê°€ ë ê¹Œ?\"ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ë Œì¦ˆ ì‚¬ì´ì¦ˆ(12.6mm)ëŠ” ì´ë¯¸ ê²°ì •ëœ ê³ ì •ê°’ì´ë©°, ì´ë¥¼ ë³€ê²½í•  ìˆ˜ ì—†ë‹¤. ì´ ê²½ìš° ëª¨ë¸ì€ í™˜ìì˜ ëˆˆ ì¡°ê±´ê³¼ ê³ ì •ëœ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„ vaulting ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµí•œë‹¤. ì„ íƒ ë¬¸ì œì—ì„œëŠ” í†µì œ ë³€ìˆ˜ê°€ í•µì‹¬ ì—­í• ì„ í•œë‹¤. ì—°êµ¬ìëŠ” ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ì—ì„œ ìµœì ì˜ ì„ íƒì„ í•˜ê¸° ìœ„í•´ í†µì œ ë³€ìˆ˜ì˜ ê°’ì„ ë³€í™”ì‹œí‚¬ ìˆ˜ ìˆë‹¤. \"í™˜ì Aì—ê²Œ 12.1mm, 12.6mm, 13.2mm ì¤‘ ì–´ë–¤ ë Œì¦ˆë¥¼ ì¶”ì²œí• ê¹Œ?\"ë¼ëŠ” ì§ˆë¬¸ì—ì„œ ë Œì¦ˆ ì‚¬ì´ì¦ˆëŠ” í†µì œ ë³€ìˆ˜ë¡œ, ì—¬ëŸ¬ ì˜µì…˜ ì¤‘ì—ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ë³€ìˆ˜ì´ë‹¤. ì´ ê²½ìš° ëª¨ë¸ì€ ê° ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì˜µì…˜ì— ëŒ€í•œ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•˜ê³ , ìµœì ì˜ ì„ íƒì„ ì œì•ˆí•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµí•´ì•¼ í•œë‹¤. ì˜ˆì¸¡ ë¬¸ì œëŠ” ì¡°ê±´ë¶€ í™•ë¥  $P(Y X)$ë¥¼ ì¶”ì •í•˜ëŠ” ë¬¸ì œì´ê³ , ì„ íƒ ë¬¸ì œëŠ” ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜ íš¨ê³¼ $E[Y(T=t) X]$ë¥¼ ì¶”ì •í•˜ëŠ” ë¬¸ì œì´ë‹¤. ì—¬ê¸°ì„œ $T$ëŠ” í†µì œ ë³€ìˆ˜(ì²˜ì¹˜)ë¥¼, $X$ëŠ” í™˜ì íŠ¹ì„±ì„, $Y$ëŠ” ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜ íš¨ê³¼ë€ í†µì œ ë³€ìˆ˜ê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì²˜ì¹˜ íš¨ê³¼ì˜ í‰ê· ì„ ì˜ë¯¸í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì§•ì€ ë³¸ì§ˆì ìœ¼ë¡œ ì„ íƒì˜ ë¬¸ì œì´ë‹¤. í™˜ìì—ê²Œ ìµœì ì˜ ë Œì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ” ê²ƒì´ ëª©í‘œì´ê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ê¸°ì¡´ ì—°êµ¬ëŠ” ì˜ˆì¸¡ ëª¨ë¸ì˜ ì„±ëŠ¥ í–¥ìƒì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì„ íƒì˜ ë¬¸ì œë¥¼ ì œëŒ€ë¡œ í•´ê²°í•˜ì§€ ëª»í•œë‹¤. 1.2 ì˜ˆì¸¡ ëª¨ë¸ì´ ì„ íƒì˜ ë¬¸ì œì— í™œìš©ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ  --- ë Œì¦ˆ ì‚¬ì´ì§•ì´ ì„ íƒì˜ ë¬¸ì œì„ì„ í™•ì¸í–ˆìœ¼ë¯€ë¡œ, ì´ì œ ì˜ˆì¸¡ ëª¨ë¸ì´ ì„ íƒì˜ ë¬¸ì œì— í™œìš©ë˜ê¸° ì–´ë ¤ìš´ ì´ìœ ë¥¼ ë¶„ì„í•œë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì´ ê°€ì§€ëŠ” í•œê³„ì ì€ ë¬´ì—‡ì¸ì§€, ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒì„ í•  ë•Œì˜ ìœ„í—˜ì„±ì— ëŒ€í•´ ì´ì•¼ê¸°í•œë‹¤. 1.2.1 ìƒê´€ê´€ê³„ì— ê¸°ë°˜í•œ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ ì˜ˆì¸¡ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ê°„ì˜ ë™ì‹œ ë³€ë™ íŒ¨í„´, ì¦‰ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•˜ì—¬ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•œë‹¤. ìƒê´€ê´€ê³„ëŠ” ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ì¼ì •í•œ íŒ¨í„´ì´ë‚˜ ë™ë°˜ ë³€ë™ì´ ì¡´ì¬í•¨ì„ ì˜ë¯¸í•˜ë©°, í•œ ë³€ìˆ˜ê°€ ì»¤ì§ˆ ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ì¼ì •í•˜ê²Œ ì»¤ì§€ê±°ë‚˜ ì‘ì•„ì§€ëŠ” ê²½í–¥ì„ ë³´ì´ëŠ” í†µê³„ì  ê´€ê³„ë¥¼ ë§í•œë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì€ ìƒê´€ê´€ê³„ë§Œì„ í•™ìŠµí•˜ë¯€ë¡œ ìˆ¨ê²¨ì§„ êµë€ë³€ìˆ˜(Confounding Variable)ì˜ ì¡´ì¬ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•œë‹¤. êµë€ë³€ìˆ˜(Confounding Variable) : ì²˜ì¹˜(Treatment)ì™€ ê²°ê³¼(Outcome) ëª¨ë‘ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì œ3ì˜ ë³€ìˆ˜ë¡œ, ì²˜ì¹˜ì™€ ê²°ê³¼ ì‚¬ì´ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ì™œê³¡ì‹œí‚¤ëŠ” ë³€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. êµë€ë³€ìˆ˜ëŠ” ì²˜ì¹˜ ì„ íƒê³¼ ê²°ê³¼ì— ë™ì‹œì— ì˜í–¥ì„ ì£¼ì–´ ê°€ì§œ ì¸ê³¼ê´€ê³„ë¥¼ ë§Œë“¤ì–´ë‚´ê±°ë‚˜ ì‹¤ì œ ì¸ê³¼ê´€ê³„ë¥¼ ìˆ¨ê¸¸ ìˆ˜ ìˆë‹¤. êµë€í¸í–¥(Confounding Bias) : êµë€ë³€ìˆ˜ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” í†µê³„ì  í¸í–¥ìœ¼ë¡œ, ì²˜ì¹˜ì™€ ê²°ê³¼ ì‚¬ì´ì˜ ì‹¤ì œ ì¸ê³¼ê´€ê³„ê°€ ì™œê³¡ë˜ì–´ ë‚˜íƒ€ë‚˜ëŠ” í˜„ìƒì„ ì˜ë¯¸í•œë‹¤. ì´ëŠ” ì²˜ì¹˜ ê·¸ë£¹ ê°„ì— êµë€ë³€ìˆ˜ì˜ ë¶„í¬ê°€ ë‹¤ë¥¼ ë•Œ ë°œìƒí•˜ë©°, ì²˜ì¹˜ íš¨ê³¼ì˜ ì •í™•í•œ ì¶”ì •ì„ ë°©í•´í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ìˆ™ë°• ê°€ê²©ê³¼ ë§¤ì¶œ ë°ì´í„°ì—ì„œ ê°€ê²©ì´ ë†’ì„ìˆ˜ë¡ ë§¤ì¶œì´ ë†’ì•„ ë³´ì´ëŠ” íŒ¨í„´ì´ ê´€ì°°ë˜ë©´, ì˜ˆì¸¡ ëª¨ë¸ì€ \"ê°€ê²©ì„ ì˜¬ë¦¬ë©´ ë§¤ì¶œì´ ì˜¤ë¥¸ë‹¤\"ê³  ê²°ë¡ ì„ ë‚´ë¦°ë‹¤. í•˜ì§€ë§Œ ì´ íŒ¨í„´ ë’¤ì—ëŠ” ì„±ìˆ˜ê¸°/ë¹„ìˆ˜ê¸° ì—¬ë¶€ë¼ëŠ” ìˆ¨ê²¨ì§„ êµë€ë³€ìˆ˜ê°€ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œëŠ” ì„±ìˆ˜ê¸°ì— ê°€ê²©ê³¼ ë§¤ì¶œì´ ë™ì‹œì— ìƒìŠ¹í•˜ê³ , ë¹„ìˆ˜ê¸°ì— ê°€ê²©ê³¼ ë§¤ì¶œì´ ë™ì‹œì— í•˜ë½í•˜ëŠ” ê²ƒì´ë©°, ê°€ê²©ê³¼ ë§¤ì¶œ ì‚¬ì´ì—ëŠ” ì§ì ‘ì ì¸ ì¸ê³¼ê´€ê³„ê°€ ì—†ë‹¤. ì´ëŸ¬í•œ ìƒê´€ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ì˜¤í•´í•˜ë©´ \"ë§¤ì¶œì„ ì˜¬ë¦¬ë ¤ë©´ ê°€ê²©ì„ ì˜¬ë ¤ì•¼ í•œë‹¤\"ëŠ” ì˜ëª»ëœ ê²°ë¡ ì— ì´ë¥¼ ìˆ˜ ìˆê³ , ì‹¤ì œ íš¨ê³¼ê°€ ìˆëŠ” ê°œì…(ì˜ˆ: ë¹„ìˆ˜ê¸° í”„ë¡œëª¨ì…˜) ëŒ€ì‹  ë¶€ì ì ˆí•œ ê°€ê²© ì •ì±…ì„ ì„ íƒí•˜ê²Œ ë§Œë“ ë‹¤. ì¸ê³¼ê´€ê³„: í•œ ë³€ìˆ˜ê°€ ë³€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ê·¸ ë³€í™”ì— ì˜í•´ ì§ì ‘ì Â·ì²´ê³„ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ì•„ ë³€í™”í•˜ëŠ” ê´€ê³„ë¥¼ ì˜ë¯¸ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì˜ˆì¸¡ ì„±ëŠ¥ì´ ì¢‹ë‹¤ê³  í•´ì„œ ì¸ê³¼ê´€ê³„ë¥¼ ì˜ ì°¾ì•˜ë‹¤ëŠ” ë³´ì¥ì´ ì—†ìœ¼ë©°, ë Œì¦ˆ ì‚¬ì´ì§•ê³¼ ê°™ì€ ì„ íƒ ë¬¸ì œì—ì„œëŠ” ì¸ê³¼ê´€ê³„ê°€ í•„ìš”í•˜ì§€ë§Œ ì˜ˆì¸¡ ëª¨ë¸ì€ ì´ë¥¼ ì œê³µí•˜ì§€ ëª»í•œë‹¤. ë”°ë¼ì„œ ì˜ˆì¸¡ ëª¨ë¸ì— ê¸°ë°˜í•´ ì„ íƒì„ í•˜ëŠ” ê²ƒì€ ìœ„í—˜í•˜ë©°, ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì´ í•„ìš”í•˜ë‹¤. ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì„ í†µí•´ ì²˜ì¹˜ (í†µì œë³€ìˆ˜, ë Œì¦ˆì‚¬ì´ì¦ˆ)ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆê³  ì´ë¥¼ í†µí•´ ì˜¬ë°”ë¥¸ ì„ íƒì„ í•  ìˆ˜ ìˆë‹¤. 1.2.2 ë°ì´í„° ì•ˆì— ì¡´ì¬í•˜ëŠ” í†µê³„ì  í¸í–¥ ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ë°ì´í„°ì— í†µê³„ì  í¸í–¥(êµë€í¸í–¥)ì´ ì¡´ì¬í•˜ë©´ ì˜ˆì¸¡ ëª¨ë¸ì€ ì˜ëª»ëœ íŒ¨í„´, ì¦‰ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤. ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œ ë°ì´í„°ëŠ” í†µê³„ì  í¸í–¥ì„ ê°€ì§€ê³  ìˆë‹¤. ì´ëŠ” ë Œì¦ˆ í¬ê¸°ë³„(ì²˜ì¹˜ë³„)ë¡œ í™˜ìì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤ëŠ” ì˜ë¯¸ì´ë©°, ë°ì´í„°ì—ì„œ í™•ì¸ëœë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„_Feature_DensityPlot.png\" alt=\"ì²˜ì¹˜ë³„ Feature Density Plot\"> <figcaption>ê·¸ë¦¼ 1: ì²˜ì¹˜ë³„ í™˜ì ëˆˆì˜ í¬ê¸° ë¶„í¬ ë¹„êµ</figcaption> </figure> ê·¸ë¦¼ 1ì€ ì²˜ì¹˜(ë Œì¦ˆ ì‚¬ì´ì¦ˆ)ë³„ë¡œ í™˜ìì˜ ëˆˆ í¬ê¸°ì— ëŒ€í•œ ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ê·¸ë¦¼ 1ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ì²˜ì¹˜ë³„ë¡œ í™˜ìì˜ ëˆˆ í¬ê¸° ë¶„í¬(ATA, ACD)ê°€ ëª…í™•íˆ ë‹¤ë¥´ë‹¤. ì´ëŠ” ì˜ì‚¬ê°€ í™˜ìì˜ ê°œë³„ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•œ ë Œì¦ˆë¥¼ ì„ íƒí•˜ëŠ” ì„ìƒì  íŒë‹¨ì— ë”°ë¥¸ ê²°ê³¼ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ëˆˆì´ ì‘ì€ í™˜ìì—ê²ŒëŠ” ì‘ì€ ë Œì¦ˆë¥¼, ëˆˆì´ í° í™˜ìì—ê²ŒëŠ” í° ë Œì¦ˆë¥¼ ì„ íƒí•˜ëŠ” ê²½í–¥ì´ ìˆë‹¤. ì´ëŸ¬í•œ ì„ íƒ í¸í–¥ì€ êµë€í¸í–¥ì„ ìœ ë°œí•œë‹¤. í™˜ìì˜ ëˆˆ í¬ê¸°, ë‚˜ì´, ê°ë§‰ ë‘ê»˜ ë“±ì€ ë Œì¦ˆ ì„ íƒê³¼ ìˆ˜ìˆ  ê²°ê³¼ ëª¨ë‘ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” êµë€ë³€ìˆ˜ì´ê¸° ë•Œë¬¸ì´ë‹¤. ì•ì„œ 1.2.1ì—ì„œ ì–¸ê¸‰í•œ ë°”ì™€ ê°™ì´, ë°ì´í„°ì— í†µê³„ì  í¸í–¥ì´ ì¡´ì¬í•  ê²½ìš° ì˜ˆì¸¡ ëª¨ë¸ì€ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ ì˜ëª»ëœ íŒ¨í„´ì„ í•™ìŠµí•  ìˆ˜ ìˆìœ¼ë©°, ì´ëŸ¬í•œ íŒ¨í„´ì— ê¸°ë°˜í•œ ì„ íƒì€ ìœ„í—˜í•˜ë‹¤. ë˜í•œ í†µê³„ì  í¸í–¥ì€ ë Œì¦ˆì˜ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì¸ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì™œê³¡í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì¦ˆë³„ë¡œ í™˜ì ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë‹¨ìˆœíˆ ê²°ê³¼ë§Œ ë¹„êµí•´ì„œëŠ” ë Œì¦ˆ ì‚¬ì´ì¦ˆì˜ íš¨ê³¼ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤. í™˜ìì˜ ê°œë³„ íŠ¹ì„±ì„ ë™ì¼í•˜ê²Œ ë§ì¶˜ ìƒíƒœì—ì„œ ë Œì¦ˆ íš¨ê³¼ë¥¼ ë¹„êµí•´ì•¼ í•œë‹¤. ì´ëŠ” ì‹¤í—˜ì„ ì„¤ê³„í•  ë•Œ ëŒ€ì¡°êµ°ê³¼ ì²˜ì¹˜êµ°ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤. ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ ë™ì¼í•˜ê²Œ ë§ì¶°ì•¼ë§Œ ì‹¤í—˜ ì¡°ê±´ë§Œì˜ ì˜í–¥ì„ ì¶”ì •í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤. í•˜ì§€ë§Œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ ë¬´ì‘ìœ„ ëŒ€ì¡° ì‹¤í—˜(RCT, Randomized Controlled Trial)ì„ ì§„í–‰í•  ìˆ˜ ì—†ë‹¤. ë¬´ì‘ìœ„ ëŒ€ì¡° ì‹¤í—˜ : ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ ì‹¤í—˜ì„ ì§„í–‰í•˜ëŠ” ë°©ë²• ë”°ë¼ì„œ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê³  ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•´ì•¼ í•œë‹¤. ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì€ ì´ëŸ¬í•œ êµë€í¸í–¥ì„ í†µê³„ì ìœ¼ë¡œ ì œê±°í•˜ì—¬ ì‹¤ì œ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œê³µí•œë‹¤. êµ¬ì²´ì ì¸ ë°©ë²•ì€ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ë‹¤ë£¬ë‹¤. 1.3 ì˜ˆì¸¡ í‰ê°€ì§€í‘œì˜ í•œê³„ --- ì˜ˆì¸¡ ëª¨ë¸ì˜ ë˜ ë‹¤ë¥¸ í•œê³„ëŠ” ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°(ë°˜ì‚¬ì‹¤, Counterfactual)ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤. ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì˜ˆì¸¡ ëª¨ë¸ì„ ê³ ë„í™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì§„í–‰ë˜ì—ˆìœ¼ë©°, ì˜ˆì¸¡ ì„±ëŠ¥ì§€í‘œë¥¼ í†µí•´ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•˜ê³  ìˆë‹¤. í•˜ì§€ë§Œ ìµœì ì˜ ë Œì¦ˆë¥¼ ì¶”ì²œí•˜ê¸° ìœ„í•´ì„œëŠ” ìˆ˜ìˆ ë°›ì€ ë Œì¦ˆ ì™¸ì— ë‹¤ë¥¸ ë Œì¦ˆ í¬ê¸°ì— ëŒ€í•œ ê²°ê³¼ë„ ì˜ˆì¸¡í•´ì•¼ í•œë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ëŠ” ì‹¤ì œë¡œ ì¼ì–´ë‚˜ì§€ ì•Šì€ ìƒí™©ì— ëŒ€í•œ í‰ê°€ê°€ ë¶ˆê°€ëŠ¥í•˜ë‹¤ëŠ” ì ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, í™˜ì Aê°€ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ê³  ì´ì— ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤ê³  ê°€ì •í•œë‹¤. ì´ë•Œ ì¶”ì²œ ëª¨ë¸ì´ ì¦ëª…í•´ì•¼ í•˜ëŠ” ê²ƒì€ \"ë§Œì•½ ì´ í™˜ìê°€ 13.2mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ë‹¤ë©´ ì–´ë–¤ ê²°ê³¼ê°€ ë‚˜ì™”ì„ê¹Œ?\"ì´ë‹¤. í•˜ì§€ë§Œ ì´ í™˜ìëŠ” ì´ë¯¸ 12.6mm ë Œì¦ˆë¡œ ìˆ˜ìˆ ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì—, 13.2mm ë Œì¦ˆì˜ ê²°ê³¼ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤. ì´ë ‡ê²Œ ì‹¤ì œë¡œëŠ” ê´€ì°°ë˜ì§€ ì•Šì€ ê°€ìƒì˜ ìƒí™©ì„ \"ë°˜ì‚¬ì‹¤(Counterfactual)\"ì´ë¼ê³  í•œë‹¤. ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ë“¤ì€ ê´€ì°°ëœ ë°ì´í„°ë§Œìœ¼ë¡œ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì—, ì´ëŸ° ë°˜ì‚¬ì‹¤ì  ìƒí™©ì— ëŒ€í•œ ì¶”ì •ì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ê·¸ë¦¬ê³  ì´ì— ëŒ€í•œ í‰ê°€ì§€í‘œë¥¼ ê³ ë ¤í•˜ì§€ ì•ŠëŠ”ë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì´ëŸ° ë°˜ì‚¬ì‹¤ì  ìƒí™©ì„ í†µê³„ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ë¡ ì„ ì œê³µí•˜ë©°, ì´ì— ëŒ€í•œ í‰ê°€ì§€í‘œë„ í•¨ê»˜ ê³ ë ¤í•´ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•œë‹¤. Method & Result - ì¸ê³¼ì¶”ë¡ ì— ëŒ€í•œ ì†Œê°œì™€ ì¸ê³¼ì¶”ë¡ ì„ í†µí•´ ì´ë¥¼ í•´ê²°í•œ ë°©ë²• ì†Œê°œ --- ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ì„ íƒì˜ ë¬¸ì œì—ì„œ ì˜ˆì¸¡ ëª¨ë¸ì€ í•œê³„ì ì„ ê°€ì§„ë‹¤. ì´ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ ë„ì…í•˜ê³ ì í•œë‹¤. ì¸ê³¼ì¶”ë¡ ì´ë€ ì˜í•™, ë§ˆì¼€íŒ…, êµìœ¡ ë“±ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ë¡ ìœ¼ë¡œ ì •ì±… ë° ë¹„ì¦ˆë‹ˆìŠ¤ì— ìˆ˜ë°˜ëœ ìˆ˜ë§ì€ ì˜ì‚¬ê²°ì •ë“¤ì˜ ì›ì¸ê³¼ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ëŠ” ë°©ë²•ì´ë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì˜ì‚¬ê²°ì •ì„ ë•ëŠ” ë°©ë²•ë¡ ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì •ë°€ì˜í•™ì—ì„œëŠ” í™˜ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ìˆ˜ìˆ ë°©ë²•ì„ ì„ íƒì„ ê²°ì •í•˜ê³ , êµìœ¡ì—ì„œëŠ” í•™ìƒë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ êµìœ¡ ë°©ë²•ì„ ê²°ì •í•˜ê³ , ì˜¨ë¼ì¸ ë§ˆì¼€íŒ…ì—ì„œëŠ” ì‚¬ìš©ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ë§ì¶¤í˜• ê´‘ê³ ë¥¼ ì œê³µí•˜ê³ , ì˜¤í”„ë¼ì¸ ì •ì±… í‰ê°€ì—ì„œëŠ” ì§€ì—­ë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì •ì±…ì„ ê²°ì •í•  ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ì²˜ì¹˜ë€ ì¸ê³¼ì¶”ë¡ ì—ì„œ ì˜ì‚¬ê²°ì •ì˜ ëŒ€ìƒ(ì˜ˆ: ìˆ˜ìˆ  ë°©ë²•, ì•½ë¬¼ íˆ¬ì—¬, êµìœ¡ì •ì±… ë“±)ì„ ì˜ë¯¸í•œë‹¤. ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œëŠ” ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ì²˜ì¹˜ì— í•´ë‹¹í•œë‹¤. ì¸ê³¼ì¶”ë¡ ì„ í†µí•´ ë Œì¦ˆì˜ ì‚¬ì´ì¦ˆì™€ ê²°ê³¼ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ë¶„ì„í•˜ê³  ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ê·¸ë¦¬ê³  ì´ë¥¼ í†µí•´ ìµœì ì˜ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ê²°ì •í•˜ëŠ”ë° ë„ì›€ì„ ì¤„ ìˆ˜ ìˆë‹¤. ì¸ê³¼ì¶”ë¡ ì„ ë„ì…í•¨ìœ¼ë¡œì¨ ë‹¤ìŒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤. ì²«ì§¸, í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê³  ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•  ìˆ˜ ìˆì–´ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì˜í–¥ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ë‘˜ì§¸, ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°(ë°˜ì‚¬ì‹¤ì  ìƒí™©)ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤. ì…‹ì§¸, ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼(Heterogeneous Treatment Effects)ë¥¼ ì¶”ì •í•˜ì—¬ í™˜ìë³„ë¡œ ìµœì í™”ëœ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•  ìˆ˜ ìˆë‹¤. ì´ë²ˆ ë³´ê³ ì„œì—ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ê²°ê³¼ì— ëŒ€í•´ì„œë§Œ ì†Œê°œí•˜ë©°, ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ê°œì¸í™”ëœ ì¶”ì²œì„ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œëŠ” ì¶”í›„ì— ë‹¤ë£¨ë„ë¡ í•œë‹¤. 2.1 ì¸ê³¼ì¶”ë¡  ë°©ë²•ì„ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±° ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì¸ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì²˜ëŸ¼ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ë³€ìˆ˜ëŠ” í†µì œí•´ì•¼ í•œë‹¤. ì¦‰ ë Œì¦ˆ ì‚¬ì´ì¦ˆë³„ë¡œ í™˜ìì˜ ëˆˆì˜ í¬ê¸°ì— ëŒ€í•œ ë¶„í¬ê°€ ê°™ì•„ì•¼ í•œë‹¤. í•˜ì§€ë§Œ ì•ì„œ í™•ì¸í•œ ê²ƒì²˜ëŸ¼ ë°ì´í„°ì—ëŠ” í¸í–¥ì´ ì¡´ì¬í•˜ë©°, ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ RCTë¥¼ ìˆ˜í–‰í•  ìˆ˜ ì—†ë‹¤. ì´ë•Œ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•  ìˆ˜ ìˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„ê²°ê³¼ë¹„êµ.png\" alt=\"ì²˜ì¹˜ë³„ ê²°ê³¼ ë¹„êµ\"> <figcaption>ê·¸ë¦¼ 2: ì²˜ì¹˜ë³„ ê²°ê³¼ ë¶„í¬ ë¹„êµ</figcaption> </figure> ê·¸ë¦¼ 2ëŠ” ì²˜ì¹˜ë³„ ê²°ê³¼ì˜ ë¶„í¬ë¥¼ ì‹œê°í™”í•œ ê²ƒì´ë‹¤. ì•ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ì²˜ì¹˜ë³„ë¡œ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ì²˜ì¹˜ë³„ ë¶„í¬ë¥¼ ë¹„êµí•´ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ì—†ë‹¤. ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•´ì•¼ í•œë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì‚¬ì´ì¦ˆë³„ì”ì°¨ë¶„í¬.png\" alt=\"ê²°ê³¼ì—ì„œ êµë€ë³€ìˆ˜ì— ëŒ€í•œ ì˜í–¥ ì œê±°í›„ ë¶„í¬\"> <figcaption>ê·¸ë¦¼ 3: ì”ì°¨í™”ë¥¼ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ê²°ê³¼ ë¶„í¬</figcaption> </figure> ê·¸ë¦¼ 3ì€ ê·¸ë¦¼ 2ì˜ ê²°ê³¼ì—ì„œ ì”ì°¨í™”ë¥¼ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ê²ƒì´ë‹¤. ì”ì°¨í™”ë€ ê²°ê³¼(vaulting)ì— ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì™¸í•œ ë‹¤ë¥¸ ë³€ìˆ˜(êµë€ë³€ìˆ˜)ê°€ ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œê±°í•´ êµë€ë³€ìˆ˜ë¥¼ í†µì œí•œ ê²ƒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ë§Œë“œëŠ” ë°©ë²•ë¡ ì´ë‹¤. ì”ì°¨í™”ë¥¼ í†µí•´ í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ë©´ ë Œì¦ˆ ì‚¬ì´ì¦ˆë³„ë¡œ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ê°™ì•„ì§€ë¯€ë¡œ ë Œì¦ˆ ì‚¬ì´ì¦ˆê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ì´ì²˜ëŸ¼ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì€ ë°ì´í„°ê°€ ê°€ì§€ê³  ìˆëŠ” í†µê³„ì  í¸í–¥ì„ í™•ì¸í•˜ê³  ì´ë¥¼ ì œê±°í•´ ì²˜ì¹˜ì˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê³  ì¸ê³¼ì ì¸ ê´€ê³„ë¥¼ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ë¡ ì´ë‹¤. Conclusion - ì—°êµ¬ ìš”ì•½ ë° í–¥í›„ ê³„íš --- ë³¸ ì—°êµ¬ì—ì„œëŠ” ë Œì¦ˆ ì‚¬ì´ì§• ë¬¸ì œì—ì„œ ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì ì„ ë¶„ì„í•˜ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì˜ ë„ì…ì„ ì œì•ˆí•˜ì˜€ë‹¤. ê¸°ì¡´ ì˜ˆì¸¡ ëª¨ë¸ì€ ì„ íƒì˜ ë¬¸ì œì—ì„œ í†µê³„ì  í¸í–¥ì„ ê³ ë ¤í•˜ì§€ ëª»í•˜ê³  ì¸ê³¼ê´€ê³„ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì§€ ëª»í•˜ëŠ” í•œê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤. ì´ëŸ¬í•œ í•œê³„ë¥¼ ê·¹ë³µí•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì„ ë„ì…í•˜ì—¬ í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê³  ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ì˜€ë‹¤. ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡ ì˜ ë„ì…ì„ í†µí•´ ë‹¤ìŒê³¼ ê°™ì€ íš¨ê³¼ë¥¼ ê¸°ëŒ€í•  ìˆ˜ ìˆë‹¤. ì²«ì§¸, í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ì—¬ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì‹¤ì œ ì¸ê³¼ì  ì˜í–¥ì„ ì •í™•íˆ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. ë‘˜ì§¸, ì¸¡ì •ë˜ì§€ ì•Šì€ ë°ì´í„°(ë°˜ì‚¬ì‹¤ì  ìƒí™©)ì— ëŒ€í•œ í‰ê°€ë¥¼ ê³ ë ¤í•˜ì—¬ ëª¨ë¸ì˜ ì‹ ë¢°ë„ë¥¼ í™•ë³´í•  ìˆ˜ ìˆë‹¤. ì…‹ì§¸, ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼(Heterogeneous Treatment Effects)ë¥¼ ì¶”ì •í•˜ì—¬ í™˜ìë³„ë¡œ ìµœì í™”ëœ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì œì•ˆí•  ìˆ˜ ìˆë‹¤. í–¥í›„ ì—°êµ¬ì—ì„œëŠ” ê°œì¸í™”ëœ ì²˜ì¹˜ íš¨ê³¼ ì¶”ì •ì„ í†µí•œ ê°œì¸í™”ëœ ì¶”ì²œ ì‹œìŠ¤í…œ ê°œë°œì— ì§‘ì¤‘í•  ì˜ˆì •ì´ë‹¤. ì´ë¥¼ í†µí•´ í™˜ìë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ë Œì¦ˆ ì‚¬ì´ì¦ˆ ì¶”ì²œì´ ê°€ëŠ¥í•œ ëª¨ë¸ì„ êµ¬ì¶•í•˜ê³ ì í•œë‹¤."
  },
  {
    "id": "Fitness/Workout Log/250713",
    "title": "ê°„ë‹¨í•œ ë‹¬ë¦¬ê¸° + ì‚°ì±…",
    "date": "2025-07-13",
    "excerpt": "ìš´ë™ì¼ì§€",
    "category": "Workout Log",
    "tags": [],
    "content": "í™•ì‹¤íˆ ëŸ°ë‹ ìì£¼ ì•ˆí•˜ë‹ˆê¹Œ ì§„ì§œ ì•ˆë›°ì–´ì§„ë‹¤. ê¾¸ì¤€íˆ ë›°ì–´ë³´ì."
  },
  {
    "id": "English Study/TOEFL/í† í”Œì¤€ë¹„",
    "title": "TOEFL 100ì  ì´ìƒ ëª©í‘œë¡œ ê³µë¶€",
    "date": "2025-07-13",
    "excerpt": "OMSCS ì§€ì›ì„ ìœ„í•œ í† í”Œ ê³µë¶€",
    "category": "English Study",
    "tags": [
      "TOEFL",
      "ì˜ì–´",
      "ëŒ€í•™ì›",
      "OMSCS"
    ],
    "content": "ë°°ê²½ OMSCSë¥¼ ì§€ì›í•˜ê¸° ìœ„í•´ì„œëŠ” í† í”Œ 100ì  ì´ìƒì´ í•„ìš”í•œë°ìš”, ì´ë¥¼ ìœ„í•´ì„œ í† í”Œ ê³µë¶€ë¥¼ ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê³µë¶€í•˜ë©´ì„œ ì–´ë–»ê²Œ ê³µë¶€í•´ì•¼í• ì§€ ì •ë¦¬í•´ë‚˜ê°€ê³  ìˆìŠµë‹ˆë‹¤. ëª©í‘œ ê¸°ê°„ì€ 3~4ê°œì›” ì •ë„ì…ë‹ˆë‹¤. ì˜¬í•´ 11ì›”ë¶€í„°ëŠ” OSMCì— ì„œë¥˜ë¥¼ ë„£ìœ¼ë ¤ê³  í•©ë‹ˆë‹¤. í•™ìŠµ ê³„íš ì„¸ì›Œë³´ê¸° ê³µë¶€ ì „ëµ ìœ íŠœë¸Œ ë§í¬ - ğŸ€í† í”Œ ë…í•™ì„ ë§ˆìŒë¨¹ìœ¼ì‹  ë¶„ë“¤ì„ ìœ„í•œ ì²« ë²ˆì§¸ ê°€ì´ë“œ (ì‹œí—˜ ì´í•´, ê³µë¶€ ë°©ë²•, ë‘ë ¤ì›€ ê·¹ë³µ, ì°¸ê³  ë§í¬) ëª…í™•í•œ ëª©í‘œ ê¸°ê°„ì„ ì¡ê³  ì§‘ì¤‘í•´ì„œ ê³µë¶€í•˜ê¸° ì‹œí—˜êµ¬ì„±ì— ëŒ€í•´ ì •í™•íˆ ì´í•´í•˜ê³  ì‹œì‘í•˜ê¸° ì‹œí—˜ êµ¬ì„± Reading 2ê°œ ì§€ë¬¸ x 10ë¬¸ì œ. ì§€ë¬¸í•˜ë‚˜ë‹¹ ì•½ 700ë‹¨ì–´. ì´ 35ë¶„ì •ë„. Listening 2ê°œ ëŒ€í™” (ê° 3ë¶„) x 5ë¬¸ì œ 3ê°œ ê°•ì˜ (ê° 3-5ë¶„) x 6ë¬¸ì œ ì´ 36ë¶„ ì •ë„. Speaking (1) ë…ë¦½í˜• (15ì´ˆ ì¤€ë¹„, 45ì´ˆ ë‹µë³€) (2), (3) í†µí•©í˜• - ì½+ë“£+ë§ (30ì´ˆ ì¤€ë¹„, 60ì´ˆ ë‹µë³€) (4) í†µí•©í˜• - ë“£+ë§ (20ì´ˆ ì¤€ë¹„, 60ì´ˆ ë‹µë³€) ì´ 16ë¶„ ì •ë„. Writing (1) í†µí•©í˜• - ì½+ë“£+ì“° (20ë¶„) (2) í† ë¡ í˜• - ì½+ì“° (10ë¶„) ì´ 30ë¶„ì •ë„. ì°¸ê³  ì‚¬ì´íŠ¸ ğŸ”—ì°¸ê³  ì‚¬ì´íŠ¸ ë§í¬ ETS TOEFL ê³µì‹ ì›¹ì‚¬ì´íŠ¸ ì‹œí—˜ êµ¬ì„± ë° ë¬¸ì œ ìœ í˜• ì•ˆë‚´ https://www.ets.org/toefl/test-takers... ETS TOEFL TestReady ì›¹ì‚¬ì´íŠ¸ (ëª¨ì˜ê³ ì‚¬, ì•¡í‹°ë¹„í‹°) https://testready.ets.org/test-prep-l... í…ŒìŠ¤íŠ¸ê¸€ë¼ì´ë” ì›¹ì‚¬ì´íŠ¸ https://www.testglider.com/ko/toefl ì°¸ê³ ìë£Œ [[í† í”Œê³µë¶€/í† í”Œ ê³µë¶€ë²•] ì´ˆë³´ìë¥¼ ìœ„í•œ í† í”Œ ë…í•™ ê°€ì´ë“œ : A to Z](https://beyondourlimit.tistory.com/68) : ë¸”ë¡œê·¸, ê¸°ë³¸ì ì¸ ê°œë… ì•Œê¸° ì¢‹ë‹¤"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-13-ì¡°ì–¸í•˜ëŠ” í‘œí˜„",
    "title": "ì¡°ì–¸í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-13",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë°°ìš´ ê²ƒë“¤ Try to drink more water Drink more water (X, ëª…ë ¹í•˜ëŠ” ëŠë‚Œì´ ë‚¨) ë°œìŒ íŒ! t -> d Try to get more sleep Try to cut down on smoking cut down on : ~ì„ ì¤„ì´ë‹¤ Try not to skip breakfast. Make sure to + Make sure to take probiotics every day. Make sure to take Vitamin C. Make sure not to eat too much junk food. wipe down : ë§ë”íˆ ë‹¦ë‹¤ come to work stock up : ëŒ€ëŸ‰ìœ¼ë¡œ ì‚¬ë†“ë‹¤. ê°„ë‹¨í•œ ì‘ë¬¸ I read a book , atomic habits that tell me how to make habit. The book says that try to split the things to very small things. Because it make me to do more. So the lesson in the book is that make sure to make things, clear, attractive, easy. í”¼ë“œë°±. AI ì‘ë¬¸ í”¼ë“œë°±: \"I read a book , atomic habits\" â†’ ì‰¼í‘œ ì• ê³µë°± ì œê±°, ì±… ì œëª©ì€ ë”°ì˜´í‘œ ì‚¬ìš© \"that tell me\" â†’ ë‹¨ìˆ˜ ì£¼ì–´ì´ë¯€ë¡œ \"tells me\" \"how to make habit\" â†’ \"how to build habits\" (ë³µìˆ˜í˜•, ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„) \"says that try to\" â†’ \"says to try to\" ë˜ëŠ” \"says that I should try to\" \"split the things to very small things\" â†’ \"break things down into smaller pieces\" \"Because it make me to do more\" â†’ \"makes\" (ë‹¨ìˆ˜), \"make me do\" (to ë¶ˆí•„ìš”) \"make sure to make things, clear, attractive, easy\" â†’ ë¬¸ì¥ êµ¬ì¡° ê°œì„  í•„ìš” ìˆ˜ì •ëœ ì‘ë¬¸: I read a book called \"Atomic Habits\" that tells me how to build habits. The book says to try to break things down into smaller pieces. This makes me do more because it reduces the psychological burden. So the lesson in the book is to make sure to make things clear, attractive, and easy. ì˜ì–´ ì‘ë¬¸ ì—°ìŠµ (í•œê¸€ ë‚´ìš© ê¸°ë°˜) I think habits are the most important thing in life. According to research by Professor Wendy Wood at Duke University, about 45% of our behavior comes from habits. The book \"Atomic Habits\" suggests the 2-minute rule. Try to break down what you need to do into very small tasks that can be finished in 2 minutes. I applied this to my life for about 1-2 months and it was really effective. The biggest reason is that it reduces psychological burden. Usually, we procrastinate because of psychological pressure. When I break down tasks into small 2-minute pieces, this burden almost disappears. By achieving small things, I can gradually do more things with that sense of accomplishment. For breaking bad habits, you should make them difficult to do. Human willpower has limits, so environment is the most important thing. I had a bad habit of looking at my phone late at night and sleeping late. To solve this, I bought an alarm clock. Now I don't bring my phone to bed and turn it off as soon as I get home. The key lessons are: Try to break tasks into very small pieces to reduce psychological burden Make sure to create an environment where you can't do bad habits Remember that willpower is limited, so focus on changing your environment"
  },
  {
    "id": "Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™",
    "title": "ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™",
    "date": "2025-07-13",
    "excerpt": "ì§€ê¸ˆë³´ë‹¤ ì¡°ê¸ˆ ë” ì–´ë ¤ìš´ ì¼ì— ë„ì „í•˜ì",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì—ì„œëŠ” ê¾¸ì¤€í•¨ì„ ì–´ë–»ê²Œ ì´ì–´ê°ˆ ê²ƒì¸ì§€, ê·¸ë¦¬ê³  ì´ì— ë„ì›€ì„ ì£¼ëŠ” ë„êµ¬ë“¤ì„ ì†Œê°œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ ì¤‘ì—ì„œ 'ê³¨ë”” ë½ìŠ¤ì˜ ë²•ì¹™'ì— ëŒ€í•´ ì´ì•¼ê¸°í•œ ë¶€ë¶„ì„ ì •ë¦¬í•´ë´…ë‹ˆë‹¤. <figure> <img src=\"/post/ê³¨ë””ë½ìŠ¤ì˜ë²•ì¹™/ê³¨ë””ë½ìŠ¤ì˜ë²•ì¹™_ê·¸ë˜í”„_ì´ë¯¸ì§€.jpg\" alt=\"ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ ê·¸ë˜í”„\" /> <figcaption>ê·¸ë¦¼1.ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™: ë„ˆë¬´ ì‰¬ìš°ë©´ ì§€ë£¨í•˜ê³ , ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í¬ê¸°í•˜ê²Œ ëœë‹¤. ì ë‹¹íˆ ë„ì „ì ì¸ ìˆ˜ì¤€ì´ ê°€ì¥ íš¨ê³¼ì ì´ë‹¤.</figcaption> </figure> ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™(Goldilocks Rule)ì˜ ë²•ì¹™ì´ë€ ì¼ì˜ ë‚œì´ë„ê°€ ë„ˆë¬´ ì‰¬ìš°ë©´ ì§€ë£¨í•˜ê³ , ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í¬ê¸°í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì—, ì¡°ê¸ˆ ì–´ë ¤ìš´ ì •ë„ì˜ ë‚œì´ë„ê°€ ì¼ì— ëª°ì…í•˜ëŠ”ë° íš¨ê³¼ì ì´ë¼ëŠ” ì´ì•¼ê¸°ì…ë‹ˆë‹¤. > ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì´ë€ ì¸ê°„ì€ ìì‹ ì´ í•  ìˆ˜ ìˆëŠ” ì í•©í•œ ì¼ì„ í•  ë•Œ ë™ê¸°ê°€ ê·¹ëŒ€í™”ë˜ëŠ” ê²½í—˜ì„ í•œë‹¤ëŠ” ê²ƒì´ë‹¤. > ì§€ë‚˜ì¹˜ê²Œ ì–´ë ¤ì›Œì„œë„ ì•ˆë˜ë©° ì§€ë‚˜ì¹˜ê²Œ ì‰¬ì›Œì„œë„ ì•ˆëœë‹¤. ë”± ë“¤ì–´ë§ì•„ì•¼ í•œë‹¤. > > â€” ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ ë²•ì¹™ì„ ì´í•´í•˜ê³  í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì–´ë–¤ ì¼ì„ ì²˜ìŒ ì‹œì‘í•˜ëŠ”ê±°ë¼ë©´, ê·¸ ìì²´ë¡œ ì‹¬ë¦¬ì  ë¶€ë‹´ê°ì´ í¬ê³  ë‚œì´ë„ê°€ ì–´ë µë‹¤ê³  ëŠê»´ì§€ê¸° ë•Œë¬¸ì— ì •ë§ ì‰¬ìš´ ì¼ë¡œ ì‘ê²Œ ìª¼ê°œëŠ”ê²Œ íš¨ê³¼ì ì¼ ê²ƒì…ë‹ˆë‹¤. ê·¸ í›„ì— ì–´ëŠì •ë„ ë°˜ë³µí•˜ë©´ì„œ ìŠµê´€ì´ ë˜ì—ˆë‹¤ë©´ ì¡°ê¸ˆì”© ë‚œì´ë„ë¥¼ ì˜¬ë ¤ê°€ì•¼ ì§€ë£¨í•¨ì„ ëŠë¼ì§€ ì•Šê³  ê¾¸ì¤€íˆ í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. ì´ ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì€ ë‡Œì˜ ê¸´ì¥ë„ë¥¼ ì–´ë–»ê²Œ ìœ ì§€í•  ê²ƒì¸ì§€ì™€ë„ ì—°ê²°ë©ë‹ˆë‹¤. ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™ê³¼ì˜ ì—°ê²° ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì€ ì‹¬ë¦¬í•™ì˜ \"ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™(Yerkes-Dodson Law)\"ê³¼ ë°€ì ‘í•œ ê´€ë ¨ì´ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/ê³¨ë””ë½ìŠ¤ì˜ë²•ì¹™/ì—¬í‚¤ìŠ¤ë„ìŠ¨ë²•ì¹™_ê·¸ë˜í”„.png\" alt=\"ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™ ê·¸ë˜í”„\" /> <figcaption>ê·¸ë¦¼2. ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™: ê¸´ì¥ë„(ê°ì„± ìˆ˜ì¤€)ì™€ ì„±ê³¼ì˜ ê´€ê³„. ì ì ˆí•œ ê¸´ì¥ë„ì—ì„œ ìµœê³ ì˜ ì„±ê³¼ë¥¼ ë³´ì´ì§€ë§Œ, ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ì€ ê¸´ì¥ë„ì—ì„œëŠ” ì„±ê³¼ê°€ ë–¨ì–´ì§„ë‹¤.</figcaption> </figure> > ì˜ˆë¥´í¬ìŠ¤-ë„ë“œìŠ¨ ë²•ì¹™ì´ë€ ê¸´ì¥ë„(ê°ì„± ìˆ˜ì¤€)ì™€ ì„±ê³¼ ê°„ì˜ ê´€ê³„ë¥¼ ì„¤ëª…í•˜ëŠ” ë²•ì¹™ìœ¼ë¡œ, ì ì ˆí•œ ê¸´ì¥ë„ì—ì„œ ìµœê³ ì˜ ì„±ê³¼ë¥¼ ë³´ì´ì§€ë§Œ, ë„ˆë¬´ ë‚®ê±°ë‚˜ ë†’ì€ ê¸´ì¥ë„ì—ì„œëŠ” ì„±ê³¼ê°€ ë–¨ì–´ì§„ë‹¤ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ë„ˆë¬´ ì‰¬ìš´ ì¼: ê¸´ì¥ë„ê°€ ë‚®ì•„ì„œ ì§€ë£¨í•¨ì„ ëŠë¼ê³  ì„±ê³¼ê°€ ë–¨ì–´ì§ ë„ˆë¬´ ì–´ë ¤ìš´ ì¼: ê¸´ì¥ë„ê°€ ë„ˆë¬´ ë†’ì•„ì„œ ìŠ¤íŠ¸ë ˆìŠ¤ì™€ ë¶ˆì•ˆìœ¼ë¡œ ì„±ê³¼ê°€ ë–¨ì–´ì§ ì ë‹¹íˆ ë„ì „ì ì¸ ì¼: ì ì ˆí•œ ê¸´ì¥ë„ë¥¼ ìœ ì§€í•˜ì—¬ ìµœê³ ì˜ ì„±ê³¼ì™€ ëª°ì…ì„ ê²½í—˜ ì‚¬ëŒë§ˆë‹¤ ìµœì ì˜ ê¸´ì¥ë„ ìˆ˜ì¤€ì´ ë‹¤ë¥´ê¸° ë•Œë¬¸ì—, ìì‹ ì—ê²Œ ë§ëŠ” ë‚œì´ë„ë¥¼ ì°¾ì•„ê°€ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì‰¬ìš´ ê²ƒë¶€í„° ì‹œì‘í•´ì„œ ì ì§„ì ìœ¼ë¡œ ë‚œì´ë„ë¥¼ ë†’ì—¬ê°€ë©° ìì‹ ë§Œì˜ \"ê³¨ë””ë½ìŠ¤ ì¡´\"ì„ ì°¾ì•„ê°€ëŠ” ê²ƒì´ íš¨ê³¼ì ì…ë‹ˆë‹¤. ì •ë¦¬ ê³¨ë””ë½ìŠ¤ì˜ ë²•ì¹™ì„ ì‹¤ìƒí™œì— ì ìš©í•˜ëŠ” ë°©ë²•: ë‚œì´ë„ ì¡°ì ˆ ì „ëµ ì§€ë£¨í•¨ì„ ëŠë‚„ ë•Œ: ë‚œì´ë„ë¥¼ 10-15% ë†’ì—¬ì„œ ë„ì „ê°ì„ ë§Œë“¤ì–´ë¼ ì••ë„ë‹¹í•  ë•Œ: ë‚œì´ë„ë¥¼ ë‚®ì¶”ê³  ë” ì‘ì€ ë‹¨ìœ„ë¡œ ìª¼ê°œë¼ ì ì • ìˆ˜ì¤€: ì•½ê°„ì˜ ê¸´ì¥ê°ì´ ìˆì§€ë§Œ í¬ê¸°í•˜ì§€ ì•Šì„ ìˆ˜ ìˆëŠ” ìˆ˜ì¤€ ì‹¤ì²œ ë°©ë²• ìŠµê´€ í˜•ì„± ì´ˆê¸°: ì •ë§ ì‰¬ìš´ ê²ƒë¶€í„° ì‹œì‘ (ì˜ˆ: ë§¤ì¼ 1ë¶„ ëª…ìƒ) ìŠµê´€ ì•ˆì •í™” í›„: ì ì§„ì ìœ¼ë¡œ ë‚œì´ë„ ìƒìŠ¹ (ì˜ˆ: 5ë¶„ â†’ 10ë¶„ â†’ 15ë¶„) ì§€ì†ì  ëª¨ë‹ˆí„°ë§: ìì‹ ì˜ ëª°ì…ë„ì™€ ë™ê¸°ë¥¼ ì²´í¬í•˜ë©° ì¡°ì ˆ í•µì‹¬ ì›ì¹™ > \"ë„ˆë¬´ ì‰¬ìš°ë©´ ì§€ë£¨í•˜ê³ , ë„ˆë¬´ ì–´ë ¤ìš°ë©´ í¬ê¸°í•œë‹¤. ì ë‹¹íˆ ë„ì „ì ì¸ ìˆ˜ì¤€ì´ ê°€ì¥ íš¨ê³¼ì ì´ë‹¤.\""
  },
  {
    "id": "Guitar/Practice Log/250713",
    "title": "ë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°±",
    "date": "2025-07-12",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [],
    "content": "í¬ë¡œë§¤í‹± 80bpm 16bit. ì²˜ìŒì—” ì˜ ì•ˆëœë‹¤ê°€ ë‚˜ì¤‘ì— ì†í’€ë¦¬ë‹ˆê¹Œ ì˜ ë¨ ìŠ¤ì¼€ì¼ 0~4. ì´ì œ ë‹¤ ì™¸ìš¸ ë•Œê°€ ëëŠ”ë°.. ã…ã… ê³¡ ì—°ìŠµ ë¸ë¦¬ìŠ¤íŒŒì´ìŠ¤ ê³ ë°±. ë§‰ ë¹¡ì„¸ê²ŒëŠ” ì•ˆí•˜ê³  ê·¸ëƒ¥ í•œë²ˆì”© ì³ë³´ëŠ” ì •ë„?"
  },
  {
    "id": "Guitar/Practice Log/250712",
    "title": "ì–´ë–»ê²Œ",
    "date": "2025-07-12",
    "excerpt": "ê¸°íƒ€ì—°ìŠµì¼ì§€",
    "category": "Practice Log",
    "tags": [],
    "content": "ì—°ìŠµ ì²´í¬ 7ì›” 12. í†  [ ] í¬ë¡œë©”í‹±"
  },
  {
    "id": "Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ìŠµê´€ì¶”ì ",
    "title": "ìŠµê´€ì¶”ì ì— ëŒ€í•˜ì—¬",
    "date": "2025-07-12",
    "excerpt": "ìŠµê´€ì¶”ì ì€ ì–´ë–¤ ìŠµê´€ì„ ë§Œë“¤ê¸° ìœ„í•œ í›Œë¥­í•œ ë„êµ¬ë‹¤",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì—ì„œ ì´ì•¼ê¸°í•˜ëŠ” ì»¨ì…‰ì€ ì•„ì£¼ ëª…í™•í•©ë‹ˆë‹¤. ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” 'ëª…í™•í•´ì•¼ í•˜ë©°', 'í•˜ê¸° ì‰¬ì›Œì•¼ í•˜ê³ ', 'ë§Œì¡±ê°ì„ ì¤˜ì•¼ í•œë‹¤'. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ì¢‹ì€ ë°©ë²• ì¤‘ í•˜ë‚˜ë¡œ ìŠµê´€ ì¶”ì ì„ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìŠµê´€ì„ ì¶”ì í•œë‹¤ëŠ” ê±´ ë‚´ê°€ ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê³  ìˆëŠ”ì§€ì™€ ì–´ë–¤ ì¼ì„ í•´ì•¼í•˜ëŠ”ì§€ë¥¼ ì ê³  í™•ì¸í•´ë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‚´ê°€ ì–´ë–¤ ì¼ì„ í•´ì•¼í•˜ëŠ”ì§€ ì ì–´ì„œ í•´ì•¼í•  ì¼ì„ ëª…í™•íˆ ë§Œë“¤ê³ , í–ˆì„ ë•ŒëŠ” ì´ë¥¼ ê¸°ë¡í•˜ë©´ì„œ ì„±ì·¨ê°ì„ ëŠë‚„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” ì„±ì·¨ê°ì„ ë°›ì•„ì•¼ í•œë‹¤. ì´ë¥¼ ìœ„í•´ ìŠµê´€ì¶”ì ì€ ì¢‹ì€ ë„êµ¬ë‹¤. íŠ¹íˆ ì €ì—ê²ŒëŠ” ë§¤ì¼ í•´ë‚˜ê°€ë©´ì„œ ê¾¸ì¤€íˆ ìŒ“ì´ëŠ” ëŠë‚Œì´ ê°•ë ¥í•œ ë™ê¸°ë¶€ì—¬ë¡œ ì‘ìš©í•©ë‹ˆë‹¤. ê°œë°œì„ í•˜ëŠ” ì…ì¥ì—ì„œ ê°€ì¥ ëŒ€í‘œì ì¸ê²Œ ê¹ƒí—™ì˜ ì”ë””ë°­ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. <figure> <img src=\"/post/ìŠµê´€ì¶”ì /ê¹ƒí—™ì”ë””ë°­.png\" alt=\"ê¹ƒí—™ ì”ë””ë°­\"> <figcaption>ê¹ƒí—™ ì”ë””ë°­</figcaption> </figure> ë‚´ê°€ ê¾¸ì¤€íˆ ì»¤ë°‹(ì½”ë“œ ì‘ì—…)ì„ í•˜ê³  ìˆëŠ”ê²Œ í•œëˆˆì— ë³´ì´ê³ , ì´ê²Œ ì´ ì¼ì„ ë” ê¾¸ì¤€íˆ í•˜ëŠ”ë° ë™ê¸°ë¶€ì—¬ë¥¼ í•´ì¤ë‹ˆë‹¤. > ê¾¸ì¤€íˆ ìŒ“ì—¬ê°€ëŠ” ëŠë‚Œì€ êµ‰ì¥í•œ ë™ê¸°ë¶€ì—¬ë¥¼ ì œê³µí•œë‹¤. ë˜í•œ ì´ë ‡ê²Œ ìŒ“ì—¬ê°€ëŠ” ëŠë‚Œì„ ë°›ê¸° ìœ„í•´ ë¸”ë¡œê·¸ë„ ì‘ì„±í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ê³µë¶€í•œ ê²ƒë“¤ì„ ê¸€ë¡œ ì‘ì„±í•˜ëŠ” ì—°ìŠµì„ í•˜ê¸°ë„ í•˜ê³ , ë˜ ì´ë ‡ê²Œ ê³µë¶€í•œ ê²ƒë“¤ì´ í•˜ë‚˜ì”© ìŒ“ì—¬ê°€ëŠ”ê²Œ ëˆˆì— ë³´ì´ê¸° ë•Œë¬¸ì— ì„±ì·¨ê°ì„ ì¤ë‹ˆë‹¤. ê·¸ë˜ì„œ ê·¸ëƒ¥ ê³µë¶€í•˜ëŠ” ê²ƒë³´ë‹¤ ì €ëŠ” ë¸”ë¡œê·¸ì— ê¸€ë¡œ ë‚¨ê¸°ë©´ì„œ í•˜ë‚˜ì”© ìŒ“ì—¬ê°€ëŠ” ê±¸ ëˆˆìœ¼ë¡œ ë³¼ ë•Œ ë” í° ì„±ì·¨ê°ê³¼ ë™ê¸°ë¶€ì—¬ë¥¼ ë°›ìŠµë‹ˆë‹¤."
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-11-ê¸ì •ì ì¸í”¼ë“œë°±ì£¼ê¸°",
    "title": "ê¸ì •ì ì¸ í”¼ë“œë°± ì£¼ëŠ” í‘œí˜„ë“¤ ë³µìŠµ",
    "date": "2025-07-11",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ë¦¬ë·° ë°ì´ I like the way I appreciate it I know that its hard I want you to know that you call me any time. This too shall pass. I was really impressed with. It couldnt be any better I couldnt have done it without you. You did a great job on work Im happy to see that I can tell that I believe in you Youve made it this far. You`re almost there"
  },
  {
    "id": "Causal Inference/ill-conditioned-matrix-theory",
    "title": "Ill-conditioned í–‰ë ¬ê³¼ ê³ ìœ ê°’: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½",
    "date": "2025-07-11",
    "excerpt": "ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œì™€ ill-conditioned í–‰ë ¬, ê³ ìœ ê°’ì˜ ê´€ê³„ë¥¼ ìì„¸íˆ ì„¤ëª…",
    "category": "Causal Inference",
    "tags": [
      "Theory",
      "Linear Algebra",
      "Matrix Analysis"
    ],
    "content": "Ill-conditioned í–‰ë ¬ê³¼ ê³ ìœ ê°’: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½ ê°œìš” ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ill-conditioned í–‰ë ¬ ë¬¸ì œë¡œ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. ì´ ê¸€ì—ì„œëŠ” ì´ ê°œë…ë“¤ì„ ë‹¨ê³„ë³„ë¡œ ìì„¸íˆ ì„¤ëª…í•©ë‹ˆë‹¤. ê³ ìœ ê°’(Eigenvalue)ì´ë€? 1.1 ê¸°ë³¸ ê°œë… ê³ ìœ ê°’ì€ í–‰ë ¬ì˜ ì¤‘ìš”í•œ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ìŠ¤ì¹¼ë¼ ê°’ì…ë‹ˆë‹¤. ìˆ˜í•™ì  ì •ì˜: í–‰ë ¬ Aì— ëŒ€í•´, 0ì´ ì•„ë‹Œ ë²¡í„° vì™€ ìŠ¤ì¹¼ë¼ Î»ê°€ ë‹¤ìŒì„ ë§Œì¡±í•  ë•Œ: $$Av = \\lambda v$$ Î»ë¥¼ Aì˜ ê³ ìœ ê°’, vë¥¼ ê³ ìœ ë²¡í„°ë¼ê³  í•©ë‹ˆë‹¤. 1.2 ì§ê´€ì  ì´í•´ ê³ ìœ ê°’ì€ í–‰ë ¬ì´ ë²¡í„°ë¥¼ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ëŠ˜ë¦¬ê±°ë‚˜ ì¤„ì´ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì˜ˆì‹œ: $$ A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix} $$ ê³ ìœ ê°’: $\\lambda_1 = 2$, $\\lambda_2 = 3$ ê³ ìœ ë²¡í„°: $v_1 = [1, 0]$, $v_2 = [0, 1]$ Î»â‚ = 2: xì¶• ë°©í–¥ìœ¼ë¡œ 2ë°° ëŠ˜ë¦¼ Î»â‚‚ = 3: yì¶• ë°©í–¥ìœ¼ë¡œ 3ë°° ëŠ˜ë¦¼ 1.3 ê³ ìœ ê°’ì˜ ì˜ë¯¸ Î» > 1: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ í™•ëŒ€ 0 < Î» < 1: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ì¶•ì†Œ Î» = 0: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ì™„ì „íˆ ì••ì¶• (ì •ë³´ ì†ì‹¤) Î» < 0: í•´ë‹¹ ë°©í–¥ìœ¼ë¡œ ë°˜ì „ Ill-conditioned í–‰ë ¬ì´ë€? 2.1 ì¡°ê±´ìˆ˜(Condition Number) ì¡°ê±´ìˆ˜ëŠ” í–‰ë ¬ì´ ì–¼ë§ˆë‚˜ \"ë¶ˆì•ˆì •í•œì§€\"ë¥¼ ì¸¡ì •í•˜ëŠ” ì§€í‘œì…ë‹ˆë‹¤. ì •ì˜: $$ \\kappa(A) = \\ A\\ \\times \\ A^{-1}\\ $$ ì—¬ê¸°ì„œ $\\ A\\ $ëŠ” í–‰ë ¬ Aì˜ ë…¸ë¦„(norm)ì…ë‹ˆë‹¤. 2.2 Ill-conditionedì˜ ì˜ë¯¸ Well-conditioned: ì¡°ê±´ìˆ˜ê°€ ì‘ìŒ (â‰ˆ 1) Ill-conditioned: ì¡°ê±´ìˆ˜ê°€ í¼ (â‰« 1) 2.3 ì™œ ë¬¸ì œê°€ ë˜ëŠ”ê°€? Ill-conditioned í–‰ë ¬ì—ì„œëŠ” ì‘ì€ ì…ë ¥ ë³€í™”ê°€ í° ì¶œë ¥ ë³€í™”ë¥¼ ì•¼ê¸°í•©ë‹ˆë‹¤. ì˜ˆì‹œ: $$ A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1.001 \\end{bmatrix} $$ $$ A^{-1} \\approx \\begin{bmatrix} 1001 & -1000 \\\\ -1000 & 1000 \\end{bmatrix} $$ ì¡°ê±´ìˆ˜ $\\approx 4000$ (ë§¤ìš° í¼!) ì…ë ¥ì— ì‘ì€ ë…¸ì´ì¦ˆê°€ ìˆìœ¼ë©´ í•´ê°€ í¬ê²Œ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ Ill-conditionedê°€ ë°œìƒí•˜ëŠ” ì´ìœ  3.1 Non-identification ë¬¸ì œ ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” ë¬´í•œíˆ ë§ì€ ì²˜ì¹˜ ìˆ˜ì¤€ì´ ì¡´ì¬í•©ë‹ˆë‹¤: ì´ì§„ ì²˜ì¹˜: $T \\in \\{0, 1\\}$ (2ê°œ ê°’) ì—°ì†í˜• ì²˜ì¹˜: $T \\in [0, 1]$ (ë¬´í•œ ê°œ ê°’) 3.2 Generalized R-lossì˜ íŠ¹ì„± ì´ì§„ ì²˜ì¹˜ R-loss: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X)(T - \\pi(X)))^2]$$ ì—°ì†í˜• ì²˜ì¹˜ R-loss: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X,T)(T - \\pi(X)))^2]$$ 3.3 í•µì‹¬ ì°¨ì´ì  ì´ì§„ ì²˜ì¹˜: Ï„(X)ëŠ” Xì—ë§Œ ì˜ì¡´ (ìœ í•œì°¨ì›) ì—°ì†í˜• ì²˜ì¹˜: Ï„(X,T)ëŠ” Xì™€ T ëª¨ë‘ì— ì˜ì¡´ (ë¬´í•œì°¨ì›) 3.4 í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´ ì—°ì†í˜• ì²˜ì¹˜ë¥¼ í–‰ë ¬ë¡œ í‘œí˜„í•˜ë©´: $$ \\begin{bmatrix} \\tau(x_1,t_1) & \\tau(x_1,t_2) & \\cdots & \\tau(x_1,t_n) \\\\ \\tau(x_2,t_1) & \\tau(x_2,t_2) & \\cdots & \\tau(x_2,t_n) \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\tau(x_m,t_1) & \\tau(x_m,t_2) & \\cdots & \\tau(x_m,t_n) \\end{bmatrix} $$ ì´ í–‰ë ¬ì€ ë§¤ìš° í° í¬ê¸°ë¥¼ ê°€ì§€ë©°, ë§ì€ í–‰ë“¤ì´ ê±°ì˜ ì„ í˜• ì¢…ì†ì…ë‹ˆë‹¤. ê³ ìœ ê°’ì´ 0ì— ê°€ê¹Œì›Œì§€ëŠ” ì´ìœ  4.1 ì„ í˜• ì¢…ì†ì„± ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ Ï„(x,t) í•¨ìˆ˜ë“¤ì€ ì„œë¡œ ë§¤ìš° ìœ ì‚¬í•©ë‹ˆë‹¤: $$\\tau(x, 0.1) \\approx \\tau(x, 0.11) \\approx \\tau(x, 0.12) \\cdots$$ ì´ëŠ” í–‰ë ¬ì˜ í–‰ë“¤ì´ ê±°ì˜ ì„ í˜• ì¢…ì†ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. 4.2 ê³ ìœ ê°’ì˜ ì˜ë¯¸ ê³ ìœ ê°’ì´ 0ì— ê°€ê¹Œì›€ = í•´ë‹¹ ë°©í–¥ì˜ ì •ë³´ê°€ ê±°ì˜ ì—†ìŒ ì˜ˆì‹œ: í–‰ë ¬ Aì˜ ê³ ìœ ê°’: $\\lambda_1 = 10$, $\\lambda_2 = 0.001$, $\\lambda_3 = 0.0001$ $\\lambda_1 = 10$: ì´ ë°©í–¥ì€ ì •ë³´ê°€ í’ë¶€ $\\lambda_2 = 0.001$: ì´ ë°©í–¥ì€ ì •ë³´ê°€ ê±°ì˜ ì—†ìŒ $\\lambda_3 = 0.0001$: ì´ ë°©í–¥ì€ ê±°ì˜ ì •ë³´ ì—†ìŒ 4.3 ì—°ì†í˜• ì²˜ì¹˜ì—ì„œì˜ ìƒí™© ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ”: ëŒ€ë¶€ë¶„ì˜ ê³ ìœ ê°’ì´ 0ì— ê°€ê¹Œì›€ ëª‡ ê°œì˜ ê³ ìœ ê°’ë§Œ ì˜ë¯¸ ìˆìŒ í–‰ë ¬ì´ ê±°ì˜ íŠ¹ì´(singular)ì— ê°€ê¹Œì›€ ì™œ ë¬¸ì œê°€ ë˜ëŠ”ê°€? 5.1 ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± ì‘ì€ ë…¸ì´ì¦ˆ â†’ í° ì˜¤ì°¨ ì›ë˜ ë¬¸ì œ: $Ax = b$ ë…¸ì´ì¦ˆê°€ ìˆëŠ” ë¬¸ì œ: $(A + \\epsilon)\\tilde{x} = b + \\delta$ ê²°ê³¼: $\\ x - \\tilde{x}\\ $ê°€ ë§¤ìš° í´ ìˆ˜ ìˆìŒ 5.2 í•´ì˜ ìœ ì¼ì„± ë¶€ì¡± ë¬´ìˆ˜íˆ ë§ì€ í•´ê°€ ì¡´ì¬: $$\\tau(x,t) = \\tau_0(x,t) + \\epsilon(x,t)$$ ì—¬ê¸°ì„œ $\\epsilon(x,t)$ëŠ” R-lossë¥¼ ê±°ì˜ ë³€í™”ì‹œí‚¤ì§€ ì•ŠëŠ” í•¨ìˆ˜ 5.3 ì¶”ì •ì˜ ì–´ë ¤ì›€ ì–´ë–¤ í•´ê°€ \"ì •ë‹µ\"ì¸ì§€ ì•Œ ìˆ˜ ì—†ìŒ í•´ê²° ë°©ë²•: Tikhonov ì •ê·œí™” 6.1 ê¸°ë³¸ ì•„ì´ë””ì–´ ì •ê·œí™” í•­ ì¶”ê°€: $$L(\\tau) = \\mathbb{E}[(Y - \\mu_0(X) - \\tau(X,T)(T - \\pi(X)))^2] + \\lambda\\ \\tau\\ ^2$$ 6.2 ìˆ˜í•™ì  íš¨ê³¼ í–‰ë ¬ í‘œí˜„: $$(A^TA + \\lambda I)x = A^Tb$$ ê³ ìœ ê°’ ë³€í™”: ì›ë˜ ê³ ìœ ê°’: $\\lambda_i$ ì •ê·œí™” í›„: $\\lambda_i + \\lambda$ 6.3 ì™œ ë„ì›€ì´ ë˜ëŠ”ê°€? 0ì— ê°€ê¹Œìš´ ê³ ìœ ê°’ë“¤ì´ Î»ë§Œí¼ ì¦ê°€ í–‰ë ¬ì´ ë” ì•ˆì •ì (well-conditioned)ì´ ë¨ ìœ ì¼í•œ í•´ ë³´ì¥ ì‹¤ì œ ì˜ˆì‹œ 7.1 ê°„ë‹¨í•œ ì˜ˆì‹œ ì—°ì†í˜• ì²˜ì¹˜ í•¨ìˆ˜: $$\\tau(x,t) = 2t + 0.1\\sin(10t)$$ ë¬¸ì œ: sin(10t) í•­ì´ R-lossì— ê±°ì˜ ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŒ í•´ê²°: ì •ê·œí™”ë¥¼ í†µí•´ \"ë¶€ë“œëŸ¬ìš´\" í•¨ìˆ˜ ì„ í˜¸ 7.2 ìˆ˜ì¹˜ì  ì˜ˆì‹œ ì›ë˜ í–‰ë ¬ Aì˜ ê³ ìœ ê°’: $[10, 0.1, 0.01, 0.001, \\ldots]$ ì •ê·œí™” í›„ ($\\lambda=1$): $[11, 1.1, 1.01, 1.001, \\ldots]$ ì¡°ê±´ìˆ˜ê°€ í¬ê²Œ ê°œì„ ë©ë‹ˆë‹¤. ê²°ë¡  ì—°ì†í˜• ì²˜ì¹˜ R-learnerì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œëŠ”: ë¬´í•œì°¨ì› í•¨ìˆ˜ ê³µê°„ì—ì„œ ë°œìƒ Ill-conditioned í–‰ë ¬ë¡œ ìˆ˜í•™ì  í‘œí˜„ ê³ ìœ ê°’ë“¤ì´ 0ì— ê°€ê¹Œì›€ìœ¼ë¡œ íŠ¹ì§• Tikhonov ì •ê·œí™”ë¡œ í•´ê²° ì´ëŸ¬í•œ ì´í•´ëŠ” ì—°ì†í˜• ì²˜ì¹˜ íš¨ê³¼ ì¶”ì •ì˜ ì´ë¡ ì  ê¸°ë°˜ì„ ì œê³µí•©ë‹ˆë‹¤. ì°¸ê³  ìë£Œ ì¶”ì²œ ë„ì„œ: Golub, G. H., & Van Loan, C. F. (2013). Matrix computations. JHU press. Trefethen, L. N., & Bau, D. (1997). Numerical linear algebra. SIAM. í•µì‹¬ ë…¼ë¬¸: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. Hansen, P. C. (1998). Rank-deficient and discrete ill-posed problems: numerical aspects of linear inversion. ì˜¨ë¼ì¸ ìë£Œ: MIT OpenCourseWare: Linear Algebra Khan Academy: Eigenvalues and Eigenvectors"
  },
  {
    "id": "Causal Inference/Paper Review/review-Towards-R-learner-with-Continuous-Treatments",
    "title": "[Paper Review] Towards R-learner with Continuous Treatments",
    "date": "2025-07-11",
    "excerpt": "ì—°ì†í˜• ì²˜ì¹˜ë¥¼ ìœ„í•œ R-learnerë¥¼ ì–´ë–»ê²Œ êµ¬í˜„í•  ìˆ˜ ìˆëŠ”ì§€ì— ëŒ€í•œ ë…¼ì˜",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link ë…¼ë¬¸ì˜ ë°°ê²½ --- Quasi-Oracle Estimation of Heterogeneous Treatment Effects ì—ì„œ ê°œì¸í™” ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí–ˆìŠµë‹ˆë‹¤. <small> * ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ : ì–´ë–¤ ì²˜ì¹˜ë¥¼ í–ˆì„ ë•Œ ê°œì¸ë³„ë¡œ ì–´ë–¤ íš¨ê³¼ê°€ ìˆì„ì§€ ì¶”ì •í•œ ê²ƒ</small> ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ê±´ ì¸ê³¼ì¶”ë¡ ì˜ ê°€ì¥ í•µì‹¬ì ì¸ ë¬¸ì œì´ë©°, ì´ëŠ” ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ í†µì°°ì„ ì œê³µí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì •ë°€ì˜í•™ì—ì„œëŠ” í™˜ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì²˜ì¹˜ ì„ íƒì„ ê²°ì •í•˜ê³ , êµìœ¡ì—ì„œëŠ” í•™ìƒë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ êµìœ¡ ë°©ë²•ì„ ê²°ì •í•˜ê³ , ì˜¨ë¼ì¸ ë§ˆì¼€íŒ…ì—ì„œëŠ” ì‚¬ìš©ìë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ë§ì¶¤í˜• ê´‘ê³ ë¥¼ ì œê³µí•˜ê³ , ì˜¤í”„ë¼ì¸ ì •ì±… í‰ê°€ì—ì„œëŠ” ì§€ì—­ë³„ ì²˜ì¹˜ íš¨ê³¼ë¥¼ ì¶”ì •í•˜ì—¬ ì •ì±…ì„ ê²°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ì¦‰, ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì•Œê²Œ ë˜ë©´ ì–´ë–¤ ì„ íƒì— ëŒ€í•œ ê·¼ê±°ë¥¼ ì œê³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ë…¼ë¬¸ì—ì„œëŠ” ì´ì§„ ì²˜ì¹˜ì˜ ê°œì¸í™” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì´ì•¼ê¸°í–ˆê³ , ì´ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ í™•ì¥í•´ì„œ ì—°ì†í˜• ì²˜ì¹˜ì— ëŒ€í•´ì„œë„ ì´ë¥¼ ì ìš©í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ì„ ì´ ë…¼ë¬¸ì—ì„œ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤. [[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects](/posts/Causal%20Inference/review-Quasi-Oracle-Estimation-of-Heterogeneous-Treatment-Effects) <- ì´ ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·°ëŠ” ì—¬ê¸°ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. > ê¸°ì¡´ì˜ ë°©ë²•ì„ í™•ì¥í•  ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œì™€ ì´ë¥¼ í•´ê²°í•œ ë°©ë²•ë¡ ì— ëŒ€í•œ ì´ì•¼ê¸°ì— ì§‘ì¤‘í•´ì„œ ì´ ë…¼ë¬¸ì„ ì´í•´í–ˆìŠµë‹ˆë‹¤. ê°„ë‹¨íˆ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤ ì´ì§„ ì²˜ì¹˜ì—ì„œëŠ” ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ ìˆì—ˆëŠ”ë°, ì´ë¥¼ ì—°ì†í˜• ì²˜ì¹˜ë¡œ í™•ì¥í•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•œë‹¤. (ìœ ì¼í•´ë¥¼ ê°€ì§€ì§€ ì•ŠëŠ” ë¬¸ì œ) ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ 2ê°€ì§€ ë°©ë²•ì„ ì œì•ˆí•œë‹¤. Tikhonov regularization zero-constraining operatorë¥¼ í†µí•´ì„œ í•´ê²°í•œë‹¤. ë…¼ë¬¸ ë‚´ìš© ì •ë¦¬ --- Abstract --- > However, extending the R-learner framework from binary to continuous treatments introduces a non-identifiability issue, as the functional zero constraint inherent to the conditional average treatment effect cannot be directly imposed in the R-loss under continuous treatments ì´ ë…¼ë¬¸ì˜ í•µì‹¬ ì£¼ì¥ì…ë‹ˆë‹¤. > binary ë¥¼ continuousë¡œ í™•ì¥í•˜ë©´ non-identifiability issueê°€ ë°œìƒí•œë‹¤. ì´ê±¸ í•´ê²°í•˜ê¸° ìœ„í•œ ê³¼ì •ì„ identification strategyë¼ê³  ì¹­í•˜ê³  ìˆìŠµë‹ˆë‹¤. 2ê°€ì§€ ê³¼ì •ì„ í†µí•´ ì´ë¥¼ êµ¬í˜„í•œë‹¤ê³  í•©ë‹ˆë‹¤. Tikhonov regularization zero-constraining operator ì™œ ì´ìŠˆê°€ ë°œìƒí•˜ê³  ì–´ë–»ê²Œ í•´ê²°í–ˆëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ”ê²Œ ì´ ë…¼ë¬¸ì˜ í•µì‹¬ì´ë¼ê³  ì´í•´ë©ë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ ë…¼ë¬¸ì—ì„œ generalized R-lossë¥¼ ì •ì˜í–ˆëŠ”ë°, ì´ ìˆ˜ì‹ì´ ì–´ë–»ê²Œ ìœ ë„ë˜ëŠ”ì§€ë¥¼ ì´í•´í•˜ëŠ” ë¶€ë¶„ë„ í° ë„ì›€ì´ ë©ë‹ˆë‹¤. Introduction --- > Estimating heterogeneous treatment effects is fundamental in causal inference and provides insights into various fields, including precision medicine, education, online marketing, and offline policy evaluation ê°œì¸ë³„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ê±´ ë‹¤ì–‘í•œ ë¶„ì•¼ì—ì„œ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ì œê³µí•´ì¤„ ìˆ˜ ìˆê¸°ì— ì¤‘ìš”í•œ ë¬¸ì œì…ë‹ˆë‹¤. The treatment effect heterogeneity can be quantified by: $$ \\begin{equation} \\tau(x, t) = E[Y^{(t)} - Y^{(0)} X = x] \\end{equation} $$ $t = 0$ : reference treatment level heterogeneous treatment effectsëŠ” conditional average treatment effect (CATE)ë¡œ ì •ì˜ë©ë‹ˆë‹¤. ì´ëŠ” ì¡°ê±´ë¶€ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ê³„ì‚°í•œ ê°’ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. (í™˜ìì˜ ì¡°ê±´ì´ xì¼ ë•Œì˜ ATE) > On the contrary, the R-learner and its variants (Kennedy, 2023) target the treatment effect estimatio Towards Optimal Doubly Robust Estimation of Heterogeneous Causal Effects ê¸°ì¡´ì˜ ë‹¤ë¥¸ ì—°êµ¬ë“¤ì€ ì´ CATEë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ì§€ ì•Šì•˜ì§€ë§Œ R-learner ëª¨ë¸ì€ ì´ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ì¶”ì •í•˜ê³  ìˆìŠµë‹ˆë‹¤. > The R-learner capitalizes on the decomposition of the outcome model initially proposed by Robinson (1988) in partially linear models and extends for machine learning-based treatment effect estimation (Nie and Wager, 2021) R-learnerëŠ” partially linear modelì—ì„œ ì œì•ˆí•œ ìˆ˜í•™ì  ë¶„í•´ ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ ì‚¼ì•„ì„œ ì´ë¥¼ ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ìœ¼ë¡œ í™•ì¥ëœ ê°œë…ì…ë‹ˆë‹¤. ì´ì— ëŒ€í•œ ê°„ë‹¨í•œ ì •ë¦¬ëŠ” What is FWL?ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¸ê³  ë…¼ë¬¸: Robinson, P. M. (1988). Root-N-consistent semiparametric regression. Econometrica, 56(4), 931-954. [ë…¼ë¬¸ ë§í¬] Nie, X., & Wager, S. (2021). Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2), 299-319. [ë…¼ë¬¸ ë§í¬] > Notably, when using the two nuisance functions estimated under flexible models, the R-learner preserves the oracle property of treatment effect estimation as though the nuisance functions were known. Despite these advantages, the current R-learner framework applies only to binary or categorical treatments. R-learnerì˜ ì¤‘ìš”í•œ íŠ¹ì§•ì€ ìœ ì—°í•œ ëª¨ë¸ì„ í†µí•´ ë‘ ê°œì˜ nuisance functionì„ ì¶”ì •í•˜ë”ë¼ë„, ë§ˆì¹˜ ì´ í•¨ìˆ˜ë“¤ì´ ì •í™•íˆ ì•Œë ¤ì§„ ê²ƒì²˜ëŸ¼ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì˜ oracle ì„±ì§ˆì„ ìœ ì§€í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ì¥ì ì—ë„ ë¶ˆêµ¬í•˜ê³ , í˜„ì¬ R-learner í”„ë ˆì„ì›Œí¬ëŠ” ì´ì§„í˜• ë˜ëŠ” ë²”ì£¼í˜• ì²˜ì¹˜ì—ë§Œ ì ìš©í•  ìˆ˜ ìˆë‹¤ëŠ” í•œê³„ê°€ ìˆìŠµë‹ˆë‹¤. > In this article, we extend the R-learner framework to estimate the conditional average treatment effect flexibly with continuous treatments. This extension is nontrivial in both identification and estimation. Echoing the approach of Nie and Wager (2021), we focus on adapting the generalized R-learner loss function with continuous treatments. ì´ ë…¼ë¬¸ì—ì„œëŠ” R-learner í”„ë ˆì„ì›Œí¬ë¥¼ ì—°ì†í˜• ì²˜ì¹˜ì— ëŒ€í•´ì„œ í™•ì¥í•´ì„œ CATEë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. ì´ í™•ì¥ì€ ì‹ë³„ê³¼ ì¶”ì • ëª¨ë‘ì—ì„œ ì–´ë ¤ìš´ ë¬¸ì œì…ë‹ˆë‹¤. Nieì™€ Wager (2021)ì˜ ì ‘ê·¼ ë°©ì‹ì„ ë”°ë¼ì„œ ì—°ì†í˜• ì²˜ì¹˜ì— ëŒ€í•œ ì¼ë°˜í™”ëœ R-learner ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì ìš©í•˜ëŠ” ë°©ë²•ì„ ì¤‘ì ì ìœ¼ë¡œ ë‹¤ë£¹ë‹ˆë‹¤. ì´ì§„ì²˜ì¹˜ ($T = 1$ or $T = 0$ ì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°)ì™€ ë‹¤ë¥´ê²Œ ì—°ì†í˜•ì¼ ê²½ìš°ì— ë°œìƒí•˜ëŠ” ì–´ë ¤ì›€ë“¤ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ë©° ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. > Unlike the binary-treatment case, we demonstrate that directly minimizing the generalized R-loss does not uniquely identify $\\tau (x, t)$ but instead identifies a broad class of functions. ì´ì§„ì²˜ì¹˜ì™€ ë‹¬ë¦¬, ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” R-lossë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒë§Œìœ¼ë¡œëŠ” ìœ ì¼í•œ CATE í•¨ìˆ˜ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ê³ , ëŒ€ì‹  í•¨ìˆ˜ë“¤ì˜ ë„“ì€ ì§‘í•©ë§Œì„ ì‹ë³„í•  ìˆ˜ ìˆë‹¤ê³  ì„¤ëª…í•©ë‹ˆë‹¤. ì´ê²ƒì´ ë°”ë¡œ non-identifiability issueì˜ í•µì‹¬ì…ë‹ˆë‹¤. ì´ì§„ì²˜ì¹˜ì˜ ê²½ìš° zero condition ($\\tau (x, t): \\tau (x, 0) \\equiv 0$)ì„ ë§Œì¡±í•˜ëŠ” ê²ƒì´ ì‰½ì§€ë§Œ, ì—°ì†í˜• ì²˜ì¹˜ì˜ ê²½ìš°ì—ëŠ” ì´ë¥¼ ë§Œì¡±í•˜ëŠ” ê²ƒì´ ì‰½ì§€ ì•Šê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ë³´ë©´, ì´ì§„ì²˜ì¹˜ì˜ ê²½ìš° zero conditionì„ ë§Œì¡±í•˜ê³  ì´ê²Œ cate functionì„ ìœ ì¼í•˜ê²Œ ì¶”ì •í•˜ë„ë¡ í•´ì£¼ì§€ë§Œ, ì—°ì†í˜• ì²˜ì¹˜ì˜ ê²½ìš°ì—ëŠ” zero conditionì„ ë§Œì¡±í•˜ëŠ” ê²ƒì´ ì‰½ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ë¥¼ ë§Œì¡±í•˜ëŠ” í•¨ìˆ˜ë“¤ì˜ ë„“ì€ ì§‘í•©ë§Œì„ ì¶”ì •í•˜ê²Œ ëœë‹¤ê³  ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ non-identifiability ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ â„“2-ì •ê·œí™”ëœ R-learnerë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì´ ë°©ë²•ì€ Tikhonov ì •ê·œí™” ì›ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. Tikhonov ì •ê·œí™”ë€? Tikhonov ì •ê·œí™”ëŠ” ill-posed problem(ì˜ ì •ì˜ë˜ì§€ ì•Šì€ ë¬¸ì œ)ì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” non-identifiability ë¬¸ì œê°€ ë°”ë¡œ ì´ëŸ° ill-posed problemì˜ ëŒ€í‘œì ì¸ ì˜ˆì…ë‹ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´: ë¬¸ì œê°€ ë„ˆë¬´ ìœ ì—°í•´ì„œ í•´ê°€ ë¬´ìˆ˜íˆ ë§ì„ ë•Œ ì¶”ê°€ì ì¸ ì œì•½ì¡°ê±´(ì •ê·œí™”)ì„ ë„ì…í•˜ì—¬ í•´ë¥¼ ìœ ì¼í•˜ê²Œ ë§Œë“¦ ìˆ˜í•™ì ìœ¼ë¡œëŠ” f Â² ê°™ì€ ì •ê·œí™” í•­ì„ ì†ì‹¤ í•¨ìˆ˜ì— ì¶”ê°€ 2ë‹¨ê³„ ì¶”ì • ê³¼ì • ë…¼ë¬¸ì˜ í•µì‹¬ì€ 2ë‹¨ê³„ ì¶”ì • ê³¼ì •ì„ í†µí•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤: 1ë‹¨ê³„: ì¤‘ê°„ í•¨ìˆ˜ ì¶”ì • $$\\tilde{\\tau}(x, t) = \\tau(x, t) - \\mathbb{E}\\{\\tau(X, T) \\mid X = x\\}$$ 2ë‹¨ê³„: Zero-constraining operatorë¥¼ í†µí•œ ë³€í™˜ $$\\tau(x, t) = \\text{ë³€í™˜}(\\tilde{\\tau}(x, t))$$ Zero-constraining operatorì˜ ì—­í•  ì´ operatorëŠ” ì¶”ì •ëœ í•¨ìˆ˜ê°€ í•­ìƒ zero conditionì„ ë§Œì¡±í•˜ë„ë¡ ë³´ì¥í•©ë‹ˆë‹¤: Ï„(x, 0) = 0 (ê¸°ì¤€ ì²˜ì¹˜ ìˆ˜ì¤€ì—ì„œì˜ íš¨ê³¼ëŠ” 0) ì´ë¥¼ í†µí•´ ìœ ì¼í•œ í•´ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŒ Sieve ë°©ë²•ë¡  Sieve ë°©ë²•ì€ ë¬´í•œì°¨ì› í•¨ìˆ˜ ê³µê°„ì„ ìœ í•œì°¨ì›ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤: ê¸°ë³¸ ì•„ì´ë””ì–´: ë¬´í•œì°¨ì› í•¨ìˆ˜ë¥¼ ìœ í•œê°œì˜ ê¸°ì € í•¨ìˆ˜ë“¤ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ê·¼ì‚¬ ì˜ˆ: ë‹¤í•­ì‹, ìŠ¤í”Œë¼ì¸, í‘¸ë¦¬ì— ê¸‰ìˆ˜ ë“± ìˆ˜í•™ì  í‘œí˜„: $$\\tau(x, t) \\approx \\sum_i \\beta_i \\phi_i(x, t)$$ ì—¬ê¸°ì„œ $\\phi_i(x, t)$ëŠ” ê¸°ì € í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤. ì €ì°¨ì› í–‰ë ¬ê³¼ Non-identification ì¼ë°˜ì ì¸ sieve íšŒê·€ì™€ ë‹¬ë¦¬, ì´ ë…¼ë¬¸ì—ì„œëŠ” ì €ì°¨ì› í–‰ë ¬ì´ ë“±ì¥í•©ë‹ˆë‹¤. ì´ëŠ” ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ë°œìƒí•˜ëŠ” non-identification ë¬¸ì œì˜ ìˆ˜í•™ì  í‘œí˜„ì…ë‹ˆë‹¤. í•µì‹¬ ê°œë…: Generalized R-lossì˜ non-identification íŠ¹ì„± ë•Œë¬¸ì— í–‰ë ¬ì´ ill-conditionedê°€ ë¨ í–‰ë ¬ì˜ ê³ ìœ ê°’ë“¤ì´ 0ì— ê°€ê¹Œì›Œì ¸ì„œ ìˆ˜ì¹˜ì  ë¶ˆì•ˆì •ì„± ë°œìƒ ì´ëŠ” í–‰ë ¬ êµë€ ì´ë¡ (matrix perturbation theory)ê³¼ ìŠ¤í™íŠ¸ëŸ´ ë¶„ì„ì´ í•„ìš”í•œ ì´ìœ  ìì„¸í•œ ì´ë¡ ì  ë°°ê²½: Ill-conditioned í–‰ë ¬ê³¼ ê³ ìœ ê°’: ì—°ì†í˜• ì²˜ì¹˜ R-learnerì˜ ì´ë¡ ì  ë°°ê²½ ìˆ˜ë ´ ì†ë„ì˜ íŠ¹ì§• í•µì‹¬ ê²°ê³¼: > \"nuisance í•¨ìˆ˜ë“¤ì´ oP(n^(-1/4)) ìˆ˜ë ´ ì†ë„ë¡œ ê·¼ì‚¬ë  ìˆ˜ ìˆë‹¤ë©´, ì¶”ì •ëŸ‰ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ê²°ê³¼ ëª¨ë¸ì˜ ë§¤ë„ëŸ¬ì›€ì— ì˜ì¡´í•˜ì§€ ì•Šê³ , ì˜¤ì§ CATEì™€ propensity score í•¨ìˆ˜ì˜ ë§¤ë„ëŸ¬ì›€ì—ë§Œ ì˜ì¡´í•œë‹¤\" ì˜ë¯¸: ê²°ê³¼ ëª¨ë¸ì˜ ë³µì¡ì„±ì— ê´€ê³„ì—†ì´ ì¢‹ì€ ì„±ëŠ¥ ë³´ì¥ CATEì™€ propensity scoreë§Œ ì˜ ì¶”ì •ë˜ë©´ ë¨ ì´ëŠ” double robustnessì˜ ì—°ì†í˜• ë²„ì „ ì ê·¼ì  ì •ê·œì„±ê³¼ ì¶”ë¡  ì ê·¼ì  ì •ê·œì„±: ì¶”ì •ëŸ‰ì´ ì •ê·œë¶„í¬ë¡œ ìˆ˜ë ´ ì´ë¥¼ í†µí•´ ì‹ ë¢°êµ¬ê°„ê³¼ ê°€ì„¤ê²€ì • ê°€ëŠ¥ ë‹«íŒ í˜•íƒœ ë¶„ì‚° ì¶”ì •ëŸ‰: ë³µì¡í•œ ë¶€íŠ¸ìŠ¤íŠ¸ë© ì—†ì´ë„ ë¶„ì‚° ê³„ì‚° ê°€ëŠ¥ ê³„ì‚° íš¨ìœ¨ì„± í–¥ìƒ ì°¸ê³  ë…¼ë¬¸: Tikhonov, A. N. (1963). On the solution of ill-posed problems and the method of regularization. Doklady Akademii Nauk SSSR, 151(3), 501-504. [ë…¼ë¬¸ ë§í¬] Bhatia, R. (2013). Matrix analysis. Springer Science & Business Media. [ì±… ë§í¬] Chen, X. (2007). Large sample sieve estimation of semi-nonparametric models. Handbook of econometrics, 6, 5549-5632. [ë…¼ë¬¸ ë§í¬] í•µì‹¬ ê°œë…ë“¤: Ill-posed problem: í•´ê°€ ìœ ì¼í•˜ì§€ ì•Šê±°ë‚˜ ë¶ˆì•ˆì •í•œ ë¬¸ì œ Tikhonov regularization: ì •ê·œí™”ë¥¼ í†µí•œ ill-posed problem í•´ê²° Sieve method: ë¬´í•œì°¨ì›ì„ ìœ í•œì°¨ì›ìœ¼ë¡œ ê·¼ì‚¬í•˜ëŠ” ë°©ë²• Matrix perturbation theory: í–‰ë ¬ì˜ ì‘ì€ ë³€í™”ê°€ ê³ ìœ ê°’ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ ì—°êµ¬ Spectral analysis: í–‰ë ¬ì˜ ê³ ìœ ê°’ê³¼ ê³ ìœ ë²¡í„° ë¶„ì„ 1.1 Setup and notation ì´ë¡ ì  ë°°ê²½ê³¼ ê´€ë ¨í•œ ìˆ˜ì‹ì„ ì •ë¦¬í•©ë‹ˆë‹¤. $\\{Z_i = (X_i, T_i, Y_i)\\}_{i=1}^n$ : independent and identically distributed samples from the distribution of $(X, T, Y)$ $X = (X^{(1)}, \\ldots, X^{(d)})$ : $d$-dimensional vector of covariates. $Y^{(t)}$ : potential outcome had the unit received treatment level $T = t \\in \\mathbb{R}$ $\\tau(x, t)$ : causal estimand defined in (1) Under Rubin's causal model framework (Rubin, 1974), $$ \\begin{equation} \\tau(x, t) = E[Y^{(t)} - Y^{(0)} X = x] \\end{equation} $$ > Due to the fundamental problem in causal inference that not all potential outcomes can be observed for a particular unit, $\\tau(x, t)$ is not identifiable without further assumptions. We employ common assumptions for continuous treatments (Kennedy et al., 2017). ì¸ê³¼ì¶”ë¡ ì—ì„œëŠ” ëª¨ë“  ê´€ì¸¡ê°’ì„ í™•ì¸í•  ìˆ˜ ì—†ëŠ” ìƒíƒœì—ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œ 3ê°€ì§€ì˜ ê°€ì •ì´ ì¡´ì¬í•©ë‹ˆë‹¤. Assumption 1 (No unmeasured confounding). We have $\\{Y^{(t)}\\}_{t \\in \\mathcal{T}} \\perp\\!\\!\\!\\perp T \\mid X$ Assumption 2 (Stable unit and treatment value). When $T = t \\in \\mathcal{T}$, we have $Y = Y^{(t)}$ Assumption 3 (Positivity). There exists an $\\epsilon > 0$ such that the generalized propensity score $f(T = t \\mid X = x) \\in (\\epsilon, 1/\\epsilon)$ for any $(x, t) \\in \\mathcal{X} \\times \\mathcal{T}$. Notation: For any vector $v$, $\\ v\\ $ denotes its $\\ell_2$ norm For any random variable $W \\in \\mathcal{W}$, $f(w)$ and $P(w)$ denote its probability density function and probability measure For any function $g(w)$: $P_n\\{g(W)\\} = \\frac{1}{n}\\sum_{i=1}^n g(W_i)$ denotes its empirical expectation $\\ g\\ _{L_2} = \\left\\{\\int_{w \\in \\mathcal{W}} g^2(w) dw\\right\\}^{1/2}$ denotes its $L_2$ norm $\\ g\\ _{L_2^P} = \\left\\{\\int_{w \\in \\mathcal{W}} g^2(w) dP(w)\\right\\}^{1/2}$ denotes its $L_2^P$ norm $\\ g\\ _{\\mathcal{W}} = \\sup_{w \\in \\mathcal{W}} g(w) $ denotes its $L_\\infty$ norm $L_2^P(\\mathcal{W})$ represents the function space of all $g(w)$ with a bounded $L_2^P$ norm When $g(w)$ is a multivariate function, denote $\\ g\\ _{\\mathcal{W}} = \\sup_{w \\in \\mathcal{W}} \\ g(w)\\ $ Nuisance Functions: Conditional outcome mean: $m(x) = E(Y \\mid X = x)$ Generalized propensity score: $\\varpi(t \\mid x) = f(T = t \\mid X = x)$ Full conditional outcome mean model: $$\\mu(x, t) = E(Y \\mid X = x, T = t)$$ Observation noises: $$\\varepsilon_i = Y_i - \\mu(X_i, T_i), \\quad i = 1, \\ldots, n$$ where $E(\\varepsilon_i \\mid X_i, T_i) = 0$, following the definition of $\\mu(x, t)$. Generalized R-learner --- 2.1 The generalized R-loss --- > We first generalize the idea of the Robinson's residual (Robinson, 1988; Nie and Wager, 2021) to the continuous-treatment scenario. ì´ë²ˆ ì ˆì—ì„œëŠ” ì¼ë°˜í™”ëœ R-lossë¥¼ ì–´ë–»ê²Œ ìœ ë„í•˜ëŠ”ì§€ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. The unconfoundedness and stable unit and treatment value imply: $$ \\begin{equation} Y_i^{(T_i)} = \\mu(X_i, T_i) + \\varepsilon_i = \\mu(X_i, 0) + \\tau(X_i, T_i) + \\varepsilon_i \\tag{3} \\end{equation} $$ > í•´ì„: > - ì²« ë²ˆì§¸ ë“±ì‹: Assumption 2ì™€ equation (2)ì—ì„œ ìœ ë„ > - ë‘ ë²ˆì§¸ ë“±ì‹: Assumption 1ê³¼ $\\tau(x, t)$ì˜ ì •ì˜ì—ì„œ ìœ ë„ > - íŠ¹ì§•: ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ì´ë©° ì¶”ê°€ì ì¸ êµ¬ì¡°ì  ê°€ì •ì´ ì—†ìŒ Step 2: ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ ê³„ì‚°: Given $X_i$, taking the conditional expectation on (3) leads to: $$ \\begin{equation} m(X_i) = E\\left[Y_i^{(T_i)} \\mid X = X_i\\right] = \\mu(X_i, 0) + E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} \\tag{4} \\end{equation} $$ > ì „ì²´ ê¸°ëŒ“ê°’ ë²•ì¹™ ì ìš©: > $$E(\\varepsilon_i \\mid X_i) = E\\left[E(\\varepsilon_i \\mid X_i, T_i) \\mid X_i\\right] = E[0 \\mid X_i] = 0$$ ì¼ë°˜í™”ëœ Propensity Score ê¸°ëŒ“ê°’: The notation $E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\}$ in (4) highlights the dependency of the conditional expectation on the generalized propensity score as: $$E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} = \\int_{t \\in \\mathcal{T}} \\tau(X_i, t) \\varpi(t \\mid X_i) dt$$ > í•µì‹¬: ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” ì ë¶„ì„ í†µí•´ ëª¨ë“  ì²˜ì¹˜ ìˆ˜ì¤€ì— ëŒ€í•œ ê°€ì¤‘ í‰ê· ì„ ê³„ì‚° Step 3: ì”ì°¨ ë„ì¶œ: By subtracting (4) from (3) on both left- and right-hand sides, we have: $$ \\begin{equation} Y_i^{(T_i)} - m(X_i) = \\tau(X_i, T_i) - E_{\\varpi}\\{\\tau(X, T) \\mid X = X_i\\} + \\varepsilon_i \\tag{5} \\end{equation} $$ > í•´ì„: > - ì¢Œë³€: ê´€ì°°ëœ ê²°ê³¼ì—ì„œ ì¡°ê±´ë¶€ í‰ê· ì„ ëº€ ê°’ (Robinson's residual) > - ìš°ë³€: ì²˜ì¹˜íš¨ê³¼ì—ì„œ í‰ê·  ì²˜ì¹˜íš¨ê³¼ë¥¼ ëº€ ê°’ + ë…¸ì´ì¦ˆ Step 4: ì†ì‹¤ í•¨ìˆ˜ ë„ì¶œ: By treating the left-hand side of (5) as the response and the right-hand side except $\\varepsilon_i$ as the mean function, we derive the following population loss function: $$ \\begin{equation} L_c(h) = E\\left[\\left\\{Y - m(X) - h(X, T) + E_{\\varpi}\\{h(X, T) \\mid X\\}\\right\\}^2\\right] \\tag{6} \\end{equation} $$ > í•µì‹¬ íŠ¹ì§•: > - ìµœì í•´: $h = \\tau$ì—ì„œ ìµœì†Œí™”ë¨ > - ì¼ë°˜í™”: ì´ì§„ ì²˜ì¹˜ R-learnerì˜ ìì—°ìŠ¤ëŸ¬ìš´ í™•ì¥ > - ì°¸ê³ : Nie and Wager (2021, Â§7)ì˜ ë‹¤ì¤‘ ì²˜ì¹˜ ì„¤ì •ì—ì„œ ìœ ì‚¬í•œ ì†ì‹¤í•¨ìˆ˜ ë“±ì¥ ì´ì§„ ì²˜ì¹˜ì™€ì˜ ì—°ê²°: > In particular, under the binary-treatment case, $\\tau(x, t)$ reduces to $\\{\\tau(x, 0), \\tau(x, 1)\\}$, where $\\tau(x, 0) = E(Y^{(0)} - Y^{(0)} \\mid X = x) = 0$ for any $x \\in \\mathcal{X}$, and $\\tau(x, 1)$ is the conditional average treatment effect of interest. It suffices to estimate $\\tau(x, 1)$ by solving the $h(\\cdot, 1)$ that minimizes (6), after imposing a zero condition of $h(\\cdot, 0)$: $$ \\begin{equation} h(x, 0) = 0, \\quad \\text{for any } x \\in \\mathcal{X} \\tag{7} \\end{equation} $$ > Zero condition: ê¸°ì¤€ ì²˜ì¹˜ ìˆ˜ì¤€ì—ì„œì˜ íš¨ê³¼ëŠ” 0ìœ¼ë¡œ ì„¤ì • ì´ì§„ ì²˜ì¹˜ R-lossë¡œì˜ í™˜ì›: ìœ ë„ëœ ì¼ë°˜í™”ëœ R-lossê°€ ê¸°ì¡´ì˜ ì´ì§„ ì²˜ì¹˜ì—ì„œì˜ loss functionìœ¼ë¡œ ìœ ë„ë  ìˆ˜ ìˆìŒì„ ì„¤ëª…í•©ë‹ˆë‹¤. > More specifically, observing that under (7) one has $h(X, T) - E_e\\{h(X, T) \\mid X\\} = \\{T - e(X)\\}h(X, 1)$ a.s., where $e(x) = \\text{pr}(T = 1 \\mid X = x)$ is the propensity score, the R-loss function (6) reduces to: $$ \\begin{equation} L_b(h) = E\\left[\\left\\{Y - m(X) - \\{T - e(X)\\}h(X, 1)\\right\\}^2\\right] \\tag{8} \\end{equation} $$ > í•µì‹¬: > - ì¡°ê±´: $h(x, 0) = 0$ (zero condition) > - ê²°ê³¼: $h(X, T) - E_e\\{h(X, T) \\mid X\\} = \\{T - e(X)\\}h(X, 1)$ > - ì˜ë¯¸: ì¼ë°˜í™”ëœ R-lossê°€ ì´ì§„ ì²˜ì¹˜ì˜ ê³ ì „ì  R-lossë¡œ í™˜ì›ë¨ > í•µì‹¬: Tê°€ 1 or 0ì¸ ìƒí™©ì—ì„œ zero conditionì„ ê³ ë ¤í•´ (6)ë²ˆ ìˆ˜ì‹ì„ ì „ê°œí•˜ë©´ ê¸°ì¡´ì˜ ì´ì§„ ì²˜ì¹˜ì—ì„œì˜ ì†ì‹¤í•¨ìˆ˜ ìˆ˜ì‹ê³¼ ê°™ì€ (8)ë²ˆ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 2.2 Non-identification of the generalized R-loss --- > The R-learner for continuous treatment will have poor estimation performance, due to the non-unique identifiability of the generalized R-loss í•µì‹¬ ë¬¸ì œ: ì´ ì ˆì—ì„œëŠ” non-identification ë¬¸ì œì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤. ì—°ì†í˜• ì²˜ì¹˜ì—ì„œ ì¼ë°˜í™”ëœ R-lossë¥¼ ì§ì ‘ ìµœì†Œí™”í•˜ë©´ ìœ ì¼í•œ í•´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ëŠ” ê²ƒì´ í•µì‹¬ ë¬¸ì œì…ë‹ˆë‹¤. í•´ì§‘í•© ì •ì˜: $$ \\begin{equation} S = \\{h \\mid h(X, T) = \\tau(X, T) + s(X) \\text{ a.s., for any } s \\in L_2^P(X)\\} \\tag{9} \\end{equation} $$ > í•´ì„: $S$ëŠ” ëª©í‘œ í•¨ìˆ˜ $\\tau(X, T)$ì— ê³µë³€ëŸ‰ $X$ì˜ í•¨ìˆ˜ $s(X)$ë¥¼ ë”í•œ ëª¨ë“  í•¨ìˆ˜ë“¤ì˜ ì§‘í•© ê²€ì¦ ê³¼ì •: > It is easy to check that for any $h \\in S$: $$ Y - m(X) - [h(X, T) - E_{\\varpi}\\{h(X, T) \\mid X\\}] = Y - m(X) - [\\tau(X, T) - E_{\\varpi}\\{\\tau(X, T) \\mid X\\}] \\text{ a.s.} $$ > ì˜ë¯¸: $S$ì— ì†í•œ ëª¨ë“  í•¨ìˆ˜ $h$ê°€ ë™ì¼í•œ ì†ì‹¤ê°’ì„ ê°€ì§ Non-identification ë¬¸ì œ: > From (6), any function $h \\in S$ minimizes the generalized R-loss $L_c(\\cdot)$. Therefore, when $T$ is continuous, directly minimizing the generalized R-loss fails to uniquely identify the target estimand $\\tau(x, t)$, as there are infinitely many solutions in $S$. > í•µì‹¬ ë¬¸ì œ: > - ì—°ì†í˜• ì²˜ì¹˜ì—ì„œëŠ” ë¬´í•œíˆ ë§ì€ í•´ê°€ ì¡´ì¬ > - ìœ ì¼í•œ CATE í•¨ìˆ˜ë¥¼ ì‹ë³„í•  ìˆ˜ ì—†ìŒ > - ì´ëŠ” ill-posed problemì˜ ì „í˜•ì ì¸ ì˜ˆ ì´ë¡ ì  ê·¼ê±°: > This result theoretically substantiates the ill-posedness of estimating $\\tau(x, t)$ by minimizing the empirical counterpart of $L_c(\\cdot)$ using nonparametric estimators, and also explains the failure-to-estimate issue illustrated in Fig. 1. > Part (i) of Proposition 1 below rigorously proves that $S$ in fact contains all minima of $L_c(\\cdot)$ in $L_2^P(X, T)$. ì´ì§„ ì²˜ì¹˜ì™€ì˜ ëŒ€ë¹„: > In contrast, minimizing the binary-treatment R-loss (8) which incorporates the zero condition (7), can successfully identify $\\tau$, because (7) narrows the general solution set $S$ into: $$ \\begin{equation} S^\\natural = \\{h \\mid h(X, T) = \\tau(X, T) \\text{ a.s.}\\} \\tag{10} \\end{equation} $$ > í•µì‹¬ ì°¨ì´: > - ì—°ì†í˜•: $S$ (ë¬´í•œíˆ ë§ì€ í•´) > - ì´ì§„í˜•: $S^\\natural$ (ìœ ì¼í•œ í•´) > - ì›ì¸: Zero condition (7)ì´ í•´ì§‘í•©ì„ ì¢í˜€ì¤Œ Proposition 1 - Suppose Assumptions 1â€“2 hold. Then, the following statements hold: ì´ ë¶€ë¶„ì€ generalized loss function (6) ì„ ìµœì†Œí™”í•˜ë©´ hëŠ” ë‹¤ì–‘í•œ í•´ë¥¼ ê°€ì§€ì§€ë§Œ, zero-conditionì„ ë…¹ì—¬ë‚´ ì •ë¦¬í•œ binary loss function (8) ì„ ìµœì†Œí™”í•˜ë©´ hëŠ” ìœ ì¼í•œ í•´ë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. > Part (i): ì²˜ì¹˜ Tê°€ ì—°ì†í˜•ì¸ ê²½ìš°, SëŠ” ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ë‹µí•´ë‹¤. > - ìµœì í™” ë¬¸ì œ: $$ \\begin{equation} \\arg\\min_{h \\in L_2^P(X,T)} L_c(h) \\tag{11} \\end{equation} $$ > - í•´ì§‘í•©: $S$ > Part (ii): ì²˜ì¹˜ Tê°€ ì´ì§„í˜•ì¸ ê²½ìš°, SëŠ” ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ë‹µí•´ë‹¤. > - ìµœì í™” ë¬¸ì œ: $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ > - í•´ì§‘í•©: $S^\\natural$ > ê²°ê³¼: ê´€ì‹¬ í•¨ìˆ˜ë“¤ì˜ ì§‘í•© $\\mathcal{L}_b = \\{h \\mid h(\\cdot, 1) \\in L_2^P(X) \\text{ and } h(X, 0) = 0 \\text{ a.s.}\\}$ ì¤‘ì—ì„œ, (10)ì˜ $S^\\natural$ê°€ ë‹¤ìŒ ìµœì í™” ë¬¸ì œì˜ í•´ì§‘í•©ì´ë‹¤: > - ìµœì í™” ë¬¸ì œ: $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ > - í•´ì§‘í•©: $S^\\natural$ > ì˜ë¯¸: ì´ì§„í˜• ì²˜ì¹˜ì—ì„œëŠ” zero conditionì´ ì ìš©ëœ R-loss $L_b(h)$ì˜ ìµœì†Œí™”ê°€ ìœ ì¼í•œ í•´ $S^\\natural$ë¥¼ ì œê³µ í•µì‹¬ ì°¨ì´ì : êµ¬ë¶„ ì—°ì†í˜• ì²˜ì¹˜ ì´ì§„í˜• ì²˜ì¹˜ ------ ------------- ------------- ìµœì í™” ë¬¸ì œ $\\arg\\min_{h \\in L_2^P(X,T)} L_c(h)$ $\\arg\\min_{h \\in \\mathcal{L}_b} L_b(h)$ í•´ì§‘í•© $S$ (ë¬´í•œíˆ ë§ì€ í•´) $S^\\natural$ (ìœ ì¼í•œ í•´) ì œì•½ì¡°ê±´ ì—†ìŒ $h(X, 0) = 0$ (zero condition) ì‹ë³„ ê°€ëŠ¥ì„± âŒ ë¶ˆê°€ëŠ¥ âœ… ê°€ëŠ¥ 2.3 One-step nonparametric identification with a functional zero constraint > ì´ì§„ ì²˜ì¹˜ì˜ ê²½ìš° zero-conditionë¥¼ loss functionì— ë…¹ì—¬ë‚¼ ìˆ˜ ìˆì–´ ìœ ì¼í•´ë¥¼ ì–»ì„ ìˆ˜ ìˆì—ˆì§€ë§Œ, ì—°ì† ì²˜ì¹˜ì˜ ê²½ìš° ìœ ì¼í•´ë¥¼ ì–»ì§€ ëª»í•œë‹¤. ê·¸ëŸ¼ optimization problemì„ í’€ ë•Œ zero conditionì„ ì ìš©í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ? ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. $$ \\begin{equation} \\arg \\min_{h \\in L_2^P(X,T) \\cap \\{h \\mid h(x,0) = 0 \\text{ for any } x \\in \\mathcal{X}\\}} L_c(h) \\tag{12} \\end{equation} $$ ê·¸ëŸ¬ë‚˜ Proposition 2ëŠ” ì´ ì „ëµë„ $\\tau(x, t)$ì˜ ë¹„ëª¨ìˆ˜ì  ì‹ë³„ì„ ë‹¬ì„±í•˜ëŠ” ë° ê³„ì† ì‹¤íŒ¨í•œë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Proposition 2. Assumptions 1â€“2ê°€ ì„±ë¦½í•˜ê³ , $(X, T)$ê°€ ìœ ê³„ ë°€ë„í•¨ìˆ˜ë¥¼ ê°€ì§„ë‹¤ê³  ê°€ì •í•˜ì. ì¦‰, $$\\sup_{(x,t) \\in \\mathcal{X} \\times \\mathcal{T}} f(x, t) < \\infty$$ $\\tau^{\\vee}(x, t \\mid s)$ë¥¼ ë‹¤ìŒê³¼ ê°™ì€ í˜•íƒœë¥¼ ì·¨í•˜ëŠ” í•¨ìˆ˜ë¼ê³  í•˜ì: $$ \\begin{equation} \\tau^{\\vee}(x, t \\mid s) = \\begin{cases} \\tau(x, t) + s(x) & \\text{when } t \\neq 0 \\\\ 0 & \\text{when } t = 0 \\end{cases} \\tag{13} \\end{equation} $$ ì—¬ê¸°ì„œ $s$ëŠ” $L_2^P(X)$ì˜ ì„ì˜ì˜ í•¨ìˆ˜ì´ë‹¤. ê·¸ëŸ¬ë©´ ì„ì˜ì˜ $s \\in L_2^P(X)$ì— ëŒ€í•´ $\\tau^{\\vee}(x, t \\mid s)$ëŠ” (12)ë¥¼ í•´ê²°í•œë‹¤. > í•µì‹¬: ì„ì˜ì˜ $\\tau^{\\vee}(x, t \\mid s)$ëŠ” zero condition (7)ì„ ë§Œì¡±í•˜ë©´ì„œë„ ì—¬ì „íˆ ì§‘í•© $S$ì— ì†í•  ìˆ˜ ìˆë‹¤. ì¦‰, zero conditionì„ ìµœì í™”ë¬¸ì œì— ê°•ì œë¡œ ì ìš©í•´ë„ $t \\neq 0$ì¸ ë¶€ë¶„ì—ì„œëŠ” ì—¬ì „íˆ ê°™ì€ non-identification ë¬¸ì œë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. 2.4 Two-step Tikhonov identification and â„“â‚‚ regularization R-learner --- 2.3ì—ì„œ ê°•ì œë¡œ zero conditionì„ ì ìš©í•´ì„œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë° ì‹¤íŒ¨í–ˆê³ , ì´ì œ ì •ê·œí™”í•­ì„ ë„ì…í•´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤. Step I: ëª¨ì§‘ë‹¨ ìˆ˜ì¤€ì—ì„œ ì£¼ì–´ì§„ $\\rho > 0$ì— ëŒ€í•´ (11)ì˜ $\\ell_2$-ì •ê·œí™” ë³€í˜•ì„ í•´ê²°í•©ë‹ˆë‹¤: $$ \\begin{equation} \\tau_\\rho = \\arg \\min_{h \\in L_2^P(X,T)} L_{c,\\ell_2}(h \\mid \\rho) = \\arg \\min_{h \\in L_2^P(X,T)} \\left[ L_c(h) + \\rho \\ h\\ _{L_2^P}^2 \\right] \\tag{14} \\end{equation} $$ > The new loss $L_{c,\\ell_2}(h \\mid \\rho)$ is strictly convex over $L_2^P(X, T)$ due to the addition of a strictly convex functional $\\rho \\ h\\ _{L_2^P}^2 = \\rho E\\{h^2(X, T)\\}$. > Thus minimizing $L_{c,\\ell_2}(h \\mid \\rho)$ becomes well-posed and yields a unique functional minimum $\\tau_\\rho$. Theorem 1 explicitly characterizes this unique minimum. ê°•í•œ (ì—„ê²©í•œ) convex functionì„ loss functionì— ì¶”ê°€í•˜ì—¬, loss functionì„ convexí•˜ê²Œ ë§Œë“¤ì–´ ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆë‹¤ëŠ” ì´ì•¼ê¸°ì…ë‹ˆë‹¤. ì´ë¡ ì  ë°°ê²½: Tikhonov regularizationê³¼ convex optimizationì˜ ìì„¸í•œ ì›ë¦¬ëŠ” Tikhonov Regularizationê³¼ Convex Optimizationì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Theorem 1. ì§‘í•© $S$ ë‚´ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ ì¤‘ê°„ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì: $$ \\begin{equation} \\tilde{\\tau}(x, t) = \\tau(x, t) - E\\{\\tau(X, T) \\mid X = x\\} \\tag{15} \\end{equation} $$ Assumptions 1â€“2ê°€ ì„±ë¦½í•˜ê³  $\\tau \\in L_2^P(X, T)$ì¼ ë•Œ, ì£¼ì–´ì§„ $\\rho > 0$ì— ëŒ€í•´ (14)ì˜ í•´ì§‘í•©ì€ ë‹¤ìŒê³¼ ê°™ë‹¤: $$S_\\rho = \\{h \\mid h(X, T) = \\tau_\\rho(X, T) \\text{ a.s.}\\}$$ ì—¬ê¸°ì„œ $$\\tau_\\rho(x, t) = (1 + \\rho)^{-1} \\tilde{\\tau}(x, t)$$ Theorem 1ì˜ ì˜ë¯¸: Theorem 1ì€ $\\tau_\\rho$ì— $(1 + \\rho)$ ì¸ìˆ˜ë¥¼ ê³±í•˜ì—¬ ì§‘í•© $S$ ë‚´ì˜ ì¤‘ê°„ í•¨ìˆ˜ $\\tilde{\\tau}$ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤: $$\\tilde{\\tau} = (1 + \\rho)\\tau_\\rho$$ ì´ $\\tilde{\\tau}$ëŠ” ì›ë˜ R-loss $L_c(h)$ì˜ í•´ê°€ ë©ë‹ˆë‹¤. Step II: Zero-constraining Operator Step IIì—ì„œëŠ” $\\tilde{\\tau}$ë¥¼ zero-constraining operator $\\mathcal{C}(\\cdot): L_2^P(X, T) \\to L_2^P(X, T)$ë¥¼ í†µí•´ ë³€í™˜í•©ë‹ˆë‹¤: $$ \\begin{equation} \\mathcal{C}(h)(x, t) = h(x, t) - h(x, 0) \\quad \\text{for any } h \\in L_2^P(X, T) \\end{equation} $$ ë”°ë¼ì„œ: $$\\mathcal{C}(\\tilde{\\tau})(x, t) = \\mathcal{C}((1 + \\rho)\\tau_\\rho)(x, t) = (1 + \\rho)\\{\\tau_\\rho(x, t) - \\tau_\\rho(x, 0)\\}$$ Zero-constraining Operatorì˜ ì—­í• : Zero-constraining Operatorì˜ í•µì‹¬ ì›ë¦¬: ì—°ì‚°ì $\\mathcal{C}(\\cdot)$ëŠ” ë³€í™˜ì„ ê±°ì¹˜ëŠ” ëª¨ë“  í•¨ìˆ˜ê°€ zero condition (7)ì„ ë§Œì¡±í•˜ë„ë¡ ë³´ì¥ $\\tau$ëŠ” ì§‘í•© $S$ì—ì„œ zero condition (7)ì„ ë§Œì¡±í•˜ëŠ” ìœ ì¼í•œ í•¨ìˆ˜ ë”°ë¼ì„œ í•´ì§‘í•© $S$ì˜ ì„ì˜ í•¨ìˆ˜ë¥¼ ë³€í™˜í•˜ë©´ $\\tau$ë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŒ $\\tilde{\\tau} \\in S$ì´ë¯€ë¡œ, Step IIì˜ $\\mathcal{C}(\\tilde{\\tau})$ëŠ” ê¶ê·¹ì ìœ¼ë¡œ $\\tau$ë¥¼ ì‹ë³„í•¨ > í•µì‹¬: 2ë‹¨ê³„ ê³¼ì •ì„ í†µí•´ non-identification ë¬¸ì œë¥¼ í•´ê²°í•˜ê³  ìœ ì¼í•œ CATE í•¨ìˆ˜ $\\tau(x, t)$ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Supplementary material for \"Towards R-learner with Continuous Treatments\" S1 : $l_2$ regularization R-learner : Formal algorithm and more discussions ë…¼ë¬¸ì—ì„œ ì˜ë¬¸ì´ ë“¤ì—ˆë˜ ë¶€ë¶„ë“¤ ì •ë¦¬ --- ì™œ ì´ì§„ ì²˜ì¹˜ì—ì„œëŠ” Zero Conditionì´ ì ìš©ë˜ê³ , ì—°ì† ì²˜ì¹˜ì—ì„œëŠ” ì ìš©ë˜ì§€ ì•Šì„ê¹Œ? ì´ì§„ ì²˜ì¹˜ì—ì„œì˜ Zero Condition ì´ì§„ ì²˜ì¹˜ì˜ íŠ¹ì„±: ì²˜ì¹˜ ìˆ˜ì¤€: $T \\in \\{0, 1\\}$ (ì´ì‚°ì ) ëª©í‘œ: $\\tau(x, 1)$ (ì²˜ì¹˜ íš¨ê³¼)ë§Œ ì¶”ì • ê¸°ì¤€ì : $T = 0$ (í†µì œ ê·¸ë£¹) Zero Conditionì˜ ìì—°ìŠ¤ëŸ¬ìš´ ì ìš©: $$\\tau(x, 0) = E[Y^{(0)} - Y^{(0)} \\mid X = x] = 0$$ > ì´ìœ : > - $T = 0$ì€ \"ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì€ ìƒíƒœ\"ë¥¼ ì˜ë¯¸ > - ê°™ì€ ìƒíƒœì—ì„œì˜ ì°¨ì´ëŠ” ë‹¹ì—°íˆ 0 > - ì´ëŠ” ìì—°ìŠ¤ëŸ¬ìš´ ì œì•½ì¡°ê±´ì´ ë¨ ìˆ˜í•™ì  íš¨ê³¼: í•´ì§‘í•© $S$ì—ì„œ $h(x, 0) = 0$ ì¡°ê±´ì„ ì¶”ê°€ $S^\\natural = \\{h \\mid h(X, T) = \\tau(X, T) \\text{ a.s.}\\}$ë¡œ ì¶•ì†Œ ìœ ì¼í•œ í•´ ë³´ì¥ ì—°ì† ì²˜ì¹˜ì—ì„œì˜ ë¬¸ì œ ì—°ì† ì²˜ì¹˜ì˜ íŠ¹ì„±: ì²˜ì¹˜ ìˆ˜ì¤€: $T \\in \\mathbb{R}$ (ì—°ì†ì ) ëª©í‘œ: $\\tau(x, t)$ for all $t \\in \\mathcal{T}$ (ëª¨ë“  ì²˜ì¹˜ ìˆ˜ì¤€ì—ì„œì˜ íš¨ê³¼) ê¸°ì¤€ì : $T = 0$ì´ ì„ì˜ì ì¼ ìˆ˜ ìˆìŒ Zero Condition ì ìš©ì˜ í•œê³„: > $L_b$ë¥¼ í†µí•´ ì´ì§„ì²˜ì¹˜ì—ì„œëŠ” zero-condtionì„ loss í•¨ìˆ˜ì— ë…¹ì—¬ë‚¼ ìˆ˜ ìˆì—ˆëŠ”ë°, ì—°ì†í˜•ì—ì„œëŠ” ì´ë¥¼ loss functionì— ë…¹ì—¬ë‚¼ ìˆ˜ ì—†ë‹¤. ë¬¸ì œëŠ” â€œzero conditionì„ R-loss ì•ˆì—ì„œ ì§ì ‘ êµ¬í˜„í•˜ê¸° ì–´ë µë‹¤â€ëŠ” ì ì…ë‹ˆë‹¤. Binary Tì—ì„œëŠ” Tâˆˆ{0, 1}ì´ë¼ (T-e(X))h(X,1) í˜•íƒœë¡œ ì‹ì„ ì¬ì •ë¦¬í•˜ë©´ h(x,0)=0ì´ ìë™ìœ¼ë¡œ ë‚´ì¬ë©ë‹ˆë‹¤. í•µì‹¬ ì°¨ì´ì  ìš”ì•½ êµ¬ë¶„ ì´ì§„ ì²˜ì¹˜ ì—°ì† ì²˜ì¹˜ ------ ----------- ----------- ì²˜ì¹˜ íŠ¹ì„± ì´ì‚°ì  $\\{0, 1\\}$ ì—°ì†ì  $\\mathbb{R}$ ê¸°ì¤€ì  $T = 0$ (ìì—°ìŠ¤ëŸ¬ì›€) $T = 0$ (ì„ì˜ì ) Zero Condition ìˆ˜ì‹ì—ì„œ ì‰½ê²Œ ê°•ì œ ê°€ëŠ¥ ë¬´í•œì°¨ì› ì œì•½ â†’ êµ¬í˜„ ë‚œì´ë„ ë†’ìŒ í•´ì§‘í•©(ì´ë¡ ) $S^\\natural$ (ìœ ì¼) $S$ (ë¬´í•œ) ì‹ë³„ ê°€ëŠ¥ì„±(ì‹¤ì „) âœ… ê°€ëŠ¥ âŒ ì¶”ê°€ ì¥ì¹˜ í•„ìš” \"h(x, 0)=0ë§Œ ê°•ì œí•˜ë©´ ë˜ì§€ ì•Šë‚˜?\" â€“ ì™œ ì‹¤ì „ì—ì„œëŠ” ì•ˆ í†µí• ê¹Œ? --- ìˆ˜í•™ì ìœ¼ë¡œëŠ” $h(x,0)=0$ì„ ì •í™•íˆ ë¶€ê³¼í•˜ë©´ $s(x)=âˆ’\\tau(x,0)=0$ì´ ë˜ì–´ í•´ê°€ ìœ ì¼í•©ë‹ˆë‹¤. ë¬¸ì œëŠ” ëª¨ë¸ í•™ìŠµ ë‹¨ê³„ì—ì„œ ì´ ë¬´í•œì°¨ì› ì œì•½ì„ ê·¸ëŒ€ë¡œ êµ¬í˜„í•˜ê¸° ì–´ë µë‹¤ëŠ” ì ì— ìˆìŠµë‹ˆë‹¤. ì‹¤ë¬´ì—ì„œëŠ” $h$ë¥¼ ì‹ ê²½ë§Â·íŠ¸ë¦¬Â·ì»¤ë„ ë“± ìœ ì—°í•œ í•¨ìˆ˜ ê·¼ì‚¬ê¸°ë¡œ íŒŒë¼ë¯¸í„°í™”í•˜ê³ , ì†ì‹¤ì„ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•ìœ¼ë¡œ ìµœì†Œí™”í•©ë‹ˆë‹¤. ì´ë•Œ â€œ$t=0$ì—ì„œ ë°˜ë“œì‹œ 0â€ì´ë¼ëŠ” ì œì•½ì„ ë„¤íŠ¸ì›Œí¬ ì¶œë ¥ ì „ì²´ì— ê°•ì œí•˜ê¸°ê°€ ì‰½ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ í•™ìŠµ ê³¼ì •ì—ì„œëŠ” $\\hat{h}(x,t) = \\tau(x,t) + s(x)$ ê¼´ì˜ í•¨ìˆ˜ë„ ì†ì‹¤ì„ ë™ì¼í•˜ê²Œ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ìœ : R-loss ì•ˆì˜ $E[h(X,T)\\mid X]$ í•­ì—ì„œ $s(x)$ê°€ ìƒì‡„ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì¦‰, zero conditionì„ ì½”ë“œì— ëª…ì‹œí•˜ì§€ ì•Šìœ¼ë©´ ì—¬ì „íˆ ì‹ë³„ ì‹¤íŒ¨(ill-posed)ê°€ ë°œìƒí•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì €ìë“¤ì€ ë‹¤ìŒê³¼ ê°™ì€ ë‘ ë‹¨ê³„ ì ˆì°¨ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. 1ë‹¨ê³„: ì œì•½ ì—†ì´ $\\tilde{h}$ë¥¼ í•™ìŠµ 2ë‹¨ê³„: íŠ¹ìˆ˜í•œ ì‚¬ìƒ(\"zero-constraining operator\")ìœ¼ë¡œ $\\tilde{h}$ë¥¼ $t=0$ì—ì„œ 0ì´ ë˜ë„ë¡ ë³€í™˜ ì¶”ê°€ë¡œ Tikhonov ì •ê·œí™”ë¥¼ ë„£ì–´ ìˆ˜ì¹˜ì  ì•ˆì •ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤. ì™œ theorem 1ì—ì„œ ì¤‘ê°„í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ”ê±°ì§€? ì™œ êµ³ì´ â€˜ì¤‘ê°„ í•¨ìˆ˜(tilde tau)â€™ë¥¼ í•œ ë²ˆ ë” ì •ì˜í•˜ëŠëƒë¥¼ í•œë§ˆë””ë¡œ ìš”ì•½í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì•„ìš”. ë¹„ì‹ë³„(non-identification)ì˜ ì›ì¸ R-lossë¥¼ ê·¸ëŒ€ë¡œ ìµœì†Œí™”í•˜ë©´ $$h(x,t) = Ï„(x,t) + s(x)$$ ì²˜ëŸ¼ â€œx ì—ë§Œ ì˜ì¡´í•˜ëŠ” í•¨ìˆ˜â€ê°€ ì–¼ë§ˆë“ ì§€ ë§ë¶™ì–´ë„ ì†ì‹¤ê°’ì´ ë˜‘ê°™ìŠµë‹ˆë‹¤. ì´ s(x) ë•Œë¬¸ì— ìµœì†Œê°’ì´ ë¬´í•œíˆ ë§ì•„ì ¸ì„œ í•´ê°€ ìœ ì¼í•˜ì§€ ì•Šì•„ìš”. ì•„ì´ë””ì–´: â€˜x í‰ê· ì„ 0ìœ¼ë¡œ ë§Œë“œëŠ”â€™ í•¨ìˆ˜ë¡œ ì¤‘ì‹¬í™”(center)í•˜ê¸° s(x)ëŠ” t ì™€ ë¬´ê´€í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ Ï„ ì—ì„œ â€œx ì— ë”°ë¥¸ í‰ê· ì¹˜â€ë§Œ ì™ ë¹¼ë²„ë¦¬ë©´ s(x)ê°€ ë” ì´ìƒ ë¶™ì„ ìˆ˜ ì—†ê² ì£ ? ê·¸ê²Œ ë°”ë¡œ $$\\tilde{\\tau}(x,t) = \\tau(x,t) âˆ’ E[ \\tau(X,T) X=x ]$$ ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ tilde tau ëŠ” ì–´ë–¤ x ì— ëŒ€í•´ì„œë„ t ê°’ ì „ì²´ë¥¼ í‰ê·  ë‚´ë©´ 0 ì´ ë©ë‹ˆë‹¤. ì¦‰ â€œë§ë¶™ì¼ ìˆ˜ ìˆëŠ” s(x)â€ ì—¬ì§€ê°€ ì‚¬ë¼ì§‘ë‹ˆë‹¤. Why Do We Introduce the Centered Function tilde_tau(x,t)? ì •ê·œí™”ì™€ì˜ ì—°ê²° L2 ì •ê·œí™”(Tikhonov)ë¡œ $$argmin_h { R-loss(h) + \\rhoâ€–hâ€–_2^2 }$$ ë¥¼ í’€ë©´ ìœ ì¼í•œ í•´ $\\tau_\\rho$ ê°€ ë‚˜ì˜µë‹ˆë‹¤. Theorem 1 ì€ â€œê·¸ ìœ ì¼í•œ í•´ê°€ ë°”ë¡œ tilde tau ë¥¼ (1+Ï) ë¡œ ë‚˜ëˆˆ ê²ƒâ€ì„ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. ì¦‰ $\\tilde{\\tau} = (1+\\rho)\\tau_\\rho$. ê²°êµ­ 1ë‹¨ê³„ì—ì„œ ì •ê·œí™”ë¡œ ìœ ì¼í•˜ê²Œ ì°¾ì€ Ï„_Ï ë¡œë¶€í„° 2ë‹¨ê³„ì—ì„œ ë‹¤ì‹œ (1+Ï)ì„ ê³±í•´ tilde tau ë¥¼ ë³µì›í•˜ê³ , ë§ˆì§€ë§‰ìœ¼ë¡œ zero-constraining operator ë¡œ ì›ë˜ Ï„ ë¥¼ ì–»ìŠµë‹ˆë‹¤. í•œ ì¤„ ê²°ë¡  ì¤‘ê°„ í•¨ìˆ˜ tilde tau ë¥¼ ì“°ëŠ” ì´ìœ ëŠ” â€œx ì—ë§Œ ì˜ì¡´í•˜ëŠ” ë¶ˆí•„ìš”í•œ s(x)ë¥¼ ì œê±°í•´ ì‹ë³„ ë¬¸ì œë¥¼ ê¹¨ë—í•˜ê²Œ ë§Œë“¤ì–´ ì£¼ê¸°â€ì…ë‹ˆë‹¤. ê·¸ë ‡ê²Œ ì¤‘ì‹¬í™”í•œ ë’¤ ì •ê·œí™”ë¥¼ ê±¸ì–´ì•¼ ìœ ì¼í•œ í•´ë¥¼ ì•ˆì „í•˜ê²Œ ì°¾ì„ ìˆ˜ ìˆì–´ìš”. ì¶”ì •ê°’ì´ ì‘ì•„ì§€ëŠ” ê±¸ ë³´ì™„í•˜ê¸° ìœ„í•´ (1+Ï)ì„ ê³±í•˜ëŠ” ì´ìœ  ì•„ë˜ ìˆœì„œëŒ€ë¡œ â€˜ì™œ (1 + Ï)ë¥¼ ê³±í•´ì•¼ í•˜ë‚˜?â€™ë¥¼ ì²˜ìŒë¶€í„° ë‹¤ì‹œ í’€ì–´ë³¼ê²Œìš”. (ì—¬ê¸°ì„œ ÏëŠ” ì •ê·œí™” ì„¸ê¸°, pë¼ê³  ì“°ì˜€ë˜ ê¸°í˜¸ì…ë‹ˆë‹¤.) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì•„ì£¼ ë‹¨ìˆœí•œ 1-ì°¨ì› ì˜ˆë¡œ ì‹œì‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ ëª©í‘œ : ì–´ë–¤ â€œì§„ì§œ ê°’â€ Î¸ ë¥¼ ì•Œì•„ë‚´ê³  ì‹¶ë‹¤. â€¢ í•˜ì§€ë§Œ ë¬¸ì œ(ë°ì´í„°)ê°€ ë¶ˆì•ˆì •í•´ì„œ ì •ê·œí™”ë¥¼ ë„£ì–´ í’€ê¸°ë¡œ í–ˆë‹¤. ì •ê·œí™”ëœ ìµœì†Œí™” ë¬¸ì œ (í‰ê· ì œê³±ì˜¤ì°¨ + Ï Ã— ê°’^2) â†“ F(h) = (h âˆ’ Î¸)Â² / 2 + Ï hÂ² â†’ ì—¬ê¸°ì„œ hê°€ ìš°ë¦¬ê°€ êµ¬í•  ì¶”ì •ì¹˜. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì´ ë¬¸ì œë¥¼ ì§ì ‘ í’€ì–´ë³´ì â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ F(h)ë¥¼ hì— ëŒ€í•´ ë¯¸ë¶„í•´ì„œ 0ìœ¼ë¡œ ë‘ë©´ (h âˆ’ Î¸) + 2Ïh = 0 â‡’ h (1 + 2Ï) = Î¸ â‡’ h = Î¸ / (1 + 2Ï) â€» ë…¼ë¬¸Â·í¬ìŠ¤íŠ¸ì—ì„œëŠ” 2 ëŒ€ì‹  1ì´ ë¶™ë„ë¡ (1/2)ê³„ìˆ˜ ë“±ìœ¼ë¡œ ë§ì¶° ë†“ì•˜ê¸° ë•Œë¬¸ì— ìµœì¢…ê³„ìˆ˜ëŠ” â€œ1 + Ïâ€ê°€ ë©ë‹ˆë‹¤. ì´ë¦„ë§Œ ë‹¤ë¥´ê³  ë³¸ì§ˆì€ ê°™ì•„ìš”. ê²°êµ­ ì •ê·œí™”ë¥¼ ë„£ê³  ìµœì†Œí™”í•˜ë©´ h = Î¸ / (1 + Ï) ì¦‰, ì§„ì§œ ê°’ì´ (1 + Ï)ë°°ë§Œí¼ ì‘ì•„ì ¸ ë²„ë¦° ê²°ê³¼ë¥¼ ì–»ëŠ”ë‹¤. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì™œ ì‘ì•„ì§€ë‚˜? ì§ê´€ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ ì •ê·œí™” í•­ Ï hÂ² ëŠ” â€œh ê°’ì„ 0ì— ê°€ê¹ê²Œ ëŒì–´ë‹¹ê¸°ëŠ” ìŠ¤í”„ë§â€ ê°™ì€ ì—­í• . â€¢ ì†ì‹¤ì‹ì€ â‘  (hâˆ’Î¸)Â² â†’ â€œì§„ì§œ ê°’ Î¸ì™€ ê°€ê¹Œì›Œì ¸ë¼â€ â‘¡ Ï hÂ² â†’ â€œê°’ì´ í´ìˆ˜ë¡ í˜ë„í‹°, 0ì´ ì¢‹ë‹¤â€ ë‘ í˜ì˜ íƒ€í˜‘ì ì´ Î¸ì˜ ì¶•ì†ŒíŒ h = Î¸/(1+Ï) ë¡œ ë‚˜íƒ€ë‚œ ê²ƒ. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ì¶•ì†Œ(shrinkage) í¸í–¥ì„ ì–´ë–»ê²Œ ì—†ì•¨ê¹Œ? â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ë°©ë²•ì€ ê°„ë‹¨í•˜ë‹¤. ì •ê·œí™”ë¥¼ ë„£ì–´ì„œ ì–»ì€ ê²°ê³¼ hë¥¼ ë‹¤ì‹œ (1 + Ï)ë°° í‚¤ì›Œ ì£¼ë©´ ëœë‹¤. Î¸ (ì›ë˜ ê°’) = (1 + Ï) Ã— h (ì •ê·œí™” í•´) ê·¸ë˜ì„œ ì •ê·œí™” ë’¤ ë³´ì • ë‹¨ê³„ì—ì„œ (1 + Ï)ë¥¼ ê³±í•œë‹¤. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í•¨ìˆ˜(tilde Ï„, Ï„Ï) ë¡œ ëŒì•„ì™€ì„œ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ í•¨ìˆ˜ ì „ì²´ê°€ ìŠ¤ì¹¼ë¼ Î¸ ëŒ€ì‹  â€œtilde_tau(x,t)â€ ë¼ê³  ìƒê°í•˜ë©´ ìœ„ ê³„ì‚°ì´ ì ë§ˆë‹¤ ë™ì‹œì— ì¼ì–´ë‚œë‹¤. â€¢ ì •ê·œí™”ê°€ ëë‚˜ê³  ì–»ì€ í•¨ìˆ˜ Ï„Ï(x,t)ëŠ” tilde_tau(x,t) / (1 + Ï). â€¢ ê·¸ëŸ¬ë¯€ë¡œ ì›í•˜ëŠ” tilde_tauë¥¼ ë˜ì°¾ìœ¼ë ¤ë©´ tilde_tau = (1 + Ï) Â· Ï„Ï. â† ë°”ë¡œ ì´ ì‹ì´ì—ˆì£ . â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€˜ê¼­ ê³±í•´ì•¼ í•˜ë‚˜?â€™ì— ëŒ€í•œ ê²°ë¡  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â€¢ ì ˆëŒ€ í¬ê¸°ê°€ ì¤‘ìš”í•œ ë¬¸ì œ(ì˜ˆ: ì˜ì•½ ìš©ëŸ‰, ê°€ê²© ì±…ì •)ë¼ë©´ ì¶•ì†Œëœ ê°’ì„ ê·¸ëŒ€ë¡œ ì“°ë©´ íš¨ê³¼ë¥¼ ê³¼ì†Œí‰ê°€ â†’ ì˜ëª»ëœ ì˜ì‚¬ê²°ì •. â€¢ Ïê°€ ì•„ì£¼ ì‘ì•„ì„œ (1+Ï) â‰ˆ 1 ë¼ë©´ ìƒëµí•´ë„ í° ì˜í–¥ì€ ì—†ìŒ. â€¢ ê·¸ëŸ¬ë‚˜ ì¼ë°˜ì ìœ¼ë¡œëŠ” ì •ê·œí™” â†’ ì¶•ì†Œ â†’ (1+Ï) ë°° ë˜ëŒë¦¬ê¸° ìˆœì„œë¥¼ ì§€ì¼œ ì£¼ëŠ” ê²ƒì´ í¸í–¥ ì—†ëŠ” ì¶”ì •ì„ ë³´ì¥í•œë‹¤. ì¦‰, (1 + Ï)ë¥¼ ê³±í•´ ì£¼ëŠ” ì´ìœ ëŠ” â€œì •ê·œí™”ê°€ ë§Œë“  â€˜ì¶•ì†Œ í¸í–¥â€™ì„ ì •í™•íˆ ë°˜ëŒ€ë¡œ ìƒì‡„í•´ ì›ë˜ ìŠ¤ì¼€ì¼(tilde_tau), ë” ë‚˜ì•„ê°€ ìµœì¢… Ï„(x,t)ì„ ë˜ì°¾ê¸° ìœ„í•´ì„œâ€ì…ë‹ˆë‹¤."
  },
  {
    "id": "Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ë§Œì¡±ìŠ¤ëŸ½ê²Œë§Œë“¤ì–´ë¼",
    "title": "ë§Œì¡±ìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ë¼",
    "date": "2025-07-11",
    "excerpt": "ì§€ì†í•˜ê¸° ìœ„í•´ì„œëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì´, ê·¸ë§Œë‘ê¸° ìœ„í•´ì„œëŠ” ì¦‰ê°ì ì¸ í˜ë„í‹°ê°€ í•„ìš”í•˜ë‹¤",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "ì–´ë–¤ ì¼ì„ ì§€ì†í•˜ê¸° ìœ„í•´ ì¤‘ìš”í•œ ê±´ ë­˜ê¹Œ? ë§ì€ ì‚¬ëŒë“¤ì´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ê¸° ìœ„í•´ì„œëŠ” 'ê¾¸ì¤€í•¨'ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ë§í•©ë‹ˆë‹¤. ì´ ê¾¸ì¤€í•¨ì€ ì–´ë–»ê²Œ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œìš”? ê°•ë ¥í•œ ì˜ì§€ì™€ ë‹¤ì§ì´ í•„ìš”í•œ ê±¸ê¹Œìš”? 'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì—ì„œëŠ” ì›í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ê¸° ìœ„í•œ êµ¬ì²´ì ì¸ ë°©ë²•ë“¤ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ ì¤‘ í•˜ë‚˜ëŠ” í•´ì•¼í•˜ëŠ” ì¼ì„ ì•„ì£¼ ì‘ê²Œ ìª¼ê°œë¼ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‰¬ì›Œì•¼ ë‹¬ë¼ì§„ë‹¤ ì´ì— ì´ì–´ì„œ ì €ìëŠ” ì–´ë–¤ ì¼ì„ ê¾¸ì¤€íˆ í•˜ê¸° ìœ„í•´ì„œëŠ” ë§Œì¡±ìŠ¤ëŸ½ê²Œ ë§Œë“¤ì–´ë¼ ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ ë‡Œë¥¼ ì´í•´í•´ë³´ì ìš°ë¦¬ì˜ ë‡ŒëŠ” ê³¼ê±°ì— ë§ì¶°ì ¸ ìˆìŠµë‹ˆë‹¤. ë‡Œê³¼í•™ì—ì„œ ë°í˜€ì§„ ë°”ì— ë”°ë¥´ë©´, ì¸ê°„ì˜ ë‡ŒëŠ” ìˆ˜ì‹­ë§Œ ë…„ ë™ì•ˆ ìˆ˜ë µì±„ì§‘ ì‹œëŒ€ì˜ í™˜ê²½ì— ìµœì í™”ë˜ì–´ ì§„í™”í–ˆìŠµë‹ˆë‹¤. ìš°ë¦¬ê°€ í˜„ëŒ€ ë¬¸ëª…ì— ì‚´ê¸° ì‹œì‘í•œ ê²ƒì€ ë¶ˆê³¼ ëª‡ ì²œ ë…„ì— ë¶ˆê³¼í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì§„í™”ì  ë°°ê²½ ë•Œë¬¸ì— ìš°ë¦¬ì˜ ë‡ŒëŠ” ì¥ê¸°ì ì¸ ëª©í‘œë³´ë‹¤ëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì— í›¨ì”¬ ë” ê°•í•˜ê²Œ ë°˜ì‘í•˜ë„ë¡ ì„¤ê³„ë˜ì–´ ìˆìŠµë‹ˆë‹¤. ì›ì‹œ ì‹œëŒ€ì—ëŠ” ì§€ê¸ˆ ë‹¹ì¥ ì–»ì„ ìˆ˜ ìˆëŠ” ìŒì‹, ì•ˆì „, ë²ˆì‹ ê¸°íšŒê°€ ìƒì¡´ì— ì§ê²°ë˜ì—ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ê°€ ì¥ê¸°ì ì¸ ëª©í‘œë¥¼ ë‹¬ì„±í•˜ë ¤ë©´ ì´ëŸ¬í•œ ë‡Œì˜ íŠ¹ì„±ì„ ì´í•´í•˜ê³  í™œìš©í•´ì•¼ í•©ë‹ˆë‹¤. > ìš°ë¦¬ì˜ ë‡ŒëŠ” ë‹¹ì¥ ì¦ê±°ìš´ë©´ í•˜ê³  ë‹¹ì¥ ê³ í†µìŠ¤ëŸ¬ìš°ë©´ ì•ˆí•œë‹¤. ìš°ë¦¬ì˜ ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆê¸° ë•Œë¬¸ì—, ì´ë¥¼ ì´í•´í•˜ê³  í™œìš©í•´ì„œ ê¾¸ì¤€íˆ í•  ìˆ˜ ìˆëŠ” í™˜ê²½ í˜¹ì€ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. > ìš°ë¦¬ëŠ” ë³¸ëŠ¥ì˜ ì”¨ì•—ë“¤ì— ë°˜ê¸°ë¥¼ ë“¤ì§€ ëª»í•˜ë©° ê·¸ê²ƒë“¤ê³¼ í•¨ê»˜ ë‚˜ì•„ê°€ì•¼ í•œë‹¤. ê°€ì¥ ìµœì„ ì˜ ë°©ë²•ì€ ì¥ê¸°ì ìœ¼ë¡œ ë³´ìƒì„ ì£¼ëŠ” ìŠµê´€ì—ëŠ” ì¦‰ì‹œì ì¸ ê¸°ì¨ì˜ ì¡°ê°ë“¤ì„ ë§ë¶™ì´ê³ , ê·¸ë ‡ì§€ ì•Šì€ ìŠµê´€ì—ëŠ” ê³ í†µì˜ ì¡°ê°ë“¤ì„ ë§ë¶™ì´ëŠ” ê²ƒì´ë‹¤. > > - ã€Šì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ã€‹ ì›í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ë ¤ë©´ ì¦ê²ê²Œ ë§Œë“¤ì > ìŠµê´€ì„ ê³„ì† ìœ ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œì„ í•„ìˆ˜ì ìœ¼ë¡œ ë°›ì•„ì•¼ í•œë‹¤. ë¹„ë¡ ì•„ì£¼ ì‚¬ì†Œí•œ ë°©ì‹ì¼ì§€ë¼ë„ ë§ì´ë‹¤. ì„±ê³µí–ˆë‹¤ê³  ëŠë¼ëŠ” ê²ƒì€ ìŠµê´€ì´ ì„±ê³¼ë¥¼ ëƒˆê³ , ê·¸ ì¼ì´ ë…¸ë ¥í•  ë§Œí•œ ê°€ì¹˜ê°€ ìˆë‹¤ëŠ” ì‹ í˜¸ì´ê¸° ë•Œë¬¸ì´ë‹¤. > > - ã€Šì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ã€‹ ìš°ë¦¬ëŠ” ì¦‰ê°ì ìœ¼ë¡œ ì¦ê±°ì›€ì„ ëŠë‚„ ìˆ˜ ìˆëŠ” ì¼ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì›í•˜ëŠ” ëª©í‘œë“¤ì€ ì§€ì—°ì ì¸ ë³´ìƒì„ ì œê³µí•˜ëŠ” ì¼ë“¤ì´ ë§ìŠµë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ ì›í•˜ëŠ” ëª©í‘œë¡œ ê°€ëŠ” ê¸¸ì„ ì•„ì£¼ ì˜ê²Œ ìª¼ê°œê³ , ê° ë‹¨ê³„ì—ì„œ ì¦‰ê°ì ì¸ ë³´ìƒì„ ë°›ë„ë¡ ì„¤ê³„í•´ì•¼í•©ë‹ˆë‹¤. ê°€ì¥ ì¢‹ì€ ê±´ \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì„ ë°›ë„ë¡ ì„¤ê³„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. > ì‹¤ì œë¡œ ì €ëŠ” ì´ ì „ëµìœ¼ë¡œ ìš´ë™í•˜ëŠ” ìŠµê´€ì„ ë§Œë“¤ì—ˆê³ , 6ë…„ì •ë„ ê¾¸ì¤€íˆ ì´ì–´ì„œ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ë§ì€ ì‚¬ëŒë“¤ì´ ìš´ë™ì„ ì‹œì‘í•  ë•Œ \"ì§„ì§œ ìš´ë™ ì—´ì‹¬íˆ í• ê±°ì•¼, ì§„ì§œ ë¹¡ì„¸ê²Œ í•´ì•¼ì§€\" ë“±ì˜ ë‹¤ì§ì„ í•˜ê³  ì‹œì‘í•˜ê²Œ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ë§‰ìƒ ìš´ë™ì„ í•˜ê²Œ ë˜ë©´ í˜ë“¤ê³  í”¼ê³¤í•˜ê¸° ë•Œë¬¸ì— ë¶€ì •ì ì¸ ê°ì •ì„ ê²½í—˜í•˜ê²Œ ë˜ê³  í–‰ë™ì„ ì´ì–´ê°€ì§€ ëª»í•©ë‹ˆë‹¤. ì œê°€ ìš´ë™ì„ ì‹œì‘í•  ë•Œì˜ ëª©í‘œëŠ” \"í—¬ìŠ¤ì¥ì— ë“¤ì–´ê°€ì„œ í‘¸ì‰¬ì—… 1ê°œë§Œ í•˜ê³  ì¹­ì°¬í•˜ê¸°\" ì˜€ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì•„ë¬´ ìƒê°ì—†ì´ í—¬ìŠ¤ì¥ì— ê°€ì„œ ì§„ì§œ í‘¸ì‰¬ì—… 1ê°œë§Œ í•˜ê³  \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì„ ë°›ìœ¼ë©° ìŠ¤ìŠ¤ë¡œë¥¼ ì¹­ì°¬í–ˆìŠµë‹ˆë‹¤. ì´ëŸ¬ë‹¤ë³´ë‹ˆ í—¬ìŠ¤ì¥ì— ê°€ëŠ” ê±´ ì €ì—ê²Œ ì„±ì·¨ê°ì„ ì£¼ëŠ” ì¬ë°ŒëŠ” ì¼ì´ ë˜ì—ˆê³ , ì ì  ìš•ì‹¬ì„ ëŠê»´ ê°•ë„ë¥¼ ëŠ˜ë ¤ë‚˜ê°”ìŠµë‹ˆëŒœ. í‘¸ì‰¬ì—…ì„ ê¸°ì¤€ìœ¼ë¡œ ë³´ë©´ ì²˜ìŒì—ëŠ” í•œë²ˆì— 10ê°œì •ë„ ë°–ì— ëª»í–ˆì§€ë§Œ ì§€ê¸ˆì€ 100ê°œì •ë„ëŠ” í•  ìˆ˜ ìˆëŠ”, 10ë°° ì´ìƒì˜ ê·¼ë ¥ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬í•˜ë©´ ì›í•˜ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ì„œëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì„ ë°›ë„ë¡ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ° í™˜ê²½, ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ì•¼í•©ë‹ˆë‹¤. ë§¤ì¼í•  ìˆ˜ ìˆëŠ” ì‘ì€ ëª©í‘œë“¤ë¡œ ìª¼ê°œê³ , ì´ë¥¼ í–ˆì„ ë•Œ \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì„ ë°›ë„ë¡ ì„¤ê³„ í•´ì•¼í•˜ê³ , ì´ë•Œ ì´ë¥¼ ì´ì–´ê°ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‚˜ìœ ìŠµê´€ì„ ë§Œë“¤ë ¤ë©´ í˜ë„í‹°ë¥¼ ì£¼ì ë°˜ëŒ€ë¡œ ë‚˜ìœ ìŠµê´€ì„ ì—†ì• ë ¤ë©´ ë³´ìƒì„ ì§€ì—°ì‹œí‚¤ê±°ë‚˜ íŒ¨ë„í‹°ë¥¼ ì£¼ë©´ ë©ë‹ˆë‹¤. í•˜ì§€ë§Œ ì œ ìƒê°ì— ì´ ë¶€ë¶„ì€ ì ìš©í•˜ê¸° ì–´ë ¤ì› ìŠµë‹ˆë‹¤. íœ´ëŒ€í°ì´ë‚˜ ë…¸íŠ¸ë¶ìœ¼ë¡œ ë„ˆë¬´ë‚˜ë„ ì‰½ê²Œ ì¦‰ê°ì ì¸ ë³´ìƒì„ ì–»ì„ ìˆ˜ ìˆëŠ” ì„¸ìƒì´ê³ , ì´ë¥¼ ì§€ì—°ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì•„ì§ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì œê°€ ë‚˜ë¦„ëŒ€ë¡œ ì ìš©í•˜ê³  ìˆëŠ” ë¶€ë¶„ì€ \"íšŒì˜ì ì¸ ìƒê°\"ì„ ì ìš©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. \"ìœ íŠœë¸Œ ë´ì„œ ë­í•´? ì¬ë¯¸ë„ ì—†ëŠ”ê±°\" ì´ëŸ°ì‹ìœ¼ë¡œ í”¼í•´ì•¼í•˜ëŠ” í–‰ë™ë“¤ì— ëŒ€í•´ì„œëŠ” í•˜ê³  ì‹¶ì§€ë§Œ ì°¸ëŠ”ê²Œ ì•„ë‹Œ í•˜ëŠ”ê²Œ ì˜ë¯¸ê°€ ì—†ë‹¤ê³  ìƒê°í•˜ë ¤ê³  í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì‹¤ì œë¡œ ì´ëŸ° ìƒê°ì„ ê°€ì§€ê³  ë‚˜ë‹ˆ, í•´ë´¤ì ì˜ë¯¸ì—†ëŠ” í–‰ë™ë“¤ì€ í•˜ì§€ ì•Šê²Œ ë˜ì—ˆê³ , ì‹¤ì§ˆì ìœ¼ë¡œ ë‚˜ì—ê²Œ ì„±ì·¨ê°ì„ ì£¼ëŠ” í–‰ë™ë“¤ì— ì§‘ì¤‘í•  ìˆ˜ ìˆê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬ ìš°ë¦¬ì˜ ë‡ŒëŠ” ì¦‰ê°ì ì¸ ë³´ìƒì— ì§‘ì¤‘í•œë‹¤. ë¯¸ë˜ì˜ ë³´ìƒì—ëŠ” ì§‘ì¤‘í•˜ì§€ ëª»í•œë‹¤. ì´ë¥¼ í™œìš©í•´ ì „ëµì„ ì„¸ìš°ê³  í™˜ê²½ì„ ë§Œë“¤ì. ì˜ì§€ë ¥ì˜ ë¬¸ì œê°€ ì•„ë‹ˆë‹¤! ì‘ì€ ëª©í‘œë“¤ë¡œ ìª¼ê°œê³  ì´ë¥¼ í•´ë‚˜ê°€ë©° \"ì„±ê³µí–ˆë‹¤ëŠ” ëŠë‚Œ\"ì— ì§‘ì¤‘í•˜ì í”¼í•´ì•¼ í•˜ëŠ” ì¼ë“¤ì—ëŠ” íšŒì˜ì ì¸ ìƒê°ì„ ì ìš©í•´ ë¶ˆí¸í•œ ëŠë‚Œì„ ê°€ì§€ë„ë¡ í•˜ì"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-10-ê²©ë ¤ì™€ì‹ ë¢°í•œë‹¤ëŠ”í‘œí˜„",
    "title": "ê²©ë ¤ì™€ ì‹ ë¢°í•œë‹¤ëŠ” í‘œí˜„",
    "date": "2025-07-10",
    "excerpt": "",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ ê²ƒë“¤ ì •ë¦¬ I believe in you. I believe in your ability to come out ahead. I believe in your ability to stay focused. I believe in your ability to persevere. You've made it this far. You're almost there. ê°„ë‹¨í•œ ì‘ë¬¸ As I did yesterday (ì–´ì œì²˜ëŸ¼), I wanna talk to myself. Because I need some ê²©ë ¤. I believe in you MK. I believe in your ability to come out ahead. ì§€ê¸ˆì€ í˜ë“¤ë”ë¼ë„, you will made it. Youve made it this far. Just focus on today, one step ahead. Youre almost there. í”¼ë“œë°± ì˜í•œ ì ë“¤ âœ¨ í•™ìŠµí•œ í‘œí˜„ í™œìš©: ì˜¤ëŠ˜ ë°°ìš´ \"I believe in you\" ì‹œë¦¬ì¦ˆë¥¼ ì‹¤ì œ ì‘ë¬¸ì— ë°”ë¡œ ì ìš©í•œ ì ì´ í›Œë¥­í•©ë‹ˆë‹¤ ê°ì • í‘œí˜„: ìì‹ ì—ê²Œ ê²©ë ¤ê°€ í•„ìš”í•˜ë‹¤ëŠ” ì†”ì§í•œ ë§ˆìŒì„ ì˜ì–´ë¡œ í‘œí˜„í•œ ê²ƒì´ ìì—°ìŠ¤ëŸ½ìŠµë‹ˆë‹¤ ê°œì¸ì  ë©”ì‹œì§€: MKë¼ëŠ” ì´ë¦„ì„ ë„£ì–´ì„œ ë” ê°œì¸ì ì´ê³  ì§„ì‹¬ì–´ë¦° ëŠë‚Œì„ ë§Œë“¤ì–´ëƒˆìŠµë‹ˆë‹¤ ê°œì„ í•  ë¶€ë¶„ ğŸ”§ ë¬¸ë²• ì˜¤ë¥˜: you will made it â†’ you will make it (ë™ì‚¬ì›í˜• ì‚¬ìš©) Youve â†’ You've (ì˜¬ë°”ë¥¸ ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ ì‚¬ìš©) ì–´ì¡° í†µì¼: wanna â†’ want to (ì¢€ ë” ê²©ì‹ìˆëŠ” í‘œí˜„) í•œì˜ í˜¼ìš©ì„ ì¢€ ë” ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ ê°œì„ : one step ahead â†’ one step at a time` (ë” ì¼ë°˜ì ì¸ í‘œí˜„) ìˆ˜ì • ì œì•ˆ ğŸ“ ì›ë¬¸ ìœ ì§€í•˜ë˜ ì´ë ‡ê²Œ ì“¸ ìˆ˜ë„ ìˆì–´ìš”: As I did yesterday, I want to talk to myself in English. Because I need some encouragement right now. I believe in you, MK. I believe in your ability to come out ahead. Even though it's tough right now, you will make it through. You've made it this far, and that's already incredible. Just focus on today, one step at a time. You're almost there. ì´í‰ ğŸ’­ ì˜ì–´ë¡œ ìì‹ ì—ê²Œ ê²©ë ¤í•˜ëŠ” ìŠµê´€ì´ ì •ë§ ì¢‹ìŠµë‹ˆë‹¤! ë°°ìš´ í‘œí˜„ì„ ë°”ë¡œ ì‚¬ìš©í•˜ë ¤ëŠ” ì ê·¹ì„±ì´ ì¸ìƒì ì´ì—ìš”. ë¬¸ë²•ì ì¸ ë¶€ë¶„ë§Œ ì¡°ê¸ˆì”© ë‹¤ë“¬ì–´ê°€ë©´ ë”ìš± ìì—°ìŠ¤ëŸ¬ìš´ ì˜ì–´ í‘œí˜„ë ¥ì„ ê¸°ë¥¼ ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê³„ì† ì´ëŸ° ì‹ìœ¼ë¡œ ì—°ìŠµí•˜ì„¸ìš”! ğŸŒŸ"
  },
  {
    "id": "Causal Inference/Paper Review/review-HETEROGENEOUS-TREATMENT-EFFECTS-ESTIMATION-WHEN-MACHINE-LEARNING-MEETS-MULTIPLE-TREATMENT-REGIME",
    "title": "[Paper Review] Heterogeneous Treatment Effects Estimation: When Machine Learning Meets Multiple Treatment Regime",
    "date": "2025-07-10",
    "excerpt": "HETEROGENEOUS TREATMENT EFFECTS ESTIMATION: WHEN MACHINE LEARNING MEETS MULTIPLE TREATMENT REGIME",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link"
  },
  {
    "id": "Causal Inference/Paper Review/review-Causal-Effect-Inference-for-Structured-Treatments",
    "title": "[Paper Review] Causal Effect Inference for Structured Treatments",
    "date": "2025-07-10",
    "excerpt": "Causal Effect Inference for Structured Treatments",
    "category": "Causal Inference",
    "tags": [
      "Paper Review"
    ],
    "content": "paper link"
  },
  {
    "id": "Career/Job Transition/25-07-10-lunit-cancer-screening",
    "title": "(Seoul) Research Engineer Â· AutoML ",
    "date": "2025-07-10",
    "excerpt": "ë£¨ë‹› ì±„ìš©ê³µê³  ë¶„ì„ê³¼ ì¤€ë¹„ê³¼ì •",
    "category": "Career",
    "tags": [
      "ë£¨ë‹› ì±„ìš©ê³µê³ "
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´</strong></summary> ë§í¬ Lunit, a portmanteau of â€˜Learning unit,' is a medical AI software company devoted to providing AI-powered total cancer care. Our AI solutions help discover cancer and predict cancer treatment outcomes, achieving timely and individually tailored cancer treatment. ğŸ—¨ï¸ About The Team AutoML team at Lunit automates AI product development processes to streamline time-consuming tasks and advance cutting-edge AutoML research. By combining engineering expertise with state-of-the-art deep learning techniques, our team plays an important role in accelerating product development for cancer detection and treatment. Join us in our mission to Conquer Cancer Through AI where your contributions will directly influence AI innovations that improve patient outcomes worldwide In short, we're a team that maximizes AI tech to create AI models with extreme efficiency ğŸ—¨ï¸ About The Position As a research engineer, you will work closely with the team to develop and optimize platforms for automating medical AI model research workflows Your responsibilities will involve implementing an automated product development framework for the medical domain, covering data ingestion, model training, packaging and inference You will collaborate with both research and engineering teams to bring cutting-edge AI research into real-world applications This position is ideal for someone with a strong research engineering background and deep understanding of the AI model development process ğŸš© Roles & Responsibilities Develop and implement tailored AutoML platforms to automate medical AI model research workflows Integrate AutoML pipelines into AI agent which includes data ingestion, model training, hyperparameter optimization and inference Enhance the maintainability, reliability, and efficiency of both new and existing frameworks Build robust AI infrastructure using Kubernetes, Docker, and cloud computing services (GCP, AWS, Azure) Keep up with the latest advancements in AI agents, LLMs, and AutoML research to drive innovation ğŸš© Tools Used Development Related: Django, Django REST Framework, React, Next.js, MySQL, PostgreSQL, Redis, Celery, Nginx, Go ML framework: PyTorch, Optuna Infrastructure: Google Cloud Platform, Kubernetes, Git, Docker, Helm, ArgoCD, Terraform General: Slack, Confluence, Jira Requirements ğŸ¯Qualifications 3+ years of experience in research engineering in the AI industry Master's, or Ph.D. in Computer Science or a related field Proficiency in Python, unit/integration testing, documentation, Git, collaborative code development, and Docker Solid knowledge of software design and system architecture, with a strong understanding of RESTful API design Hands-on experience with developing and operating scalable AI platforms and infrastructure, including DevOps Effective communication skills: Ability to clearly and efficiently convey information, thoughts, and ideas to other developers Proven ability to take ownership and drive projects from concept to deployment Highly responsible and detail-oriented, with a strong motivation to build high-quality, reliable solutions in line with current best practices ğŸ… Preferred Experiences Familiarity with web development, including both backend and frontend Deep insight into automating and orchestrating ML workflows Experience in designing or developing automated AI platforms Experience in designing or developing AI agents or LLM-based automation (e.g., fine-tuning, prompt engineering) Knowledge of cloud computing services (GCP, AWS, or Azure) Publications in AI or computer vision conferences, or active participation in the research community Ability to collaborate effectively as a team player in a cross-functional research-engineering environment Proactive in sharing knowledge, initiating collaborations, and promoting a positive research environment Passion for high-quality programming and software engineering to produce and maintain reliable code for the training and evaluation of models ğŸ“ How To Apply CV (resume, free format) ğŸƒâ€â™€ï¸ Hiring Process Document Screening â†’ Technical Interview(Teams) â†’ Assignment â†’ PT Interview â†’ Culture-fit Interview â†’ Onboarding After the final interview, we may proceed with reference checks if needed. ğŸ¤ Work Conditions and Environment Work type: Full Time Work location : Lunit HQ (5F, 374, Gangnam-daero, Gangnam-gu, Seoul) Salary: After Negotiation ğŸ¸ ETC If you misrepresent your experience or education or provide false or fraudulent information in or with your application, it may be grounds for cancellation of the employment Lunit is committed in providing the preferential processing to those eligible for employment protection (national merits and people with disabilities) relevant to related laws and regulations Benefits ğŸŒ» Benefits & Perks The office is at a very convenient location, just a minute away from Gangnam Station Exit 3 Meal Allowance is provided (up to 12,000 KRW per meal) when working at the office Up to 300,000 KRW is covered upon joining to decorate your personal workspace Latest computer models, such as Macs and 4K monitors are provided and can be renewed every three years Seminar registration fees and book purchases are covered Regular in-house AI and medical seminars are held Korean lessons are provided for Lunitians who do not speak Korean as their first language In-house English lessons (aka Luniversal) is provided for English development Access to high-quality AI learning resources & deep learning DevOps system Up to 1.2 million KRW worth of benefits points can be claimed annually Holiday Allowances are provided in the form of gifts or vouchers for Korean National holidays, Seollal and Chuseok Congratulatory and Condolence allowances, along with paid time off are provided Annual medical checkups and employee accident insurance are provided Expenses for monthly employees gatherings are partially covered </details> ì¤€ë¹„í•´ì•¼í•˜ëŠ” ê²ƒë“¤ ì¼ë‹¨ ë‚´ ë¶„ì•¼ì™€ ê²¹ì¹˜ëŠ” ì˜ë£Œìª½ ì±„ìš©ê³µê³ ê³ , ì—¬ê¸°ì„œ í–ˆë˜ MLê°œë°œë“¤ì— ëŒ€í•œ ê²½í—˜ì„ ì‚´ë¦´ ìˆ˜ ìˆê² ë‹¤. ì´ê²ƒë„ ì´ë²ˆì£¼ ì£¼ë§ì— ì´ê±°ì— ë§ê²Œ ì´ë ¥ì„œ ì“°ê¸°. ì—¬ê¸°ì— ë„ˆê°€ ìƒê°í•˜ëŠ” ì¤€ë¹„í•˜ë©´ ì¢‹ì€ ê²ƒë“¤ ì¶”ê°€í•´ì¤˜ ğŸ¯ í•µì‹¬ ì¤€ë¹„ì‚¬í•­ AutoML ì „ë¬¸ì„± ê°•í™” Optuna ì‹¤ìŠµ: í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” í”„ë ˆì„ì›Œí¬ ê²½í—˜ ìŒ“ê¸° AutoML íŒŒì´í”„ë¼ì¸: MLflow, DVC ë“±ì„ í™œìš©í•œ ML ì›Œí¬í”Œë¡œìš° ìë™í™” ê²½í—˜ Neural Architecture Search (NAS) ê´€ë ¨ ë…¼ë¬¸ ë° êµ¬í˜„ ê²½í—˜ Meta-learning ê¸°ì´ˆ ê°œë… í•™ìŠµ ì˜ë£Œ AI ë„ë©”ì¸ ì§€ì‹ ì˜ë£Œ ì˜ìƒ ë¶„ì„: DICOM í¬ë§·, ì˜ë£Œ ì˜ìƒ ì „ì²˜ë¦¬ ê¸°ë²• í•™ìŠµ Cancer detection papers: ë£¨ë‹›ì˜ ì£¼ìš” ë…¼ë¬¸ë“¤ ì½ì–´ë³´ê¸° (arXiv, ì˜ë£Œ AI ì»¨í¼ëŸ°ìŠ¤) FDA ìŠ¹ì¸ AI ì˜ë£Œê¸°ê¸°: ê·œì œ ìš”êµ¬ì‚¬í•­ ë° í’ˆì§ˆ ê´€ë¦¬ ê¸°ì¤€ ì´í•´ Medical imaging datasets: ChestX-ray, MIMIC ë“± ê³µê°œ ë°ì´í„°ì…‹ ê²½í—˜ ì¸í”„ë¼/DevOps ìŠ¤í‚¬ ì—…ê·¸ë ˆì´ë“œ Kubernetes ì‹¤ìŠµ: ë¡œì»¬ í´ëŸ¬ìŠ¤í„° êµ¬ì¶• ë° ML ì›Œí¬ë¡œë“œ ë°°í¬ Docker ê³ ê¸‰ í™œìš©: Multi-stage builds, ìµœì í™” ê¸°ë²• Cloud ML ì„œë¹„ìŠ¤: GCP AI Platform, Vertex AI ì‹¤ìŠµ CI/CD for ML: GitHub Actionsë¡œ ëª¨ë¸ í•™ìŠµ/ë°°í¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì›¹ ê°œë°œ ìŠ¤í‚¬ (Preferred ìš”êµ¬ì‚¬í•­) Django REST Framework: API ê°œë°œ ì‹¤ìŠµ React/Next.js ê¸°ì´ˆ: ê°„ë‹¨í•œ ML ëª¨ë¸ ë°ëª¨ í˜ì´ì§€ êµ¬ì¶• ë°ì´í„°ë² ì´ìŠ¤: PostgreSQL, Redis í™œìš© ê²½í—˜ í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ ì œì•ˆ ì˜ë£Œ ì˜ìƒ AutoML í”Œë«í¼: ê°„ë‹¨í•œ AutoML ì‹œìŠ¤í…œ êµ¬ì¶• MLOps íŒŒì´í”„ë¼ì¸: ëª¨ë¸ í•™ìŠµë¶€í„° ë°°í¬ê¹Œì§€ ìë™í™” API ì„œë²„: ML ëª¨ë¸ ì„œë¹™ REST API ê°œë°œ Kubernetes ë°°í¬: ìœ„ í”„ë¡œì íŠ¸ë“¤ì„ K8së¡œ ë°°í¬ ì´ë ¥ì„œ ì‘ì„± í¬ì¸íŠ¸ ì •ëŸ‰ì  ì„±ê³¼: ëª¨ë¸ ì„±ëŠ¥ ê°œì„ , ì²˜ë¦¬ ì‹œê°„ ë‹¨ì¶•, ì¸í”„ë¼ ë¹„ìš© ì ˆê° ë“± ìˆ˜ì¹˜ë¡œ í‘œí˜„ ì˜ë£Œ AI ê´€ë ¨ ê²½í—˜: ì§ì ‘ì ì´ì§€ ì•Šë”ë¼ë„ í—¬ìŠ¤ì¼€ì–´ ë°ì´í„° ë‹¤ë£¬ ê²½í—˜ ê°•ì¡° AutoML/MLOps í‚¤ì›Œë“œ: íŒŒì´í”„ë¼ì¸ ìë™í™”, ëª¨ë¸ ìµœì í™” ê²½í—˜ ë¶€ê° í˜‘ì—… ê²½í—˜: ì—°êµ¬íŒ€ê³¼ ê°œë°œíŒ€ ê°„ ë¸Œë¦¿ì§€ ì—­í•  ê²½í—˜ ë©´ì ‘ ì¤€ë¹„ ê¸°ìˆ  ë©´ì ‘: AutoML ê°œë…, PyTorch ì‹¬í™”, ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì œ ê³¼ì œ: ê°„ë‹¨í•œ AutoML íŒŒì´í”„ë¼ì¸ êµ¬í˜„ ë˜ëŠ” ì˜ë£Œ ì˜ìƒ ë¶„ì„ íƒœìŠ¤í¬ ì˜ˆìƒ PT ë©´ì ‘: ë³¸ì¸ì˜ í”„ë¡œì íŠ¸ë¥¼ AutoML ê´€ì ì—ì„œ ì„¤ëª…í•  ìˆ˜ ìˆë„ë¡ ì¤€ë¹„ ì»¬ì²˜í•: ì˜ë£Œ AIë¡œ í™˜ìì—ê²Œ ë„ì›€ì´ ë˜ê³  ì‹¶ë‹¤ëŠ” ë™ê¸°, ì—°êµ¬ì™€ ê°œë°œ ì‚¬ì´ì˜ ê· í˜•ê° ì¶”ê°€ í•™ìŠµ ìë£Œ ë…¼ë¬¸: \"Efficient Neural Architecture Search via Parameter Sharing\" (ENAS) ì±…: \"Hands-On Machine Learning\" (AutoML ì±•í„°) ê°•ì˜: CS330 (Stanford Meta-Learning), MLOps ê´€ë ¨ ì˜¨ë¼ì¸ ê°•ì˜ ë¸”ë¡œê·¸: Google AI Blogì˜ AutoML ê´€ë ¨ í¬ìŠ¤íŠ¸ë“¤ ë„¤íŠ¸ì›Œí‚¹ ì˜ë£Œ AI ì»¤ë®¤ë‹ˆí‹°: í•œêµ­ ì˜ë£Œ AI í•™íšŒ, ê´€ë ¨ ë°‹ì—… ì°¸ì—¬ ë£¨ë‹› ì§ì›: LinkedInìœ¼ë¡œ í˜„ì§ìì™€ ì»¤í”¼ì±— ìš”ì²­ ì»¨í¼ëŸ°ìŠ¤: MICCAI, SPIE Medical Imaging ë“± ì˜ë£Œ AI ì»¨í¼ëŸ°ìŠ¤ ë…¼ë¬¸ ì½ê¸° ğŸ“… ì£¼ì°¨ë³„ ì¤€ë¹„ ê³„íš (4ì£¼ ê°€ì •) 1ì£¼ì°¨: AutoML ê¸°ì´ˆ í•™ìŠµ + Optuna ì‹¤ìŠµ 2ì£¼ì°¨: ì˜ë£Œ AI ë…¼ë¬¸ ë¦¬ë·° + Docker/K8s ì‹¤ìŠµ 3ì£¼ì°¨: í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ êµ¬í˜„ 4ì£¼ì°¨: ì´ë ¥ì„œ ì™„ì„± + ë©´ì ‘ ì¤€ë¹„ ë‚˜ì˜ ê´€ë ¨ ì§ë¬´ê²½í—˜ê³¼ í•„ì‚´ê¸° ì¤€ë¹„í•˜ê¸°"
  },
  {
    "id": "Career/Job Transition/25-07-10-hyundai-autoever",
    "title": "í˜„ëŒ€ ì˜¤í† ì—ë²„ MLOps / AI ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´",
    "date": "2025-07-10",
    "excerpt": "í˜„ëŒ€ ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³  ë¶„ì„ê³¼ ì¤€ë¹„ê³¼ì •",
    "category": "Career",
    "tags": [
      "í˜„ëŒ€ ì˜¤í† ì—ë²„ ì±„ìš©ê³µê³ "
    ],
    "content": "ì±„ìš©ê³µê³  <details> <summary>ğŸ“‹ <strong>ì±„ìš©ê³µê³  ìƒì„¸ì •ë³´</strong></summary> [Tech] Machine Learning Engineer - MLOps / AI ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ ğŸ“ƒ ëˆ„êµ¬ë‚˜ â€‹ë§ˆìŒ â€‹ì†ì— â€‹ì´ë ¥ì„œ í•œ â€‹ì¥ì€ ìˆìœ¼ë‹ˆê¹Œ, âš¡ 1ë¶„ â€‹ì»· â€‹ì§€ì›ìœ¼ë¡œ í˜„ëŒ€ì˜¤í† ì—ë²„ë¡œì˜ â€‹ì—¬ì •ì„ ì‹œì‘í•˜ì„¸ìš”. (ì´ë ¥ì„œ â€‹ììœ¨ì–‘ì‹) âœ… MLOps â€‹/ â€‹AI ê²€ìƒ‰ ì—”ì§€ë‹ˆì–´ ğŸš€ â€‹í•©ë¥˜í•˜ì‹¤ â€‹íŒ€ì„ â€‹ì†Œê°œí•´ìš” ì–¸ì–´AIê¸°ìˆ íŒ€ ğŸ’» í•©ë¥˜í•˜ì‹œë©´ í•¨ê»˜ â€‹í•  â€‹ì—…ë¬´ì˜ˆìš” AI ëŒ€í™”í˜• ì„œë¹„ìŠ¤ë¥¼ â€‹ìœ„í•œ â€‹ë°ì´í„°ë² ì´ìŠ¤ â€‹ì„¤ê³„ ë° ê°œë°œ AI â€‹ê²€ìƒ‰ ì—”ì§„ â€‹ê°œë°œ â€‹ë° ìš´ì˜ MLOps â€‹êµ¬ì¶• ë° â€‹ìš´ì˜ ğŸ” â€‹ì´ëŸ° ë¶„ê³¼ í•¨ê»˜ â€‹í•˜ê³  ì‹¶ì–´ìš” ê²€ìƒ‰ â€‹/ ì±—ë´‡ ê´€ë ¨ ëª¨ë“ˆ ë° ì„œë¹„ìŠ¤ ê°œë°œ ê²½í—˜ Docker / Kubernetes í™œìš© ê°œë°œ ë° ë°°í¬ ê²½í—˜ Python / JAVA ê°œë°œ ê²½ë ¥ 3ë…„ ì´ìƒ í˜¹ì€ ê·¸ì— ì¤€í•˜ëŠ” ì‹¤ë ¥ ğŸ” ì´ëŸ° ë¶„ì´ë¼ë©´ ë”ìš± ì¢‹ì•„ìš” ì •ë³´ê²€ìƒ‰/SWê³µí•™ ê´€ë ¨ ì„ì‚¬ ì´ìƒ ELK(Elasticsearch, Kibana, Kubernetes, Kafka) êµ¬ì„± ì„¤ê³„ ë° ìš´ì˜ ê²½í—˜ì Neo4j / Redis ê¸°ë°˜ DB ì„¤ê³„ ë° êµ¬ì¶• ê²½í—˜ì ë²¡í„° DB (Milvus / qdrant / faiss ë“±) ê²½í—˜ì ES í”ŒëŸ¬ê·¸ì¸ ê°œë°œ ê²½í—˜ì âŒ› ì´ë ‡ê²Œ í•©ë¥˜í•´ìš” ì„œë¥˜ ì ‘ìˆ˜ â†’ ì„œë¥˜ ê²€í†  â†’ ì§ë¬´ì—­ëŸ‰í…ŒìŠ¤íŠ¸(ì½”ë”© ë˜ëŠ” ê³¼ì œí…ŒìŠ¤íŠ¸) ë° ì¸ì„±ê²€ì‚¬ â†’ 1ì°¨ë©´ì ‘ â†’ 2ì°¨ë©´ì ‘ â†’ ì²˜ìš°í˜‘ì˜ ë° ì±„ìš©ê²€ì§„ â†’ ìµœì¢… í•©ê²©ğŸ‰ ğŸ“ ë§Œë‚˜ê²Œ ë  ê·¼ë¬´ì§€ëŠ” ì—¬ê¸°ì˜ˆìš” ì„œìš¸ ê°•ë‚¨ ğŸ“Œ ì°¸ê³ í•´ ì£¼ì„¸ìš” ì±„ìš© ì‹œ ë§ˆê°ë˜ëŠ” ìƒì‹œ ì±„ìš© ê³µê³ ë¡œ ìš´ì˜ë˜ë©°, ì±„ìš© ì ˆì°¨ì™€ ì¼ì •ì€ ë³€ë™ë  ìˆ˜ ìˆì–´ìš”. ì‚¬íšŒì  ë°°ë ¤ ëŒ€ìƒì(ë³´í›ˆ ì·¨ì—…ì§€ì›ëŒ€ìƒì, ì¥ì• ì¸)ëŠ” ê´€ê³„ ë²•ë ¹ê³¼ ë‚´ê·œì— ë”°ë¼ ìš°ëŒ€í•´ìš”. ëª¨ì§‘ ë¶„ì•¼ ë° ë‹´ë‹¹ ì—…ë¬´ì— ë”°ë¼ ì˜ì–´ êµ¬ìˆ í‰ê°€, ë ˆí¼ëŸ°ìŠ¤ ì²´í¬, ë˜ëŠ” ê¸°íƒ€ ì „í˜•ì´ ì‹¤ì‹œë  ìˆ˜ ìˆì–´ìš”. ì§€ì›ìì˜ ê²½í—˜ê³¼ ì—­ëŸ‰ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ë¥¸ í¬ì§€ì…˜ì´ ë” ì í•©í•˜ë‹¤ê³  íŒë‹¨ë˜ëŠ” ê²½ìš° ì§€ì› ë¶„ì•¼ê°€ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ë°°ì¹˜ ë¶€ì„œ ë° ê·¼ë¬´ì§€ëŠ” íšŒì‚¬ ì‚¬ì •ì— ë”°ë¼ ë³€ê²½ë  ìˆ˜ ìˆì–´ìš”. ì•„ë˜ì˜ ê²½ìš°, í•©ê²©ì´ ì·¨ì†Œë˜ê±°ë‚˜ ì „í˜• ì§„í–‰ì—ì„œ ë¶ˆì´ìµì„ ë°›ìœ¼ì‹¤ ìˆ˜ ìˆì–´ìš”. ì§€ì›ì„œê°€ ì‚¬ì‹¤ê³¼ ë‹¤ë¥´ê±°ë‚˜ ì¦ë¹™ì´ ë¶ˆê°€í•  ê²½ìš° í•´ì™¸ì—¬í–‰ ê²°ê²© ì‚¬ìœ ê°€ ìˆëŠ” ê²½ìš° (ë‚¨ì„±ì˜ ê²½ìš°, íšŒì‚¬ê°€ ì§€ì •í•œ ì…ì‚¬ì¼ê¹Œì§€ ë³‘ì—­ í•„ ë˜ëŠ” ë©´ì œ í•„ìš”) ìµœì¢… í•©ê²© í›„ íšŒì‚¬ê°€ ì§€ì •í•˜ëŠ” ì…ì‚¬ì¼ì— ì…ì‚¬ ë¶ˆê°€í•œ ê²½ìš° </details> ì¤€ë¹„í•´ì•¼í•˜ëŠ” ê²ƒë“¤ ê¸°ë³¸ ê³„íš âœ… ì´ë ¥ì„œ & í¬íŠ¸í´ë¦¬ì˜¤ ì¤€ë¹„ ë‚´ê°€ í•œ ê²½í—˜ë“¤ ì¤‘ì—ì„œ ì±„ìš©ê³µê³ ì— í•„ìš”í•œ ê²½í—˜ ë½‘ì•„ì„œ ì •ë¦¬ ì´ë ¥ì„œë‘ í¬íŠ¸í´ë¦¬ì˜¤ ë§Œë“¤ê¸° ì½”ë”©í…ŒìŠ¤íŠ¸ ì¤€ë¹„ ê¸°ì¶œë¬¸ì œ ë½‘ì•„ì„œ ì •ë¦¬í•˜ê³  ëª¨ë¥´ëŠ” ê°œë… ì •ë¦¬í•˜ê¸° ë³´ì™„ ê³„íš ğŸ’¡ ê¸°ìˆ  ìŠ¤íƒ ì‹¤ë¬´ ì¤€ë¹„ MLOps ë„êµ¬ ì‹¤ìŠµ Docker/Kubernetes ì‹¤ìŠµ í”„ë¡œì íŠ¸ CI/CD íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê²½í—˜ ê²€ìƒ‰ ì—”ì§„ í•™ìŠµ Elasticsearch í•™ìŠµ ë° ì‹¤ìŠµ ê²€ìƒ‰ ì—”ì§„ í† ì´ í”„ë¡œì íŠ¸ êµ¬ì¶• ë²¡í„° DB ê²½í—˜ Milvus, qdrant ë“± ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í•™ìŠµ êµ¬ì²´ì  ì‹¤í–‰ ê³„íš í¬íŠ¸í´ë¦¬ì˜¤ í”„ë¡œì íŠ¸ ëª©ë¡ ì‘ì„± MLOps íŒŒì´í”„ë¼ì¸ êµ¬ì¶• í”„ë¡œì íŠ¸ AI ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„ í”„ë¡œì íŠ¸ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ê³„ ê²½í—˜ ì •ë¦¬ ì½”ë”©í…ŒìŠ¤íŠ¸ ì˜ì—­ ì„¸ë¶„í™” ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜ (ì´ì§„íƒìƒ‰, ë¬¸ìì—´ ì²˜ë¦¬) ML ê´€ë ¨ ì•Œê³ ë¦¬ì¦˜ ì‹œìŠ¤í…œ ì„¤ê³„ ë¬¸ì œ íƒ€ì„ë¼ì¸ ì„¤ì • 1ì£¼ì°¨: ì´ë ¥ì„œ/í¬íŠ¸í´ë¦¬ì˜¤ ì´ˆì•ˆ ì‘ì„± 2-3ì£¼ì°¨: ê¸°ìˆ  ìŠ¤íƒ ì‹¤ìŠµ í”„ë¡œì íŠ¸ 4ì£¼ì°¨: ì½”ë”©í…ŒìŠ¤íŠ¸ ì§‘ì¤‘ ì¤€ë¹„ ì‹¤ë¬´ í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ ë¯¸ë‹ˆ MLOps íŒŒì´í”„ë¼ì¸: ê°„ë‹¨í•œ ML ëª¨ë¸ ë°°í¬ ìë™í™” ê²€ìƒ‰ ì—”ì§„ í”„ë¡œì íŠ¸: ë¬¸ì„œ ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬í˜„ ë²¡í„° DB í™œìš©: ì„ë² ë”© ê¸°ë°˜ ìœ ì‚¬ë„ ê²€ìƒ‰ ì‹œìŠ¤í…œ"
  },
  {
    "id": "Career/Graduate School/ëŒ€í•™ì›ì—ëŒ€í•œê³ ë¯¼",
    "title": "ëŒ€í•™ì›ì„ ì§„í•™í•´ì•¼í• ê¹Œ? í•˜ë©´ ì–´ë””ë¡œ?",
    "date": "2025-07-10",
    "excerpt": "ë‚˜ì˜ ë‹¤ìŒ ì„ íƒì§€ëŠ” ì–´ë””ë¡œ?",
    "category": "Career",
    "tags": [
      "ëŒ€í•™ì›"
    ],
    "content": "ì»¤ë¦¬ì–´ ì„ ë°°í˜•ì—ê²Œ ë¬¼ì–´ë³¸ ë‚´ìš©ë“¤ ã…‡ã…‡ ë‚´ê°€ ìì£¼ë³´ëŠ” ê´œì°®ì€ dsê´€ë ¨ ê³µê³ ì‚¬ì´íŠ¸ì¤„í…Œë‹ˆê¹Œ, ì¼ë‹¨ ìˆëŠ” ê³µê³ ë“¤ ì«˜-ì•… í›‘ì–´ë´ë´. ê·¸ë¦¬ê³  ê³µí†µì ì¸ í‚¤ì›Œë“œë“¤ì´ ìˆì„í…Œë‹ˆê¹Œ ê·¸ê±°ìœ„ì£¼ë¡œ ìƒê°í•´ë³´ë“ ê°€. (ex. LLM, RAG, agent, ML serving, triton ë“±) zighang offercent bzpp"
  },
  {
    "id": "Career/Graduate School/ê³ ë ¤ëŒ€ì•¼ê°„ëŒ€í•™ì›",
    "title": "ê³ ë ¤ëŒ€ ì•¼ê°„ëŒ€í•™ì› ê´€ë ¨ ì •ë¦¬",
    "date": "2025-07-10",
    "excerpt": "ê¸°íšŒë¹„ìš©ê³¼ ê¸°ëŒ€ë˜ëŠ” ê°€ì¹˜",
    "category": "Career",
    "tags": [
      "ëŒ€í•™ì›"
    ],
    "content": "ë°°ê²½ ì œê°€ ì›í•˜ëŠ” ì»¤ë¦¬ì–´ì˜ ë°©í–¥ì„ ìƒê°í•´ë³¼ ë•Œ ëŒ€í•™ì›ì— ëŒ€í•œ ì„ íƒì„ ëŠ˜ ê³ ë¯¼í•˜ê²Œ ë©ë‹ˆë‹¤. AI/ML ë¦¬ì„œì²˜ë‚˜ ê´€ë ¨ ì—°êµ¬ë¥¼ í•˜ëŠ” ì¼ë“¤ì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ìµœì†Œ ì„ì‚¬ ì´ìƒì˜ í•™ë ¥ì„ ìš”êµ¬í•˜ëŠ” ê²½ìš°ê°€ ë§ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ëŠ¦ì—ˆë‚˜? ì§€ê¸ˆ ë‚˜ì´ê°€ ë§Œìœ¼ë¡œ 26ì •ë„, ë‚´ë…„ ì „ê¸°ì— ì‹œì‘í•´ë„ 29~30ì— ëë‚˜ë‹ˆê¹Œ ê·¸ë ‡ê²Œ ëŠ¦ì€ ì‹œê¸°ë„ ì•„ë‹ˆë¼ê³  ìƒê°ì´ ë“­ë‹ˆë‹¤. ê·¸ë¦¬ê³  ë‚˜ì´ê°€ ë” ë§ì•˜ë”ë¼ë„ ì´ê±¸ í†µí•´ ì–»ì„ ìˆ˜ ìˆëŠ”ê²Œ ë¶„ëª…í•˜ë‹¤ë©´, ê¸°íšŒë¹„ìš©ë³´ë‹¤ ë” í¬ë‹¤ê³  íŒë‹¨ëœë‹¤ë©´ ëŠ¦ì€ ì‹œê¸°ë€ ì—†ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ê¸°íšŒë¹„ìš© (ì‹œê°„ê³¼ ê°€ê²©) í•™ë¹„ê°€ ëŒ€ë ¥ 4ì²œì •ë„. ì¼ë‹¨ ëˆì´ ì •ë§ ë§ì´ ë“­ë‹ˆë‹¤. ì•„ì§ 3ì²œë§Œì›ë„ ëª¨ì•„ë³¸ ì  ì—†ëŠ” ì œê°€ ì´ë ‡ê²Œ í° ë¹„ìš©ì„ ê°ë‹¹í•  ìˆ˜ ìˆì„ê¹Œ? ë¼ëŠ” ìƒê°ë„ ë“­ë‹ˆë‹¤. (í•™ìê¸ˆ ëŒ€ì¶œ ë°›ê³  ì¡°ê¸ˆì”© ê°šìœ¼ë©´ ì–¸ì  ê°„ ê°šê² ì£ ...?) ê·¸ë¦¬ê³  ì‹œê°„ë„ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. í‡´ê·¼ í›„ì—ëŠ” ì˜¨ì „íˆ ì‹œê°„ì„ ë‹¤ ìŸì•„ì•¼ í•˜ê³  ì¢‹ì€ ê²°ê³¼ë¥¼ ìœ„í•´ì„œëŠ” ì£¼ë§ì—ë„ ì´ ë¶€ë¶„ë§Œ ë³´ê³  ìˆì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. 2ë…„ ë°˜ì •ë„ì˜ ì‹œê°„ë„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ê¸°ëŒ€ë˜ëŠ” ê°€ì¹˜ ì œê°€ ì›í•˜ëŠ” ì»¤ë¦¬ì–´ì˜ ë°©í–¥ì— ë„ì›€ì´ ë©ë‹ˆë‹¤. í•™ë ¥ ì„ì‚¬ë¼ëŠ” í•™ë ¥ì€ ì‚¬ì‹¤ìƒ í•„ìˆ˜ì ì´ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ í•˜ë©´ì„œ ë§Œë‚˜ëŠ” ë‹¤ë¥¸ ë¶„ë“¤ì„ ë´ë„ ë‹¤ ì„ì‚¬ ì´ìƒì˜ í•™ë ¥ì„ ê°€ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë…¼ë¬¸ ê·¸ë¦¬ê³  ì¢‹ì€ ë…¼ë¬¸ì„ ì‘ì„±í•  ìˆ˜ ìˆì–´ì•¼ë§Œ ë‹¨ìˆœ í•™ìœ„ì— ê·¸ì¹˜ì§€ ì•Šê³  ì§„ì§œ ë„ì›€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì§€ì›ì‹œê¸°ì™€ ì¤€ë¹„í•  ê²ƒë“¤ ì˜¬í•´ í›„ê¸° ì§€ì›ì€ ë†“ì³¤ê³  ë‚´ë…„ ì „ê¸°ì— ì§€ì›í•˜ì§€ ì•Šì„ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë¸”ë¡œê·¸ë“¤ì„ ì‚´í´ë³´ë‹ˆ ì¤€ë¹„í•´ì•¼í•˜ëŠ” ê±´ ê¸°ë³¸ì§€ì‹(í†µê³„, ì„ í˜•ëŒ€ìˆ˜ ë“±ë“±)ê³¼ ì—°êµ¬ê³„íšì„œ ì •ë„ì…ë‹ˆë‹¤. ê¸°ë³¸ì§€ì‹ í†µê³„, ì„ í˜•ëŒ€ìˆ˜ì™€ ê°™ì€ ì§ˆë¬¸ë“¤ì„ ë©´ì ‘ë•Œ ë°›ëŠ”ë‹¤ê³  í•©ë‹ˆë‹¤. ê²½ìŸë¥ ì´ ì•½ 6:1ì •ë„ ëœë‹¤ê³  í•˜ë‹ˆ, ì´ëŸ° ì§ˆë¬¸ì— ëŒ€í•œ ëŒ€ë‹µì„ ë¯¸ë¦¬ ì² ì €í•˜ê²Œ ì¤€ë¹„í•  í•„ìš”ê°€ ìˆì–´ë³´ì…ë‹ˆë‹¤. ì—°êµ¬ê³„íšì„œ ì´ ë¶€ë¶„ì— ëŒ€í•œ ê³ ë¯¼ì´ ë§ì´ í•„ìš”í•©ë‹ˆë‹¤. > ê·¸ë˜ì„œ ì–´ë–¤ ì—°êµ¬ë¥¼ í•˜ê³  ì‹¶ì€ê±°ì§€? ì´ ë¶€ë¶„ì´ ë§ì´ ë¹„ì–´ìˆì–´ì„œ ê¾¸ì¤€íˆ ì±„ì›Œë‚˜ê°€ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì°¸ê³  ìë£Œë“¤ Hello New World - ê³ ë ¤ëŒ€í•™êµ SW AI ëŒ€í•™ì› ë©´ì ‘ í›„ê¸° ê·€í‰ì´ ì„œì¬ - ê³ ë ¤ëŒ€í•™êµ AI ì•¼ê°„ëŒ€í•™ì› ë©´ì ‘ í›„ê¸° From data - ì¬ì§ì ë¹…ë°ì´í„°/ì¸ê³µì§€ëŠ¥ ëŒ€í•™ì› ì •ë¦¬"
  },
  {
    "id": "Career/25-07-10-visuworks-thoughts-of-my-career",
    "title": "ë‚˜ì˜ í˜„ì¬ ì»¤ë¦¬ì–´ ìƒí™©ì— ëŒ€í•œ ìƒê°",
    "date": "2025-07-10",
    "excerpt": "ë‚˜ì˜ í˜„ì¬ ì»¤ë¦¬ì–´ì— ëŒ€í•œ ê³ ì°°",
    "category": "Career",
    "tags": [
      "ë‚´ ì»¤ë¦¬ì–´ëŠ” ì–´ë””ë¡œ ê°€ì•¼í•˜ë‚˜"
    ],
    "content": "í˜„ì¬ ë‚˜ì˜ ìƒí™© ìš”ì¦˜ ì •ë§ ì»¤ë¦¬ì–´ì— ëŒ€í•œ ê±±ì •ê³¼ ê³ ë¯¼ì´ ë§ìŠµë‹ˆë‹¤... ã…ã… ì§€ê¸ˆ ë‹¤ë‹ˆê³  ìˆëŠ” íšŒì‚¬ì˜ ë„ë©”ì¸ì— ë¬¶ì—¬ìˆì§„ ì•Šì„ì§€. ì—¬ê¸°ì„œ ì´ë£¬ ì„±ê³¼ë“¤ì´ ë‚˜ì˜ ì»¤ë¦¬ì–´ì— ë„ì›€ì´ ë ì§€. ì‚¬ì‹¤ ìƒê°ë§Œí•˜ë©´ ì •ë¦¬ê°€ ì•ˆë˜ëŠ” ë¶€ë¶„ë“¤ì´ ë§ê¸° ë•Œë¬¸ì— ê¸€ë¡œ ì ì–´ë³´ë©´ì„œ ê³ ë¯¼ë“¤ì— ëŒ€í•œ ë‚˜ë¦„ì˜ ëŒ€ë‹µë“¤ì„ ì ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì €ëŠ” ì‚°ì—…ê³µí•™ì„ ì „ê³µí•˜ê³ , 6ê°œì›”ì •ë„ ë¶€íŠ¸ìº í”„ì—ì„œ ì»´í“¨í„° ë¹„ì „ ë¶€ë¶„ì„ ê³µë¶€í•˜ê³ , ì˜ë£Œ ë„ë©”ì¸ì—ì„œ ì¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë‹¤ë‹ˆê³  ìˆëŠ” íšŒì‚¬(visuworks)ëŠ” ì‹œë ¥êµì •ë³‘ì› (ë¹„ì•¤ë¹› ì•ˆê³¼)ë¥¼ ì£¼ëœ ê³ ê°ì‚¬ë¡œ ì‚¼ê³  ìˆê³  ì´ ë³‘ì›ì—ì„œ ë§Œë“¤ì–´ì¡Œë‹¤ê³ ë„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì§€ê¸ˆê¹Œì§€ í•œ í”„ë¡œì íŠ¸ëŠ” OCR pipeline ê°œë°œ ê³ ê° ìƒë‹´ìš© ì±—ë´‡ ê°œë°œ ë Œì¦ˆ ì‚¬ì´ì§• ì¶”ì²œ ì„œë¹„ìŠ¤ ê°œë°œ ì •ë„ê°€ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œ ìƒí™©ë“¤ ì œê°€ ìƒê°í•˜ëŠ” ë¬¸ì œë“¤ì„ ì •ë¦¬í•´ë³´ê³  ì–´ë–»ê²Œ í•´ê²°í•´ë³¼ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ìƒê°ë“¤ì„ ì •ë¦¬í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì—°ë´‰ì´ ì´ë ‡ê²Œ ì ë‚˜? ë˜ê²Œ ë‹¨ìˆœí•˜ê²Œ ì¼ì„ ì¼ë‹¨ ì‹œì‘í•˜ëŠ” ë§ˆìŒìœ¼ë¡œ ì´ íšŒì‚¬ì— ê³ ë¯¼ì—†ì´ ì…ì‚¬í–ˆëŠ”ë°, ì—°ë´‰ì´ ìƒê°ë³´ë‹¤ í˜„ì €íˆ ì ì–´ì„œ ì¡°ê¸ˆ ë†€ëìŠµë‹ˆë‹¤. ë” ì •í™•íˆëŠ” ì—°ë´‰ì´ ë‚®ì§€ë§Œ ì¼ë‹¨ ê²½ë ¥ì„ ìŒ“ìëŠ” ë§ˆìŒìœ¼ë¡œ ì…ì‚¬í–ˆìŠµë‹ˆë‹¤. 8ê°œì›”ì •ë„ ì¼í•˜ê³  ë‚˜ë¦„ 2ê°œì˜ í”„ë¡œì íŠ¸ë¥¼ ì„±ê³µì ìœ¼ë¡œ ëë‚¸ ì‹œì ì—ì„œ, ì—°ë´‰í˜‘ìƒì„ í–ˆì§€ë§Œ í° í­ìœ¼ë¡œ ì˜¤ë¥´ì§€ëŠ” ëª»í–ˆìŠµë‹ˆë‹¤. > ì—°ë´‰ í…Œì´ë¸”ì´ ì¢‹ì€ íšŒì‚¬ë¡œ ë¹ ë¥´ê²Œ ì˜®ê²¨ì•¼ê² ë‹¤. í•˜ì§€ë§Œ ì‰½ì§€ ì•Šë„¤ìš”... ã…  ì§€ê¸ˆ 4~5ê°œì›”ì •ë„ ì´ì§ì„ ì¤€ë¹„í•˜ê³  ìˆëŠ”ë° ëŒ€ë¶€ë¶„ì˜ ê¸°ì—…ì—ì„œ ë–¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ê·¸ëƒ¥ ì´ì •ë„ ê°€ì¹˜ë¥¼ ê°€ì§„ ëŠ¥ë ¥ì¸ê°€?ë¼ëŠ” ì˜êµ¬ì‹¬ì´ ë“¤ì§€ë§Œ ë³´ì™„í•´ì•¼í•  ì ë“¤ì„ ë³´ì™„í•˜ë©´ì„œ ì´ì§ì„ ì¤€ë¹„ ì¤‘ì…ë‹ˆë‹¤."
  },
  {
    "id": "Book/ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜ (Atomic Habits)/ì‰¬ì›Œì•¼ë‹¬ë¼ì§„ë‹¤",
    "title": "ì‰¬ì›Œì•¼ ë‹¬ë¼ì§„ë‹¤",
    "date": "2025-07-10",
    "excerpt": "ì–´ë–»ê²Œ ì¢‹ì€ ìŠµê´€ì„ ë§Œë“¤ê³ , ë‚˜ìœ ìŠµê´€ì€ ë²„ë¦´ ê²ƒì¸ê°€",
    "category": "Book",
    "tags": [
      "atomic habits",
      "ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜"
    ],
    "content": "ìŠµê´€ì˜ ì¤‘ìš”ì„± ì¸ìƒì„ ì‚´ì•„ê°€ë©´ì„œ ì—¬ëŸ¬ ì¤‘ìš”í•œ ìš”ì†Œë“¤ì´ ìˆê² ì§€ë§Œ, ê°€ì¥ ì¤‘ìš”í•œ ê±´ ìŠµê´€ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë“€í¬ ëŒ€í•™ì˜ Wendy Wood êµìˆ˜ê°€ ìˆ˜í–‰í•œ ì—°êµ¬ì— ë”°ë¥´ë©´, ìš°ë¦¬ í–‰ë™ì˜ ì•½ 45%ê°€ ìŠµê´€ì— ì˜í•œ ê²ƒì´ë¼ê³  í•©ë‹ˆë‹¤. ì œê°€ ê°€ì¥ ì¢‹ì•„í•˜ëŠ” ë¬¸êµ¬ë„ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. > Habit is second nature ì´ë ‡ê²Œ ì¤‘ìš”í•œ ìŠµê´€ì„ ì–´ë–»ê²Œ ê´€ë¦¬í•  ìˆ˜ ìˆì„ê¹Œìš”? ì¢‹ì€ ìŠµê´€ì„ ë§ì´ ë§Œë“¤ê³ , ë‚˜ìœ ìŠµê´€ì„ ë²„ë ¤ ì¢‹ì€ ì‹œìŠ¤í…œì„ ë§Œë“¤ì–´ ë‚˜ê°„ë‹¤ë©´ ì œê°€ ì›í•˜ëŠ” ì‚¶ì— ê°€ê¹Œì›Œì§ˆ ê²ƒì´ë¼ ìƒê°í•©ë‹ˆë‹¤. ì‰¬ì›Œì•¼ ë‹¬ë¼ì§„ë‹¤ 'ì•„ì£¼ ì‘ì€ ìŠµê´€ì˜ í˜'ì´ë¼ëŠ” ì±…ì—ì„œëŠ” 2ë¶„ ë²•ì¹™ì„ ì œì‹œí•©ë‹ˆë‹¤. í•´ì•¼í•˜ëŠ” ì¼ì„ ì•„ì£¼ ì‘ê²Œ ìª¼ê°œë¼ë©° 2ë¶„ì•ˆì— ëë‚¼ ìˆ˜ ìˆëŠ” ì¼ë¡œ ë‚˜ëˆ„ë¼ê³  ë§í•©ë‹ˆë‹¤. 1~2ë‹¬ì •ë„ ì´ ë¶€ë¶„ì„ ì‚¶ì— ì ìš©í•´ë´¤ëŠ”ë° ì •ë§ íš¨ê³¼ê°€ ë§ì•˜ìŠµë‹ˆë‹¤. ê°€ì¥ í° ì´ìœ ëŠ” ì‹¬ë¦¬ì  ë¶€ë‹´ê°ì„ ì¤„ì—¬ì£¼ê¸° ë•Œë¬¸ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ë³´í†µ ì¼ì„ ë¯¸ë£¨ëŠ” ì´ìœ ë¥¼ ìƒê°í•´ë³´ë©´ ì‹¬ë¦¬ì  ë¶€ë‹´ê° ë•Œë¬¸ì…ë‹ˆë‹¤. íŠ¹íˆ ì €ëŠ” ì¼ì„ ì‹œì‘í•˜ê¸°ì— ë¶€ë‹´ìŠ¤ëŸ¬ì›Œì„œ ì‹œì‘ì¡°ì°¨ ëª»í•˜ê³  ë¯¸ë£¨ê¸°ë§Œ í–ˆë˜ ê²½í—˜ì´ ë§ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° í•´ì•¼í•  ì¼ì„ 2ë¶„ì•ˆì— ëë‚¼ ìˆ˜ ìˆëŠ” ì‘ì€ ì¼ë“¤ë¡œ ìª¼ê°œë‹ˆê¹Œ ì´ëŸ° ë¶€ë‹´ê°ì´ ê±°ì˜ ì—†ì–´ì¡Œê³ , ì‘ì€ ì¼ë“¤ì„ ì„±ì·¨í•˜ë©´ì„œ ê·¸ ì„±ì·¨ê°ì— ì ì  ë” ë§ì€ ì¼ë“¤ì„ í•  ìˆ˜ ìˆê²Œ ëìŠµë‹ˆë‹¤. > ì•„ì£¼ ì‘ì€ ì¼ë“¤ë¡œ ìª¼ê°œì. 2ë¶„ì•ˆì— ëë‚¼ ìˆ˜ ìˆëŠ” ì¼ë“¤ë¡œ. ì´ê±´ ì œ ì‚¶ì˜ í•˜ë‚˜ì˜ ì›ì¹™ì²˜ëŸ¼ ë˜ì—ˆìŠµë‹ˆë‹¤. ì•„ ë¯¸ë£¨ì§€ ë§ì ë¼ê³  ìƒê°í•˜ëŠ”ê²Œ ì•„ë‹ˆë¼, ì™œ ë¯¸ë£¨ëŠ”ì§€ ì´í•´í•˜ê³ , ì‘ì€ ì¼ë“¤ë¡œ ìª¼ê°œê¸° ì‹œì‘í–ˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ë ‡ê²Œ ì‘ì€ ì„±ì·¨ë“¤ì´ ëª¨ì—¬ ë…¸ë ¥í•˜ì§€ ì•Šì•„ë„ ìë™ìœ¼ë¡œ ì‹¤í–‰ì´ ë˜ëŠ” ìŠµê´€ì´ ë˜ì–´ê°‘ë‹ˆë‹¤. ì–´ë–»ê²Œ ë‚˜ìœ ìŠµê´€ì„ ì—†ì•¨ê¹Œ ì¢‹ì€ ìŠµê´€ì„ ë§Œë“œëŠ” ê²ƒê³¼ ë°˜ëŒ€ë¡œ í•˜ê¸° ì–´ë µê²Œ ë§Œë“¤ë©´ ë©ë‹ˆë‹¤. ì‚¬ëŒì˜ ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆê¸° ë•Œë¬¸ì— í™˜ê²½ì´ ê°€ì¥ ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. Roy Baumeisterì˜ ì—°êµ¬ì— ì˜í•˜ë©´ ì˜ì§€ë ¥ì€ ì œí•œëœ ìì›ì´ë¼ê³  í•©ë‹ˆë‹¤. ì˜ì§€ë ¥ì€ ê·¼ìœ¡ì²˜ëŸ¼ ì‘ë™í•˜ë©°, ì‚¬ìš©í•˜ë©´ í”¼ë¡œí•´ì§€ê³  íœ´ì‹ì´ í•„ìš”í•´ì§€ê²Œ ë©ë‹ˆë‹¤. > ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆë‹¤. í™˜ê²½ì´ ì¤‘ìš”í•¨! Massachusetts General Hospital ì—°êµ¬ì—ì„œëŠ” ë³‘ì› ì¹´í˜í…Œë¦¬ì•„ì˜ ë¬¼ì˜ ìœ„ì¹˜ë§Œ ë°”ê¿¨ëŠ”ë°ë„ íƒ„ì‚°ìŒë£Œ íŒë§¤ 11.4% ê°ì†Œ, ìƒìˆ˜ íŒë§¤ 25.8% ì¦ê°€í–ˆë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ë‚˜ìœ ìŠµê´€ì´ ë°œê²¬ëë‹¤ë©´, ì´ë¥¼ í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì„ ì–´ë–»ê²Œ ë§Œë“¤ì§€ ê³ ë¯¼í•´ì•¼ í•©ë‹ˆë‹¤. ì €ëŠ” ë°¤ì— íœ´ëŒ€í°ì„ ë³´ë©´ì„œ ëŠ¦ê²Œ ìëŠ” ìŠµê´€ì´ ìˆì–´ì„œ ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìëª…ì¢…ì„ ìƒ€ìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ì•ŒëŒ ë•Œë¬¸ì— íœ´ëŒ€í°ì„ ì¹¨ëŒ€ì— ë“¤ê³  ê°€ì§€ ì•Šê³  ì§‘ì— ë“¤ì–´ì˜¤ë©´ ë°”ë¡œ êº¼ë²„ë¦½ë‹ˆë‹¤. > ë‚˜ìœ ìŠµê´€ì„ í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì„ ë§Œë“¤ì ì§‘ì—ì„œ ì•„ì— íœ´ëŒ€í°ì„ ë³´ì§€ ì•Šê²Œ ë˜ì—ˆê³ , ëŠ¦ê²Œ ìëŠ” ìŠµê´€ë„ í•´ê²°í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì •ë¦¬ í•´ì•¼í•  ì¼ì„ ì•„ì£¼ ì‘ê²Œ ìª¼ê°œ ì‹¬ë¦¬ì  ë¶€ë‹´ê°ì„ ì—†ì• ì -> ì‘ì€ ì„±ì·¨ë“¤ì„ ë§›ë³´ë©° ê¾¸ì¤€íˆ ì´ì–´ê°€ì ì‹¤ì œë¡œ í•´ë³´ë‹ˆ ê·¸ë™ì•ˆ ë¯¸ë£¨ë˜ ë§ì€ ì¼ë“¤ì„ ì´ì–´ê°€ê³  ìˆë‹¤. ì˜ì§€ë ¥ì€ í•œê³„ê°€ ìˆìœ¼ë‹ˆ, ë‚˜ìœ ìŠµê´€ì„ í•  ìˆ˜ ì—†ëŠ” í™˜ê²½ì„ ë§Œë“¤ì ì•ŒëŒì€ ìëª…ì¢…ìœ¼ë¡œ ëŒ€ì²´í•˜ê³ , íœ´ëŒ€í°ì€ ì§‘ì— ì˜¤ë©´ ë°”ë¡œ êº¼ë²„ë¦¬ê¸°! -> ëŠ¦ê²Œê¹Œì§€ íœ´ëŒ€í° ë³´ëŠ” ìŠµê´€ ì œê±°ë¨"
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-09-ê²©ë ¤í•˜ëŠ”í‘œí˜„",
    "title": "ê²©ë ¤í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-09",
    "excerpt": "You did a great job on, I'm happy to see that",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ê±° You did a great job on the speaking practice. I'm happy to see that you're improving. I'm happy to see that you've overcome your fear of English. I can tell that she's sad. I could tell that she broke up the her boyfriend. I can tell that you put a lof of thought into this. ê°„ë‹¨í•œ ì‘ë¬¸ Today I wanna complement myself. You did a great job on the solving problem. Working on company 1ë…„ ì¡°ê¸ˆë„˜ê²Œ, I solved a lot of problems. On the OCR project, I solve communicate problem and make greak OCR pipeline. On the chatbot project, I made chatbot which ë§Œì¡±í•˜ë‹¤ ê³ ê°ì‚¬s needs. And then, I'm happy to see that youve overcome of ë¹„êµí•˜ë©´ì„œ ë’¤ì³ì§„ë‹¤ê³  ëŠë¼ëŠ” fear. When I was 21-25, I was just frozen because of fear that ë‚œ ë’¤ì³ì¡Œê³  ì¸ìƒì€ ë§í–ˆë‹¤. I can tell that your life is great. I ì¼ì¸ë¶„ì˜ ì‚¶ì€ ì‚´ì•„ê°„ë‹¤. And normal life is always wonderful. claud-4-sonnet í”¼ë“œë°± > gemini-2-proë³´ë‹¤ ë‚˜ì€ ë“¯ ìˆ˜ì •ëœ ë²„ì „: Today I want to compliment myself. You did a great job at problem-solving. Working at a company for a little over a year, I have solved many problems. On the OCR project, I solved communication issues and built a great OCR pipeline. On the chatbot project, I created a chatbot that satisfied the client's needs. I'm happy to see that you've overcome the fear of comparing yourself to others and feeling behind. When I was 21-25, I was frozen because of the fear that I was falling behind and my life was ruined. I can tell that your life is great. You're living a fulfilling life. And a normal life is always wonderful. í”¼ë“œë°±: ì˜í•œ ì : ë°°ìš´ í‘œí˜„ë“¤ (You did a great job on, I'm happy to see that, I can tell that)ì„ ì‹¤ì œ ìƒí™©ì— ì ìš©í•˜ë ¤ê³  ë…¸ë ¥í–ˆì–´ìš” ìì‹ ì˜ ê²½í—˜ì„ ë°”íƒ•ìœ¼ë¡œ ì§„ì •ì„± ìˆëŠ” ë‚´ìš©ì„ ì¼ì–´ìš” ìê¸° ê²©ë ¤ë¼ëŠ” ì£¼ì œë¥¼ ì˜ ì„ íƒí–ˆì–´ìš” ê°œì„ ì : ë‹¨ì–´ ì„ íƒ: \"complement\" â†’ \"compliment\" (ì¹­ì°¬í•˜ë‹¤) ë¬¸ë²•: \"on the solving problem\" â†’ \"at problem-solving\" ì‹œì œ ì¼ê´€ì„±: ê³¼ê±°ì™€ í˜„ì¬ì™„ë£Œ ì‹œì œë¥¼ ìƒí™©ì— ë§ê²Œ ì‚¬ìš© í•œì˜ í˜¼ìš©: í•œêµ­ì–´ ë¶€ë¶„ì„ ì˜ì–´ë¡œ ì™„ì „íˆ ë²ˆì—­í•˜ì—¬ ìì—°ìŠ¤ëŸ½ê²Œ í‘œí˜„ ì „ì¹˜ì‚¬: \"overcome of\" â†’ \"overcome\" (overcomeì€ ì „ì¹˜ì‚¬ ì—†ì´ ì‚¬ìš©) ë‹¤ìŒì— ì‹œë„í•´ë³¼ ê²ƒ: í•œ ë¬¸ì¥ ì•ˆì—ì„œëŠ” í•œ ì–¸ì–´ë¡œ í†µì¼í•˜ê¸° ë¬¸ì¥ì„ ì“°ê¸° ì „ì— ì „ì²´ êµ¬ì¡°ë¥¼ ì˜ì–´ë¡œ ë¨¼ì € ìƒê°í•´ë³´ê¸° ë°°ìš´ í‘œí˜„ì„ ë” ë‹¤ì–‘í•œ ìƒí™©ì— ì ìš©í•´ë³´ê¸°"
  },
  {
    "id": "Causal Inference/Industry Application/what-is-statistical-bias",
    "title": "í†µê³„ì ì¸ í¸í–¥ê³¼ ì´ë¥¼ ì œê±°í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ",
    "date": "2025-07-09",
    "excerpt": "ì‹¤ì œ ë°ì´í„°ì—ì„œ í†µê³„ì  í¸í–¥ì„ í™•ì¸í•˜ê³  ì´ë¥¼ ì”ì°¨í™”ë¥¼ í†µí•´ ì œê±°í•˜ëŠ” ê³¼ì •",
    "category": "Causal Inference",
    "tags": [
      "í¸í–¥",
      "í†µê³„ì ",
      "bias"
    ],
    "content": "ë°°ê²½ --- ì¸ê³¼ì¶”ë¡ ì˜ ê¸°ë³¸ì ì¸ ê°œë…ì€ í†µê³„ì  í¸í–¥ (Statistical Bias)ì„ ì œê±°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì„ ë” ì˜ ì´í•´í•˜ê³  íš¨ê³¼ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•´ì„œëŠ”, ì´ í†µê³„ì  í¸í–¥ì„ ì´í•´í•˜ê³  í™•ì¸í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ì •ì˜ --- > í†µê³„ì  í¸í–¥(Statistical Bias)ì€ í†µê³„ ë¶„ì„ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” ì²´ê³„ì ì¸ ì˜¤ë¥˜ë¡œ, ì¸¡ì •ì´ë‚˜ ì¶”ì • ê³¼ì •ì—ì„œ ì‹¤ì œ ëª¨ì§‘ë‹¨ì˜ ëª¨ìˆ˜(parameter)ì™€ í‘œë³¸ í†µê³„ëŸ‰ ì‚¬ì´ì— ì¼ê´€ëœ ì°¨ì´ê°€ ë°œìƒí•˜ëŠ” í˜„ìƒì„ ë§í•©ë‹ˆë‹¤. ì²´ê³„ì (systematic) ì˜¤ë¥˜ë€? ì¸¡ì • ë°©ë²•ì´ë‚˜ ë¶„ì„ ë°©ë²•ì— ë¬¸ì œê°€ ìˆì–´ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ ë¬´ì‘ìœ„ê°€ ì•„ë‹Œ, ì¼ì •í•œ íŒ¨í„´ì´ë‚˜ ë°©í–¥ì„±ì„ ê°€ì§ ì²´ê³„ì  ì˜¤ë¥˜ (Systematic Error) vs ë¬´ì‘ìœ„ ì˜¤ë¥˜ (Random Error) ì²´ê³„ì  ì˜¤ë¥˜ (Systematic Error): í•­ìƒ ê°™ì€ ë°©í–¥ìœ¼ë¡œ ë°œìƒ (ì˜ˆ: í•­ìƒ ì‹¤ì œê°’ë³´ë‹¤ ë†’ê²Œ ì¸¡ì •) ì¸¡ì • íšŸìˆ˜ë¥¼ ëŠ˜ë ¤ë„ ì¤„ì–´ë“¤ì§€ ì•ŠìŒ ì¸¡ì • ë„êµ¬ë‚˜ ë°©ë²•ì˜ ê²°í•¨ì—ì„œ ë¹„ë¡¯ë¨ ë¬´ì‘ìœ„ ì˜¤ë¥˜ (Random Error): ë°©í–¥ì´ ë¶ˆê·œì¹™í•¨ (ë•Œë¡œëŠ” ë†’ê²Œ, ë•Œë¡œëŠ” ë‚®ê²Œ) ì¸¡ì • íšŸìˆ˜ë¥¼ ëŠ˜ë¦¬ë©´ í‰ê· ì ìœ¼ë¡œ ìƒì‡„ë¨ ìš°ì—°í•œ ë³€ë™ì—ì„œ ë¹„ë¡¯ë¨ ì œê°€ ì´í•´í•œ ê°œë…ì€, ëœë¤ ë…¸ì´ì¦ˆê°€ ì•„ë‹Œ êµ¬ì¡°ìƒì˜ ë¬¸ì œê°€ ìˆì–´ ê²°ê³¼ë¥¼ í™•ì¸í•˜ëŠ”ë° ì˜¤í•´ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë³´ë©´: êµ¬ì¡°ìƒ ë¬¸ì œ: ì˜¨ë¼ì¸ ì„¤ë¬¸ì¡°ì‚¬ë¡œë§Œ ì˜ê²¬ì„ ìˆ˜ì§‘ â†’ ë””ì§€í„¸ ê¸°ê¸° ì‚¬ìš©ì´ ì–´ë ¤ìš´ ê³ ë ¹ì¸µ ì˜ê²¬ ëˆ„ë½ë˜ê³  ì Šì€ ì¸µì˜ ì˜ê²¬ë§Œ ë°˜ì˜ë¨ ê²°ê³¼ì˜ ì˜¤í•´: \"ì Šì€ ì¸µì˜ ì˜ê²¬ì´ ì „ì²´ ì˜ê²¬\"ì´ë¼ê³  ì˜ëª» í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í†µê³„ì  í¸í–¥ì˜ ì¢…ë¥˜ì™€ ì„¤ëª… ì„ íƒ í¸í–¥ (Selection Bias) > ì—°êµ¬ ëŒ€ìƒì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” í¸í–¥ìœ¼ë¡œ, í‘œë³¸ì´ ëª¨ì§‘ë‹¨ì„ ì œëŒ€ë¡œ ëŒ€í‘œí•˜ì§€ ëª»í•  ë•Œ ë°œìƒí•©ë‹ˆë‹¤. ì˜ˆì‹œ: ë³‘ì› ë°ì´í„°ë§Œìœ¼ë¡œ ì§ˆë³‘ ì—°êµ¬ë¥¼ í•  ë•Œ â†’ ì¤‘ì¦ í™˜ìë§Œ í¬í•¨ë˜ì–´ ì§ˆë³‘ì˜ ì‹¬ê°ì„±ì´ ê³¼ëŒ€í‰ê°€ë¨ ìë°œì  ì°¸ì—¬ìë§Œìœ¼ë¡œ ì„¤ë¬¸ì¡°ì‚¬ â†’ íŠ¹ì • ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§„ ì‚¬ëŒë“¤ë§Œ ì°¸ì—¬í•˜ì—¬ ê²°ê³¼ê°€ í¸í–¥ë¨ ì œê°€ ì•ì„œ ì˜ˆì‹œë¥¼ ë“  ê²½ìš°ê°€ ì´ì— í•´ë‹¹í•©ë‹ˆë‹¤. í•œêµ­ ë‚¨ìì˜ í‰ê·  í‘¸ì‰¬ì—… ê°œìˆ˜ë¥¼ ì¸¡ì •í•œë‹¤ê³  í•  ë•Œ, í—¬ìŠ¤ì¥ì—ì„œ í‘¸ì‰¬ì—… ê°œìˆ˜ë¥¼ ì¸¡ì •í•œë‹¤ë©´ ì´ëŠ” í•œêµ­ ë‚¨ìë¥¼ ëŒ€í‘œí•  ìˆ˜ ìˆì„ê¹Œìš”? êµë€ë³€ìˆ˜ í¸í–¥ (Confounding Bias) > ì›ì¸ë³€ìˆ˜ì™€ ê²°ê³¼ë³€ìˆ˜ ëª¨ë‘ì— ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ì œ3ì˜ ë³€ìˆ˜(êµë€ë³€ìˆ˜)ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì»¤í”¼ ì†Œë¹„ì™€ ì‹¬ì¥ë³‘ì˜ ê´€ê³„ ì—°êµ¬ì—ì„œ í¡ì—°ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²½ìš° êµìœ¡ê³¼ ì†Œë“ì˜ ê´€ê³„ì—ì„œ ê°€ì • ë°°ê²½ì„ ê³ ë ¤í•˜ì§€ ì•Šì€ ê²½ìš° ì •ë³´ í¸í–¥ (Information Bias) > ë°ì´í„° ìˆ˜ì§‘ ê³¼ì •ì—ì„œ ë°œìƒí•˜ëŠ” í¸í–¥ìœ¼ë¡œ, ì¸¡ì • ì˜¤ë¥˜ë‚˜ ë¶„ë¥˜ ì˜¤ë¥˜ë¡œ ì¸í•´ ë°œìƒí•©ë‹ˆë‹¤. í•˜ìœ„ ìœ í˜•: íšŒìƒ í¸í–¥ (Recall Bias): ê³¼ê±° ì‚¬ê±´ì„ ê¸°ì–µí•˜ëŠ” ì •í™•ë„ ì°¨ì´ ê´€ì°°ì í¸í–¥ (Observer Bias): ì—°êµ¬ìì˜ ì£¼ê´€ì  íŒë‹¨ì´ ê°œì… ì¸¡ì • í¸í–¥ (Measurement Bias): ì¸¡ì • ë„êµ¬ë‚˜ ë°©ë²•ì˜ ê²°í•¨ ì˜ˆì‹œ: ì§ˆë³‘ í™˜ìê°€ ê±´ê°•í•œ ì‚¬ëŒë³´ë‹¤ ê³¼ê±° ë…¸ì¶œì„ ë” ì˜ ê¸°ì–µí•˜ëŠ” ê²½ìš° ì—°êµ¬ìê°€ ê°€ì„¤ì„ ì•Œê³  ìˆì–´ ê²°ê³¼ë¥¼ ì£¼ê´€ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ê²½ìš° ìƒì¡´ í¸í–¥ (Survivorship Bias) > ì„±ê³µí•˜ê±°ë‚˜ ì‚´ì•„ë‚¨ì€ ì‚¬ë¡€ì—ë§Œ ì§‘ì¤‘í•˜ê³ , ì‹¤íŒ¨í•˜ê±°ë‚˜ íƒˆë½í•œ ì‚¬ë¡€ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì„±ê³µí•œ ê¸°ì—…ê°€ë“¤ë§Œ ì¸í„°ë·°í•˜ì—¬ \"ì°½ì—… ì„±ê³µ ìš”ì¸\" ë¶„ì„ ì¡¸ì—…ìƒë§Œ ëŒ€ìƒìœ¼ë¡œ í•œ ëŒ€í•™ êµìœ¡ íš¨ê³¼ ì—°êµ¬ (ì¤‘ë„ íƒˆë½ì ì œì™¸) ë°œí‘œ í¸í–¥ (Publication Bias) > í†µê³„ì ìœ¼ë¡œ ìœ ì˜í•œ ê²°ê³¼ë‚˜ ê¸ì •ì ì¸ ê²°ê³¼ë§Œ ë°œí‘œë˜ê³ , ê·¸ë ‡ì§€ ì•Šì€ ê²°ê³¼ëŠ” ë°œí‘œë˜ì§€ ì•Šì•„ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì•½ë¬¼ íš¨ê³¼ê°€ ìˆëŠ” ì—°êµ¬ë§Œ ë…¼ë¬¸ìœ¼ë¡œ ë°œí‘œë˜ê³ , íš¨ê³¼ê°€ ì—†ëŠ” ì—°êµ¬ëŠ” ë°œí‘œë˜ì§€ ì•ŠìŒ ì„±ê³µì ì¸ ì •ì±… ì‚¬ë¡€ë§Œ ë³´ê³ ë˜ê³ , ì‹¤íŒ¨í•œ ì •ì±…ì€ ë³´ê³ ë˜ì§€ ì•ŠìŒ ì‹œê°„ ê´€ë ¨ í¸í–¥ (Time-related Bias) > ì‹œê°„ì˜ íë¦„ê³¼ ê´€ë ¨í•˜ì—¬ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. í•˜ìœ„ ìœ í˜•: ì‹œëŒ€ íš¨ê³¼ (Period Effect): íŠ¹ì • ì‹œì ì˜ í™˜ê²½ì  ìš”ì¸ ì½”í˜¸íŠ¸ íš¨ê³¼ (Cohort Effect): íŠ¹ì • ì„¸ëŒ€ì˜ ê³µí†µ ê²½í—˜ ì—°ë ¹ íš¨ê³¼ (Age Effect): ë‚˜ì´ì— ë”°ë¥¸ ìì—°ìŠ¤ëŸ¬ìš´ ë³€í™” íƒˆë½ í¸í–¥ (Attrition Bias) > ì—°êµ¬ ì§„í–‰ ì¤‘ ì°¸ì—¬ìê°€ íƒˆë½í•˜ì—¬ ë°œìƒí•˜ëŠ” í¸í–¥ì…ë‹ˆë‹¤. ì˜ˆì‹œ: ì¥ê¸° ì¶”ì  ì—°êµ¬ì—ì„œ ê±´ê°•í•œ ì‚¬ëŒë“¤ë§Œ ê³„ì† ì°¸ì—¬í•˜ê³ , ì•„í”ˆ ì‚¬ëŒë“¤ì€ íƒˆë½ ì˜¨ë¼ì¸ êµìœ¡ íš¨ê³¼ ì—°êµ¬ì—ì„œ í•™ìŠµ ëŠ¥ë ¥ì´ ë†’ì€ ì‚¬ëŒë“¤ë§Œ ëê¹Œì§€ ì°¸ì—¬ ê²°ê³¼ë¥¼ í•´ì„í•  ë•Œ ì´ëŸ° í¸í–¥ë“¤ì€ ì˜¤í•´ë¥¼ ë§Œë“­ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ¬í•œ í¸í–¥ì„ ê³ ë ¤í•˜ê³  ê²°ê³¼ë¥¼ í•´ì„í•´ì•¼í•˜ë©°, ì´ëŸ° í¸í–¥ì„ ì°¾ê³  ì œê±°í•˜ëŠ” ë°©ë²•ì´ ì¸ê³¼ì¶”ë¡ ì˜ ì£¼ëœ ëª©ì ì…ë‹ˆë‹¤. ë‚´ê°€ ë‹¤ë£¨ê³  ìˆëŠ” ë°ì´í„°ì—ì„œì˜ í†µê³„ì  í¸í–¥ --- ì €ëŠ” ì‹œë ¥êµì •ë³‘ì›ì˜ ì˜ë£Œ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì£¼ìš” ì—°êµ¬ ëª©ì ì€ í™˜ìë“¤ì—ê²Œ ë‹¤ì–‘í•œ ìˆ˜ìˆ  ë°©ë²•ì„ ì ìš©í–ˆì„ ë•Œì˜ ê²°ê³¼ë¥¼ ë¹„êµí•˜ì—¬, ìˆ˜ìˆ ì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¹ì—°íˆ í™˜ìë§ˆë‹¤ ëˆˆì˜ ìƒíƒœê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë™ì¼í•œ ìˆ˜ìˆ ì„ í•´ë„ ê²°ê³¼ëŠ” ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì´ëŸ¬í•œ ê°œì¸ì°¨ë¥¼ ê³ ë ¤í•˜ì§€ ì•Šê³  ìˆ˜ìˆ  íš¨ê³¼ë¥¼ í‰ê°€í•˜ë©´, ì•ì„œ ì„¤ëª…í•œ í¸í–¥ì´ ë°œìƒí•˜ì—¬ ì˜ëª»ëœ ê²°ë¡ ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ìˆ ë§ˆë‹¤ í™˜ìì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê³  ì´ í™˜ìë“¤ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê±´ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆë‹¤ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì´ëŠ” í˜¼ë™ í¸í–¥ (Confounding Bias)ì´ë¼ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™œ Confounding Biasì¸ê°€? êµ¬ì¡°: ì›ì¸ë³€ìˆ˜: ìˆ˜ìˆ  ë°©ë²• (Aìˆ˜ìˆ  vs Bìˆ˜ìˆ ) ê²°ê³¼ë³€ìˆ˜: ìˆ˜ìˆ  ê²°ê³¼ (ì‹œë ¥ ê°œì„  ì •ë„) í˜¼ë™ë³€ìˆ˜: í™˜ìì˜ ëˆˆ ìƒíƒœ (ê·¼ì‹œ ì •ë„, ê°ë§‰ ë‘ê»˜, ë‚˜ì´ ë“±) ë¬¸ì œì : ëˆˆ ìƒíƒœê°€ ìˆ˜ìˆ  ë°©ë²• ì„ íƒì— ì˜í–¥ì„ ë¯¸ì¹¨ ì˜ˆ: ê°ë§‰ì´ ì–‡ì€ í™˜ìëŠ” íŠ¹ì • ìˆ˜ìˆ ë§Œ ê°€ëŠ¥ ê·¼ì‹œê°€ ì‹¬í•œ í™˜ìëŠ” ë‹¤ë¥¸ ìˆ˜ìˆ  ë°©ë²• ì„ íƒ ëˆˆ ìƒíƒœê°€ ìˆ˜ìˆ  ê²°ê³¼ì—ë„ ì§ì ‘ ì˜í–¥ì„ ë¯¸ì¹¨ ì˜ˆ: ê°ë§‰ì´ ë‘êº¼ìš´ í™˜ìê°€ ì¼ë°˜ì ìœ¼ë¡œ ê²°ê³¼ê°€ ì¢‹ìŒ ì Šì€ í™˜ìê°€ íšŒë³µì´ ë¹ ë¦„ ê²°ê³¼: ëˆˆ ìƒíƒœë¥¼ í†µì œí•˜ì§€ ì•Šìœ¼ë©´, ìˆ˜ìˆ  ë°©ë²• ìì²´ì˜ íš¨ê³¼ê°€ ì•„ë‹Œ í™˜ì íŠ¹ì„±ì˜ ì°¨ì´ë¥¼ ìˆ˜ìˆ  íš¨ê³¼ë¡œ ì˜ëª» í•´ì„í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë¥¼ ì–´ë–»ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì„ê¹Œ? --- ìš”ì¦˜ ê°€ì¥ ë§ì€ ê³ ë¯¼ì„ í•˜ê³  ìˆëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. > ë…¼ë¦¬ì ìœ¼ë¡œ í¸í–¥ì´ ë°œìƒí•˜ëŠ” êµ¬ì¡°ì´ê³ , ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ì¸ê³¼ì¶”ë¡ ì„ ë„ì…í–ˆë‹¤. > ê·¸ë˜ì„œ ê²°ê³¼ëŠ”? ì–´ëŠì •ë„ ì¢‹ì•„ì¡ŒëŠ”ë°? ì–´ë–¤ ë°©ë²•ì„ ë„ì…í•œ í›„ì˜ ê²°ê³¼ë¥¼ ëª…í™•í•˜ê²Œ ë³´ì—¬ì¤˜ì•¼ í•˜ëŠ”ë°, ì´ë¥¼ ì–´ë–»ê²Œ ë³´ì—¬ì¤„ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ê³ ë¯¼ì…ë‹ˆë‹¤. ì œê°€ ë³´ê³  ìˆëŠ” ë°ì´í„°ì— ëŒ€í•œ ì„¤ëª…ì„ ì¶”ê°€ì ìœ¼ë¡œ í•˜ìë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ìˆ˜ìˆ ë°©ë²•ì˜ ì°¨ì´ : ë Œì¦ˆì‚½ì…ìˆ ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë Œì¦ˆ í¬ê¸° [12.1, 12.6, 13.2] í™˜ì ëˆˆì˜ ì¡°ê±´ : [ë‚˜ì´, ATA(ëˆˆì˜ ê°€ë¡œê¸¸ì´), ACD(ëˆˆì˜ ì„¸ë¡œê¸¸ì´), CLR(ìˆ˜ì •ì²´ì˜ ìƒíƒœ)] ìˆ˜ìˆ ë°©ë²• ë³„ë¡œ ëˆˆì˜ ì¡°ê±´ ë¶„í¬ë¥¼ í™•ì¸í•˜ê¸° --- <small>ì²˜ì¹˜ : ì¸ê³¼ì¶”ë¡ ì—ì„œ ì—°êµ¬ìê°€ ì¡°ì‘í•˜ëŠ” ë³€ìˆ˜(ì˜ˆ: ìˆ˜ìˆ  ë°©ë²•, ì•½ë¬¼ íˆ¬ì—¬). ì²˜ì¹˜ ê·¸ë£¹ê³¼ ëŒ€ì¡° ê·¸ë£¹ì„ ë¹„êµí•˜ì—¬ íš¨ê³¼ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.</small> > ì²˜ì¹˜(ìˆ˜ìˆ ë°©ë²•, ì‚½ì…í•˜ëŠ” ë Œì¦ˆí¬ê¸°)ë³„ë¡œ í™˜ì ëˆˆì˜ ë¶„í¬ê°€ ê°™ì€ê°€? <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„_Feature_DensityPlot.png\" alt=\"ì²˜ì¹˜ë³„ Feature Density Plot\"> <figcaption>ê·¸ë¦¼ 1: ì²˜ì¹˜ë³„ Feature ë¶„í¬ ë¹„êµ</figcaption> </figure> <figure> <img src=\"/post/what-is-statistical-bias/boxplot_grouped_by_size.png\" alt=\"Boxplot Grouped by Size\"> <figcaption>ê·¸ë¦¼ 2: í¬ê¸°ë³„ ê·¸ë£¹í™”ëœ ë°•ìŠ¤í”Œë¡¯</figcaption> </figure> ataì™€ clrì˜ ê²½ìš° ë¶„í¬ì˜ ì°¨ì´ê°€ ëª…í™•íˆ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. > ì²˜ì¹˜ë³„ í™˜ì ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤. ë”°ë¼ì„œ í™˜ì ëˆˆì˜ ì¡°ê±´ì„ í†µì œí•˜ì§€ ì•Šê³  ìˆ˜ìˆ  íš¨ê³¼ë¥¼ í‰ê°€í•˜ë©´ ì˜¤í•´ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì¦‰ ataì™€ clrì€ ì²˜ì¹˜ì™€ ê²°ê³¼ë³€ìˆ˜ ëª¨ë‘ì— ì˜í–¥ì„ ì£¼ëŠ” êµë€ë³€ìˆ˜ (confounding variable)ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¬ê³  ì²˜ì¹˜ë³„ ê²°ê³¼ ì°¨ì´ëŠ” ì´ êµë€ë³€ìˆ˜ì— ì˜í•´ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì²˜ì¹˜ë³„ê²°ê³¼ë¹„êµ.png\" alt=\"ì²˜ì¹˜ë³„ ê²°ê³¼ ë¹„êµ\"> <figcaption>ê·¸ë¦¼ 3: ì²˜ì¹˜ë³„ ê²°ê³¼ ë¶„í¬ ë¹„êµ</figcaption> </figure> > ë‹¨ìˆœíˆ ì²˜ì¹˜ë³„ ë¶„í¬ì˜ ì°¨ì´ë¥¼ í†µí•´ ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒë‹¨í•  ìˆ˜ ì—†ë‹¤. ê°™ì€ í¬ê¸°ì˜ ëˆˆì— 3ê°€ì§€ ì¢…ë¥˜ì˜ ë Œì¦ˆë¥¼ ëª¨ë‘ ë„£ì€ ê²°ê³¼ê°€ ìˆë‹¤ë©´, ì´ë¥¼ í†µí•´ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ë¦¼1ê³¼ 2ì—ì„œ í™•ì¸í•œ ê²ƒì²˜ëŸ¼ ë Œì¦ˆë³„ë¡œ ë‹¤ë¥¸ í¬ê¸°ì˜ ëˆˆì— ë Œì¦ˆë¥¼ ë„£ì–´ ê²°ê³¼ë¥¼ ì–»ì—ˆê¸° ë•Œë¬¸ì—, ì´ë¥¼ í†µí•´ì„œëŠ” ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ì¹˜ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì‹¤ì§ˆì ìœ¼ë¡œ ë¶„ì„í•˜ê¸° ìœ„í•´ì„œëŠ” í†µê³„ì  í¸í–¥ì„ ì œê±°í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì  --- ì˜ˆì¸¡ ëª¨ë¸ì€ ìƒê´€ê´€ê³„ì— ê¸°ë°˜í•´ íŒ¨í„´ì„ ì°¾ìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì´ë€? ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ ì´ë•Œ confounding variableì€ ì¸ê³¼ê´€ê³„ê°€ ì•„ë‹Œ ìƒê´€ê´€ê³„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ataê°€ ì»¤ì§ˆìˆ˜ë¡ ë Œì¦ˆë¥¼ ë” í° ê±¸ ì‚¬ìš©í–ˆê³ , ë”°ë¼ì„œ ê²°ê³¼ (vaulting)ê°€ ë” ì»¤ì§„ ê²½í–¥ì´ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ëª¨ë¸ì€ ataê°€ ì»¤ì§€ë©´ vaultingì´ ì»¤ì§„ë‹¤ê³  ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ìƒí™©ì—ì„œ ì˜ˆì¸¡ ëª¨ë¸ì— ê¸°ë°˜í•´ ì²˜ì¹˜ë¥¼ ê²°ì •í•˜ëŠ” ê±´ êµ‰ì¥íˆ ìœ„í—˜í•©ë‹ˆë‹¤. > ì²˜ì¹˜ì™€ ê²°ê³¼ë³€ìˆ˜ ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª¨ë¸ë§í•  í•„ìš”ê°€ ìˆê³ , ì´ë¥¼ í†µí•´ ì²˜ì¹˜ë¥¼ ê²°ì •í•  ë•Œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ìœ¼ë¡œ ì´ë¥¼ í•´ê²°í•˜ëŠ” ë°©ë²• --- > êµë€ë³€ìˆ˜ë“¤ì„ ì œê±°í•˜ê³  ì²˜ì¹˜ì™€ ê²°ê³¼ë³€ìˆ˜ ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª¨ë¸ë§í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰ ì²˜ì¹˜(T)ì™€ ê²°ê³¼(Y)ì— ëª¨ë‘ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ë³€ìˆ˜ë¥¼ $X$ë¼ê³  í–ˆì„ ë•Œ, $X$ê°€ $Y$ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œê±°í•˜ê³  $X$ê°€ $T$ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ì œê±°í•˜ê³  $Y$ì™€ $T$ ê°„ì˜ ì¸ê³¼ê´€ê³„ë¥¼ ëª¨ë¸ë§í•´ì•¼ í•©ë‹ˆë‹¤. êµë€ë³€ìˆ˜ë¥¼ ì œê±°í•˜ëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ê·¸ ì¤‘ì— í•˜ë‚˜ê°€ ì”ì°¨í™” ë°©ë²•ì…ë‹ˆë‹¤. FWL ì •ë¦¬ ì—ì„œ ì„¤ëª…í•œ ê²ƒì²˜ëŸ¼ ì”ì°¨í™” ë°©ë²•ì„ í†µí•´ êµë€ë³€ìˆ˜ë¥¼ í†µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì”ì°¨í™”ë¥¼ í•˜ê²Œ ë˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì‚¬ì´ì¦ˆë³„ì”ì°¨ë¶„í¬.png\" alt=\"ê²°ê³¼ì—ì„œ êµë€ë³€ìˆ˜ì— ëŒ€í•œ ì˜í–¥ ì œê±°í›„ ë¶„í¬\"> <figcaption>ê·¸ë¦¼ 4: ì”ì°¨í™”ë¥¼ í†µí•´ êµë€ë³€ìˆ˜ì— ëŒ€í•œ ì˜í–¥ì„ ì œê±°í•œ ê²°ê³¼ ë¶„í¬</figcaption> </figure> ê·¸ë¦¼3ê³¼ ë¹„êµí•˜ë©´, ì²˜ì¹˜(ë Œì¦ˆ í¬ê¸°)ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ êµë€ë³€ìˆ˜ë“¤ì˜ ì˜í–¥ì„ ì œê±°í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¼4ë¥¼ í†µí•´ ë Œì¦ˆ í¬ê¸°ê°€ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì”ì°¨í™”ê°€ ì˜ë˜ì—ˆëŠ”ì§€ëŠ” ì”ì°¨ê°€ ì •ê·œë¶„í¬ í˜•íƒœë¥¼ ë„ëŠ”ì§€ì™€ ì”ì°¨ê°€ êµë€ë³€ìˆ˜ì™€ ë…ë¦½ì¸ì§€ë¥¼ í™•ì¸í•˜ë©´ ë©ë‹ˆë‹¤. ì •ê·œë¶„í¬ í˜•íƒœë¥¼ ë„ê³  ìˆë‹¤ëŠ” ë¶€ë¶„ì€ ê·¸ë¦¼4ë¥¼ í†µí•´ í™•ì¸í•  ìˆ˜ ìˆê³ , êµë€ë³€ìˆ˜ì™€ ë…ë¦½ì¸ì§€ëŠ” êµë€ë³€ìˆ˜ë¡œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ì„ ë§Œë“¤ì–´ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. <figure> <img src=\"/post/what-is-statistical-bias/ì¡°ê±´ë¶€ë…ë¦½ì„±ê²€ì‚¬.png\" alt=\"êµë€ë³€ìˆ˜ë¡œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ê²°ê³¼\"> <figcaption>ê·¸ë¦¼ 5: êµë€ë³€ìˆ˜ë¡œ ì”ì°¨ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ê²°ê³¼</figcaption> </figure> ëª¨ë¸ì´ ì”ì°¨ë¥¼ ì „í˜€ ì˜ˆì¸¡í•˜ì§€ ëª»í•˜ëŠ” ê²ƒì„ í†µí•´ ë” ì´ìƒ Xì— ëŒ€í•œ ì •ë³´ê°€ Yì—ëŠ” í¬í•¨ë˜ì–´ìˆì§€ ì•Šë‹¤ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²°ê³¼ë¥¼ ì¢€ ë” ì§ê´€ì ì„ í•´ì„í•´ë³´ë©´ <figure> <img src=\"/post/what-is-statistical-bias/ê°™ì€ë Œì¦ˆí¬ê¸°_ê°™ì€ê²°ê³¼_ë‹¤ë¥¸ëˆˆ.png\" alt=\"ê°™ì€ë Œì¦ˆí¬ê¸°_ê°™ì€ê²°ê³¼_ë‹¤ë¥¸ëˆˆ\"> <figcaption>ê·¸ë¦¼ 6: ê°™ì€ ë Œì¦ˆ í¬ê¸°ì— ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆì§€ë§Œ ë‹¤ë¥¸ ëˆˆì„ ê°€ì§„ ê²½ìš°</figcaption> </figure> ê°™ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆê³  ê°™ì€ ì‚¬ì´ì¦ˆë¥¼ ì‚¬ìš©í–ˆê¸° ë•Œë¬¸ì—, ëˆˆì´ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë©´ ë Œì¦ˆ í¬ê¸°ì˜ ì˜í–¥ì„ ì œëŒ€ë¡œ íŒë‹¨í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. > ì§ê´€ì ìœ¼ë¡œ ë„ˆë¬´ë‚˜ë„ ë‹¹ì—°í•œ ë§ì´ì§€ë§Œ, ì•ì„œ ì„¤ëª…ë“œë¦° ê²ƒì²˜ëŸ¼ ì´ëŸ¬í•œ í†µê³„ì ì¸ í¸í–¥ë“¤ì„ ì œê±°í•˜ì§€ ì•Šê³  ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê²Œ ë˜ë©´ ì˜ëª»ëœ ê²°ê³¼ë¥¼ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì”ì°¨í™”í•˜ëŠ” ì½”ë“œëŠ” ì”ì°¨í™” ì½”ë“œ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì •ë¦¬ ì²˜ì¹˜ (ìˆ˜ìˆ ë°©ë²•, ë Œì¦ˆí¬ê¸°) ë³„ë¡œ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ìˆ˜ìˆ ì„ ë°›ì€ í™˜ìì˜ ëˆˆì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ìˆ˜ìˆ  ê²°ê³¼ë¥¼ ë¹„êµí•˜ëŠ” ê²ƒì€ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì •í™•íˆ ìˆ˜ìˆ  ê²°ê³¼ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•´ì„œëŠ” ê²°ê³¼ì—ì„œ êµë€ë³€ìˆ˜ì˜ ì˜í–¥ (í†µê³„ì  í¸í–¥) ì„ ì œê±°í•œ í›„ ë¹„êµí•´ì•¼ í•©ë‹ˆë‹¤."
  },
  {
    "id": "Reflections/2025-07-08-optimization-pitfall",
    "title": "ìµœì í™”ì˜ í•¨ì •ì— ëŒ€í•˜ì—¬",
    "date": "2025-07-08",
    "excerpt": "íŒ¨ë°°ì£¼ì˜ì—ì„œ ë²—ì–´ë‚˜ì",
    "category": "Reflections",
    "tags": [
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "ë°°ê²½ ì €ëŠ” ê½¤ë‚˜ ì˜¤ëœì‹œê°„ ì¼ì¢…ì˜ íŒ¨ë°°ì£¼ì˜ì—ì„œ ë²—ì–´ë‚˜ì§€ ëª»í•˜ê³  ê³ í†µìŠ¤ëŸ¬ì›Œí•˜ë©° ì‹œê°„ì„ ë³´ëƒˆìŠµë‹ˆë‹¤. ì €ë³´ë‹¤ í›¨ì”¬ ë›°ì–´ë‚œ ì‚¬ëŒë“¤ì„ ë³´ë©° ë‚˜ëŠ” ì €ëŸ° ê¸¸ì„ ê±¸ì–´ì˜¤ì§€ ì•Šì•˜ìœ¼ë‹ˆê¹Œ ì•ˆë  ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ë“¤ì— ì‚¬ë¡œì¡í˜€ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì‚¬ê³ ë°©ì‹ì„ ì–´ë–»ê²Œ ì •ì˜í•  ìˆ˜ ìˆì„ì§€ ëª°ëëŠ”ë°, \"ë‰´ìš•í„¸ê²Œ\"ë‹˜ì˜ ì˜ìƒë“¤ì„ ë³´ë©´ì„œ \"ìµœì í™”ì˜ í•¨ì •\"ì— ë¹ ì¡Œë‹¤ê³  ì •ì˜í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ìµœì í™”ì˜ í•¨ì •ì´ë€? ì¸ìƒì—ëŠ” ì–´ë–¤ ìµœì í™”ëœ ê¸¸ì´ ìˆê³ , ì´ ê¸¸ì„ ë”°ë¼ê°€ì•¼í•˜ë©°, ë”°ë¼ê°€ì§€ ëª»í•œ ë‚˜ëŠ” íŒ¨ë°°ì¬ìë¼ê³  ìƒê°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ìµœì í™”ëœ ê¸¸ì„ ì°¾ìœ¼ë ¤ê³  í•˜ë©´ì„œ ì™„ë²½ì£¼ì˜ì— ë¹ ì§€ê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì™„ë²½í•œ ê³„íšì— ëŒ€í•´ ê³ ë¯¼í•˜ë©´ì„œ ì‹œê°„ì„ ë³´ë‚´ê³ , ë§‰ìƒ ì•ìœ¼ë¡œ í•œë°œìêµ­ë„ ë‚˜ì•„ê°€ì§€ ëª»í•˜ëŠ” ìƒí™©ì— ë¹ ì§€ê²Œ ë©ë‹ˆë‹¤. ë‰´ìš•í„¸ê²Œë‹˜ ì˜ìƒ ë§í¬ >ê³„íšì´ ì›ëŒ€í•´ì§€ë©´ ë‚´ í•˜ë£¨ëŠ” ë¹„ì°¸í•´ì§„ë‹¤. ê°€ì¥ ì¸ìƒê¹Šì€ ë§ì´ì, ì œ ê³¼ê±° ì‚¶ì„ ê´€í†µí•˜ëŠ” ë§ì…ë‹ˆë‹¤. ë‰´ìš•í„¸ê²Œë‹˜ì˜ ì‚¬ê³ ë°©ì‹ ë‰´ìš•í„¸ê²Œë‹˜ì´ ë§ì”€í•˜ì‹œëŠ” ë¶€ë¶„ë“¤ì„ ì œ ì‚¶ì— ì •ë§ ë§ì€ ë„ì›€ì´ ë˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ê°€ì§€ ê°œë…ë“¤ì„ ë§ì”€í•´ì£¼ì‹œëŠ”ë°, ê³µí†µì ì¸ ê°œë…ì€ \"ìˆ˜ìš©ì˜ ìì„¸\"ê°€ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì´ëŠ” ì œê°€ ì¢‹ì•„í•˜ëŠ” \"ë²•ë¥œìŠ¤ë‹˜\"ì˜ ë§ì”€ê³¼ ë¹„ìŠ·í•œ ì ì´ ë§ìŠµë‹ˆë‹¤. ê³¼ê±°, ì–´ì©Œë©´ ì§€ê¸ˆë„ ì œê°€ ê³ í†µë°›ëŠ” ì´ìœ ë¥¼ ìƒê°í•´ë³´ë©´ ì œê°€ ì›í•˜ëŠ” ì´ìƒì ì¸ ìƒì„ ê·¸ë ¤ë†“ê³  ì´ê²ƒê³¼ ë¹„êµí•˜ë©´ì„œ ì œ í•˜ë£¨í•˜ë£¨ë¥¼ ë¹„ì°¸í•˜ê²Œ ìƒê°í–ˆê¸° ë•Œë¬¸ì´ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì—­ì„¤ì ìœ¼ë¡œ ì œ ì‚¶ì„ ë¶€ì •í•˜ê³  ì›ëŒ€í•œ ê³„íšì„ ì„¸ìš¸ìˆ˜ë¡ ì‚¶ì€ ë‚˜ì•„ì§€ì§€ ì•ŠëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê·¸ì € ë¹„ì°¸í•œ í•˜ë£¨ë¥¼ ë³´ë‚´ë©° ì‹œê°„ì„ ë³´ë‚´ê¸°ë§Œ í•  ë¿ì…ë‹ˆë‹¤. ê·¸ë˜ì„œ ì´ ìˆ˜ìš©ì˜ ìì„¸ëŠ” ì‚¶ì„ ì‚´ì•„ê°€ëŠ”ë° êµ‰ì¥íˆ ì¤‘ìš”í•œ ìì„¸ê°€ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. ì§€ê¸ˆ ë‚´ ëª¨ìŠµì´ ë‚´ê°€ ìƒìƒí–ˆë˜ ëª¨ìŠµì€ ì•„ë‹ˆì§€ë§Œ, ì¶©ë¶„íˆ í›Œë¥­í•˜ë‹¤. 1ì¸ë¶„ì˜ ì‚¶ì„ ì˜¨ì „íˆ ì‚´ì•„ë‚˜ê°€ê³  ìˆê³  ì¡°ê¸ˆì”© ë‚˜ì•„ê°€ê³  ìˆë‹¤. ì´ë ‡ê²Œ ì§€ê¸ˆ ì‚¶ì„ ì˜¨ì „íˆ ìˆ˜ìš©í•  ë•Œ, ë¹„ë¡œì†Œ ë°œì „í•  ìˆ˜ ìˆëŠ” ë‚´ê°€ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë‚˜ì˜ ì‚¶ì— ì ìš©í•´ë³´ê¸° ì›í•˜ëŠ” íšŒì‚¬ì— ì§€ì›í•˜ê³  ìˆì§€ë§Œ ê³„ì†í•´ì„œ ë–¨ì–´ì§€ê³  ìˆìŠµë‹ˆë‹¤. ë‚˜ì˜ ì´ìƒì ì¸ ìƒê³¼ ë¹„êµí•˜ë©° ë‚˜ëŠ” ì•ˆë˜ëŠ” ì¸ìƒì¸ê°€ë³´ë‹¤ í•˜ë©° ê³ ì •ì ì´ê³  íŒ¨ë°°ì ì¸ ì‚¬ê³ ë°©ì‹ì„ ê°€ì§ˆ ìˆ˜ë„ ìˆì§€ë§Œ, ì§€ê¸ˆì˜ ìƒí™©ì„ ìˆ˜ìš©í•˜ê³  ì•ìœ¼ë¡œ ë‚˜ì•„ê°€ê¸° ìœ„í•œ ë°©ë²•ì„ ì°¾ì•„ë³´ëŠ”ê²Œ ë” ì¢‹ì•„ ë³´ì…ë‹ˆë‹¤. ì´ ìˆ˜ìš©ì˜ ìì„¸ë¥¼ ë– ì˜¬ë¦¬ë©° ì‚¶ì„ ì‚´ì•„ê°€ë³´ë ¤ í•©ë‹ˆë‹¤."
  },
  {
    "id": "Reflections/2025-07-08-how-to-write-resume",
    "title": "ì´ë ¥ì„œëŠ” ì–´ë–»ê²Œ ì ì–´ì•¼í• ê¹Œ?",
    "date": "2025-07-08",
    "excerpt": "ì´ë ¥ì„œë¥¼ ì ìœ¼ë©° í–ˆë˜ ë‚˜ì˜ ê³ ë¯¼ë“¤",
    "category": "Reflections",
    "tags": [
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "ë‚˜ì˜ ë°°ê²½ ì¼í•œì§€ 1ë…„ì´ ì¡°ê¸ˆ ë„˜ëŠ” ì‹œì ì—ì„œ ì´ì§ì„ ì¤€ë¹„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì´ë ¥ì„œë¥¼ ì¤€ë¹„í•˜ë©° ë‚˜ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•˜ë©´ ì¢‹ì„ì§€ì— ëŒ€í•œ ê³ ë¯¼ë“¤ì„ í•˜ê³  ìˆìŠµë‹ˆë‹¤. ëª¨ë¥´ëŠ” ë‚´ìš©ì„ ê³µë¶€í•˜ê³  ì¢‹ì€ ì½”ë“œë¥¼ ì‘ì„±í•˜ê³  ì¢‹ì€ ëª¨ë¸ì„ ë§Œë“œëŠ” ì¼ë“¤ì€ ì–´ë µì§€ë§Œ ê·¸ ê¸¸ì´ ë‚˜ë¦„ ëª…í™•í•©ë‹ˆë‹¤. ì´ëŸ° ê³¼ì •ì„ ì ì–´ë„ 5ë…„ì´ìƒ ê±°ì³ì™”ìœ¼ë‹ˆ ìì‹ ì´ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ë‚˜ë¥¼ í‘œí˜„í•˜ëŠ” ë¶€ë¶„ë“¤ì€ ì •ë§ ì–´ë µê²Œ ëŠê»´ì§‘ë‹ˆë‹¤. ë‚˜ëŠ” ì§„ì§œ ê²½ìŸë ¥ì´ ìˆëŠ”ì§€ì— ëŒ€í•œ ì˜êµ¬ì‹¬ë¶€í„°, ë‚´ê°€ í–ˆë˜ ì¼ë“¤ì„ ì–´ë””ì„œë¶€í„° ì–´ë””ê¹Œì§€ ì„¤ëª…í•´ì•¼í•˜ëŠ”ì§€ ë“±ë“± ì´ëŸ° ê³¼ì •ì—ì„œ í–ˆë˜ ê³ ë¯¼ë“¤ì„ í•˜ë‚˜ì”© ì •ë¦¬í•´ë´…ë‹ˆë‹¤. ë‚´ ì´ë ¥ì„œì— ëŒ€í•œ í”¼ë“œë°± ì‚¬ì´ë“œí”„ë¡œì íŠ¸ë¥¼ ê°™ì´í•˜ëŠ” ê°œë°œìë¶„ > ì„íŒ©íŠ¸ê°€ ì—†ë‹¤. ì´ë ¥ì„œë¥¼ ë” ì§§ê²Œ ì“°ê³  í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ í’ë¶€í•˜ê²Œ ê°€ì ¸ê°€ëŠ”ê²Œ ì¢‹ê² ë‹¤. ì´ í”¼ë“œë°±ì— ë™ì˜í•˜ëŠ” ë¶€ë¶„ì´ ë§ì•˜ìŠµë‹ˆë‹¤. ë‚´ê°€ í•œ í”„ë¡œì íŠ¸ì˜ ê²°ê³¼ë¥¼ ìˆ˜ì¹˜ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ”ë°ì—ë§Œ ì§‘ì¤‘í–ˆì§€, ì§„ì§œ ì–´ë–¤ ë¬¸ì œë¥¼ í’€ì–´ì„œ ì–´ë–¤ ì˜í–¥ì„ ì¤¬ëŠ”ì§€ì— ëŒ€í•œ ë‚´ìš©ì´ ë¶€ì¡±í•œ ê²ƒ ê°™ë‹¤ê³  ëŠê¼ˆìŠµë‹ˆë‹¤. íŠ¹íˆ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë”°ë¡œ ì¤€ë¹„í•˜ì§€ ì•Šì•˜ë˜ ë¶€ë¶„ì€ ê¼­ ìˆ˜ì •ì´ í•—ìš”í•˜ë‹¤ê³  ëŠê»´ì§‘ë‹ˆë‹¤."
  },
  {
    "id": "Recommendation/2025-07-08-thoughts-on-review-quality",
    "title": "ë¦¬ë·° ë…¸ì´ì¦ˆì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2025-07-08",
    "excerpt": "ì¹´ì¹´ì˜¤ë§µ ë¦¬ë·° ë°ì´í„°ì˜ ë…¸ì´ì¦ˆë“¤ì— ëŒ€í•˜ì—¬",
    "category": "Recommendation",
    "tags": [
      "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì •ë¦¬",
      "ì¶”ì²œì‹œìŠ¤í…œ",
      "ì¶”ì²œì‹œë‚˜ë¦¬ì˜¤"
    ],
    "content": "ë°°ê²½ ë°ì´í„°ê³¼í•™ìë¡œ 1ë…„ ì¡°ê¸ˆ ë„˜ê²Œ ì¼í•˜ë©´ì„œ ê°€ì¥ í¬ê¸° ëŠë¼ëŠ” ë¶€ë¶„ì€, ë°ì´í„°ì˜ ì¤‘ìš”ì„±ì…ë‹ˆë‹¤. ëª¨ë¸ì€ ë°ì´í„° ì•ˆì—ì„œ íŒ¨í„´ì„ ì°¾ì„ ë¿, ê·¸ ì•ˆì— íŒ¨í„´ì´ ì—†ê±°ë‚˜ ë…¸ì´ì¦ˆê°€ í¬ë©´ ëª¨ë¸ì€ ì œ ì—­í• ì„ ëª»í•˜ê²Œ ëœë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ê·¸ë˜ì„œ ë°ì´í„°ë¥¼ ëœ¯ì–´ë³´ê³  ê·¸ ì•ˆì˜ ë…¸ì´ì¦ˆë¥¼ ì‚´í´ë³´ëŠ” ì¼ì€ êµ‰ì¥íˆ ì¤‘ìš”í•œ ì¼ì…ë‹ˆë‹¤. > ë¦¬ë·° ë°ì´í„°ì—ëŠ” ì–´ë–¤ ë…¸ì´ì¦ˆê°€ ìˆëŠ”ì§€ ì‚´í´ë³´ê³  ì´ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤ <small>ê°™ì´ ì‚¬ì´ì¦ˆ í”„ë¡œì íŠ¸ë¥¼ í•˜ëŠ” 'ì‹ ë³´í˜„'ë‹˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ì •ë¦¬í—€ìŠµë‹ˆë‹¤.</small> ë¬¸ì œ ìƒí™©ë“¤ ì—¬ëŸ¬ ë…¸ì´ì¦ˆë“¤ì´ ì¡´ì¬í•˜ëŠ”ë° ê·¸ ì¤‘ ê°€ì¥ ì‹¬ê°í•˜ë‹¤ê³  ëŠë¼ëŠ” ë¶€ë¶„ë“¤ì— ëŒ€í•´ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ì¥ì†Œ(ìŒì‹ì )ê³¼ ìƒê´€ì—†ëŠ” ì´ìœ ì¸ ë¶€ì •ì /ê¸ì •ì ì¸ ë¦¬ë·°ë“¤ ê°€ì¥ ëˆˆì— ëˆ ë¶€ë¶„ì€ ê³„ì—„ê³¼ ê·¸ í›„ì˜ ì‹œìœ„ë“¤ê³¼ ê´€ë ¨ëœ ë¦¬ë·°ë“¤ì…ë‹ˆë‹¤. ê³„ì—„ì„ ì°¬ì„±í•˜ê±°ë‚˜ ë°˜ëŒ€í•  ê²½ìš° ê´€ë ¨ëœ ì‚¬ëŒë“¤ì´ 'ëŒ“ê¸€ í…ŒëŸ¬'ë¥¼ í•˜ëŠ” ê²½ìš°ë¥¼ ë°œê²¬í–ˆìŠµë‹ˆë‹¤. ê·¸ ì™¸ì—ë„ ì œê°€ ë‹¤ë‹ˆê³  ìˆëŠ” í—¬ìŠ¤ì¥ì— ì—¬ì íŠ¸ë ˆì´ë„ˆê°€ ëš±ëš±í•˜ë‹¤ëŠ” ì´ìœ ë¡œ ë¶€ì •ì ì¸ ë¦¬ë·°ë¥¼ ë‚¨ê¸°ëŠ” ê²½ìš°ë„ ìˆì—ˆìŠµë‹ˆë‹¤. ë§ˆì¼€íŒ… ëª©ì ìœ¼ë¡œ ì‘ì„±ëœ ë¦¬ë·°ë“¤ ë§ˆì¼€íŒ…ì„ ìœ„í•´ ì˜ë„ì ìœ¼ë¡œ ì‘ì„±ëœ ë¦¬ë·°ë“¤ì…ë‹ˆë‹¤. ì˜ˆì „ì— ì•Œë°”í–ˆë˜ ìŒì‹ì ì—ì„œë„ ì´ëŸ° ë§ˆì¼€íŒ…ì„ ì§„í–‰í–ˆì—ˆìŠµë‹ˆë‹¤. AIë¡œ ì‘ì„±í•´ì„œ ë¬¸ì²´ì— í‹°ê°€ ë‚˜ëŠ” ê²½ìš°ë¼ë©´ ë‹¤ë¥´ê²Œ ì ‘ê·¼í•  ìˆ˜ë„ ìˆê² ì§€ë§Œ, ì‚¬ëŒì´ ì‘ì„±í•œ ë“¯í•œ ëŒ“ê¸€ì´ ë§ì•„ì„œ ì´ë¥¼ ì–´ë–»ê²Œ ê±°ë¥¼ ìˆ˜ ìˆëŠ”ì§€ë„ ê³ ë¯¼í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ? ì´ëŸ° ë¶€ë¶„ì„ ìƒì‡„í• ë§Œí¼ ë¦¬ë·°ê°€ ë§ìœ¼ë©´ ì´ê²Œ ìƒì‡„ê°€ ë˜ëŠ”ì§€, í˜¹ì€ ëª¨ë¸ë¡œ ì´ë¥¼ ê·¹ë³µí•  ìˆ˜ ìˆì„ì§€. ë°ì´í„°ë¥¼ í•„í„°ë§í•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ë“¤ì„ ê³ ë¯¼í•´ì•¼í• ì§€ì— ëŒ€í•œ ë…¼ì˜ê°€ ì¶”ê°€ë¡œ í•„ìš”í•œ ìƒí™©ì…ë‹ˆë‹¤. ì´ ë¶€ë¶„ë“¤ì— ëŒ€í•œ ë‚´ìš©ë„ ì¶”ê°€ë¡œ ì •ë¦¬í•  ì˜ˆì •ì…ë‹ˆë‹¤."
  },
  {
    "id": "Recommendation/2025-07-08-thoughts-on-deciding-scenario",
    "title": "ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì •í•˜ê¸° ìœ„í•œ ê³ ë¯¼",
    "date": "2025-07-08",
    "excerpt": "ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ëŠ” ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©´ì„œ ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤ëŠ” ì–´ë–»ê²Œ ì •í•˜ë©´ ì¢‹ì„ì§€ì— ëŒ€í•œ ë…¼ì˜ë¥¼ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.",
    "category": "Recommendation",
    "tags": [
      "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì •ë¦¬",
      "ì¶”ì²œì‹œìŠ¤í…œ",
      "ì¶”ì²œì‹œë‚˜ë¦¬ì˜¤"
    ],
    "content": "ë°°ê²½ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ë©° ì¶”ì²œ ì„œë¹„ìŠ¤ë¥¼ êµ¬í˜„í•´ë³´ê³  ìˆìŠµë‹ˆë‹¤. êµ¬í˜„í•˜ë©´ì„œ í•„ìš”í•œ ì—¬ëŸ¬ ê³ ë¯¼ ì¤‘ í•˜ë‚˜ëŠ” ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–»ê²Œ ì¶”ì²œì„ í•´ì¤„ ê²ƒì¸ì§€ì— ëŒ€í•œ ê³ ë¯¼ì…ë‹ˆë‹¤. ì´ë¥¼ ì¶”ì²œ ì‹œë‚˜ë¦¬ì˜¤ë¼ê³  ì •ì˜í•˜ê³  ì´ë¥¼ êµ¬ì²´í™”í•˜ëŠ” ê³¼ì •ì— ìˆìŠµë‹ˆë‹¤. ì½œë“œ ìœ ì €ì— ëŒ€í•´ ì–´ë–»ê²Œ ì¶”ì²œí•  ê²ƒì¸ê°€ ì„œë¹„ìŠ¤ë¥¼ ìƒˆë¡­ê²Œ ë§Œë“¤ê²Œ ëœë‹¤ë©´ ì´ ì„œë¹„ìŠ¤ë¥¼ ì‚¬ìš©í•˜ëŠ” ìœ ì €ëŠ” ì½œë“œ ìœ ì €ì¼ ê²ƒì…ë‹ˆë‹¤. ì´ì— ëŒ€í•´ ì–´ë–»ê²Œ ëŒ€ì²˜í•  ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ë…¼ì˜ê°€ ì£¼ëœ ë…¼ì˜ì˜€ìŠµë‹ˆë‹¤. ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œ + í•„í„°ë§ ê°€ì¥ ëŒ€í‘œì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. ì—¬ê¸°ì— ì¶”ê°€ë¡œ ë‚ ì”¨ë¼ë˜ì§€, ìœ„ì¹˜ë¼ë˜ì§€, íŠ¹ì • í•„í„°ë§ì„ ë¶™ì´ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ìœ ì € í”„ë¡œí•„ ì™„ì„± (ì˜¨ë³´ë”© ê¸°ë°˜ ì¶”ì²œ) ìœ ì €ì—ê²Œ ì •ë³´ë¥¼ ì™„ì„±í•˜ë„ë¡ ìš”ì²­í•˜ê³ , ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì„ í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ëŒ€í‘œì ìœ¼ë¡œ ì™“ì± ë‚˜ ë„·í”Œë¦­ìŠ¤ì˜ ê²½ìš°, ì²˜ìŒì— ë³¸ì¸ì´ ì¢‹ì•„í•˜ëŠ” ì˜í™”ë¥¼ ì„ íƒí•˜ë„ë¡ êµ¬ì„±í•˜ê³  ê·¸ í›„ ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¶”ì²œì„ í•´ì¤ë‹ˆë‹¤. ì´ëŸ°ì‹ìœ¼ë¡œ ìœ ì € ì •ë³´ë¥¼ ë°›ì„ ìˆ˜ ìˆëŠ” ì„œë¹„ìŠ¤ì— ëŒ€í•œ ë…¼ì˜ë¥¼ í–ˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Recommendation/2025-07-08-cold-start-solution",
    "title": "Cold Start í•´ê²° ë°©ë²•ì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2025-07-08",
    "excerpt": "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì—ì„œ ì§„í–‰í•œ cold start í•´ê²°ë°©ì•ˆê³¼ ë°ì´í„°ì˜ í•œê³„ì  ê·¹ë³µ ê³ ë¯¼",
    "category": "Recommendation",
    "tags": [
      "ì‚¬ì´ë“œí”„ë¡œì íŠ¸ì •ë¦¬",
      "ì¶”ì²œì‹œìŠ¤í…œ"
    ],
    "content": "ë°°ê²½ ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ \"ì©ì©LAB\"ì´ë¼ëŠ” ì´ë¦„ìœ¼ë¡œ ì§„í–‰í•œ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ì— ëŒ€í•œ ì •ë¦¬ì…ë‹ˆë‹¤. ë§›ì§‘ ì¶”ì²œ ì‹œìŠ¤í…œì„ êµ¬í˜„í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì—¬ëŸ¬ ê³¼ì œ ì¤‘ Cold Startë¥¼ ì–´ë–»ê²Œ í•´ê²°í• ì§€ì— ëŒ€í•´ ë…¼ì˜í•œ ë‚´ìš©ë“¤ì„ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤. Cold Start Problemì´ë€? > ì‚¬ìš©ìë‚˜ ì•„ì´í…œì— ëŒ€í•œ ì •ë³´ê°€ ì—†ê±°ë‚˜ í¬ì†Œí•œ ë¬¸ì œ ì´ ì¤‘ì—ì„œ ìœ ì €ì— ëŒ€í•œ cold start ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í’€ì§€ì— ëŒ€í•´ ê³ ë¯¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. <small>ê°™ì´ ì‚¬ì´ì¦ˆ í”„ë¡œì íŠ¸ë¥¼ í•˜ëŠ” 'ì´ìœ¤ì„ 'ë‹˜ì˜ ë¶„ì„ ê²°ê³¼ë¥¼ ì°¸ê³ í•´ ì •ë¦¬í—€ìŠµë‹ˆë‹¤.</small> ê¸°ë³¸ì ì¸ ì¶”ì²œì˜ ì»¨ì…‰ > Popularity Modelì—ì„œ Contextë¥¼ ë°˜ì˜í•˜ì—¬, ìœ ì €ê°€ ë§Œì¡±í• ë§Œí•œ ìŒì‹ì„ ì¶”ì²œí•´ì£¼ê³  ì‹¶ë‹¤. ì½œë“œ ìœ ì €ì—ê²Œ ì¸ê¸°ë„ ê¸°ë°˜ ì¶”ì²œì„ ë‚´ì£¼ëŠ” ê²ƒì²˜ëŸ¼, ê³„ì ˆê³¼ ë‚ ì”¨ë¥¼ ê³ ë ¤í•´ ì¶”ì²œì„ ë‚´ì£¼ë©´ ì¢‹ì„ ê²ƒ ê°™ë‹¤ëŠ” ì•„ì´ë””ì–´ì…ë‹ˆë‹¤. (ì˜ˆì‹œ) ìƒí™© ìœ ì €ì˜ ìƒê° ì¶”ì²œ ê°€ëŠ¥ ìŒì‹ --- --- --- ë§‘ê³  ì²­ëª…í•œ ë‚  â€œë°–ì— ë‚˜ê°€ì„œ ë¨¹ê³  ì‹¶ì–´â€ ìƒŒë“œìœ„ì¹˜, ê¹€ë°¥, ë¶„ì‹ íë¦¬ê³  ìŠµí•œ ë‚  â€œë­”ê°€ ì‹œì›í•œ ê²Œ ë•¡ê¸´ë‹¤â€ ëƒ‰ë©´, ë¬¼íšŒ, ì•„ì´ìŠ¤ì»¤í”¼ ë¹„ ì˜¬ ê²ƒ ê°™ì€ ë‚  â€œì§‘ì— ì¼ì° ê°€ê³  ì‹¶ì–´â€ êµ­ë¬¼ ìš”ë¦¬, ì¹¼êµ­ìˆ˜, í•´ì¥êµ­ ê²¨ìš¸ì²  â€œëŒ€ê²Œê°€ ì œì² ì´ë„¤?â€ ëŒ€ê²Œì°œ, ì–´íƒ•êµ­ìˆ˜, ì „ê³¨ë¥˜ ê³„ì¡€ ë‚ ì”¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— í¬í•¨ì‹œì¼œì•¼ í•˜ëŠ” ì´ìœ ë¡œ 3ê°€ì§€ë¥¼ ì œì‹œí–ˆìŠµë‹ˆë‹¤. ì™œ ê³„ì ˆ ë‚ ì”¨ ë°ì´í„°ë¥¼ ëª¨ë¸ì— í¬í•¨í•´ì•¼ í• ê¹Œ? ê°œì¸í™” ì¶”ì²œ ê°•í™” â†’ ë™ì¼í•œ ìœ ì €ë„ ë‚ ì”¨ì— ë”°ë¼ ì„ íƒì´ ë‹¬ë¼ì§ â†’ â€˜ìœ ì € + ë‚ ì”¨â€™ ì¡°í•© ê¸°ë°˜ì˜ ë” ë˜‘ë˜‘í•œ ì¶”ì²œ ê°€ëŠ¥ ëª¨ë¸ì˜ ì •ë°€ë„ í–¥ìƒ â†’ ê¸°ì¡´ ëª¨ë¸ì— ì»¨í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì¶”ê°€í•¨ìœ¼ë¡œì¨ ì˜ˆì¸¡ ì •í™•ë„ í–¥ìƒ Cold Start ìƒí™©ì—ì„œë„ ê°•ë ¥í•œ ë³´ì™„ â†’ ìœ ì € ì •ë³´ê°€ ì—†ì„ ë•Œë„, ê·¸ë‚ ì˜ ë‚ ì”¨ + ì¸ê¸° ë©”ë‰´ë¡œ í•©ë¦¬ì ì¸ ì¶”ì²œ ê°€ëŠ¥ ì—¬ê¸°ì— ì¶”ê°€ë¡œ ì €ëŠ” ì¶”ì²œì˜ ê·¼ê±°ë¥¼ ì œì‹œí•´ì¤„ ìˆ˜ ìˆê¸° ë•Œë¬¸ì—, ì‚¬ìš©ìê°€ ì¶”ì²œì˜ ê²°ê³¼ë¥¼ ë” ì‹ ë¢°í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. êµ¬í˜„ìƒí™©ì—ì„œì˜ ë¬¸ì œì , ë°ì´í„°ì˜ í•œê³„ ì‚¬ìš©í•˜ê³  ìˆëŠ” ë°ì´í„°ëŠ” ì¹´ì¹´ì˜¤ë§µì„ í¬ë¡¤ë§í•˜ì—¬ ë§Œë“  ë°ì´í„°ì…ë‹ˆë‹¤. ì¹´ì¹´ì˜¤ë§µì— ìˆëŠ” ë¦¬ë·°ëŠ” ì‚¬ìš©ìê°€ ì„œë¹„ìŠ¤ë¥¼ ì´ìš©í•œ í›„ì— ë°”ë¡œ ì‘ì„±í•˜ëŠ”ê²Œ ì•„ë‹ˆê¸° ë•Œë¬¸ì—, ì‘ì„± ì‹œì ê³¼ ì‹¤ì œ ì‚¬ìš©í•œ ì‹œì ì´ ë‹¤ë¦…ë‹ˆë‹¤. ë”°ë¼ì„œ ì‚¬ìš©í•œ ì‹œì ì˜ ë‚ ì”¨ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ í™•ì¸í•˜ê¸° ìœ„í•œ EDA ê²°ê³¼ë¥¼ ë°œí‘œìë‹˜ê»˜ì„œ ë°œí‘œí•´ì£¼ì…¨ìŠµë‹ˆë‹¤. > ë¦¬ë·° ë‚ ì§œì™€ ë°©ë¬¸ë‚ ì§œê°€ ê°™ë‹¤ëŠ” ê°€ì •ì„ íƒ€ë‹¹í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•œ ë°©ë²• ë¦¬ë·°ì–´ ì¤‘ì— ë§¤ì¼ ë¦¬ë·°ë¥¼ ì“°ëŠ” ì‚¬ëŒë§Œ ê³ ë¥´ì > ì‘ì„±í•œ ë¦¬ë·°ë“¤ì˜ ì¼ìì˜ ì°¨ì´ê°€ í‰ê· ì ìœ¼ë¡œ 1ì¼ì •ë„ ë‚˜ëŠ” ìœ ì €, ì¦‰ ë§¤ì¼ ë¦¬ë·°ë¥¼ ì‘ì„±í•˜ëŠ” ë¦¬ë·°ì–´ë“¤ì„ í†µí•´ ë°ì´í„°ë¥¼ í†µí•´ ì´ë¥¼ í•´ê²°í•˜ì ì¦‰ ë¦¬ë·° ë‚ ì§œì™€ ë°©ë¬¸ë‚ ì§œê°€ ê°™ë‹¤ê³  ì¶”ì •ë˜ëŠ” ìœ ì €ë§Œ í•„í„°ë§í•˜ëŠ” ë°©ë²•ì„ ì„ íƒí•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ë°ì´í„°ì˜ í•œê³„ë¥¼ ì–´ëŠì •ë„ëŠ” ê·¹ë³µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "English Study/SPEAK/ì§€ë‚˜ê°„ë‹¬/07-08-ì¹­ì°¬í•˜ëŠ”í‘œí˜„",
    "title": "ì¹­ì°¬í•˜ëŠ” í‘œí˜„",
    "date": "2025-07-08",
    "excerpt": "I was really impressed, It couldn`t be any better, I couldn`t have done it without you",
    "category": "English Study",
    "tags": [
      "ì˜ì–´ê³µë¶€ê¾¸ì¤€íˆ!",
      "SPEAK",
      "ì¹­ì°¬í•˜ëŠ” í‘œí˜„ë“¤"
    ],
    "content": "ì˜¤ëŠ˜ ë°°ìš´ê±° I was really impressed I was really impresed with your exhibit It couldnt be any better I couldnt have done it without you I couldnt have graduate without you ì´ê±° ì‚¬ìš©í•´ì„œ ê°„ë‹¨í•œ ì‘ë¬¸ Today, I attend side-project ëª¨ì„. We talked about recommmend system. The topic was, how to delete data noise, how to check 'ê°€ì •', distribute tasks for project. After that ë™ë£Œ ,who í”„ë¡œì íŠ¸ë¥¼ ê°™ì´í•˜ëŠ” ì‚¬ëŒ, give me advice. Because nowadays I fall ì„œë¥˜íƒˆë½, so I ask them how can I improve my ì´ë ¥ì„œ. I was really impressed with their advice. They ê³ ë¯¼í•˜ë‹¤ for me. How to improve my resume. It couldnt be any better. I was so ê°ë™ë°›ë‹¤. I always appreciate this project. They gave me so many ë„ì›€. I couldnt have be better person without them. ë¬¸ì¥ìˆ˜ì • (gemini-2.5-pro) Today, I attended a side-project meeting. We talked about recommendation systems. The main topics were how to remove noisy data, how to test our assumptions, and how to distribute tasks for the project. After the meeting, a teammate who is working on the project with me gave me some advice. Recently, I've been getting rejected at the resume screening stage, so I asked them for tips on how to improve my resume. I was really impressed with their advice. They thoughtfully considered how I could improve my resume. It couldnâ€™t have been any better. I was so moved. I always appreciate being a part of this project. They have been so helpful to me. I couldnâ€™t have become a better person without them. í”¼ë“œë°± (gemini-2.5-pro) ì „ë°˜ì ìœ¼ë¡œ ì „ë‹¬í•˜ë ¤ëŠ” ì˜ë¯¸ê°€ ëª…í™•í•´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤! ëª‡ ê°€ì§€ ë¬¸ë²•ì ì¸ ë¶€ë¶„ê³¼ ë” ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„ì„ ì¤‘ì‹¬ìœ¼ë¡œ í”¼ë“œë°±ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤. ì‹œì œ ì¼ì¹˜: ê³¼ê±°ì˜ ê²½í—˜ì— ëŒ€í•´ ì´ì•¼ê¸°í•˜ê³  ìˆìœ¼ë¯€ë¡œ ë™ì‚¬ëŠ” ê³¼ê±°í˜•ìœ¼ë¡œ ì¼ì¹˜ì‹œí‚¤ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. (ì˜ˆ: attend â†’ attended) ìì—°ìŠ¤ëŸ¬ìš´ í‘œí˜„: side-project ëª¨ì„ â†’ a side-project meeting ì²˜ëŸ¼ í•œêµ­ì–´ ë‹¨ì–´ë¥¼ ì˜ì–´ë¡œ ë°”ê¾¸ê³ , ëª…ì‚¬ ì•ì— ê´€ì‚¬(a/an/the)ë¥¼ ì ì ˆíˆ ì‚¬ìš©í•˜ë©´ ë¬¸ì¥ì´ ë” ìì—°ìŠ¤ëŸ¬ì›Œì§‘ë‹ˆë‹¤. 'ì„œë¥˜ íƒˆë½í•˜ë‹¤'ëŠ” fail the resume screening ë˜ëŠ” get rejected at the resume screening stageì™€ ê°™ì´ êµ¬ì²´ì ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 'ê³ ë¯¼í•˜ë‹¤'ëŠ” thoughtfully considered ë˜ëŠ” took the time to think about ë“±ìœ¼ë¡œ í‘œí˜„í•˜ë©´ 'ë‚˜ë¥¼ ìœ„í•´ ì‹ ì¤‘í•˜ê²Œ ìƒê°í•´ ì£¼ì—ˆë‹¤'ëŠ” ë‰˜ì•™ìŠ¤ë¥¼ ì˜ ì‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ë²•: 'ë„ì›€(help)'ì€ ì…€ ìˆ˜ ì—†ëŠ” ëª…ì‚¬ë¼ many ëŒ€ì‹  muchë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. (so much help) I couldn't have be better person â†’ I couldn't have become a better person ì²˜ëŸ¼ ì¡°ë™ì‚¬ ë’¤ì— ë™ì‚¬ ì›í˜•ì˜ ê³¼ê±°ë¶„ì‚¬í˜•(p.p.)ì„ ì‚¬ìš©í•˜ê³ , ëª…ì‚¬ ì•ì— ê´€ì‚¬ë¥¼ ë¶™ì—¬ì£¼ë©´ ë¬¸ë²•ì ìœ¼ë¡œ ì™„ì „í•œ ë¬¸ì¥ì´ ë©ë‹ˆë‹¤. ì˜¤íƒ€: recommmend â†’ recommendation`ê³¼ ê°™ì€ ì˜¤íƒ€ë¥¼ ì£¼ì˜í•˜ë©´ ë” ì™„ì„±ë„ ë†’ì€ ê¸€ì´ ë  ê²ƒ ê°™ìŠµë‹ˆë‹¤."
  },
  {
    "id": "Codility Study/ê³µë¶€ë°©í–¥",
    "title": "ì½”ë”©í…ŒìŠ¤íŠ¸ ê³µë¶€ ë°©í–¥",
    "date": "2025-07-08",
    "excerpt": "ì½”ë”©í…ŒìŠ¤íŠ¸ ê³µë¶€ ë°©í–¥ì— ëŒ€í•œ ì •ë¦¬",
    "category": "Codility Study",
    "tags": [
      "ê³ ë¯¼ì •ë¦¬"
    ],
    "content": "ë°°ê²½ ì ê¹œ ê³µë¶€í•˜ë‹¤ê°€ ë‚´ë ¤ë†“ê³  ë‹¤ì‹œ ê³µë¶€ë¥¼ ì‹œì‘í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ì£¼ë‹ˆì–´, í˜¹ì€ ì‹œë‹ˆì–´ë”ë¼ë„ ì½”ë”©í…ŒìŠ¤íŠ¸ëŠ” ì„ íƒì´ ì•„ë‹Œ í•„ìˆ˜ê°€ ì•„ë‹ê¹Œ ì‹¶ìŠµë‹ˆë‹¤. (ì ì–´ë„ 10ë…„ ì´í•˜ê¹Œì§€ëŠ”?) ê·¸ë˜ì„œ ê¾¸ì¤€íˆ ê³µë¶€í•´ë³´ë ¤ê³  í•˜ëŠ”ë° ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ ì¡ì„ì§€ì— ëŒ€í•œ ì •ë¦¬ë¥¼ í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ë˜ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ê°™ì´ í•˜ëŠ” 'ì‹ ë³´í˜„'ë‹˜ì˜ ê²½í—˜ì„ ë“¤ì–´ë³´ë‹ˆ, ë¼ì´ë¸Œì½”í…ŒëŠ” í•„ìˆ˜ì ìœ¼ë¡œ ë³´ê³  ë‚œì´ë„ê°€ ê½¤ ìˆì–´ì„œ ê¾¸ì¤€í•œ ì¤€ë¹„ê°€ í•„ìš”í•˜ë‹¤ê³  í•©ë‹ˆë‹¤. ì§€ê¸ˆë¶€í„°ë¼ë„ ê¾¸ì¤€íˆ ê³µë¶€í•´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ì½”í…Œ ê³µë¶€í•˜ê¸° ì¢‹ì€ ê³³ (GPT ì •ë¦¬) > ì™€ ì—„ì²­ ì •ë¦¬ ì˜í•´ì£¼ë„¤ìš”... ì½”ë”© í…ŒìŠ¤íŠ¸(ì½”í…Œ) ê³µë¶€ì— ë„ì›€ì´ ë˜ëŠ” ì¢‹ì€ ì‚¬ì´íŠ¸ë“¤ì„ ì•„ë˜ì— ìš©ë„ë³„ë¡œ ì •ë¦¬í•´ë“œë¦´ê²Œìš”. ê° ì‚¬ì´íŠ¸ë§ˆë‹¤ íŠ¹ì„±ì´ ë‹¤ë¥´ë‹ˆ, ë³¸ì¸ì˜ í•™ìŠµ ëª©ì (ê¸°ì´ˆ ë‹¤ì§€ê¸°, ë¬¸ì œ í’€ì´, ì‹¤ì „ ëŒ€ë¹„ ë“±)ì— ë§ì¶° ì„ íƒí•˜ì‹œë©´ ë©ë‹ˆë‹¤. --- âœ… 1. ë¬¸ì œ í’€ì´ ì¤‘ì‹¬ ë°±ì¤€ ì˜¨ë¼ì¸ ì €ì§€ (BOJ) URL: https://www.acmicpc.net/ íŠ¹ì§•: êµ­ë‚´ ì½”í…Œ ì¤€ë¹„ìë“¤ì´ ê°€ì¥ ë§ì´ ì‚¬ìš©í•˜ëŠ” ì‚¬ì´íŠ¸. ë‹¤ì–‘í•œ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜, ë‚œì´ë„ë³„ ë¬¸ì œ ì œê³µ. ë‹¨ê³„ë³„ë¡œ í’€ì–´ë³´ê¸°, ë¬¸ì œì§‘ ê¸°ëŠ¥ìœ¼ë¡œ ì²´ê³„ì ì¸ í•™ìŠµ ê°€ëŠ¥. C++, Python, Java ë“± ë‹¤ì–‘í•œ ì–¸ì–´ ì§€ì›. í”„ë¡œê·¸ë˜ë¨¸ìŠ¤ (Programmers) URL: https://programmers.co.kr/learn/challenges íŠ¹ì§•: ì‹¤ì œ ê¸°ì—… ì½”ë”©í…ŒìŠ¤íŠ¸ ë¬¸ì œ ìˆ˜ë¡ (ì¹´ì¹´ì˜¤, ë„¤ì´ë²„ ë“±). ì‹¤ì „ ê°ê°ì„ ê¸°ë¥´ê¸°ì— ì í•©. ë ˆë²¨ë³„ë¡œ ë¶„ë¥˜ë˜ì–´ ìˆì–´ì„œ ì…ë¬¸ìë¶€í„° ê³ ê¸‰ìê¹Œì§€ í™œìš© ê°€ëŠ¥. Python, JavaScript ë“± ì‹¤ë¬´ì— ë§ì´ ì“°ëŠ” ì–¸ì–´ì— ìµœì í™”. LeetCode URL: https://leetcode.com/ íŠ¹ì§•: ê¸€ë¡œë²Œ IT ê¸°ì—…(FAANG) ì½”í…Œ ë¬¸ì œ ë‹¤ìˆ˜ ìˆ˜ë¡. ë‚œì´ë„ ë‹¤ì–‘ (Easy, Medium, Hard). ì˜ì–´ ê¸°ë°˜ì´ì§€ë§Œ, ì•Œê³ ë¦¬ì¦˜ ì‹¤ë ¥ì„ ì‹¤ì§ˆì ìœ¼ë¡œ ëŒì–´ì˜¬ë¦¬ê¸°ì— íš¨ê³¼ì . ë¬¸ì œë§ˆë‹¤ ë””ìŠ¤ì»¤ì…˜ê³¼ ì†”ë£¨ì…˜ì´ ì˜ ë˜ì–´ ìˆìŒ. --- âœ… 2. ì•Œê³ ë¦¬ì¦˜ ê°œë…/ì´ë¡  í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ ë¹„ì£¼ì–¼ë¼ì´ì € URL: https://visualgo.net/ íŠ¹ì§•: ìë£Œêµ¬ì¡° ë° ì•Œê³ ë¦¬ì¦˜ ë™ì‘ ì›ë¦¬ë¥¼ ì‹œê°ì ìœ¼ë¡œ ë³´ì—¬ì¤Œ. DFS, BFS, ì •ë ¬, í™, íŠ¸ë¦¬ ë“± ê¸°ì´ˆ ê°œë… ì •ë¦½ì— íƒì›”. Tistory / velog ë¸”ë¡œê·¸ ê²€ìƒ‰ í™œìš© ë°©ë²•: \"[ë¬¸ì œ ì´ë¦„] + ì•Œê³ ë¦¬ì¦˜ í’€ì´\"ë¡œ ê²€ìƒ‰í•˜ë©´, í’€ì´ ê³¼ì • + ì‹œê°„ ë³µì¡ë„ ë¶„ì„ê¹Œì§€ ì„¤ëª…ëœ ë¸”ë¡œê·¸ ê¸€ì´ ë§ìŒ. ì´ë¡  + êµ¬í˜„ì„ ì—°ê²°ì§€ì–´ í•™ìŠµí•˜ê¸° ì¢‹ìŒ. --- âœ… 3. ëª¨ì˜ ì½”ë”© í…ŒìŠ¤íŠ¸ / ì‹¤ì „ ì—°ìŠµ ì½”ë“œì—… (CodeUp) URL: https://codeup.kr/ íŠ¹ì§•: ì…ë¬¸ìì—ê²Œ ì í•©í•œ ê¸°ì´ˆ ë¬¸ì œë¶€í„° ì•Œê³ ë¦¬ì¦˜ ë¬¸ì œê¹Œì§€ ì œê³µ. ë‹¨ê³„ë³„ í•™ìŠµìœ¼ë¡œ ì•Œê³ ë¦¬ì¦˜ ì‹¤ë ¥ì„ ìì—°ìŠ¤ëŸ½ê²Œ ëŒì–´ì˜¬ë¦¼. Codeforces URL: https://codeforces.com/ íŠ¹ì§•: ì£¼ê¸°ì ì¸ ì˜¨ë¼ì¸ ëŒ€íšŒ ìš´ì˜. ì‹¤ì „ ê°ê° ë° ì†ë„, êµ¬í˜„ ëŠ¥ë ¥ í–¥ìƒì— ë§¤ìš° íš¨ê³¼ì . ëŸ¬ì‹œì•„ ê¸°ë°˜ ì»¤ë®¤ë‹ˆí‹°ì´ë‚˜ ì˜ì–´ ì§€ì›. --- âœ… 4. ë¬¸ì œ ì¶”ì²œ/ê°€ì´ë“œ ìë£Œ GitHub ì €ì¥ì†Œ ì˜ˆì‹œ: https://github.com/tony9402/baekjoon ë°±ì¤€ ì•Œê³ ë¦¬ì¦˜ ë¶„ë¥˜ ê¸°ë°˜ ì¶”ì²œ ë¬¸ì œ ëª¨ìŒ https://github.com/ellynhan/boj-algorithm-practice ê¸°ì´ˆ â†’ ê³ ê¸‰ ìˆœì„œë¡œ ì •ë¦¬ëœ ë°±ì¤€ ë¬¸ì œì§‘ --- ğŸ” ì½”í…Œ ì¤€ë¹„ ë°©ë²• ìš”ì•½ (ì´ë¡ ì  ë°°ê²½) ê¸°ì´ˆ ê°œë… ìŠµë“: ì‹œê°„ ë³µì¡ë„, ì •ë ¬, íƒìƒ‰, ì¬ê·€, DP, ê·¸ë¦¬ë”” ë“± ì´ë¡  í•™ìŠµ. ë¬¸ì œ ìœ í˜•ë³„ í•™ìŠµ: êµ¬í˜„ â†’ ì •ë ¬ â†’ íƒìƒ‰(BFS/DFS) â†’ ìŠ¤íƒ/í â†’ DP â†’ ê·¸ë˜í”„ ìˆœì„œ ì¶”ì²œ. ë¬¸ì œ í’€ì´ + ë³µìŠµ: ì˜¤ë‹µ ë…¸íŠ¸ í•„ìˆ˜. ê°™ì€ ë¬¸ì œë¥¼ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µí•´ì„œ í’€ì–´ë³´ëŠ” ê²ƒì´ ì¤‘ìš”. ì‹¤ì „ ëª¨ì˜ í…ŒìŠ¤íŠ¸: ì‹¤ì œ ì‹œí—˜ ì‹œê°„ì²˜ëŸ¼ ì œí•œ ì‹œê°„ ì•ˆì— í‘¸ëŠ” ì—°ìŠµì´ íš¨ê³¼ì . ë‚´ ìƒê°ì€? ê¸°ì´ˆë¶€í„° í•˜ë‚˜ì”© ìŠµë“í•˜ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ ì›í•˜ëŠ” íšŒì‚¬ ê¸°ì¶œë¬¸ì œë¶€í„° í’€ì–´ë³´ë©´ì„œ ì±„ì›Œë‚˜ê°€ëŠ” ê²ƒë„ ì¢‹ì€ ë°©í–¥ì¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¹ì¥ 7ì›”ì— ìˆëŠ” í˜„ëŒ€ ì˜¤í† ì—ë²„ ì§€ì›í•˜ê³  ì—¬ê¸° ì½”í…Œ ê¸°ì¶œë¬¸ì œë¶€í„° í’€ì–´ë³´ë ¤ê³  í•©ë‹ˆë‹¤. ê´€ë ¨ ë‚´ìš©ë“¤ì„ í•˜ë‚˜ì”© ì •ë¦¬í•´ë³´ê³˜ìŠµë‹ˆë‹¤."
  },
  {
    "id": "Reflections/2025-07-03-importance-of-numbers",
    "title": "ìˆ˜ì¹˜ì˜ ì¤‘ìš”ì„±",
    "date": "2025-07-03",
    "excerpt": "ìˆ˜ì¹˜ë¡œ í‘œí˜„í•˜ëŠ”ê²Œ ì™œ ì¤‘ìš”í•œì§€ì— ëŒ€í•œ ìƒê° ì •ë¦¬",
    "category": "Reflections",
    "tags": [
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": ""
  },
  {
    "id": "Causal Inference/what-is-fwl",
    "title": "FWL(Frisch-Waugh-Lovell) ì •ë¦¬ë€?",
    "date": "2025-07-02",
    "excerpt": "í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ê¸° ìœ„í•œ ë°©ë²•ë¡ ",
    "category": "Causal Inference",
    "tags": [
      "Causal Inference"
    ],
    "content": "ì°¸ê³ ìë£Œ ì‹¤ë¬´ë¡œ í†µí•˜ëŠ” ì¸ê³¼ì¶”ë¡  with python ë„¤ì´ë²„ë¸”ë¡œê·¸_ (ì„ í˜•ëª¨í˜•) 04 Frisch-Waugh-Lovell ì •ë¦¬, ìˆ˜ì‹ í’€ì´ê°€ ì˜ë˜ì–´ ìˆìŒ ë°°ê²½ ì„¤ëª… --- ì¸ê³¼ì¶”ë¡ ì´ë€ \"ì›ì¸ -> ê²°ê³¼\"ì˜ íŒ¨í„´ì¸ \"ì¸ê³¼ê´€ê³„\"ë¥¼ ì°¾ëŠ”ë°ì— ëª©ì ì´ ìˆìŠµë‹ˆë‹¤. ì´ íŒ¨í„´ì„ ì°¾ëŠ”ê±´ ì–´ë ¤ìš´ ì¼ì¸ë°, ê·¸ ì´ìœ  ì¤‘ í•˜ë‚˜ëŠ” ì›ì¸ê³¼ ê²°ê³¼ì— ëª¨ë‘ ì˜í–¥ì„ ì£¼ëŠ” êµë€ë³€ìˆ˜ ë•Œë¬¸ì…ë‹ˆë‹¤. êµë€ë³€ìˆ˜(confounding variable)ë€ ì¢…ì†ë³€ìˆ˜ì™€ ë…ë¦½ë³€ìˆ˜ì— ëª¨ë‘ ì˜í–¥ì„ ì¤˜ ì˜ëª»ëœ ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ë„ë¡ ë§Œë“œëŠ” ë³€ìˆ˜ë¥¼ ë§í•©ë‹ˆë‹¤. ì¦‰ êµë€ë³€ìˆ˜ê°€ ì˜ëª»ëœ íŒ¨í„´ì„ ì°¾ë„ë¡ ìœ ë„í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. !confounding variable ì˜ˆì‹œê·¸ë¦¼ ê°€ì¥ ëŒ€í‘œì ì¸ ì˜ˆì‹œë¡œ \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ê³¼ \"ìƒì–´ ì–´íƒíšŸìˆ˜\"ë¥¼ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \"ê¸°ì˜¨\"ì´ë¼ëŠ” ë³€ìˆ˜ëŠ” \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ê³¼ \"ìƒì–´ì˜ ê³µê²©íšŸìˆ˜\"ì— ëª¨ë‘ ì˜í–¥ì„ ì¤ë‹ˆë‹¤. ê·¸ë˜ì„œ \"ê¸°ì˜¨\"ì´ ì˜¬ë¼ê°€ì„œ \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ì´ ì¦ê°€í•˜ê³  \"ìƒì–´ì˜ ê³µê²©íšŸìˆ˜\"ê°€ ì¦ê°€í•œ ê²ƒì¸ë°, \"ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰\"ì´ ì¦ê°€í•´ì„œ \"ìƒì–´ì˜ ê³µê²©íšŸìˆ˜\"ê°€ ì¦ê°€í–ˆë‹¤ëŠ” ìƒê°ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. ì¦‰ \"êµë€ ë³€ìˆ˜\"ëŠ” ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ì˜ëª» í•´ì„í•  ê°€ëŠ¥ì„±ì„ ë§Œë“¤ê²Œ ë©ë‹ˆë‹¤. ë”°ë¼ì„œ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ë³¼ ë• êµë€ë³€ìˆ˜ë¥¼ ì œê±°í•´ì£¼ëŠ”ê²Œ êµ‰ì¥íˆ ì¤‘ìš”í•©ë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ì„ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. > êµë€ë³€ìˆ˜ë¥¼ ì—†ì• ì•¼ í•œë‹¤ëŠ” ê±´ ì´í•´í–ˆì–´, ê·¸ëŸ¬ë©´ ì´ê±¸ ì–´ë–»ê²Œ ì—†ì•¨ ê±´ë°? ê°€ì¥ ì¢‹ì€ ê±´ êµë€ë³€ìˆ˜ë¥¼ íŒŒì•…í•˜ê³  ì‹¤í—˜ ì„¤ê³„ë¥¼ í†µí•´ ì—†ì• ëŠ” ê²ƒì…ë‹ˆë‹¤. ìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \"ê¸°ì˜¨\"ì´ë¼ëŠ” êµë€í¸ìˆ˜ë¥¼ íŒŒì•…í•˜ê³ , ì´ë¥¼ í†µì œí•œ í›„ì— ë‘ ë³€ìˆ˜ ê°„ì˜ ê´€ê³„ë¥¼ ì‚´í´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ êµë€ë³€ìˆ˜ë¥¼ íŒŒì•…í•˜ë”ë¼ë„ ì´ë¥¼ í†µì œí•  ìˆ˜ ì—†ì„ ë•Œê°€ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìˆ˜ìˆ ë°©ë²•(ë…ë¦½ë³€ìˆ˜)ê³¼ ìˆ˜ìˆ  í›„ ê²°ê³¼(ì¢…ì†ë³€ìˆ˜)ì˜ ê´€ê³„ì—ì„œëŠ” ë‚˜ì´, ìƒí™œìŠµê´€ ë“±ì˜ êµë€ë³€ìˆ˜ê°€ ìˆìŠµë‹ˆë‹¤. êµë€ë³€ìˆ˜ë¥¼ íŒŒì•…í•˜ì§€ë§Œ ì´ë¥¼ í†µì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. > ì´ë¥¼ ìœ„í•´ êµë€ë³€ìˆ˜ì˜ ì˜í–¥ì„ \"í†µê³„ì \"ìœ¼ë¡œ ì œê±°í•´ì£¼ëŠ” ë°©ë²•ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. FWL Theorem ì´ë€? --- > ì•ì„œ ì„¤ëª…í•œ êµë€ë³€ìˆ˜ì˜ ì˜í–¥, ì¦‰ í¸í–¥ì„ ë³´ì •í•˜ê¸° ìœ„í•œ ë°©ë²•ì…ë‹ˆë‹¤. FWL ì •ë¦¬ì˜ ì´ë¡ ì  ë°°ê²½ì€ ë‹¤ì¤‘íšŒê·€ì˜ ê°•ë ¥í•¨ì— ìˆìŠµë‹ˆë‹¤. > ë‹¤ì¤‘ íšŒê·€ ë¶„ì„ì—ì„œ íšŒê·€ ê³„ìˆ˜ì˜ ì˜ë¯¸ëŠ” ëª¨ë¸ì˜ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ì˜ íš¨ê³¼ë¥¼ ê³ ë ¤í•œ í›„ ì–»ì€ ë™ì¼ ì„¤ëª…ë³€ìˆ˜ì˜ ì´ë³€ëŸ‰ê³„ìˆ˜ë‹¤. ì´ëŠ” ë‹¤ì¤‘íšŒê·€ëª¨í˜•ì—ì„œ íŠ¹ì • ê´€ì‹¬ ë³€ìˆ˜ì˜ OLS ì¶”ì •ê³„ìˆ˜ëŠ”, (1) ëª¨ë“  ì„¤ëª…ë³€ìˆ˜ë¥¼ í•œêº¼ë²ˆì— í¬í•¨í•˜ì—¬ íšŒê·€ë¶„ì„í•œ ê²°ê³¼ì™€, (2) í†µì œ ë³€ìˆ˜ë“¤ì— ëŒ€í•´ ì¢…ì†ë³€ìˆ˜ì™€ ê´€ì‹¬ ë³€ìˆ˜ ê°ê°ì˜ ì”ì°¨ë¥¼ êµ¬í•œ ë’¤ ì´ ì”ì°¨ë“¤ ê°„ì— íšŒê·€ë¶„ì„í•œ ê²°ê³¼ê°€ ë™ì¼í•˜ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. ìˆ˜í•™ì  í‘œí˜„ ë‹¤ì¤‘íšŒê·€ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: $$Y = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_k X_k + \\epsilon$$ ì—¬ê¸°ì„œ $Y$ëŠ” ì¢…ì†ë³€ìˆ˜, $X_1$ì€ ê´€ì‹¬ ë³€ìˆ˜, $X_2, \\ldots, X_k$ëŠ” í†µì œ ë³€ìˆ˜ë“¤ì…ë‹ˆë‹¤. FWL ì •ë¦¬ì— ë”°ë¥´ë©´, $\\beta_1$ì˜ OLS ì¶”ì •ì¹˜ëŠ” ë‹¤ìŒ ë‘ ë°©ë²•ìœ¼ë¡œ ë™ì¼í•˜ê²Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ë°©ë²• 1: ì „ì²´ ëª¨ë¸ì—ì„œ ì§ì ‘ ì¶”ì • $$\\hat{\\beta}_1 = \\frac{\\text{Cov}(X_1, Y)}{\\text{Var}(X_1)}$$ ë°©ë²• 2: ì”ì°¨í™”ë¥¼ í†µí•œ ì¶”ì • $Y$ë¥¼ $X_2, \\ldots, X_k$ì— ëŒ€í•´ íšŒê·€í•˜ì—¬ ì”ì°¨ $e_Y$ ê³„ì‚° $X_1$ì„ $X_2, \\ldots, X_k$ì— ëŒ€í•´ íšŒê·€í•˜ì—¬ ì”ì°¨ $e_{X_1}$ ê³„ì‚° $e_Y$ë¥¼ $e_{X_1}$ì— ëŒ€í•´ íšŒê·€í•˜ì—¬ $\\hat{\\beta}_1$ ì¶”ì • $$e_Y = Y - \\hat{Y}_{X_2,\\ldots,X_k}$$ $$e_{X_1} = X_1 - \\hat{X_1}_{X_2,\\ldots,X_k}$$ $$\\hat{\\beta}_1 = \\frac{\\text{Cov}(e_{X_1}, e_Y)}{\\text{Var}(e_{X_1})}$$ ì‹¤ì œë¡œ ìˆ˜ì‹ì ìœ¼ë¡œë„ ê°™ê³  (ì°¸ê³ ìë£Œ 2 ì°¸ê³ ) í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´ë„ ê°™ìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì—ì„œ ë‹¤ì¤‘íšŒê·€ëŠ” ì˜ˆì¸¡ ëª¨ë¸ì´ ì•„ë‹Œ ì¸ê³¼ê´€ê³„ë¥¼ ì°¾ëŠ” ëª¨ë¸ë¡œ ì‚¬ìš©ë©ë‹ˆë‹¤. R-learner, ì¸ê³¼ì¶”ë¡  ëª¨ë¸ë¡œ êµ¬í˜„í•˜ê¸° --- ì •ë¦¬í•˜ë©´ FWLì´ë€ ì”ì°¨í™”ë¥¼ í†µí•´ êµë€ë³€ìˆ˜ë¥¼ ì§êµí™” ì‹œí‚¤ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. FWLì„ meta learner í˜•íƒœë¡œ êµ¬í˜„í•œê²Œ R-learnerì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì”ì°¨í™”ë¥¼ ìœ„í•œ 2ê°œì˜ nuisance functionì´ í•„ìš”í•˜ê³ , CATEë¥¼ ì¶”ì •í•˜ëŠ” ëª¨ë¸ì´ í•„ìš”í•©ë‹ˆë‹¤. R-learnerì˜ ëª©ì  í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤: $$\\mathcal{L}(\\tau) = \\mathbb{E}\\left[\\left(Y - m(X) - \\tau(X)(T - e(X))\\right)^2\\right]$$ ì—¬ê¸°ì„œ: $m(X) = \\mathbb{E}[Y X]$: ê²°ê³¼ ëª¨ë¸ (outcome model) $e(X) = \\mathbb{E}[T X]$: ì²˜ë¦¬ ëª¨ë¸ (treatment model) $\\tau(X)$: ì¡°ê±´ë¶€ í‰ê·  ì²˜ë¦¬ íš¨ê³¼ (CATE) ë‹¤ì¤‘íšŒê·€ì™€ ë¹„êµí•´ì„œ FWLì€ meta leanerë¡œ êµ¬í˜„í•  ë•Œì˜ ì¥ì ì€ 'ë¹„ì„ í˜•ì ì¸ íŒ¨í„´'ì„ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ êµ¬í˜„ëœ R-learnerë¥¼ í†µí•´ CATE(Conditional Average Treatment Effect)ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Causal Inference/correlation-vs-causation",
    "title": "ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ì˜ ì´í•´",
    "date": "2025-06-28",
    "excerpt": "ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ì˜ ì°¨ì´ì ì„ ì´í•´í•˜ê³ , ì˜ˆì¸¡ ëª¨ë¸ì˜ í•œê³„ì™€ ì¸ê³¼ì¶”ë¡ ì˜ ì¤‘ìš”ì„±ì— ëŒ€í•œ ì´ì•¼ê¸°",
    "category": "Causal Inference",
    "tags": [
      "ìƒê´€ê´€ê³„",
      "ì¸ê³¼ê´€ê³„",
      "ì¸ê³¼ì¶”ë¡ "
    ],
    "content": "> ìƒê´€ê´€ê³„ì™€ ì¸ê³¼ê´€ê³„ë¥¼ ì´í•´í•˜ê³  êµ¬ë¶„í•˜ëŠ” ê±´ ê²°ê³¼ë¥¼ í•´ì„í•  ë•Œ ì¤‘ìš”í•œ ì ì…ë‹ˆë‹¤. ìƒê´€ê´€ê³„ë€, ë‘ ë³€ìˆ˜ ì‚¬ì´ì— ì¼ì •í•œ íŒ¨í„´ì´ë‚˜ ë™ë°˜ ë³€ë™ì´ ì¡´ì¬í•¨ì„ ì˜ë¯¸í•˜ë©°, í•œ ë³€ìˆ˜ê°€ ì»¤ì§ˆ ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ì¼ì •í•˜ê²Œ ì»¤ì§€ê±°ë‚˜(ì–‘ì˜ ìƒê´€), ì‘ì•„ì§€ëŠ”(ìŒì˜ ìƒê´€) ê²½í–¥ì„ ë³´ì´ëŠ” í†µê³„ì  ê´€ê³„ë¥¼ ë§í•©ë‹ˆë‹¤. ì¸ê³¼ê´€ê³„ë€, í•œ ë³€ìˆ˜ê°€ ë³€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ê°€ ê·¸ ë³€í™”ì— ì˜í•´ ì§ì ‘ì Â·ì²´ê³„ì ìœ¼ë¡œ ì˜í–¥ì„ ë°›ì•„ ë³€í™”í•˜ëŠ” ê´€ê³„ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤ ë³´í†µ ë‘ ë³€ìˆ˜ê°„ì˜ ê´€ê³„ë¥¼ ì‚´í´ë³¼ ë•Œ ìƒê´€ê´€ê³„ë¥¼ ì‚´í´ë³´ê³¤ í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ìƒê´€ê´€ê³„ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ í•´ì„í•˜ì§€ ì•Šë„ë¡ ì£¼ì˜í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ê³¼ ì¼ì¼ ê¸°ì˜¨ ì‚¬ì´ì—ëŠ” ë†’ì€ ê¸°ì˜¨ì¼ìˆ˜ë¡ íŒë§¤ëŸ‰ì´ ì¦ê°€í•˜ëŠ” ê°•í•œ ì–‘ì˜ ìƒê´€ê´€ê³„ê°€ ê´€ì°°ë˜ì§€ë§Œ, ì´ëŠ” ê¸°ì˜¨ì´ ì•„ì´ìŠ¤í¬ë¦¼ íŒë§¤ëŸ‰ì„ ì§ì ‘ â€œì›ì¸â€í•œë‹¤ê³  ë‹¨ì •í•  ìˆ˜ ì—†ìœ¼ë©°, ì‚¬ëŒë“¤ì´ ë”ìš´ ë‚ ì”¨ì— ë°–ì— ë‚˜ì™€ íŒë§¤ëŸ‰ì´ ëŠ˜ì–´ë‚˜ëŠ” ë“± ë‹¤ì–‘í•œ ì œ3ì˜ ìš”ì¸ì´ ë™ì‹œì— ì‘ìš©í–ˆì„ ê°€ëŠ¥ì„±ì„ ë°°ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. > ì˜ˆì¸¡ ëª¨ë¸ì€ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ë³€ìˆ˜ ê°„ì˜ ë™ì‹œ ë³€ë™ íŒ¨í„´, ì¦‰ ìƒê´€ê´€ê³„ë¥¼ í•™ìŠµí•´ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. ì´ ëª¨ë¸ì˜ ì˜ˆì¸¡ê²°ê³¼ëŠ” ì¸ê³¼ê´€ê³„ë¥¼ ë³´ì¥í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ê°€ê²© ë§¤ì¶œ --- --- --- 1 100 1000 2 150 900 3 600 10000 4 700 12000 ì˜ˆë¥¼ ë“¤ì–´ ìˆ™ë°• ê°€ê²©ê³¼ ë§¤ì¶œì´ ì£¼ì–´ì§„ ë°ì´í„°ì—ì„œ ê°€ê²©ì´ ë†’ì„ìˆ˜ë¡ ë§¤ì¶œì´ ë†’ì•„ ë³´ì¸ë‹¤ë©´, ëª¨ë¸ì€ â€œê°€ê²©ì´ ì˜¤ë¥´ë©´ ë§¤ì¶œì´ ì˜¤ë¥¸ë‹¤â€ê³  ì˜ˆì¸¡í•  ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ íŒ¨í„´ ë’¤ì—ëŠ” â€œì„±ìˆ˜ê¸°/ë¹„ìˆ˜ê¸° ì—¬ë¶€â€ë¼ëŠ” ìˆ¨ê²¨ì§„ ì™¸ë¶€ ìš”ì¸ì´ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°€ê²©ì´ ë¹„ìˆ˜ê¸°ì— ë‚®ì•„ì§€ê³  ì„±ìˆ˜ê¸°ì— ì˜¬ë¼ê°€ëŠ” ë™ì‹œì— ë§¤ì¶œë„ í•¨ê»˜ ë³€í–ˆë‹¤ë©´, ì‹¤ì œë¡œëŠ” ê³„ì ˆì„±ì´ ë§¤ì¶œì„ ê²°ì •ì§“ëŠ” ì§„ì§œ ì›ì¸ì¸ë°, ë‹¨ìˆœ ì˜ˆì¸¡ ëª¨ë¸ì€ ì´ë¥¼ êµ¬ë¶„í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ìƒê´€ê´€ê³„ì— ê¸°ë°˜í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì¸ê³¼ê´€ê³„ë¡œ ì˜¤í•´í•˜ë©´ â€œë§¤ì¶œì„ ì˜¬ë¦¬ë ¤ë©´ ê°€ê²©ì„ ì˜¬ë ¤ì•¼ í•œë‹¤â€ëŠ” ì˜ëª»ëœ ê²°ë¡ ì— ì´ë¥¼ ìˆ˜ ìˆê³ , ì‹¤ì œ íš¨ê³¼ê°€ ìˆëŠ” ê°œì…(ì˜ˆ: ë¹„ìˆ˜ê¸° í”„ë¡œëª¨ì…˜) ëŒ€ì‹  ë¶€ì ì ˆí•œ ê°€ê²© ì •ì±…ì„ ì„ íƒí•˜ê²Œ ë§Œë“­ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì´ëŸ¬í•œ ìˆ¨ê²¨ì§„ êµë€ ìš”ì¸ê¹Œì§€ ê³ ë ¤í•´, ì¡°ì‘ ê°€ëŠ¥í•œ ì›ì¸ì´ ì‹¤ì œë¡œ ê²°ê³¼ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ì„ í†µê³„ì ìœ¼ë¡œ ë¶„ë¦¬Â·ì¶”ì •í•¨ìœ¼ë¡œì¨ ì˜¬ë°”ë¥¸ ì˜ì‚¬ê²°ì •ì„ ê°€ëŠ¥í•˜ê²Œ í•©ë‹ˆë‹¤."
  },
  {
    "id": "Causal Inference/what-is-causal-inference",
    "title": "ì¸ê³¼ì¶”ë¡ ì´ë€?",
    "date": "2025-06-20",
    "excerpt": "ì¸ê³¼ì¶”ë¡ ì˜ ê°œë…ì— ëŒ€í•´ ì†Œê°œí•˜ëŠ” ì´ì•¼ê¸°",
    "category": "Causal Inference",
    "tags": [
      "ìƒê´€ê´€ê³„",
      "ì¸ê³¼ê´€ê³„",
      "ì¸ê³¼ì¶”ë¡ "
    ],
    "content": "â€œì¸ê³¼ì¶”ë¡ â€ (Causal Inference)ì€ ìµœê·¼ì— ì˜ë£Œ ë¶„ì•¼, ë§ˆì¼€íŒ… ë¶„ì•¼ ë“± ì„ íƒì— ëŒ€í•œ ë¶„ì„ì´ í•„ìš”í•œ ê³³ì— ìì£¼ ì‚¬ìš©ë˜ëŠ” ê°œë…ì…ë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ë‹¤ìŒê³¼ ê°™ì€ ì§ˆë¬¸ë“¤ì— ë‹µì„ í•©ë‹ˆë‹¤. â€œë§ˆì¼€íŒ… ë¹„ìš©ì„ ëŠ˜ë ¸ë”ë‹ˆ ë§¤ì¶œì´ ì¦ê°€í–ˆë„¤, ë§ˆì¼€íŒ… ë¹„ìš©ì„ ëŠ˜ë ¤ì„œ ê·¸ëŸ°ê±´ê°€?â€ . â€œí™˜ìì—ê²Œ Aë¼ëŠ” ì•½ë¬¼ì„ ì²˜ë°©í–ˆë”ë‹ˆ ìƒíƒœê°€ ê´œì°®ì•„ì¡Œì–´. A ë•ë¶„ì¸ê°€?â€ ì´ëŸ¬í•œ ì§ˆë¬¸ë“¤ì€ ì „í†µì ìœ¼ë¡œ ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ í†µí•´ ì¦ëª…ë˜ì–´ì™”ìŠµë‹ˆë‹¤. ì‹¤í—˜í•˜ë ¤ëŠ” ì¡°ê±´ ì™¸ì— ë‹¤ë¥¸ ì¡°ê±´ì€ ëª¨ë‘ í†µì œí•œ í›„ì— ì‹¤í—˜ê²°ê³¼ë¥¼ ë¹„êµí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìƒˆë¡œ ê°œë°œí•œ ë¹„ë£Œì˜ íš¨ê³¼ë¥¼ ê²€ì¦í•˜ë ¤ê³  í•©ë‹ˆë‹¤. ê°™ì€ í’ˆì¢…ì˜ ì˜¥ìˆ˜ìˆ˜ ë¬˜ëª© 100í¬ê¸°ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´, ì‹¤í—˜êµ°ì—ëŠ” ìƒˆ ë¹„ë£Œë¥¼ í‘œì¤€ëŸ‰ë§Œí¼ íˆ¬ì…í•˜ê³  ëŒ€ì¡°êµ°ì—ëŠ” ê¸°ì¡´ ë¹„ë£Œ(í˜¹ì€ ë¹„ë£Œë¥¼ ì „í˜€ ì£¼ì§€ ì•ŠìŒ)ë¥¼ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ì²˜ë¦¬í•©ë‹ˆë‹¤. ì´ë•Œ ë¬¼ì£¼ê¸°, ì¼ì¡°ëŸ‰, í† ì–‘ ì„±ë¶„, ì˜¨ë„ ë“± ë‚˜ë¨¸ì§€ ì¬ë°° ì¡°ê±´ì€ ë‘ ê·¸ë£¹ì—ì„œ ì™„ì „íˆ ë™ì¼í•˜ê²Œ ìœ ì§€í•©ë‹ˆë‹¤. ì¼ì • ê¸°ê°„ í›„ ë‘ ê·¸ë£¹ì˜ ìƒì¥ ì†ë„, ìˆ˜í™•ëŸ‰, ì‘ë¬¼ ê±´ê°• ìƒíƒœë¥¼ ë¹„êµí•˜ë©´, ì˜¤ë¡œì§€ â€œë¹„ë£Œ ì¢…ë¥˜â€ì˜ ì°¨ì´ë§Œì´ ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì³¤ë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì€ ê°€ì¥ í•©ë¦¬ì ì´ì§€ë§Œ, í˜„ì‹¤ ë¬¸ì œì— ì ìš©í•˜ê¸°ì—” ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ë§ˆë¸”ì˜ ë©€í‹°ë²„ìŠ¤ ì„¸ê³„ê´€ì´ ì•„ë‹ˆë¼ë©´, 2025ë…„ 7ì›” 1ì¼ì— ë§ˆì¼€íŒ… ë¹„ìš©ì„ 100ë§Œì›ê³¼ 1000ë§Œì›ì„ ëª¨ë‘ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í•œ ëª…ì˜ í™˜ìì—ê²Œ ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ 12.1ë¥¼ ì‚½ì…í•œ í›„ ê²°ê³¼ë¥¼ í™•ì¸í•˜ê³ , ë Œì¦ˆë¥¼ ëº€ í›„ 12.6ë¥¼ ì‚½ì…í•´ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¦‰, ëŒ€ì¡°êµ°ê³¼ ì‹¤í—˜êµ°ì„ ì„¤ì •í•˜ëŠ”ë° ì–´ë ¤ì›€ì´ ìˆìŠµë‹ˆë‹¤. ì¸ê³¼ì¶”ë¡ ì€ ì´ë¥¼ í†µê³„ì ìœ¼ë¡œ ë³´ì™„í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ë©´, ì˜¨ë¼ì¸ ì‡¼í•‘ëª°ì—ì„œ Aë¼ëŠ” ê´‘ê³  ìº í˜ì¸ì´ íŒë§¤ëŸ‰ì— ë¯¸ì¹œ ì˜í–¥ì„ ì•Œê³  ì‹¶ì„ ë•Œ, ì‹¤ì œë¡œëŠ” ë™ì¼í•œ ê³ ê°ì—ê²Œ ê´‘ê³ ë¥¼ ë³´ì—¬ì£¼ì§€ ì•Šì€ ìƒíƒœì™€ ë³´ì—¬ì¤€ ìƒíƒœë¥¼ ëª¨ë‘ ê²½í—˜ì‹œí‚¬ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì¸ê³¼ì¶”ë¡  ê¸°ë²•ì„ í™œìš©í•©ë‹ˆë‹¤. ì´ë•Œ ê³ ê°ì˜ ì—°ë ¹, ì„±ë³„, ê³¼ê±° êµ¬ë§¤ ì´ë ¥ ë“± ë‹¤ì–‘í•œ ê³ ê° íŠ¹ì„±ì„ ë³´ê³ , ê´‘ê³ ë¥¼ ë³¸ ê·¸ë£¹ê³¼ ë³´ì§€ ì•Šì€ ê·¸ë£¹ ê°„ì— ì´ íŠ¹ì„± ë¶„í¬ë¥¼ ë¹„ìŠ·í•˜ê²Œ ë§ì¶°(ì˜ˆ: ì„±í–¥ì ìˆ˜ ë§¤ì¹­) ê°€ìƒì˜ ëŒ€ì¡°êµ°ì„ ë§Œë“­ë‹ˆë‹¤. ì´í›„ ë‘ ê·¸ë£¹ì˜ í‰ê·  êµ¬ë§¤ì•¡ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ë©´, â€œê´‘ê³  íš¨ê³¼â€ë§Œì„ í†µê³„ì ìœ¼ë¡œ ë¶„ë¦¬í•´ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ì‹¤ì œ ì‹¤í—˜ì„ ìˆ˜í–‰í•˜ê¸° ì–´ë ¤ìš´ ìƒí™©ì—ì„œë„ ë§ˆì¹˜ ë¬´ì‘ìœ„ëŒ€ì¡°ì‹œí—˜ê³¼ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì¸ê³¼ì  í•´ì„ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•  ë•Œë„, ë§ˆì¹˜ ê´‘ê³  íš¨ê³¼ë¥¼ ë¶„ì„í•˜ë“¯ì´ í™˜ìë§ˆë‹¤ ë‚˜ì´Â·êµ´ì ˆë„Â·ê°ë§‰ ë‘ê»˜ ê°™ì€ íŠ¹ì§•ë“¤ì„ ê³ ë ¤í•´ â€œ12.1, 12.6, 13.2 í¬ê¸° ë Œì¦ˆâ€ë¥¼ ì“´ í™˜ìë“¤ê³¼ ë¹„ìŠ·í•œ ì¡°ê±´ì˜ ê°€ìƒì˜ ëŒ€ì¡°êµ°ì„ ë§Œë“¤ì–´ ë¹„êµí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, 30ëŒ€ ê·¼ì‹œ í™˜ì ì¤‘ì— 12.6mm ë Œì¦ˆë¥¼ ì‚¬ìš©í•œ ê·¸ë£¹ê³¼ 12.1mm ë Œì¦ˆë¥¼ ì‚¬ìš©í•œ ê·¸ë£¹ì´ ë¹„ìŠ·í•œ ëˆˆ êµ¬ì¡°ì™€ ì‹œë ¥ì¹˜ë¥¼ ê°–ê³  ìˆë„ë¡ í†µê³„ì ìœ¼ë¡œ ì¡°ì •í•œ ë’¤, ë‘ ê·¸ë£¹ì˜ ìˆ˜ìˆ  í›„ ì‹œë ¥ ê°œì„  ì •ë„ ì°¨ì´ë¥¼ ë¶„ì„í•˜ë©´ â€œ12.6mm ë Œì¦ˆë¥¼ ì“°ë©´ ì‹œë ¥ì´ ì–¼ë§ˆë‚˜ ë” ì¢‹ì•„ì§€ëŠ”ì§€â€ë¥¼ ì¸ê³¼ì ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ì–»ì€ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì‹¤ì œë¡œëŠ” ì¨ë³´ì§€ ì•Šì€ 12.3mm ê°™ì€ ì¤‘ê°„ í¬ê¸°ì˜ ë Œì¦ˆê°€ ê°€ì ¸ì˜¬ íš¨ê³¼ë„ ì˜ˆì¸¡í•  ìˆ˜ ìˆê³ , ìµœì¢…ì ìœ¼ë¡œëŠ” ê° í™˜ìì—ê²Œ ê°€ì¥ íš¨ê³¼ì ì¸ ë Œì¦ˆ í¬ê¸°ë¥¼ ê³¼í•™ì ìœ¼ë¡œ ì¶”ì²œí•´ ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Causal Inference/Paper Review/review-Multi-Study-R-Learner-for-Estimating-Heterogeneous-Treatment-Effects-Across-Studies-Using-Statistical-Machine-Learning",
    "title": "[Paper Review] Multi-Study R-Learner for Estimating Heterogeneous Treatment Effects Across Studies Using Statistical Machine Learning",
    "date": "2025-01-16",
    "excerpt": "Multi-Study R-Learner ë…¼ë¬¸ì— ëŒ€í•œ ë¦¬ë·° ë° ë¶„ì„",
    "category": "Causal Inference",
    "tags": [
      "R-Learner",
      "Heterogeneous Treatment Effects",
      "Multi-Study",
      "Statistical Machine Learning",
      "Paper Review"
    ],
    "content": "paper link"
  },
  {
    "id": "Career/Graduate School/OMSCSê´€ë ¨",
    "title": "OMSCS(Georgia Tech ì˜¨ë¼ì¸ CS ì„ì‚¬) ì§€ì› ì¤€ë¹„ ê³„íš",
    "date": "2025-01-15",
    "excerpt": "ì„¸ê³„ ìµœê³  ê°€ì„±ë¹„ CS ì„ì‚¬ í”„ë¡œê·¸ë¨ ë¶„ì„ê³¼ ì§€ì› ì „ëµ",
    "category": "Career",
    "tags": [
      "ëŒ€í•™ì›",
      "OMSCS",
      "Georgia Tech",
      "ì»´í“¨í„°ì‚¬ì´ì–¸ìŠ¤"
    ],
    "content": "ë°°ê²½ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ë¶„ì•¼ì—ì„œ ë” ê¹Šì´ ìˆëŠ” ì»¤ë¦¬ì–´ë¥¼ ìŒ“ê¸° ìœ„í•´ ì„ì‚¬ ê³¼ì •ì„ ê³ ë ¤í•˜ë˜ ì¤‘, OMSCS(Georgia Tech Online Master of Science in Computer Science)ë¼ëŠ” í˜ì‹ ì ì¸ í”„ë¡œê·¸ë¨ì„ ë°œê²¬í–ˆë‹¤. ì„¸ê³„ í†±ê¸‰ CS í”„ë¡œê·¸ë¨ì„ ì˜¨ë¼ì¸ìœ¼ë¡œ, ê·¸ê²ƒë„ ê·¹ë„ë¡œ ì €ë ´í•œ ë¹„ìš©ìœ¼ë¡œ ì´ìˆ˜í•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ ë§¤ë ¥ì ì´ë‹¤. OMSCSë€? ğŸ“Š ê¸°ë³¸ ì •ë³´ ì •ì‹ëª…ì¹­: Georgia Institute of Technology Online Master of Science in Computer Science ê°œì„¤ì—°ë„: 2014ë…„ (Udacity, AT&Tì™€ í˜‘ë ¥) ì´ ë¹„ìš©: ì•½ $7,000-$8,500 (í•œí™” 900ë§Œ-1,100ë§Œì›) í•™ìœ„: ìº í¼ìŠ¤ì™€ ë™ì¼í•œ Georgia Tech MS in Computer Science (ì˜¨ë¼ì¸ í‘œê¸° ì—†ìŒ) ê¸°ê°„: í‰ê·  2-3ë…„ (íŒŒíŠ¸íƒ€ì„ìœ¼ë¡œ ì§„í–‰ ê°€ëŠ¥) ê³¼ì • êµ¬ì„±: 10ê°œ ê³¼ì • (30í•™ì ) í•©ê²©ë¥ : ì•½ 70% âœ… ì£¼ìš” ì¥ì  ì••ë„ì ì¸ ê°€ì„±ë¹„ ì „ì²´ í”„ë¡œê·¸ë¨ ë¹„ìš©ì´ $7,000-$8,500 (í•œí™” 900ë§Œ-1,100ë§Œì›) íƒ€ ëª…ë¬¸ëŒ€ ì˜¨ìº í¼ìŠ¤ í”„ë¡œê·¸ë¨ ëŒ€ë¹„ 1/5~1/10 ìˆ˜ì¤€ ëª…ë¬¸ëŒ€ ë¸Œëœë“œ + ë™ë“±í•œ í•™ìœ„ Georgia TechëŠ” CS ë¶„ì•¼ ì„¸ê³„ 8ìœ„ (US News 2024) ì¡¸ì—…ì¥ì— \"ì˜¨ë¼ì¸\" í‘œê¸° ì—†ìŒ ì‹¤ë¦¬ì½˜ë°¸ë¦¬ì—ì„œ ì¸ì •ë°›ëŠ” ë¸Œëœë“œ ë‹¤ë¥¸ ë¸”ë¡œê·¸ ê¸€ë“¤ì´ë‚˜ ë§í¬ë“œì¸ ë´¤ì„ ë•Œ job interview ê¸°íšŒë„ í™•ì‹¤íˆ ë” ì–»ì„ ìˆ˜ ìˆë‹¤. í•´ì™¸ ì´ì§ë„ ê³ ë ¤í•˜ê³  ìˆìœ¼ë‹ˆê¹Œ ì§€ê¸ˆ ìƒí™©ì—ì„œ í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ì¢‹ì€ ì„ íƒì´ ì•„ë‹ê¹Œ. ìœ ì—°ì„± í’€íƒ€ì„ ì§ì¥ê³¼ ë³‘í–‰ ê°€ëŠ¥ ìì‹ ì˜ í˜ì´ìŠ¤ë¡œ ì§„ë„ ì¡°ì ˆ ì „ ì„¸ê³„ ì–´ë””ì„œë‚˜ ìˆ˜ê°• ê°€ëŠ¥ ë³´í†µ 1ê³¼ëª©ë‹¹ ì£¼ë‹¹ 20~40ì‹œê°„ì •ë„ í•„ìš”í•˜ë‹¤ê³  í•¨. ì‹¤ë¬´ ì¤‘ì‹¬ ì»¤ë¦¬í˜ëŸ¼ Machine Learning, AI, Computer Vision ë“± í˜„ì—… ì—”ì§€ë‹ˆì–´ë“¤ì´ ì§ì ‘ í™œìš©í•  ìˆ˜ ìˆëŠ” ìŠ¤í‚¬ í”„ë¡œì íŠ¸ ê¸°ë°˜ í•™ìŠµ âš ï¸ ë‹¨ì  ë° ê³ ë ¤ì‚¬í•­ ë†’ì€ ìê¸°ê´€ë¦¬ ìš”êµ¬ ì˜¨ë¼ì¸ íŠ¹ì„±ìƒ ê°•í•œ ìê¸° ë™ê¸°ë¶€ì—¬ í•„ìš” ì‹œê°„ ê´€ë¦¬ì™€ í•™ìŠµ ê³„íš ìˆ˜ë¦½ ëŠ¥ë ¥ ì¤‘ìš” ì¡¸ì—…ì´ ì–´ë ¤ì›€ ë„¤íŠ¸ì›Œí‚¹ ì œí•œ êµìˆ˜ë‚˜ ë™ë£Œë“¤ê³¼ì˜ ì§ì ‘ì ì¸ ë§Œë‚¨ ë¶€ì¡± ì—°êµ¬ ê¸°íšŒë‚˜ ì¸í„´ì‹­ ì—°ê²° ì–´ë ¤ì›€ Redisì—ì„œ í™œë°œí•˜ê²Œ ì´ì•¼ê¸°ê°€ ë‚˜ì˜¤ê³  ìˆê¸´ í•¨. ê·¸ë˜ë„ ë„¤íŠ¸ì›Œí‚¹ì€ ë§ì´ ë¶€ì¡±í•¨. í•œêµ­ì—ì„œì˜ ì¸ì‹ ì•„ì§ êµ­ë‚´ì—ì„œëŠ” ì¸ì§€ë„ê°€ ë‚®ìŒ ì „í†µì ì¸ í•œêµ­ ê¸°ì—…ì—ì„œëŠ” í‰ê°€ ë¶ˆë¶„ëª… ê·¸ë˜ë„ ì•Œê³  ìˆëŠ” ì‚¬ëŒë“¤ì€ ì¶©ë¶„íˆ ì¢‹ê²Œ ë´ì£¼ëŠ” ê±¸ë¡œ ì´í•´ ì¤‘ ì–¸ì–´ ì¥ë²½ ëª¨ë“  ìˆ˜ì—…ì´ ì˜ì–´ë¡œ ì§„í–‰ TOEFL 100ì  ì´ìƒ í•„ìš” í† í”Œ ì¤€ë¹„ê°€ ê°€ì¥ ê¸‰í•˜ë‹¤. ì§€ì› ì¼ì • ë° ì „ëµ ğŸ“… ì§€ì› ì¼ì • Spring í•™ê¸° ì§€ì› ë§ˆê°ì¼: 8ì›” 15ì¼ Fall í•™ê¸° ì§€ì› ë§ˆê°ì¼: 3ì›” 1ì¼ Priority Decision: ë§ˆê°ì¼ ì´ì „ë¶€í„° ìˆœì°¨ì  í•©ê²© ë°œí‘œ ë‚´ë…„ ê°€ì„ í•™ê¸° ëª©í‘œë¡œ 3ì›”ê¹Œì§€ ì¤€ë¹„ ì„œë¥˜ ê°–ì¶”ê¸° ì¤€ë¹„í•´ì•¼ í•˜ëŠ” ê²ƒë“¤ [x] OMSCS í”„ë¡œê·¸ë¨ ìƒì„¸ ì¡°ì‚¬ ì™„ë£Œ [ ] TOEFL ê¸°ì¡´ ì ìˆ˜ í™•ì¸ ë˜ëŠ” ì‘ì‹œ ì¼ì • ì¡ê¸° [ ] ì¶”ì²œì¸ 3ëª…ì—ê²Œ ì—°ë½ (8ì›” 10ì¼ê¹Œì§€ ì œì¶œ ìš”ì²­) ì¶”ì²œì¸ë“¤ì—ê²Œ ë¦¬ë§ˆì¸ë” ë©”ì‹œì§€ í•„ìš”í•œ ì •ë³´ ì¦‰ì‹œ ì œê³µ [ ] ëŒ€í•™êµ ì˜ë¬¸ ì„±ì ì¦ëª…ì„œ ë°œê¸‰ ì‹ ì²­ [ ] Personal Statement ì´ˆì•ˆ ì‘ì„± ì‹œì‘ [ ] ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ê²½í—˜ + OMSCS ì§€ì› ë™ê¸° ëª…í™•íˆ [ ] ì˜¨ë¼ì¸ ìƒ˜í”Œë“¤ ì°¸ê³ í•˜ì—¬ ë¹ ë¥´ê²Œ ì‘ì„± [ ] ì´ë ¥ì„œ OMSCS ë§ì¶¤ ì—…ë°ì´íŠ¸ ê¸°ìˆ  í”„ë¡œì íŠ¸, í”„ë¡œê·¸ë˜ë° ê²½í—˜ ê°•ì¡° ğŸ“… í•©ê²© í›„ ê³„íš (2025.9 ~ 2026.1) [ ] TOEFL ì ìˆ˜ í–¥ìƒ (í•©ê²© í›„ì—ë„ ê³„ì†) ì‹¤ì œ ìˆ˜ì—…ì—ì„œ í•„ìš”í•œ Speaking/Writing ì‹¤ë ¥ [ ] CS ê¸°ì´ˆ ì§€ì‹ ë³´ê°• ìë£Œêµ¬ì¡°/ì•Œê³ ë¦¬ì¦˜ ì²´ê³„ì  ë³µìŠµ Python, Java ë“± í”„ë¡œê·¸ë˜ë° ì–¸ì–´ ìˆ™ë ¨ë„ í–¥ìƒ [ ] OMSCS ì»¤ë®¤ë‹ˆí‹° í™œë™ í•©ê²©ì ê·¸ë£¹ ì°¸ì—¬ ì²« í•™ê¸° ìˆ˜ê°• ê³¼ëª© ë¦¬ì„œì¹˜ [ ] ìˆ˜í•™ ê¸°ì´ˆ ê°•í™” ì„ í˜•ëŒ€ìˆ˜, í™•ë¥ í†µê³„, ì´ì‚°ìˆ˜í•™ ë³µìŠµ [ ] Foundational Course ì„ íƒ ì²˜ìŒì—ëŠ” 1ê°œ ê³¼ëª©ë§Œ ìˆ˜ê°• ê¶Œì¥ CS 6515 (Algorithms) ë˜ëŠ” CS 6601 (AI) ê³ ë ¤ [ ] í•™ìŠµ í™˜ê²½ êµ¬ì¶• ì˜¨ë¼ì¸ í•™ìŠµì— ìµœì í™”ëœ í™˜ê²½ ì„¤ì • ì‹œê°„ ê´€ë¦¬ ì‹œìŠ¤í…œ êµ¬ì¶• --- ğŸ¯ í•µì‹¬ ìš°ì„ ìˆœìœ„ ğŸ”¥ HIGH (í•„ìˆ˜) TOEFL 100ì  ì´ìƒ - ê°€ì¥ ì¤‘ìš”í•œ ì •ëŸ‰ì  ì§€í‘œ Personal Statement - í•©ê²©ì„ ì¢Œìš°í•˜ëŠ” í•µì‹¬ ìš”ì†Œ ì¶”ì²œì„œ 3ê°œ - ì œ3ì ê´€ì ì—ì„œì˜ ê²€ì¦ ğŸ“ˆ MEDIUM (ì¤‘ìš”) CS ê¸°ì´ˆ ì§€ì‹ - ë©´ì ‘ì´ë‚˜ ì¶”ê°€ ì§ˆë¬¸ ëŒ€ë¹„ í”„ë¡œê·¸ë˜ë° í”„ë¡œì íŠ¸ - ì‹¤ë¬´ ì—­ëŸ‰ ì¦ëª… ì„±ì ì¦ëª…ì„œ - ê¸°ë³¸ ìê²© ìš”ê±´ ğŸ’¡ LOW (í”ŒëŸ¬ìŠ¤ ìš”ì†Œ) OMSCS ì»¤ë®¤ë‹ˆí‹° í™œë™ - ì¶”ê°€ ì •ë³´ ìˆ˜ì§‘ ê¸°ìˆ  ë¸”ë¡œê·¸ - í•™ìŠµ ì˜ì§€ í‘œí˜„ --- â° ì£¼ê°„ í•™ìŠµ ê³„íš (ì˜ˆì‹œ) í‰ì¼ (ì›”-ê¸ˆ) ì•„ì¹¨ 1ì‹œê°„: ì˜ì–´ ê³µë¶€ (TOEFL ì¤€ë¹„) ì €ë… 1ì‹œê°„: CS ê¸°ì´ˆ ë˜ëŠ” í”„ë¡œê·¸ë˜ë° ì£¼ë§ í† ìš”ì¼ 3-4ì‹œê°„: ì§€ì›ì„œ ì‘ì„± ë° í”„ë¡œì íŠ¸ ì¼ìš”ì¼ 2-3ì‹œê°„: ë³µìŠµ ë° ë‹¤ìŒ ì£¼ ê³„íš ì´ ì£¼ë‹¹ ì•½ 10-12ì‹œê°„ íˆ¬ì ì»¤ë¦¬ì–´ì— ë¯¸ì¹  ì˜í–¥ ê¸ì •ì  ì˜í–¥ ê¸°ìˆ ì  ê¹Šì´: í˜„ì¬ ì‹¤ë¬´ ê²½í—˜ì— ì´ë¡ ì  ë°±ê·¸ë¼ìš´ë“œ ì¶”ê°€ ê¸€ë¡œë²Œ ë„¤íŠ¸ì›Œí¬: ì „ ì„¸ê³„ OMSCS ë™ë¬¸ë“¤ê³¼ì˜ ì—°ê²° ê²½ë ¥ ë°œì „: ML ì—”ì§€ë‹ˆì–´, ë¦¬ì„œì¹˜ ì‚¬ì´ì–¸í‹°ìŠ¤íŠ¸ í¬ì§€ì…˜ ì§„ì¶œ ê°€ëŠ¥ ë¸Œëœë“œ ê°€ì¹˜: Georgia Tech í•™ìœ„ì˜ ì‹ ë¢°ë„ í˜„ì‹¤ì  ê³ ë ¤ì‚¬í•­ ì‹œê°„ íˆ¬ì: ì£¼ë‹¹ 15-20ì‹œê°„ í•™ìŠµ ì‹œê°„ í•„ìš” ì—…ë¬´ ë³‘í–‰: í˜„ì¬ ì§ì¥ê³¼ì˜ ê· í˜• ìœ ì§€ ì „ëµ í•„ìš” ROI ê´€ì : í•™ìœ„ ì·¨ë“ í›„ ì‹¤ì œ ì—°ë´‰ ìƒìŠ¹ ë° ì»¤ë¦¬ì–´ ê¸°íšŒ ì¦ê°€ ì—¬ë¶€ ê²°ë¡  OMSCSëŠ” í˜„ì¬ ë‚´ ìƒí™©ì— ë§¤ìš° ì í•©í•œ ì„ íƒì§€ë¡œ ë³´ì¸ë‹¤. íŠ¹íˆ: ê²½ì œì  ë¶€ë‹´ ìµœì†Œí™”: ê¸°ì¡´ ì§ì¥ ìœ ì§€í•˜ë©´ì„œ ì§„í–‰ ê°€ëŠ¥ ì‹¤ë¬´ ì—°ê³„: í˜„ì¬ ë°ì´í„° ì‚¬ì´ì–¸ìŠ¤ ì—…ë¬´ì™€ ì§ì ‘ì  ì—°ê´€ ë¯¸ë˜ ê°€ì¹˜: AI/ML ë¶„ì•¼ ì „ë¬¸ì„± ê°•í™”ë¡œ ì»¤ë¦¬ì–´ í™•ì¥ ë‹¤ë§Œ ì„±ê³µì ì¸ ì™„ì£¼ë¥¼ ìœ„í•´ì„œëŠ” ì² ì €í•œ ì‹œê°„ ê´€ë¦¬ì™€ í•™ìŠµ ê³„íšì´ í•„ìˆ˜ì ì´ë‹¤. ë¬´ì—‡ë³´ë‹¤ ì™œ ì´ í”„ë¡œê·¸ë¨ì„ í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ ëª…í™•í•œ ë™ê¸°ë¶€ì—¬ê°€ ê°€ì¥ ì¤‘ìš”í•  ê²ƒ ê°™ë‹¤. Spring 2026 ì…í•™ì„ ëª©í‘œë¡œ ì²´ê³„ì ì¸ ì¤€ë¹„ë¥¼ ì‹œì‘í•´ë³´ì! ğŸ¯ --- ì°¸ê³  ìë£Œ OMSCS ê³µì‹ ì›¹ì‚¬ì´íŠ¸ OMSCS ì§€ì› ê°€ì´ë“œë¼ì¸ OMSCS ì»¤ë®¤ë‹ˆí‹° ë° í›„ê¸°"
  },
  {
    "id": "Data Science/jupyter-notebook-test",
    "title": "ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì™„ì „ í…ŒìŠ¤íŠ¸ - ì…€ì—ì„œ ë©”íƒ€ë°ì´í„° ì„¤ì •",
    "date": "2025-01-12",
    "excerpt": "ì²« ë²ˆì§¸ ì…€ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.",
    "category": "Data Science",
    "tags": [
      "jupyter",
      "python",
      "ë°ì´í„°ë¶„ì„",
      "pandas",
      "numpy",
      "matplotlib"
    ],
    "content": "--- title: \"ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì™„ì „ í…ŒìŠ¤íŠ¸ - ì…€ì—ì„œ ë©”íƒ€ë°ì´í„° ì„¤ì •\" date: \"2025-01-12\" category: \"Data Science\" tags: [\"jupyter\", \"python\", \"ë°ì´í„°ë¶„ì„\", \"pandas\", \"numpy\", \"matplotlib\"] excerpt: \"ì²« ë²ˆì§¸ ì…€ì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ì„¤ì •í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì œì…ë‹ˆë‹¤.\" --- # ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì™„ì „ í…ŒìŠ¤íŠ¸ ì´ê²ƒì€ ë¸”ë¡œê·¸ì—ì„œ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì´ ì™„ì „íˆ ì–´ë–»ê²Œ í‘œì‹œë˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸í•˜ëŠ” ì˜ˆì œì…ë‹ˆë‹¤. ë©”íƒ€ë°ì´í„° ì„¤ì • ë°©ë²• ì²« ë²ˆì§¸ ì…€ì— YAML í˜•ì‹ìœ¼ë¡œ ë©”íƒ€ë°ì´í„°ë¥¼ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤: ``yaml --- title: \"í¬ìŠ¤íŠ¸ ì œëª©\" date: \"ë‚ ì§œ\" category: \"ì¹´í…Œê³ ë¦¬ëª…\" tags: [\"íƒœê·¸1\", \"íƒœê·¸2\", \"íƒœê·¸3\"] excerpt: \"í¬ìŠ¤íŠ¸ ìš”ì•½\" --- `` ë°ì´í„° ë¶„ì„ ì˜ˆì œ Pythonì„ ì‚¬ìš©í•œ ì™„ì „í•œ ë°ì´í„° ë¶„ì„ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤. ì£¼ìš” ë‚´ìš©: ë°ì´í„° ìƒì„± ë° ë¡œë“œ ê¸°ë³¸ í†µê³„ ë¶„ì„ ë°ì´í„° ì‹œê°í™” import pandas as pd import numpy as np import matplotlib.pyplot as plt print(\"íŒ¨í‚¤ì§€ë¥¼ ì„±ê³µì ìœ¼ë¡œ importí–ˆìŠµë‹ˆë‹¤!\") # ìƒ˜í”Œ ë°ì´í„° ìƒì„± np.random.seed(42) data = { \"x\": np.random.randn(100), \"y\": np.random.randn(100) * 2 + 5 } df = pd.DataFrame(data) print(\"ìƒ˜í”Œ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.\") df.head() # ê¸°ë³¸ í†µê³„ëŸ‰ í™•ì¸ print(\"ê¸°ë³¸ í†µê³„ëŸ‰:\") df.describe() # ê²°ë¡  ì´ ì˜ˆì œì—ì„œëŠ” ë‹¤ìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤: ë°ì´í„° ìƒì„±: NumPyë¥¼ ì‚¬ìš©í•˜ì—¬ ëœë¤ ë°ì´í„°ë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤ ê¸°ë³¸ ë¶„ì„: Pandasë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸°ë³¸ í†µê³„ëŸ‰ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤ ì½”ë“œ ì‹¤í–‰: ì£¼í”¼í„° ë…¸íŠ¸ë¶ì˜ ì½”ë“œ ì…€ê³¼ ì¶œë ¥ì´ ì˜ í‘œì‹œë˜ëŠ”ì§€ í™•ì¸í–ˆìŠµë‹ˆë‹¤ ì£¼í”¼í„° ë…¸íŠ¸ë¶ì´ ë¸”ë¡œê·¸ì—ì„œ ì™„ë²½í•˜ê²Œ í‘œì‹œë˜ê³  ìˆìŠµë‹ˆë‹¤! ğŸ‰"
  },
  {
    "id": "Portfolio/chatbot",
    "title": "AI Chatbot Project",
    "date": "2024-12-01",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "AI Chatbot Project í”„ë¡œì íŠ¸ ê°œìš” ë³‘ì› ìƒë‹´ ì—…ë¬´ë¥¼ ìë™í™”í•˜ê¸° ìœ„í•œ RAG(Retrieval-Augmented Generation) ê¸°ë°˜ ì§€ëŠ¥í˜• ì±—ë´‡ ì‹œìŠ¤í…œì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì¹´ì¹´ì˜¤í†¡ ì±„ë„ì„ í†µí•´ ê³ ê°ì—ê²Œ ì œê³µë˜ë©°, ë°˜ë³µì ì¸ ìƒë‹´ ì§ˆë¬¸ì— ëŒ€í•œ ìë™ ì‘ë‹µì„ í†µí•´ ìƒë‹´ì‚¬ì˜ ì—…ë¬´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ RAG ê¸°ë°˜ ë‹µë³€ ìƒì„±: ë¬¸ì„œ ê²€ìƒ‰ê³¼ ìƒì„± ëª¨ë¸ì„ ê²°í•©í•œ ì •í™•í•œ ë‹µë³€ ì œê³µ ë§¥ë½ ì¸ì‹ ëŒ€í™”: ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ í™œìš©í•œ ì—°ì†ì ì¸ ìƒë‹´ ì§€ì› ë‹¤êµ­ì–´ ì§€ì›: í•œêµ­ì–´ ë° ì˜ì–´ ì§€ì› ì‹¤ì‹œê°„ ì‘ë‹µ: ë¹ ë¥¸ ì‘ë‹µ ì†ë„ë¡œ ì‚¬ìš©ì ê²½í—˜ í–¥ìƒ ìë™ ëª¨ë‹ˆí„°ë§: LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ ìë™ í‰ê°€ ì‹œìŠ¤í…œ ê¸°ìˆ  ìŠ¤íƒ Backend: Python, FastAPI NLP: Transformers, spaCy, OpenAI GPT Vector Database: Qdrant, ChromaDB Embedding: BGE-m3 Deployment: Docker, AWS Monitoring: Slack API í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±… ë¬¸ì„œ ê²€ìƒ‰ ì„±ëŠ¥ ìµœì í™” ë¬¸ì œ: ì˜ë¯¸ì  ê²€ìƒ‰ì˜ í•œê³„ë¡œ ì¸í•œ ë‚®ì€ ê²€ìƒ‰ ì •í™•ë„ í•´ê²°ì±…: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ êµ¬ì¶•: Qdrantë¥¼ í™œìš©í•œ ê³ ì„±ëŠ¥ ë²¡í„° ê²€ìƒ‰ ì‹œìŠ¤í…œ êµ¬ì¶• ë©”íƒ€ë°ì´í„° í•„í„°ë§: ì˜ì‚¬ë³„, ì§„ë£Œê³¼ë³„ ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ì •ë°€ ê²€ìƒ‰ êµ¬í˜„ ì„ë² ë”© ëª¨ë¸ ì„ ì •: BGE-m3 ëª¨ë¸ì„ ë‹¤ì–‘í•œ ì˜µì…˜ê³¼ ë¹„êµí•˜ì—¬ ìµœì  ì„±ëŠ¥ í™•ì¸ í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§: ê³ ìœ ëª…ì‚¬(ì˜ì‚¬ ì´ë¦„ ë“±) ì²˜ë¦¬ ê°œì„ ì„ ìœ„í•œ í‚¤ì›Œë“œ ì¶”ì¶œ ë° í•„í„°ë§ ì§ˆë¬¸ ì„¸ë¶„í™”: ë³µí•© ì§ˆë¬¸ì„ ë‹¨ìˆœ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í•´í•˜ì—¬ ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ ëŒ€í™” ë§¥ë½ ë°˜ì˜: Chat historyë¥¼ í™œìš©í•œ ì§ˆë¬¸ ì¬ì •ì˜ ê²°ê³¼: ê²€ìƒ‰ ì •í™•ë„ 95% ì´ìƒ ë‹¬ì„± ì‚¬ìš©ì ë§Œì¡±ë„ í–¥ìƒ ë¬¸ì œ: ì±—ë´‡ì´ ë‹µë³€í•  ìˆ˜ ì—†ëŠ” ì§ˆë¬¸ì— ëŒ€í•œ ë°˜ë³µ ì‘ë‹µìœ¼ë¡œ ì¸í•œ ì‚¬ìš©ì ë¶ˆë§Œ í•´ê²°ì±…: ì§ˆë¬¸ ë¶„ë¥˜ ëª¨ë¸: ë‹µë³€ ê°€ëŠ¥/ë¶ˆê°€ëŠ¥ ì§ˆë¬¸ì„ ì‚¬ì „ íŒë³„í•˜ëŠ” ë¶„ë¥˜ ëª¨ë¸ ì¶”ê°€ ìƒë‹´ì‚¬ ì—°ê²°: ë‹µë³€ ë¶ˆê°€ ì§ˆë¬¸ ì‹œ ì¦‰ì‹œ ì‹¤ì œ ìƒë‹´ì‚¬ë¡œ ì—°ê²°í•˜ëŠ” ì‹œìŠ¤í…œ êµ¬í˜„ ê²°ê³¼: ì‚¬ìš©ì ë§Œì¡±ë„ 90% ì´ìƒ ë‹¬ì„± ìœ ì§€ë³´ìˆ˜ ë¹„ìš© ìµœì í™” ë¬¸ì œ: ìˆ˜ë™ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì¸í•œ ë†’ì€ ìš´ì˜ ë¹„ìš© (í•˜ë£¨ 30ë¶„ ì´ìƒ ì†Œìš”) í•´ê²°ì±…: ìë™ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ: LLM ê¸°ë°˜ ëŒ€í™” í’ˆì§ˆ í‰ê°€ ìë™í™” í•µì‹¬ ì§€í‘œ ì¶”ì : ë‹µë³€ ì¶œì²˜ ì í•©ì„±: ë‹µë³€ì´ ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œ ìƒì„±ë˜ì—ˆëŠ”ì§€ í‰ê°€ ì§ˆë¬¸-ë¬¸ì„œ ì—°ê´€ì„±: ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ ê°„ì˜ ê´€ë ¨ì„± í‰ê°€ ì„ê³„ì¹˜ ê¸°ë°˜ ì•Œë¦¼: í’ˆì§ˆ ì ìˆ˜ê°€ ì„ê³„ì¹˜ ì´í•˜ì¸ ëŒ€í™”ë§Œ ì•Œë¦¼ ì „ì†¡ ê²°ê³¼: ëª¨ë‹ˆí„°ë§ ë¹„ìš© 80% ê°ì†Œ ë‹µë³€ ìƒì„± ìµœì í™” í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ ë‹¤ìŒ 3ê°€ì§€ ê´€ì ì—ì„œ í”„ë¡¬í”„íŠ¸ë¥¼ ìµœì í™”í–ˆìŠµë‹ˆë‹¤: ë¬¸ì„œ ì‚¬ìš©ì„±: ë‹µë³€ì´ ì–¼ë§ˆë‚˜ ë¬¸ì„œì— ê¸°ë°˜í–ˆëŠ”ì§€ ì¹œì ˆì„±: ì–¼ë§ˆë‚˜ ì¹œì ˆí•˜ê³  ìƒëƒ¥í•˜ê²Œ ì‘ë‹µí•˜ëŠ”ì§€ ì •í™•ë„: ì–¼ë§ˆë‚˜ ì§ˆë¬¸ì— ë§ëŠ” ë‹µë³€ì„ í•˜ëŠ”ì§€ í• ë£¨ì‹œë„¤ì´ì…˜ ë°©ì§€ ì˜ë£Œ ì •ë³´ì˜ ì •í™•ì„±ì€ ë§¤ìš° ì¤‘ìš”í•˜ë¯€ë¡œ: ê²€ìƒ‰ëœ ë¬¸ì„œì—ì„œë§Œ ë‹µë³€ ìƒì„±í•˜ë„ë¡ ê°•ì œ RAGAS ì»¨ì…‰ì„ í™œìš©í•œ ë‹µë³€ ê¸°ë°˜ì„± ìë™ í‰ê°€ LLMì´ ì¶”ì¶œí•œ ëª…ì œ/ê°œë…ì˜ ë¬¸ì„œ ê¸°ë°˜ì„± ê²€ì¦ ê²°ê³¼ ë° ì„±ê³¼ ë‹µë³€ ì •í™•ë„: 95% ì´ìƒ ë‹¬ì„± ì‚¬ìš©ì ë§Œì¡±ë„: 90% ì´ìƒ ë‹¬ì„± ëª¨ë‹ˆí„°ë§ ìë™í™”: ìœ ì§€ë³´ìˆ˜ ë¹„ìš© 80% ê°ì†Œ ì—…ë¬´ íš¨ìœ¨ì„±: ê¸°ì¡´ ìƒë‹´ ì¤‘ 65% ì±—ë´‡ìœ¼ë¡œ ëŒ€ì²´ ì‘ë‹µ ì†ë„: í‰ê·  0.5ì´ˆ ì´ë‚´ í•™ìŠµí•œ ì  RAG ì‹œìŠ¤í…œì˜ ì‹¤ì œ ì ìš© ë°©ë²•ê³¼ í•œê³„ì  ì‚¬ìš©ì ê²½í—˜ì„ ê³ ë ¤í•œ ì±—ë´‡ ì„¤ê³„ì˜ ì¤‘ìš”ì„± ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ êµ¬ì¶• ë°©ë²• ì˜ë£Œ ë¶„ì•¼ì—ì„œì˜ AI ì‹œìŠ¤í…œ ì‹ ë¢°ì„± í™•ë³´ ë°©ë²•"
  },
  {
    "id": "Portfolio/lenze_size_rec",
    "title": "Lens Size Recommendation System",
    "date": "2024-11-15",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "Lens Size Recommendation System í”„ë¡œì íŠ¸ ê°œìš” ì‹œë ¥êµì •ì„¼í„°ì—ì„œ ë Œì¦ˆ ì‚½ì…ìˆ ì„ ìœ„í•œ ìµœì ì˜ ë Œì¦ˆ í¬ê¸°ë¥¼ ì¶”ì²œí•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë°˜ ì‹œìŠ¤í…œì„ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì˜ì‚¬ì˜ ê²½í—˜ê³¼ ë…¸í•˜ìš°ì— ì˜ì¡´í•˜ë˜ ë Œì¦ˆ ì„ íƒ ê³¼ì •ì„ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ê°ê´€í™”í•˜ì—¬, í™˜ìë³„ ë§ì¶¤í˜• ë Œì¦ˆ ì‚¬ì´ì¦ˆë¥¼ ì¶”ì²œí•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ ì˜ˆì¸¡ ëª¨ë¸: í™˜ì ì •ë³´ì™€ ë Œì¦ˆ í¬ê¸°ë¥¼ ì…ë ¥ë°›ì•„ ìˆ˜ìˆ  ê²°ê³¼(Vault) ì˜ˆì¸¡ ì¸ê³¼ì¶”ë¡  ê¸°ë°˜: í†µê³„ì  í¸í–¥ì„ ì œê±°í•œ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ì œê³µ ì˜í•™ì  ì§ê´€ ë°˜ì˜: ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ ì„¤ê³„ í•´ì„ ê°€ëŠ¥ì„±: ì˜ˆì¸¡ ê²°ê³¼ì— ëŒ€í•œ ê·¼ê±°ì™€ ì„¤ëª… ì œê³µ ì‹¤ì‹œê°„ ì¶”ë¡ : BentoML ê¸°ë°˜ ê³ ì„±ëŠ¥ ëª¨ë¸ ì„œë¹™ ê¸°ìˆ  ìŠ¤íƒ Backend: Python, Flask, FastAPI Machine Learning: Scikit-learn, LightGBM, XGBoost Causal Inference: Orthogonalization, Double ML Model Serving: BentoML, Docker Database: PostgreSQL Monitoring: Airflow, Flask Dashboard í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ëª¨ë¸ êµ¬ì¶• ë¬¸ì œ: ë‹¨ìˆœí•œ ì˜ˆì¸¡ ì„±ëŠ¥ë§Œìœ¼ë¡œëŠ” ì„ íƒì˜ ì‹ ë¢°ë„ë¥¼ ë³´ì¥í•  ìˆ˜ ì—†ìŒ í•´ê²°ì±…: ì¸ê³¼ì¶”ë¡  ì ìš©: êµë€í¸í–¥ ë“± í†µê³„ì  í¸í–¥ì„ ì œê±°í•˜ì—¬ ì¸ê³¼ì  ê´€ê³„ ê¸°ë°˜ ì˜ˆì¸¡ ì§êµí™”(Orthogonalization): ì˜ˆì¸¡ ë³€ìˆ˜ì™€ êµë€ ë³€ìˆ˜ë¥¼ ë¶„ë¦¬í•˜ì—¬ í¸í–¥ ì œê±° ì˜í•™ì  ì§ê´€ ë°˜ì˜: ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ ì„¤ê³„ ê²°ê³¼: ê²°ì •ê³„ìˆ˜(RÂ²) 0.7 ë‹¬ì„±ìœ¼ë¡œ ì˜ˆì¸¡ ì‹ ë¢°ë„ í™•ë³´ ì‚¬ìš©ì ì‹ ë¢°ë„ ë° ë§Œì¡±ë„ í–¥ìƒ ë¬¸ì œ: ë†’ì€ í‰ê°€ì§€í‘œê°€ ì‚¬ìš©ì ì‹ ë¢°ì™€ ë§Œì¡±ì„ ë³´ì¥í•˜ì§€ ì•ŠìŒ í•´ê²°ì±…: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê·¼ê±° ì œì‹œ: AI ì˜ˆì¸¡ ê²°ê³¼ì™€ í•¨ê»˜ ì˜í•™ì  ê·¼ê±° ì œê³µ ì‚¬ìš©ì í”¼ë“œë°± ë°˜ì˜: ì£¼ê¸°ì ì¸ í”¼ë“œë°± ìˆ˜ì§‘ ë° UI/ë¦¬í¬íŒ… ê¸°ëŠ¥ ê°œì„  ì°¨ë³„í™” ì¦ëª…: ì—°êµ¬ ë‚´ìš©ì„ í•™ìˆ  ë…¼ë¬¸ìœ¼ë¡œ ì‘ì„±í•˜ì—¬ ì œí’ˆ ì°¨ë³„ì  ì…ì¦ ê²°ê³¼: ì‚¬ìš©ì ì¹œí™”ì ì¸ ì„œë¹„ìŠ¤ ì„¤ê³„ë¡œ ì‹ ë¢°ë„ í–¥ìƒ ë°ì´í„° í’ˆì§ˆ ë° íŠ¹ì„± ë¶„ì„ ë¬¸ì œ: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì˜ ë°ì´í„° í’ˆì§ˆê³¼ ì¼ê´€ì„± í™•ë³´ í•´ê²°ì±…: ì¢…í•©ì  EDA: ì¢Œì•ˆ/ìš°ì•ˆ ì°¨ì´, ê²€ì‚¬ ì¥ë¹„ ì˜¤ë¥˜, ì˜ì‚¬ë³„ ì°¨ì´ ë“± ë¶„ì„ ë°ì´í„° ê²€ì¦: OCR ê²°ê³¼, ìˆ˜ê¸° ì…ë ¥ ë°ì´í„°ì˜ ì™„ì „ì„± ê²€ì¦ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: ë Œì¦ˆ íšŒì „ê°ë„, ì¢…ë¥˜, ë‘ê»˜ ë“± ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ í”¼ì²˜ ì¶”ê°€ ê²°ê³¼: ë°ì´í„° í’ˆì§ˆ í–¥ìƒ ë° ì˜ˆì¸¡ ì„±ëŠ¥ ê°œì„  ëª¨ë¸ë§ ì ‘ê·¼ë²• ëª¨ë¸ë§ ì¡°ê±´ ì„¤ì • ì„œë¹„ìŠ¤ ëª©í‘œì— ë§ëŠ” ëª¨ë¸ ì¡°ê±´ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í–ˆìŠµë‹ˆë‹¤: ë‹¨ì¡°ì„± ì¡°ê±´: íŠ¹ì • ë³€ìˆ˜ì™€ ì˜ˆì¸¡ê°’ ê°„ì˜ ë‹¨ì¡° ì¦ê°€/ê°ì†Œ ê´€ê³„ ìœ ì§€ í•´ì„ ê°€ëŠ¥ì„±: ëª¨ë¸ì´ í•´ì„ ê°€ëŠ¥í•˜ê±°ë‚˜ ê²°ê³¼ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆì–´ì•¼ í•¨ ëª¨ë¸ ì„ íƒ ë° ìµœì í™” Linear Regression ê³„ì—´: Lasso, Ridge, ElasticNet (íšŒê·€ ê³„ìˆ˜ë¡œ ë‹¨ì¡°ì„± í™•ë³´) GBDT ê³„ì—´: LightGBM, XGBoost, CatBoost (monotonic constraint ì ìš©) ì•™ìƒë¸”: Votingê³¼ Stackingì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹: Bayesian optimization í™œìš© ì¼ë°˜í™” ì„±ëŠ¥ í™•ë³´ Data Leakage ë°©ì§€: ì‹œê°„ ê¸°ë°˜ test set êµ¬ì„± ë° unknown ì²˜ë¦¬ ì˜í•™ì  ë…¼ë¦¬ ê²€ì¦: íšŒê·€ ê³„ìˆ˜, feature importance, SHAP ë¶„ì„ì„ í†µí•œ ì •ì„±í‰ê°€ ì •ê¸°ì  ì„±ëŠ¥ í™•ì¸: íŠ¹ì • ì£¼ê¸°ë¡œë§Œ test set í™•ì¸í•˜ì—¬ ê³¼ì í•© ë°©ì§€ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ ëª¨ë¸ ì„œë¹™ BentoML í™œìš©: Docker ì´ë¯¸ì§€ë¡œ íŒ¨í‚¤ì§•í•˜ì—¬ ë²„ì „ ê´€ë¦¬ ì˜¨í”„ë ˆë¯¸ìŠ¤ ë°°í¬: ê¸°ì¡´ ì„œë²„ ë¦¬ì†ŒìŠ¤ í™œìš©í•œ ì¶”ë¡  ì„œë²„ êµ¬ì¶• ë°ì´í„° ì¼ê´€ì„± í™•ë³´ Feature Store êµ¬ì¶•: í•™ìŠµê³¼ ì¶”ë¡ ì—ì„œ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ ë°ì´í„° ì²˜ë¦¬ FastAPI ê¸°ë°˜: ORMì„ í™œìš©í•œ ë°ì´í„° ê´€ë¦¬ ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ Flask Dashboard: ëª¨ë¸ ì„±ëŠ¥ ë° ë°ì´í„° ë¶„í¬ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ Airflow ìŠ¤ì¼€ì¤„ë§: ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ ë°ì´í„° ì—…ë°ì´íŠ¸ í˜„ì¬ ì—°êµ¬ ë° ê°œì„  ë°©í–¥ í•œê³„ì  ì¸ì‹ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ í•œê³„ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ë¶„ì„í–ˆìŠµë‹ˆë‹¤: ì¸¡ì • í•œê³„: ê²€ì‚¬ ì¥ë¹„ëŠ” í™ì±„ ìœ—ë¶€ë¶„ë§Œ ì¸¡ì • ê°€ëŠ¥ êµ¬ì¡°ì  ìš”ì†Œ: ì‹¤ì œ ë Œì¦ˆ ì‚½ì… ìœ„ì¹˜(ì„¬ëª¨ì²´ ë‚­) ì •ë³´ ë¶€ì¡± ëª¨ë¸ í•œê³„: Linear/GBDT ëª¨ë¸ë¡œëŠ” latent space êµ¬ì„± ì–´ë ¤ì›€ í–¥í›„ ì—°êµ¬ ë°©í–¥ êµ¬ì¡°ì  ìš”ì†Œ ì¶”ì •: ë³´ì´ì§€ ì•ŠëŠ” í•´ë¶€í•™ì  êµ¬ì¡°ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ ê°œë°œ Latent Space ëª¨ë¸ë§: ë³µì¡í•œ êµ¬ì¡°ì  ê´€ê³„ë¥¼ í•™ìŠµí•  ìˆ˜ ìˆëŠ” ê³ ê¸‰ ëª¨ë¸ ì—°êµ¬ ê²°ê³¼ ë° ì„±ê³¼ ì˜ˆì¸¡ ì‹ ë¢°ë„: ê²°ì •ê³„ìˆ˜(RÂ²) 0.7 ë‹¬ì„± í†µê³„ì  í¸í–¥ ì œê±°: ì¸ê³¼ì¶”ë¡  ê¸°ë°˜ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ì˜ˆì¸¡ ì œê³µ ì‚¬ìš©ì ë§Œì¡±ë„: ë„ë©”ì¸ ì§€ì‹ ê¸°ë°˜ ê·¼ê±° ì œì‹œë¡œ ì‹ ë¢°ë„ í–¥ìƒ ì‹œìŠ¤í…œ ì•ˆì •ì„±: ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜ í•™ìŠµí•œ ì  ì¸ê³¼ì¶”ë¡ ì„ í†µí•œ ì˜ˆì¸¡ ëª¨ë¸ì˜ ì‹ ë¢°ì„± í™•ë³´ ë°©ë²• ì˜ë£Œ ë¶„ì•¼ì—ì„œì˜ AI ì‹œìŠ¤í…œ ì„¤ê³„ ë° ê²€ì¦ ë°©ë²• ì‚¬ìš©ì ì¤‘ì‹¬ì˜ ì„œë¹„ìŠ¤ ì„¤ê³„ ë° í”¼ë“œë°± ë°˜ì˜ ê³¼ì • ëŒ€ìš©ëŸ‰ ë°ì´í„° ì²˜ë¦¬ ë° ëª¨ë¸ ì„œë¹™ ì‹œìŠ¤í…œ êµ¬ì¶•"
  },
  {
    "id": "Portfolio/segmentation",
    "title": "Segmentation Model",
    "date": "2024-10-20",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ íƒì§€ ëª¨ë¸ í”„ë¡œì íŠ¸ ê°œìš” ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ì€ ë‹¹ë‡¨ë³‘ í™˜ìì—ê²Œ ìì£¼ ë°œìƒí•˜ëŠ” í•©ë³‘ì¦ìœ¼ë¡œ, í˜ˆê´€ ì†ìƒê³¼ ìƒˆë¡œìš´ í˜ˆê´€ ìƒì„±ìœ¼ë¡œ ì¸í•´ ì‹¤ëª…ê¹Œì§€ ìœ ë°œí•  ìˆ˜ ìˆëŠ” ì‹¬ê°í•œ ì§ˆí™˜ì…ë‹ˆë‹¤. ì´ˆê¸° ì§„ë‹¨ì´ ë§¤ìš° ì¤‘ìš”í•œ ì´ ë³‘ì¦ì„ ì•ˆì €ì‚¬ì§„ì„ í†µí•´ íƒì§€í•˜ëŠ” ì„œë¹„ìŠ¤ë¥¼ ê°œë°œí–ˆìŠµë‹ˆë‹¤. ì•ˆì €ì‚¬ì§„ì€ ëˆˆì˜ ì•ˆìª½(ë§ë§‰ ë¶€ë¶„)ì„ ì´¬ì˜í•˜ì—¬ í˜ˆê´€ ìƒíƒœë¥¼ ê´€ì°°í•  ìˆ˜ ìˆê²Œ í•´ì£¼ëŠ” ì˜ë£Œ ì˜ìƒì…ë‹ˆë‹¤. !ì•ˆì €ì‚¬ì§„ ì´ í”„ë¡œì íŠ¸ëŠ” ì˜ì‚¬ë“¤ì˜ ì§„ë‹¨ì„ ì§€ì›í•˜ëŠ” ëª©ì ìœ¼ë¡œ ê°œë°œë˜ì—ˆìœ¼ë©°, ì…ì‚¬ ì „ ë¶€íŠ¸ìº í”„ ê¸°ê°„ ì¤‘ ê¸°ì—…ì—°ê³„ í”„ë¡œì íŠ¸ë¡œ ì‹œì‘í•˜ì—¬ ì…ì‚¬ í›„ì—ë„ ì§€ì†ì ìœ¼ë¡œ ë°œì „ì‹œì¼°ìŠµë‹ˆë‹¤. ê´€ë ¨ ë§í¬: í”„ë¡œì íŠ¸ Github ë°œí‘œì˜ìƒ ìœ íŠœë¸Œ ë°ì´í„° í˜„í™© ë° ë„ì „ê³¼ì œ í”„ë¡œì íŠ¸ ì§„í–‰ ì¤‘ ê°€ì¥ í° ì´ìŠˆëŠ” ë°ì´í„° ë¶€ì¡±ì´ì—ˆìŠµë‹ˆë‹¤. ì´ˆê¸° ë°ì´í„° ìƒí™© ì˜¤í”ˆ ë°ì´í„°ì…‹ í™œìš©: ì•½ 1,500ê°œ ì´ë¯¸ì§€ ë ˆì´ë¸” ë³´ìœ ìœ¨: ì „ì²´ì˜ 80% ì •ë„ ë°ì´í„° í’ˆì§ˆ ë¬¸ì œ: ë¶€ì ì ˆí•œ ë ˆì´ë¸” ë‹¤ìˆ˜ ì¡´ì¬ ë°ì´í„° í™•ë³´ ê³¼ì • ì…ì‚¬ í›„ ì„ìƒíŒ€ê³¼ í˜‘ì—…í•˜ì—¬ ë°ì´í„° ê²€ìˆ˜ ë° ì¶”ê°€ í™•ë³´ë¥¼ ì§„í–‰í–ˆì§€ë§Œ, ì—¬ì „íˆ ì œí•œëœ ë°ì´í„° í™˜ê²½ì—ì„œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•´ì•¼ í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ë§ ì „ëµ ë° ì„±ëŠ¥ ê°œì„  ëª¨ë¸ ì„ íƒ: U-Net ê¸°ë°˜ ë°ì´í„°ì˜ íŠ¹ì§•ìƒ ì „ì²´ ì´ë¯¸ì§€ ì¤‘ ë ˆì´ë¸”(ë§ˆìŠ¤í¬)ì´ í¬í•¨ëœ ë¶€ë¶„ì´ ë§¤ìš° ì‘ì€ ì˜ë£Œ ë„ë©”ì¸ì˜ íŠ¹ì„±ìƒ, U-Net ê³„ì—´ ëª¨ë¸ì´ ê°€ì¥ ë†’ì€ ì„±ëŠ¥ì„ ë³´ì´ëŠ” ê²ƒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì‹¤ì œ ì—°êµ¬ ê²°ê³¼ì™€ í”„ë¡œì íŠ¸ ë‚´ ë¹„êµ ì‹¤í—˜ì—ì„œë„ U-Netì´ ìµœê³  ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•œ í•µì‹¬ ê¸°ë²•ë“¤ ì „ì²˜ë¦¬: CLAHE ì ìš© ëª…ì•”ëŒ€ë¹„ í–¥ìƒ ê¸°ë²•ì¸ CLAHEë¥¼ ì ìš©í•˜ì—¬ ë°ì´í„°ì˜ íŠ¹ì§•ì„ ë” ëª…í™•í•˜ê²Œ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤. !CLAHE ì ìš© ê²°ê³¼ ìœ¡ì•ˆìœ¼ë¡œë„ íŠ¹ì§•ì´ ë” ëª…í™•í•˜ê²Œ ë³´ì´ë©°, ì‹¤ì œ ì„±ëŠ¥ë„ í–¥ìƒë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ ê³¼ë„í•œ ëª…ì•”ëŒ€ë¹„ í–¥ìƒì€ false positiveë¥¼ ì¦ê°€ì‹œí‚¤ë¯€ë¡œ ì‹¤í—˜ì„ í†µí•´ ì ì • ìˆ˜ì¤€ì„ ì„¤ì •í–ˆìŠµë‹ˆë‹¤. Contrastive Learning ê¸°ë°˜ ì‚¬ì „í•™ìŠµ ImageNet ì‚¬ì „í•™ìŠµ ê°€ì¤‘ì¹˜ê°€ ì•ˆì €ì‚¬ì§„ê³¼ ê°™ì€ íŠ¹ìˆ˜í•œ ì˜ë£Œ ì˜ìƒì—ëŠ” íš¨ê³¼ì ì´ì§€ ì•Šë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ì„ í†µí•´ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ëŒ€ì‹  íšŒì‚¬ ë‚´ ë³´ìœ í•œ ëŒ€ëŸ‰ì˜ ì•ˆì €ì‚¬ì§„(ë ˆì´ë¸” ì—†ìŒ)ì„ í™œìš©í•˜ì—¬ contrastive learning ê¸°ë°˜ ì‚¬ì „í•™ìŠµì„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ê²°ê³¼: ì„±ëŠ¥ 10-15% í–¥ìƒ ë‹¬ì„± Multi-Task Learning U-Netì˜ ì¸ì½”ë”-ë””ì½”ë” êµ¬ì¡°ë¥¼ í™œìš©í•˜ì—¬ segmentation ì™¸ì— reconstruction taskë¥¼ ì¶”ê°€ë¡œ ìˆ˜í–‰í•˜ëŠ” multi-task learningì„ ì ìš©í–ˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì„œë¹™ ë° ë°°í¬ ë°°í¬ ì „ëµ ì„œë¹„ìŠ¤ ì´ˆê¸° ë‹¨ê³„ì˜ íŠ¹ì„±ì„ ê³ ë ¤í•˜ì—¬ ë¹ ë¥¸ ëª¨ë¸ ë°°í¬ê°€ ê°€ëŠ¥í•œ ë°©ë²•ì„ ì±„íƒí–ˆìŠµë‹ˆë‹¤. ê¸°ìˆ  ìŠ¤íƒ ëª¨ë¸ ìµœì í™”: TensorRT engine ë³€í™˜ ì„œë¹™ í”Œë«í¼: Triton Server ì¥ì : ëª¨ë¸ ì¶”ë¡  ìµœì í™” ìš©ì´ GPU ìì› ê´€ë¦¬ ìë™í™” ìš”ì²­/ì‘ë‹µ ì²˜ë¦¬ ê°„ì†Œí™” í”„ë¡œì íŠ¸ ì„±ê³¼ í•­ëª© ë‚´ìš© ------ ------ í”„ë¡œì íŠ¸ëª… ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ ë³‘ë³€ íƒì§€ ëª¨ë¸ë§ ë° ì„œë¹„ìŠ¤ ê°œë°œ ê¸°ê°„ 2024ë…„ 4ì›” ~ 2025ë…„ 6ì›” (3ê°œì›”) ì£¼ìš” ì„±ê³¼ â€¢ Contrastive Learning ì ìš©ìœ¼ë¡œ ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ<br>â€¢ ì œí•œëœ ë°ì´í„°ì—ì„œë„ ëª¨ë¸ íŒë³„ë ¥ ê°œì„ <br>â€¢ Multi-Task Learningìœ¼ë¡œ ë³‘ë³€ íƒì§€ ì„±ëŠ¥ ìµœì í™”<br>â€¢ Segmentation ì„±ëŠ¥: IoU ê°œì„ , Dice Score 0.8 ë‹¬ì„± ì—­í•  â€¢ ìµœì‹  ì—°êµ¬ ë¶„ì„ ë° ëª¨ë¸ ì„¤ê³„ ì ìš©<br>â€¢ ë°ì´í„° ë¶€ì¡± ë¬¸ì œ í•´ê²°ì„ ìœ„í•œ ê¸°ë²• ì ìš©<br>â€¢ ë°ì´í„° í’ˆì§ˆ ê°œì„  ë° ì •ì œ<br>â€¢ ëª¨ë¸ ìµœì í™” ë° í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ê¸°ìˆ  ìŠ¤íƒ PyTorch, Triton-Server, gRPC, Docker"
  },
  {
    "id": "Portfolio/ocr_pipeline",
    "title": "OCR Pipeline System",
    "date": "2024-10-20",
    "excerpt": "",
    "category": "",
    "tags": [],
    "content": "OCR Pipeline System í”„ë¡œì íŠ¸ ê°œìš” ë³‘ì› ê²€ì‚¬ ê²°ê³¼ ì´ë¯¸ì§€ì—ì„œ ì‹¤ì‹œê°„ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ DBì— ìë™ ì ì¬í•˜ëŠ” OCR íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œì„ êµ¬ì¶•í–ˆìŠµë‹ˆë‹¤. ê¸°ì¡´ ìˆ˜ì‘ì—…ìœ¼ë¡œ ì§„í–‰ë˜ë˜ ë°ì´í„° ì…ë ¥ ê³¼ì •ì„ ìë™í™”í•˜ì—¬ ê²€ì•ˆì‚¬ì˜ ì—…ë¬´ íš¨ìœ¨ì„±ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ì£¼ìš” ê¸°ëŠ¥ ì‹¤ì‹œê°„ OCR ì²˜ë¦¬: ê²€ì‚¬ ì™„ë£Œ ì¦‰ì‹œ ì´ë¯¸ì§€ì—ì„œ ë°ì´í„° ì¶”ì¶œ ê³ ì • ì´ë¯¸ì§€ ìµœì í™”: ê²€ì‚¬ ì¥ë¹„ë³„ ê³ ì •ëœ ì´ë¯¸ì§€ êµ¬ì¡° í™œìš© ë‹¤ì¤‘ OCR ì—”ì§„: TrOCR ê¸°ë°˜ ê³ ì •ë°€ í…ìŠ¤íŠ¸ ì¸ì‹ ë°ì´í„° í’ˆì§ˆ ìë™í™”: ë¶„í¬ ë¶„ì„ ë° ì´ìƒì¹˜ íƒì§€ ìë™í™” ì‹œìŠ¤í…œ ì¼ê´€ì„± ê²€ì¦: ê³ ê°ë²ˆí˜¸ ë“± ê³ ì •ê°’ë“¤ì˜ ë¬´ê²°ì„± ê²€ì¦ ìë™ ëª¨ë‹ˆí„°ë§: Slack ê¸°ë°˜ ì‹¤ì‹œê°„ ì—ëŸ¬ ì•Œë¦¼ ë° ì‘ì—… ìš”ì•½ ì•ˆì •ì  íŒŒì´í”„ë¼ì¸: 1% ë¯¸ë§Œ ì—ëŸ¬ìœ¨ì˜ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜ ê¸°ìˆ  ìŠ¤íƒ OCR Engine: TrOCR (Microsoft) Image Processing: OpenCV, PIL Backend: Python, gRPC, FastAPI Model Serving: Triton Server Containerization: Docker Monitoring: Slack API Database: PostgreSQL í•µì‹¬ ê¸°ìˆ ì  ë„ì „ê³¼ í•´ê²°ì±… OCR ì •í™•ë„ 99% ë‹¬ì„± ë¬¸ì œ: ê²€ì‚¬ ê²°ê³¼ì˜ ì •í™•ì„±ì´ ë§¤ìš° ì¤‘ìš”í•˜ë¯€ë¡œ ë†’ì€ OCR ì •í™•ë„ í•„ìš” í•´ê²°ì±…: ê³ ì • ì´ë¯¸ì§€ êµ¬ì¡° í™œìš©: ê²€ì‚¬ ì¥ë¹„ë³„ ê³ ì •ëœ ì´ë¯¸ì§€ ë ˆì´ì•„ì›ƒì„ í™œìš©í•˜ì—¬ Text Detection ë‹¨ê³„ ìƒëµ TrOCR ëª¨ë¸ ì ìš©: Text Recognitionì— íŠ¹í™”ëœ TrOCR ëª¨ë¸ ì‚¬ìš©ìœ¼ë¡œ ì •í™•ë„ í–¥ìƒ ì´ë¯¸ì§€ ì „ì²˜ë¦¬: ë…¸ì´ì¦ˆ ì œê±°, ëŒ€ë¹„ ê°œì„  ë“± ì „ì²˜ë¦¬ ê³¼ì • ìµœì í™” ê²°ê³¼ í›„ì²˜ë¦¬: OCR ê²°ê³¼ ì •ì œ ë° ê²€ì¦ ë¡œì§ êµ¬í˜„ ë°ì´í„° í’ˆì§ˆ ìë™í™”: ê° ê°’ì˜ ë¶„í¬ ë¶„ì„ ë° ì´ìƒì¹˜ íƒì§€ ìë™í™” ì‹œìŠ¤í…œ êµ¬ì¶• ì¼ê´€ì„± ê²€ì¦: ê³ ê°ë²ˆí˜¸ ë“± ê³ ì •ê°’ë“¤ì˜ ì¼ê´€ì„± ê²€ì¦ ë¡œì§ ê°œë°œ ê²°ê³¼: OCR ì •í™•ë„ 99% ë‹¬ì„± ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ë¬¸ì œ: ì‹¤ì‹œê°„ ë°ì´í„° ì¶”ì¶œì„ ìœ„í•œ ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ ìš´ì˜ í•„ìš” í•´ê²°ì±…: ê°ì²´ ì§€í–¥ ì„¤ê³„: ê° ê¸°ëŠ¥ì„ ë‹´ë‹¹í•˜ëŠ” ê°ì²´ë“¤ë¡œ íŒŒì´í”„ë¼ì¸ êµ¬ì„± ë‹¨ê³„ë³„ ì—ëŸ¬ ì²˜ë¦¬: ì´ë¯¸ì§€ ì†¡ìˆ˜ì‹ , ì „ì²˜ë¦¬, OCR, í›„ì²˜ë¦¬, DB ì ì¬ ê° ë‹¨ê³„ë³„ ëª…í™•í•œ ì—ëŸ¬ ì²˜ë¦¬ ë¹„ë™ê¸° ì²˜ë¦¬: 1ì´ˆ ì´ë‚´ ì‘ë‹µì„ ìœ„í•œ ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬ì¡° êµ¬í˜„ ìë™ ëª¨ë‹ˆí„°ë§: ê° ê°ì²´ë³„ ëª¨ë‹ˆí„°ë§ ìë™í™” ê²°ê³¼: ì—ëŸ¬ìœ¨ 1% ë¯¸ë§Œì˜ ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë°©í•´ ìµœì†Œí™” ë¬¸ì œ: ê²€ì•ˆì‚¬ì˜ ê¸°ì¡´ ì—…ë¬´ í”„ë¡œì„¸ìŠ¤ì— ë³€í™”ë¥¼ ì£¼ì§€ ì•Šìœ¼ë©´ì„œ OCR ì„œë¹„ìŠ¤ ì¶”ê°€ í•´ê²°ì±…: Client-Server êµ¬ì¡°: ë³‘ì› ì»´í“¨í„°ì˜ clientê°€ ì´ë¯¸ì§€ë¥¼ ì „ì†¡ í›„ ê¸°ì¡´ \"ì•„ì´ë¦¬ë”\" í”„ë¡œê·¸ë¨ì— ìë™ ì ì¬ gRPC í†µì‹ : ë¹ ë¥¸ ì´ë¯¸ì§€ ì „ì†¡ì„ ìœ„í•œ gRPC í”„ë¡œí† ì½œ ì‚¬ìš© ê¶Œí•œ ë¬¸ì œ í•´ê²°: ë‹¤ì–‘í•œ ë³‘ì› í™˜ê²½ì— ë§ëŠ” ë°°í¬ ë°©ì‹ (Python/Go) ì‘ë‹µ ì†ë„ ìµœì í™”: 1ì´ˆ ì´ë‚´ ì‘ë‹µìœ¼ë¡œ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì§€ì—° ìµœì†Œí™” ê²°ê³¼: ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ì— ë°©í•´ ì—†ì´ OCR ì„œë¹„ìŠ¤ ì„±ê³µì  í†µí•© ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜ Client í”„ë¡œê·¸ë¨ ëª©ì : ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ë°©í•´ ì—†ì´ OCR ì„œë²„ë¡œ ì´ë¯¸ì§€ ì „ì†¡ ê¸°ìˆ : gRPCë¥¼ í†µí•œ ê³ ì† ì´ë¯¸ì§€ ì „ì†¡ í™˜ê²½ ëŒ€ì‘: ë‹¤ì–‘í•œ ë³‘ì› ì»´í“¨í„° í™˜ê²½ì— ë§ëŠ” ë°°í¬ (Python/Go) ìë™í™”: ì´ë¯¸ì§€ ì „ì†¡ í›„ ê¸°ì¡´ \"ì•„ì´ë¦¬ë”\" í”„ë¡œê·¸ë¨ì— ìë™ ì ì¬ OCR Server ë¹„ë™ê¸° ì²˜ë¦¬: ì´ë¯¸ì§€ ìˆ˜ì‹  ì¦‰ì‹œ ì‘ë‹µ í›„ ë°±ê·¸ë¼ìš´ë“œì—ì„œ OCR ì²˜ë¦¬ ëª¨ë¸ ì„œë¹™: Triton Serverë¥¼ í†µí•œ TrOCR ëª¨ë¸ ê³ ì„±ëŠ¥ ì„œë¹™ GPU í™œìš©: ì˜¨í”„ë ˆë¯¸ìŠ¤ GPU ì„œë²„ë¥¼ í†µí•œ ê³ ì† ì¶”ë¡  ê³ ì • ì´ë¯¸ì§€ ì²˜ë¦¬: ê²€ì‚¬ ì¥ë¹„ë³„ pixel ê¸°ë°˜ ì¡°ê±´ë¬¸ìœ¼ë¡œ Text Detection ìƒëµ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ Slack ì•Œë¦¼: ì—ëŸ¬ ë°œìƒ ì‹œ ì‹¤ì‹œê°„ ì•Œë¦¼ ì‘ì—… ìš”ì•½: í•˜ë£¨ ì‘ì—…ëŸ‰ ë° ì„±ê³µë¥  ìë™ ë³´ê³  ê°ì²´ë³„ ëª¨ë‹ˆí„°ë§: ê° ê¸°ëŠ¥ë³„ ë…ë¦½ì ì¸ ëª¨ë‹ˆí„°ë§ ë¡œê·¸ ê´€ë¦¬: ê¹”ë”í•œ ë¡œê·¸ êµ¬ì¡°ë¡œ ì‰¬ìš´ ë¬¸ì œ ì§„ë‹¨ í•µì‹¬ ìµœì í™” í¬ì¸íŠ¸ Text Detection ìƒëµ ê²€ì‚¬ ê²°ê³¼ ì´ë¯¸ì§€ê°€ ê³ ì •ëœ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆë‹¤ëŠ” íŠ¹ì„±ì„ í™œìš©: ê²€ì‚¬ ì¥ë¹„ë³„ ì´ë¯¸ì§€ ë²„ì „ ë° í˜ì´ì§€ êµ¬ë¶„ Pixel ê°’ ê¸°ë°˜ ì¡°ê±´ë¬¸ìœ¼ë¡œ ì˜ì—­ ë¶„í•  Text Recognition ë‹¨ê³„ì—ë§Œ ì§‘ì¤‘í•˜ì—¬ ì²˜ë¦¬ ì†ë„ í–¥ìƒ TrOCR ëª¨ë¸ ì„ íƒ ë‹¤ì–‘í•œ OCR ëª¨ë¸ ë¹„êµ í›„ TrOCR ì„ íƒ: ì •í™•ë„: ë²¤ì¹˜ë§ˆí¬ì—ì„œ 98% ì •í™•ë„ ì‹¤ì œ ì„±ëŠ¥: ìˆ«ì ì¸ì‹ 100% ì •í™•ë„ í•œêµ­ì–´ ì§€ì›: ë¬¸ì ì¸ì‹ì€ ë‹¨ìœ„ ì •ë³´ë¡œ ê³ ì • ìœ„ì¹˜ í™œìš© ë¹„ë™ê¸° ì²˜ë¦¬ êµ¬ì¡° ì‚¬ìš©ì ê²½í—˜ì„ ìœ„í•œ ì‘ë‹µ ì†ë„ ìµœì í™”: ì´ë¯¸ì§€ ìˆ˜ì‹  ì¦‰ì‹œ ì‘ë‹µ (1ì´ˆ ì´ë‚´) ë°±ê·¸ë¼ìš´ë“œì—ì„œ OCR ì²˜ë¦¬ ìƒë‹´ ì‹œì ê¹Œì§€ ì¶©ë¶„í•œ ì²˜ë¦¬ ì‹œê°„ í™•ë³´ ê²°ê³¼ ë° ì„±ê³¼ OCR ì •í™•ë„: 99% ë‹¬ì„± ì—ëŸ¬ìœ¨: 1% ë¯¸ë§Œì˜ ì•ˆì •ì ì¸ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• ì²˜ë¦¬ ì†ë„: 1ì´ˆ ì´ë‚´ ì‘ë‹µìœ¼ë¡œ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì§€ì—° ìµœì†Œí™” ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨: ì½”ë“œ êµ¬ì¡° ê°œì„ ìœ¼ë¡œ ìœ ì§€ë³´ìˆ˜ íš¨ìœ¨ 10ë°° ì¦ê°€ ëª¨ë‹ˆí„°ë§ ë¹„ìš©: ìë™í™”ë¥¼ í†µí•œ ëª¨ë‹ˆí„°ë§ ì‹œê°„ 90% ê°ì†Œ í•™ìŠµí•œ ì  ê³ ì • ì´ë¯¸ì§€ êµ¬ì¡°ë¥¼ í™œìš©í•œ OCR ìµœì í™” ë°©ë²• ì‹¤ì‹œê°„ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ì˜ ì•ˆì •ì„± í™•ë³´ ë°©ë²• ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ì™€ì˜ í†µí•© ì‹œ ì‚¬ìš©ì ê²½í—˜ ê³ ë ¤ì˜ ì¤‘ìš”ì„± ê°ì²´ ì§€í–¥ ì„¤ê³„ë¥¼ í†µí•œ ìœ ì§€ë³´ìˆ˜ì„± í–¥ìƒ ìë™í™”ëœ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œì˜ êµ¬ì¶• ë° ìš´ì˜ ë°©ë²•"
  },
  {
    "id": "Recommendation/2024-07-05-recommendation-system-interest",
    "title": "ì¶”ì²œì‹œìŠ¤í…œì— ê´€ì‹¬ìˆëŠ” ì´ìœ ",
    "date": "2024-07-04 23:30:00 +0900",
    "excerpt": "",
    "category": "Recommendation",
    "tags": [
      "Recommendation"
    ],
    "content": "ì ê¹ ì¶”ì²œì‹œìŠ¤í…œì„ ê³µë¶€í–ˆì—ˆëŠ”ë°, ë‹¤ì‹œ ê³µë¶€ë¥¼ ì‹œì‘í•˜ë©´ì„œ í”„ë¡œì íŠ¸ë¥¼ í•˜ë‚˜ì”© ìŒ“ì•„ê°€ë ¤ê³  í•œë‹¤. ê·¸ ì „ì— ë‚´ê°€ ì™œ ê´€ì‹¬ì´ ìˆëŠ”ì§€, ë˜ ì–´ë–¤ í”„ë¡œì íŠ¸ë¥¼ í•´ë³´ê³  ì‹¶ì€ì§€ ì •ë¦¬í•´ë³´ë ¤ í•œë‹¤. ì¶”ì²œì‹œìŠ¤í…œì´ë€? ì¶”ì²œì‹œìŠ¤í…œì€ ê²€ìƒ‰ê³¼ ë¹„ìŠ·í•œ ëª©ì ì„ ê°€ì§€ê³  ìˆë‹¤. > ë§ì€ ì •ë³´ ì†ì—ì„œ í•„ìš”í•œ ì •ë³´ë¥¼ í•„í„°ë§í•˜ëŠ” ê²ƒ \"ì§„ì§œ ë§ì€ ì •ë³´ë“¤, ì»¨í…ì¸ ë“¤ì´ ìˆëŠ”ë° ìœ ì €ì—ê²Œ ì–´ë–¤ ê²ƒì„ ë³´ì—¬ì¤˜ì•¼í• ê¹Œ?\"ì— ëŒ€í•œ ë‹µì„ í•˜ëŠ” ê²ƒì´ë‹¤. ì™œ ì¤‘ìš”í• ê¹Œ? ì¶”ì²œì‹œìŠ¤í…œì´ í•„ìš”í•œ ê³³ì€ ëŒ€í‘œì ìœ¼ë¡œ OTT íšŒì‚¬ë“¤ì´ ë– ì˜¤ë¥¸ë‹¤. ì†Œë¹„ìëŠ” ì–¸ì œ êµ¬ë…ì„ ê·¸ë§Œë‘˜ê¹Œ? > ë‹¹ì—°í•˜ê²Œë„ ë” ì´ìƒ ë³¼ê²Œ ì—†ë‹¤ê³  ëŠê»´ì§ˆë•Œ ê·¸ë§Œë‘˜ ê²ƒì´ë‹¤. ì „ì²´ ì»¨í…ì¸ ì— ë¹„í•´ ì†Œë¹„í•œ ì»¨í…ì¸ ëŠ” ì†Œìˆ˜ì¼í…ë° ì™œ ë³¼ê²Œ ì—†ë‹¤ê³  ëŠê»´ì§ˆê¹Œ? > ë­˜ ë´ì•¼í• ì§€ ëª¨ë¥´ê² ì–´ì„œ, ì–´ë–¤ê²Œ ë‚´ ì¬ë°Œì„ì§€ ëª°ë¼ì„œ. ê·¸ë˜ì„œ ì†Œë¹„ìì—ê²Œ ì·¨í–¥ì— ë§ëŠ” ì»¨í…ì¸ ë¥¼ ê¾¸ì¤€íˆ ë…¸ì¶œí•´ì¤˜ì•¼í•˜ê³ , ê·¸ë˜ì•¼ ì†Œë¹„ìê°€ ë– ë‚˜ì§€ ì•Šë„ë¡ ë§Œë“¤ ìˆ˜ ìˆë‹¤. ì–´ë ¤ìš´ ì  ë‚´ê°€ ìƒê°í–ˆì„ ë•Œ ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬ì„±í•˜ëŠ”ë° ê°€ì¥ ì–´ë ¤ìš´ ê²ƒì€ ì†Œë¹„ìì˜ í‰ê°€ ë°ì´í„°ê°€ ì ê¸° ë•Œë¬¸ì¸ ê²ƒ ê°™ë‹¤. ì–´ë–¤ ì‚¬ëŒì´ í‰ê°€í•œê²Œ ì ìœ¼ë©´ ë‹¹ì—°íˆ ì·¨í–¥ì„ ë¶„ì„í•˜ê¸° ì–´ë µê³ , ê·¸ë˜ì„œ ì¶”ì²œí•´ì£¼ê¸°ë„ ì–´ë µë‹¤. í‰ê°€ ë°ì´í„°ê°€ ë§ì€ ì‚¬ëŒì—ê²Œ ì˜ ì¶”ì²œí•´ì£¼ëŠ” ê²ƒë„ êµ‰ì¥íˆ ì¤‘ìš”í•˜ì§€ë§Œ, í‰ê°€ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì‚¬ëŒì—ê²Œ ì–´ë–»ê²Œ ì¶”ì²œí•´ì¤„ì§€ì— ëŒ€í•œ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ê²Œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒ ê°™ë‹¤. í•´ë³´ê³  ì‹¶ì€ í”„ë¡œì íŠ¸ ìš”ì¦˜ í•´ë³´ë ¤ëŠ” í”„ë¡œì íŠ¸ëŠ” LLMì„ ì´ìš©í•´ì„œ ì†Œë¹„ìì˜ ì·¨í–¥ì„ ì•Œì•„ë‚´ëŠ” ê²ƒì´ë‹¤. êµ¬ì²´ì ìœ¼ë¡œëŠ” Multi-Agent-Systemìœ¼ë¡œ ì¶”ì²œì‹œìŠ¤í…œì„ êµ¬ì¶•í•´ë³´ë ¤ê³  í•˜ê³ , ë§¨ ì•ë‹¨ì— ì·¨í–¥ì„ ë¶„ì„í•˜ëŠ” Agentë¥¼ êµ¬ì¶•í•´ ì‚¬ìš©í•´ë³´ë ¤ê³  í•œë‹¤. ë¨¼ì € ì˜í™”ë¡œ ì‹œì‘í•´ì„œ ì±…ê³¼ ê°™ì€ ë¶„ì•¼ë¡œë„ í™•ì¥í•´ë³´ë ¤ê³  í•œë‹¤. ì·¨í–¥ì„ ë¶„ì„í•œë‹¤ëŠ”ê²Œ êµ‰ì¥íˆ ì• ë§¤í•œ ë¶€ë¶„ì¸ ê²ƒ ê°™ì•„ì„œ ê³ ë¯¼ì´ ë§ì´ í•„ìš”í•  ê²ƒ ê°™ë‹¤. ì´ê²Œ ê°€ëŠ¥í•´ì§„ë‹¤ë©´ ë§ì€ ì‚°ì—…ì—ì„œ ì •ë§ ë§¤ë ¥ì ìœ¼ë¡œ ëŠë¼ëŠ” ê¸°ìˆ ì´ ì•„ë‹ê¹Œ? ì´ 3ëª…ì´ì„œ ì´ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•  ê²ƒ ê°™ì€ë° ê·¸ ê³¼ì •ì„ ë¸”ë¡œê·¸ì— ë‚¨ê²¨ë³´ë ¤ê³  í•œë‹¤."
  },
  {
    "id": "Reflections/2024-07-04-interest-concerns",
    "title": "ê´€ì‹¬ì‚¬ì— ëŒ€í•œ ê³ ë¯¼",
    "date": "2024-07-04 19:49:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "Interest"
    ],
    "content": "í˜„ì¬ ë‚˜ì˜ ìƒí™© ì´ì œ ë°ì´í„°ê³¼í•™ìë¡œ ì¼í•œì§€ 3ê°œì›”ì •ë„ ë˜ì—ˆë‹¤. ì•„ì´í ì´ë¼ëŠ” ëª¨ë‘ì˜ ì—°êµ¬ì†Œì—ì„œ ìš´ì˜í•˜ëŠ” ë¶€íŠ¸ìº í”„ë¥¼ ì§„í–‰í–ˆì—ˆëŠ”ë°, ì´ë•Œ ê¸°ì—…ì—°ê³„í”„ë¡œì íŠ¸ë¡œ ì§„í–‰í–ˆë˜ ê¸°ì—…ì— ì·¨ì—…ì„ í–ˆë‹¤. ë¶„ì•¼ëŠ” ì•ˆê³¼ì˜ë£Œë¶„ì•¼ì´ë©°, ë¹„ì•¤ë¹› ë°ì€ì„¸ìƒ ì•ˆê³¼ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ ìŠ¤íƒ€íŠ¸ì—…ì´ë‹¤. ì´ë•Œ ì§„í–‰í•œ í”„ë¡œì íŠ¸ëŠ” ë‹¹ë‡¨ë³‘ì„± ë§ë§‰ë³‘ì¦ì˜ ë³‘ë³€ë“¤ì„ íƒì§€í•´ë‚´ëŠ” ëª¨ë¸ì„ ë§Œë“œëŠ” ê²ƒì´ì—ˆë‹¤. ì´ì— ëŒ€í•œ ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. ë°œí‘œìë£Œ ë°œí‘œë§í¬ ê²°ê³¼ì ìœ¼ë¡œëŠ” ì˜ë£Œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  Segmentation ì‘ì—…ì„ ìˆ˜í–‰í•˜ê³  ë°œì „ì‹œì¼œë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤. ë˜í•œ ì´ê²Œ ì–´ë–»ê²Œ ì“°ì¼ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ê³ ë¯¼ë„ í•´ë³¼ ìˆ˜ ìˆì–´ì„œ ì¢‹ì•˜ë‹¤. ê·¸ë ‡ê²Œ ì¸ê³µì§€ëŠ¥ì„ í†µí•´ ê±´ê°•ì— ê¸°ì—¬í•  ìˆ˜ ìˆê² ë‹¤ëŠ” ê¿ˆì„ ê°€ì§€ê³  ì…ì‚¬ë¥¼ í•˜ê²Œ ë˜ì—ˆë‹¤. ëŒ€ë¶€ë¶„ì˜ ìŠ¤í† ë¦¬ê°€ ê·¸ëŸ¬í•˜ë“¯, ë‚˜ ì—­ì‹œ ê¸°ëŒ€ì™€ ë§ì´ ë‹¤ë¥¸ íšŒì‚¬ìƒí™œì„ í•˜ê²Œ ë˜ì—ˆë‹¤. ì•ˆê³¼ ë¶„ì•¼ì—ì„œì˜ ì¸ê³µì§€ëŠ¥ ê°œë°œí˜„í™© ì˜ë£Œ ë¶„ì•¼ ì¤‘ì—ì„œ íŠ¹íˆ ì•ˆê³¼ì— ëŒ€í•´ì„œë§Œ ì•Œê³ , ê·¸ë˜ì„œ ì•ˆê³¼ì— ëŒ€í•´ì„œë§Œ í•œì •í•´ì„œ ì´ì•¼ê¸°í•˜ë ¤ê³  í•œë‹¤. ë¹„ì¥¬ì›ìŠ¤ ë‚´ê°€ ì¼í•˜ê³  ìˆëŠ” ë¹„ì¥¬ì›ìŠ¤ë¼ëŠ” ê¸°ì—…ì€ ë‚´ê°€ ëŠë¼ê¸°ì— ê°€ì¥ ì„±ì·¨ê°€ ì ë‹¤. ì˜ë£Œì¸ì¦ì„ ë°›ì€ ëª¨ë¸ë„ ì—†ê³ , ë°›ìœ¼ë ¤ë©´ ì•„ì§ ë©€ì—ˆë‹¤. ìì„¸í•˜ê²Œ ì´ì•¼ê¸°í•  ìˆ˜ëŠ” ì—†ì§€ë§Œ, íšŒì‚¬ì˜ ë°©í–¥ë„ ë‹¹ì¥ì˜ ìˆ˜ìµì„ ë§Œë“œëŠ” ê²ƒì— ì§‘ì¤‘í•˜ê³  ìˆë‹¤. ë”°ë¼ì„œ ë‚´ê°€ ì›í–ˆë˜ ë¶€ë¶„ê³¼ëŠ” ë§ì´ ë‹¤ë¥´ê²Œ ì¼ì„ í•˜ê³  ìˆë‹¤. ë‹¤ë¥¸ íšŒì‚¬ë¡œ ì´ì§í•˜ë©´ ë˜ëŠ” ê±¸ê¹Œ? ì„ ë‘ê¸°ì—…ë“¤ì€ ì–´ë–»ê²Œ í•˜ê³  ìˆì„ê¹Œ? ë·°ë…¸ ë·°ë…¸ì—ì„œëŠ” ì•ˆì €ì‚¬ì§„ì„ í†µí•œ ì§ˆë³‘ ë¶„ë¥˜ ëª¨ë¸ì„ ì¶œì‹œí•˜ê³ , ê²°ê³¼ì— CAMë„ ê°™ì´ ë³´ì—¬ì¤€ë‹¤. (CAMì„ í†µí•´ ëª¨ë¸ì´ ì–´ëŠ ë¶€ë¶„ì„ ì§‘ì¤‘ì ìœ¼ë¡œ ë³´ê³  ìˆëŠ”ì§€ 'ì¶”ì •'í•  ìˆ˜ ìˆìŒ) ë³‘ì›ì— ìˆëŠ” ì˜ì‚¬ë¶„ê»˜ ë“£ê¸°ë¡œëŠ” ì „í˜€ ì•ˆì“°ì¸ë‹¤ê³  í•œë‹¤. ì •í™•ë„ê°€ ë‚®ê³ , ëª‡ë²ˆ ì •í™•ë„ê°€ ë‚®ë‹¤ëŠ” ìƒê°ì´ ë“¤ë©´ ë°”ë¡œ ì•ˆì“°ê²Œ ëœë‹¤ê³  í•œë‹¤. êµ³ì´? ì´ëŸ° ë§ˆìŒì´ í° ê²ƒ ê°™ë‹¤. ë©”ë””ë¹„ì „ ê°€ì¥ ì¬ë¯¸ìˆì–´ ë³´ì´ëŠ” ê¸°ì—…ì´ë‹¤. ì•ˆì €ì‚¬ì§„ì„ í†µí•´ ì‹¬ì§ˆí™˜ ìœ„í—˜ë„ ì˜ˆì¸¡, ì½©íŒ¥ ìœ„í—˜ë„ ì˜ˆì¸¡ì„ ì§„í–‰í•œë‹¤. ì‹œë¦¬ì¦ˆB(100ì–µ)íˆ¬ìë¥¼ ë°›ê³ , ìŸìŸí•œ ì´ì‚¬ì§„ë“¤ê³¼ í•¨ê»˜ ëª¨ë¸ì„ ê°œë°œí•˜ê³  ì˜ë£Œì¸ì¦ì„ ì§„í–‰í•˜ê³  ìˆë‹¤. ì•„ë§ˆë„ êµ­ë‚´ëŠ” ìˆ˜ê°€ê°€ ë‚®ê³  ë³´ìˆ˜ì ì´ë¼ëŠ” ì´ìœ  ë•Œë¬¸ì— ë¯¸êµ­ FDA ìŠ¹ì¸ì„ ì§„í–‰ ì¤‘ì´ë©°, ì½©íŒ¥ ëª¨ë¸ì˜ ê²½ìš°ì—ëŠ” ìŠ¹ì¸ì„ ë°›ì€ ê²ƒìœ¼ë¡œ ì•Œê³  ìˆë‹¤. ë§í¬ë“œì¸ì„ íŒ”ë¡œì‰í•˜ë©° ë³´ê³  ìˆëŠ”ë°, í•™ë¬¸ì ì¸ ë°œí‘œë„ ë§ì´ í•˜ëŠ” ê²ƒìœ¼ë¡œ ë³´ì¸ë‹¤. í™ˆí˜ì´ì§€ì—ì„œë„ ë°œí–‰í•œ ë…¼ë¬¸ë“¤ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë‹¤ë§Œ ê³¼ì—° ìˆ˜ìµì´ ë‚ ê¹Œ? ì— ëŒ€í•´ì„œëŠ” ê¶ê¸ˆí•˜ê³  ë‚¬ìœ¼ë©´ ì¢‹ê² ë‹¤. ë‚˜ì˜ ì§„ë¡œì— ëŒ€í•œ ê³ ë¯¼ ì´ëŸ° ìƒí™©ì†ì—ì„œ ë‚˜ëŠ” ë‚˜ì˜ í¥ë¯¸ì— ëŒ€í•´ í•œë²ˆ ë” ìƒê°í•´ë³´ê²Œ ëë‹¤. ê·¸ë¦¬ê³  ì´ëŸ° í¥ë¯¸ì— ë”°ë¼ ì§„ë¡œë¥¼ ê²°ì •í•˜ê²Œ ë˜ì—ˆë‹¤. ê±´ê°•ì— ê¸°ì—¬í•˜ê³  ì‹¶ì€ê°€? ê³¼ì—° ë‚˜ëŠ” ì§„ì§œ ê±´ê°•ì— ê¸°ì—¬í•˜ê³  ì‹¶ì„ê¹Œ? ë‹¹ì¥ì— ë‚˜ì˜ ì˜í–¥ì´ ì „í˜€ ëŠê»´ì§€ì§€ ì•ŠëŠ” ìƒí™©ì—ì„œ, ì´ëŸ° ê¿ˆì„ ê°€ì§€ê³  ê°œë°œì„ ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆì„ê¹Œ? ë‚˜ì˜ ëŒ€ë‹µì€ \"ì•„ë‹ˆ, ì „í˜€\" . í•­ìƒ ëŠë¼ì§€ë§Œ ë‚˜ëŠ” ì†Œë¹„ìì™€ ë°€ì ‘í•œ ë¶„ì•¼ì—ì„œ ê°œë°œì„ í•˜ê³  ì‹¶ë‹¤. ë‚´ê°€ ë§Œë“  ê²ƒë“¤ì´ ë°”ë¡œ ì‚¬ëŒë“¤ì—ê²Œ ì˜í–¥ì„ ë¯¸ì³¤ìœ¼ë©´ ì¢‹ê² ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ëŠ ë¶„ì•¼? ê³ 3ë•Œ ë‚´ ê¿ˆì€ PD ì˜€ë‹¤. ë¬¼ë¡  ì´ê³¼ì˜€ì§€ë§Œ, ì»¨í…ì¸ ë“¤ì„ ì†Œë¹„í•˜ë©° ì‚¶ì„ ì´ì–´ë‚˜ê°ˆ ìˆ˜ ìˆì—ˆê³  ë§ì€ ì˜ê°ì„ ë°›ì•˜ê¸° ë•Œë¬¸ì´ë‹¤. ë‚˜ë„ ì¢‹ì€ ì»¨í…ì¸ ë¥¼ ë§Œë“¤ì–´ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ê¸ì •ì ì¸ ì˜í–¥ì„ ì£¼ê³  ì‹¶ì—ˆë‹¤. í•‘ê³„ì§€ë§Œ ì´ëŸ° ìƒê°ì„ ë°œì „ì‹œí‚¬ ìƒí™©ì´ ì•„ë‹ˆì—ˆê³  ìì—°ìŠ¤ëŸ½ê²Œ í˜ëŸ¬ê°”ë‹¤. ì‹œê°„ì´ ì§€ë‚˜ ì¸ê³µì§€ëŠ¥ìœ¼ë¡œ ì»¨í…ì¸ ë¥¼ ë§Œë“œëŠ” ì„¸ìƒì´ ì™”ë‹¤. ì§ì ‘ì ìœ¼ë¡œ ì»¨í…ì¸ ë¥¼ ìƒì„±í•˜ëŠ” ì¼ì„ í•˜ëŠ” ê²ƒë„ ì¢‹ì§€ë§Œ, ë‚´ê°€ ì›í•˜ëŠ” ì»¨í…ì¸ ë“¤ì„ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¥¼ ê°œë°œí•˜ëŠ” ê²ƒë„ ì¬ë°Œì„ ê²ƒ ê°™ë‹¤ëŠ” ìƒê°ì´ ë§ì´ ë“ ë‹¤. ì¤€ë¹„í•  ê²ƒë“¤ ì •ë¦¬í•˜ìë©´ ì˜ë£Œë¶„ì•¼ ë°ì´í„° ê³¼í•™ìì—ì„œ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ì„ ê°œë°œí•˜ëŠ” ìª½ìœ¼ë¡œ ì»¤ë¦¬ì–´ë¥¼ ì´ì–´ë‚˜ê°€ê³  ì‹¶ë‹¤. í˜¹ì€ ì»¨í…ì¸ ë¥¼ ë‹¤ë£¨ëŠ” ê¸°ì—…ì˜ ë°ì´í„° ê³¼í•™ìë„ ê´œì°®ì„ ê²ƒ ê°™ë‹¤. ì–´ì¨Œë“  ì»¨í…ì¸ ë¥¼ ë‹¤ë£¨ëŠ” ìª½ìœ¼ë¡œ ì»¤ë¦¬ì–´ë¥¼ ê°€ì§€ê³  ì‹¶ë‹¤. ì´ë¥¼ ìœ„í•´ í¬ê²Œ 2ê°€ì§€ ë°©í–¥ìœ¼ë¡œ ì¤€ë¹„í•˜ë ¤ê³  í•œë‹¤. ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ ì¶”ì²œì‹œìŠ¤í…œ ì´ë¥¼ ìœ„í•´ ì§€ê¸ˆ í•˜ê³  ìˆëŠ” ê²ƒì€ ìƒì„±í˜• ì¸ê³µì§€ëŠ¥ ìŠ¤í„°ë”” (ëª¨ë‘ì˜ ì—°êµ¬ì†Œ í’€ììŠ¤ì¿¨) ì¶”ì²œì‹œìŠ¤í…œ (í”„ë¡œì íŠ¸, ê°œì¸ê³µë¶€) ì •ë¦¬ ì•ìœ¼ë¡œ ê³µë¶€í•´ì•¼í•  ê²ƒë“¤ì´ ë§ë‹¤. íƒ‘ì„ ìŒ“ì•„ê°€ëŠ” ê³¼ì •ì„ ê¾¸ì¤€íˆ ë¸”ë¡œê·¸ì— ë‚¨ê¸°ë ¤ê³  í•œë‹¤. í•˜ë‚˜ì”© í•˜ë‹¤ë³´ë©´ ëª»í•˜ëŠ” ì¼ì€ ê±°ì˜ ì—†ë‹¤ê³  ìƒê°í•œë‹¤. ê¾¸ì¤€íˆ ì¤€ë¹„í•´ì„œ ë‚´ê°€ ì›í•˜ëŠ” ì»¤ë¦¬ì–´ë¥¼ ë§Œë“¤ì–´ ë‚˜ê°€ì!"
  },
  {
    "id": "Data Science/Tableau/2024-07-01-tableau-python-cloud-upload",
    "title": "íƒœë¸”ëŸ¬ pythonìœ¼ë¡œ ë°ì´í„° cloud ì—…ë¡œë“œ",
    "date": "2024-07-01 12:00:00 +0900",
    "excerpt": "",
    "category": "Tableau",
    "tags": [
      "Tableau",
      "Python"
    ],
    "content": "íƒœë¸”ëŸ¬ prepì„ ì´ìš©í•´ ë°ì´í„° íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•´ íƒœë¸”ëŸ¬ í´ë¼ìš°ë“œì— ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ êµ¬ì¶•í•  ìˆ˜ë„ ìˆì§€ë§Œ, ë²„ê·¸ë„ ë§ê³  ì œí•œì‚¬í•­ì´ ë§ë‹¤ê³  ëŠê»´ì ¸ íŒŒì´ì¬ì—ì„œ ì²˜ë¦¬í•œ í›„ ì—…ë¡œë“œí•˜ëŠ” ê²ƒì„ ì„ íƒí–ˆìŠµë‹ˆë‹¤. ì—…ë¡œë“œ í•˜ê¸° ìœ„í•´ì„œëŠ” 2ê°€ì§€ ê³¼ì •ì´ í•„ìš”í•©ë‹ˆë‹¤. csv to hyper hypter to cloud ìˆœì„œëŒ€ë¡œ ì½”ë“œë¥¼ ê³µìœ í•˜ë©´ì„œ ê°„ë‹¨íˆ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. csv to hyper íƒœë¸”ëŸ¬ í´ë¼ìš°ë“œì—ëŠ” hyper ë°ì´í„°íƒ€ì…ì´ ë“¤ì–´ê°€ì•¼í•´ì„œ ì •ì œëœ csv íŒŒì¼ì„ hyperë¡œ ë°”ê¿”ì¤˜ì•¼ í•©ë‹ˆë‹¤. csv íŒŒì¼ì„ ë°”ê¾¸ëŠ” ê²ƒë§Œ ë‹¤ë£¨ì§€ë§Œ ë‹¤ë¥¸ íŒŒì¼ë“¤ë„ ì‰½ê²Œ ì‘ìš©í•  ìˆ˜ ìˆì„ ê²ƒì´ë¼ ìƒê°í•©ë‹ˆë‹¤. 3ê°œì˜ ì¸ìë¥¼ ë°›ëŠ”ë° csv ê²½ë¡œ, hyper ì €ì¥í•  ê²½ë¡œ, csv íŒŒì¼ íƒ€ì… ì •ì˜í•œ ê°ì²´ ê²½ë¡œ. csvë¡œ ì €ì¥í•˜ëŠ” ê³¼ì •ì—ì„œ ë§˜ëŒ€ë¡œ ë°ì´í„°íƒ€ì…ì´ ë°”ë€ŒëŠ” ë¬¸ì œê°€ ìˆì–´ì„œ íƒ€ì…ì„ ì •ì˜í•œ ê°ì²´ë„ ë”°ë¡œ ì €ì¥í•œ í›„ì— ë¶ˆëŸ¬ì˜¬ ë•Œ ì°¸ê³ í•˜ë„ë¡ í–ˆìŠµë‹ˆë‹¤. hyper íƒ€ì… ë°ì´í„°ë¥¼ ë°©ì‹ì€ ë¹ˆ í…Œì´ë¸”ì„ í•˜ë‚˜ ë§Œë“¤ê³  ì±„ì›Œë‚˜ê°€ëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì–´ë–¤ í…Œì´ë¸”ì„ ë§Œë“¤ ê²ƒì¸ì§€ë¥¼ ì •ì˜í•´ì¤˜ì•¼í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ columns ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ì˜í•´ì¤ë‹ˆë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œëŠ” ì–´ë–¤ ê°’ì„ ì¶”ê°€í• ì§€ ì •ì˜í•´ì¤˜ì•¼í•©ë‹ˆë‹¤. ì´ê²Œ row ë¦¬ìŠ¤íŠ¸ë¥¼ ì •ì˜í•˜ëŠ” ì´ìœ ì…ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì£¼ì˜ê¹Šê²Œ ë´ì•¼í•  ê²ƒì€ nullê°’ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ì¸ë° pandas dataframeì˜ NaNê°’ì„ Noneìœ¼ë¡œ ë°”ê¿”ì„œ ë„£ì–´ì¤˜ì•¼í•©ë‹ˆë‹¤. ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë°ì´í„°íƒ€ì…ì´ ë§ì§€ ì•Šë‹¤ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤. ``python def csv_to_hyper(csv_file_path, hyper_file_path, dtype_dict_path): # CSV íŒŒì¼ ì½ê¸° dtype_dict = load_data(dtype_dict_path) df = pd.read_csv(csv_file_path, dtype=dtype_dict) # í…Œì´ë¸” ì •ì˜ í•´ì£¼ê¸° columns = [] for index, value in df.dtypes.items(): value = str(value) if value == 'object': sql_type = SqlType.text() elif \"float\" in value: sql_type = SqlType.double() elif \"int\" in value: sql_type = SqlType.int() columns.append(TableDefinition.Column(index, sql_type, Nullability.NULLABLE)) # Hyperì— ì¶”ê°€í•  ê°’ ê³„ì‚°í•˜ê¸° rows = [] for row in df.itertuples(index=False, name=None): tmp_row = [] for item in row: if pd.isna(item): tmp_row.append(None) else: tmp_row.append(item) rows.append(tuple(tmp_row)) with HyperProcess(telemetry=Telemetry.SEND_USAGE_DATA_TO_TABLEAU) as hyper: with Connection(endpoint=hyper.endpoint, database=hyper_file_path, create_mode=CreateMode.CREATE_AND_REPLACE) as connection: # ìŠ¤í‚¤ë§ˆ ìƒì„± connection.catalog.create_schema('Extract') # í…Œì´ë¸” ì •ì˜ table_definition = TableDefinition( table_name=TableName(\"Extract\", \"Extract\"), columns=columns ) connection.catalog.create_table(table_definition) # ë°ì´í„°ë¥¼ Hyper íŒŒì¼ë¡œ ì‚½ì… with Inserter(connection, table_definition) as inserter: inserter.add_rows(rows=rows) inserter.execute() ` hyper to cloud ì—¬ëŸ¬ ë¼ì´ë¸ŒëŸ¬ê°€ ìˆì§€ë§Œ ê°œì¸ì ìœ¼ë¡œëŠ” \"tableau_api_lib\"ì„ ì¶”ì²œí•©ë‹ˆë‹¤. ê´€ë ¨í•´ì„œëŠ” ë‹¤ìŒ ë¸”ë¡œê·¸ë¥¼ ì°¸ê³ í•˜ë©´ ì‰½ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì°¸ê³ í•˜ë©´ ì¢‹ì€ ë¸”ë¡œê·¸ `python def hyper_to_cloud(hyper_file_path, datasource_name): config = { 'tableau_prod': { 'server': 'https://prod-apnortheast-a.online.tableau.com', 'api_version': '3.23', # 'username': '<YOUR_PROD_USERNAME>', # 'password': '<YOUR_PROD_PASSWORD>', 'personal_access_token_name': 'í† ê·¼ ì´ë¦„', 'personal_access_token_secret': 'í† ê·¼ ì •ë³´', 'site_name': 'ì‚¬ì´íŠ¸ì´ë¦„', 'site_url': 'ì‚¬ì´íŠ¸ì´ë¦„' } } conn = TableauServerConnection(config_json=config, env='tableau_prod') conn.sign_in() # ë°ì´í„° ì†ŒìŠ¤ ì—…ë¡œë“œ response = conn.publish_data_source( datasource_file_path=hyper_file_path, datasource_name=datasource_name, project_id='í”„ë¡œì íŠ¸ ì•„ì´ë””', embed_credentials_flag=False ) # ì—…ë¡œë“œ ê²°ê³¼ í™•ì¸ if response.status_code == 201: print(f\"Datasource '{datasource_name}' published successfully.\") else: print(f\"Failed to publish datasource: {response.json()}\") `` ì´ë ‡ê²Œ 2ê°€ì§€ í•¨ìˆ˜ë¥¼ í†µí•´ íƒœë¸”ëŸ¬ì— ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤ë¥¼ ì—…ë¡œë“œí•˜ê³  ìˆìŠµë‹ˆë‹¤."
  },
  {
    "id": "Reflections/2024-04-10-job-hunting-important-things",
    "title": "ì·¨ì¤€í•  ë•Œ ì¤‘ìš”í•œ ê²ƒ",
    "date": "2024-04-10 12:00:00 +0900",
    "excerpt": "",
    "category": "Reflections",
    "tags": [
      "ì·¨ì¤€"
    ],
    "content": "ì €ëŠ” í˜„ì¬ ì˜ë£Œì¸ê³µì§€ëŠ¥ íšŒì‚¬ì—ì„œ ë°ì´í„°ê³¼í•™ìë¡œ ì¼í•˜ê³  ìˆìŠµë‹ˆë‹¤. ì œê°€ ë°ì´í„°ê³¼í•™ìë¥¼ ì¤€ë¹„í•˜ë©´ì„œ ê°€ì¥ ì¤‘ìš”í•˜ê²Œ ìƒê°í–ˆê³ , ë„ì›€ì´ ëë˜ ìƒê°ì„ ê³µìœ í•´ë³´ë ¤ í•©ë‹ˆë‹¤. > íšŒì‚¬ì™€ ë¹„ìŠ·í•œ ê³ ë¯¼ì„ í•˜ê³  ë‚´ê°€ ê°€ì§„ ìì›ìœ¼ë¡œ í•´ê²°í•´ë³´ëŠ” ê²ƒ ë°ì´í„°ê³¼í•™ì ë°ì´í„°ê³¼í•™ìë€ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ì´ê±¸ ë°ì´í„°ë¡œ í•´ê²°í•´ë‚˜ê°€ëŠ” ì‚¬ëŒì„ ëœ»í•©ë‹ˆë‹¤. ì´ëŠ” í¬ê²Œ 2ê°€ì§€ ê³¼ì •ìœ¼ë¡œ ë‚˜ëˆ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¸ì œë¥¼ ì •ì˜ ë‚´ê°€ ê°€ì§„ ìì›ê³¼ ê¸°ìˆ ë¡œ í•´ê²° í”„ë¡œì íŠ¸ë¥¼ í•˜ê³  ì´ë¥¼ í¬íŠ¸í´ë¦¬ì˜¤ë¥¼ ë§Œë“¤ ë•ŒëŠ” ì´ 2ê°€ì§€ ê³¼ì •ì´ ê¼­ ì˜ ë‹´ê²¨ìˆì–´ì•¼ í•©ë‹ˆë‹¤. > ì–´ë–»ê²Œ ë¬¸ì œë¥¼ ì •ì˜í–ˆìœ¼ë©°, ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ê³¼ì •ì—ì„œ ì–´ë–¤ ê³ ë¯¼ë“¤ì„ í—€ëŠ”ì§€. íšŒì‚¬ íšŒì‚¬ëŠ” ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  ê·¸ê±¸ í•´ê²°í•´ë‚˜ê°€ë©° ìˆ˜ìµì„ ë‚´ëŠ” ì§‘ë‹¨ì…ë‹ˆë‹¤. íšŒì‚¬ì—ì„œ ì¼í•˜ëŠ” ë°ì´í„°ê³¼í•™ìë“¤ì€ ë³¸ì¸ì´ ì •ì˜í•˜ê±°ë‚˜ ìš´ì˜ì§„ì—ì„œ ì •ì˜í•œ ë¬¸ì œë¥¼ íšŒì‚¬ì˜ ìì›ê³¼ ë³¸ì¸ì˜ ê¸°ìˆ ë“¤ë¡œ í•´ê²°í•´ë‚˜ê°‘ë‹ˆë‹¤. > ë°ì´í„°ê³¼í•™ìë¥¼ ì¤€ë¹„í•˜ëŠ” ì‚¬ëŒê³¼ íšŒì‚¬ì—ì„œ ë°ì´í„°ê³¼í•™ìë¡œ ì¼í•˜ëŠ” ì‚¬ëŒì˜ ê³¼ì •ì€ ì™„ì „íˆ ë™ì¼í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”í•œ ë¶€ë¶„ ê²°êµ­ ì¤‘ìš”í•œ ê²ƒì€ ë‚˜ì˜ ê³¼ì •ë“¤ê³¼ íšŒì‚¬ì˜ ê³¼ì •ì´ ì–¼ë§ˆë‚˜ ê²¹ì¹˜ëŠ”ì§€ ì…ë‹ˆë‹¤. ì´ êµì§‘í•©ì´ í´ìˆ˜ë¡ ë³¸ì¸ì˜ í”„ë¡œì íŠ¸ í˜¹ì€ í¬íŠ¸í´ë¦¬ì˜¤ëŠ” ë§¤ë ¥ì ìœ¼ë¡œ ë³´ì´ê²Œ ë©ë‹ˆë‹¤. ì¦‰, ë‚´ê°€ ìƒê°í–ˆì„ ë•Œ ì´ëŸ° ë¬¸ì œê°€ ìˆê³  ë˜ëŠ” ì¤‘ìš”í•˜ë‹¤ê³  ìƒê°í•œë‹¤. ê·¸ë˜ì„œ ë‚˜ëŠ” ì´ëŸ° ë¬¸ì œë¥¼ ì´ë ‡ê²Œ í’€ì–´ë´¤ë‹¤. í‘¸ëŠ” ê³¼ì •ì—ì„œëŠ” ì´ëŸ° ì–´ë ¤ì›€ì´ ìˆì—ˆê³  ì´ë ‡ê²Œ í•´ê²°í•´ë‚˜ê°”ë‹¤. ì´ê²Œ í¬íŠ¸í´ë¦¬ì˜¤ì— ì˜ ë‹´ê²¨ìˆì–´ì•¼í•˜ë©°, ì·¨ì—…ì„ ì¤€ë¹„í•  ë•Œ ê°€ì¥ ì¤‘ìš”í•œ ì ì´ë¼ê³  ìƒê°í•©ë‹ˆë‹¤. ì‹¤ì œë¡œ ì €ë„ ì œê°€ ì •ì˜í•˜ê³  í•´ê²°í–ˆë˜ ë¬¸ì œê°€ í˜„ì¬ ì œê°€ íšŒì‚¬ì—ì„œ í•´ê²°í•˜ê³  ìˆëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ë©´ì ‘ì—ì„œë„ ì´ê±¸ ê°€ì¥ ì¢‹ê²Œ ë´ì£¼ì…¨ê³  ì·¨ì—…ì— ê°€ì¥ ë„ì›€ì´ ë§ì´ ëë‹¤ê³  ìƒê°í•©ë‹ˆë‹¤. ë§Œì•½ ì œê°€ ë‹¤ë¥¸ ë¶„ì•¼ë¡œ ì´ì§ì„ ì¤€ë¹„í•œë‹¤ë©´, ìœ„ì—ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ ì‚¬ì´ë“œ í”„ë¡œì íŠ¸ë¥¼ ì¤€ë¹„í•  ê²ƒì…ë‹ˆë‹¤. ë‹¹ì—°í•´ë³´ì´ëŠ” ìƒê°ì´ì§€ë§Œ, ìƒê°ë³´ë‹¤ ë§ì´ ë†“ì¹  ìˆ˜ ìˆì–´ í•œë²ˆ ì •ë¦¬í•´ë´¤ìŠµë‹ˆë‹¤. ì·¨ì—…ì„ ì¤€ë¹„í•˜ì‹œëŠ” ë¶„ë“¤ì´ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆìœ¼ë©´ ì¢‹ê² ìŠµë‹ˆë‹¤!"
  },
  {
    "id": "Reflections/2024-03-12-aifell-review",
    "title": "ì•„ì´í  í›„ê¸°",
    "date": "2024-03-12",
    "excerpt": "6ê°œì›” ë™ì•ˆì˜ ì•„ì´í  í›„ê¸°",
    "category": "Reflections",
    "tags": [
      "ì•„ì´í ",
      "ì´ëŸ°ì €ëŸ° ìƒê°ë“¤"
    ],
    "content": "ì €ëŠ” 23.9 ~ 24.2 ë™ì•ˆ ì•„ì´í  6ê¸° ë¦¬ì„œì¹˜ ê³¼ì •ì„ ìˆ˜ë£Œí–ˆê³ , íšŒê³  ê²¸ í›„ê¸°ë¥¼ ì ì–´ë³´ë ¤ í•©ë‹ˆë‹¤. !ì¡¸ì—…ì¦ ì‹ ì²­í–ˆì„ ë•Œì˜ ë‚˜ì˜ ìƒí™© ì¼ë‹¨ ì œê°€ ì–´ë–¤ ìƒíƒœì—ì„œ ì•„ì´í ì„ ë“¤ì—ˆëŠ”ì§€ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤. ì¼ë‹¨ ì €ëŠ” ì‚°ì—…ê³µí•™ì„ ì „ê³µí–ˆê³ , í†µê³„í•™ì„ ë¶€ì „ê³µí–ˆìŠµë‹ˆë‹¤. í•™ë¶€ìƒë•Œ MLì— ê´€ì‹¬ì´ ë§ì•„ì„œ í˜¼ìì„œ ê³µë¶€í–ˆì—ˆê³ , ê´€ë ¨ í”„ë¡œì íŠ¸ë„ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. DL ë¶€ë¶„ì€ ì˜ ëª°ëëŠ”ë°, ì‹œì‘í•˜ê¸° 1ë‹¬ì „ì— \"ë°‘ë°”ë‹¥ë¶€í„° ì‹œì‘í•˜ëŠ” ë”¥ëŸ¬ë‹\" 2íšŒë…ì •ë„ í–ˆìŠµë‹ˆë‹¤. ì œ ë°°ê²½ì§€ì‹ì€ í†µê³„í•™ + ì»´í“¨í„°ê³µí•™ + DL ì¡°ê¸ˆ. ì •ë„ë¡œ ì •ë¦¬í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ë‚´ê°€ ìƒê°í•˜ëŠ” ë‚œì´ë„ ì²˜ìŒ 1~2ë‹¬ ì •ë„ëŠ” ê·¸ ì „ì— ê³µë¶€í–ˆë˜ ê²ƒë“¤ì´ë¼ ë³µìŠµí•˜ëŠ” ëŠë‚Œìœ¼ë¡œ ê³µë¶€í–ˆê³ , ë‚˜ë¨¸ì§€ëŠ” ê°œë…ì •ë„ë§Œ ì•„ëŠ” ìƒíƒœì—ì„œ ê³µë¶€ë¥¼ í–ˆìŠµë‹ˆë‹¤. ê·¸ë˜ì„œ ë”°ë¼ê°€ëŠ”ë° ì–´ë ¤ì›€ì€ ì—†ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ì–‘ì´ ì›Œë‚™ ë§ì•„ì„œ í‰ê· ì ìœ¼ë¡œ ì¶”ê°€ê³µë¶€ë¥¼ 3~4ì‹œê°„, ì ì–´ë„ 1ì‹œê°„ì”©ì€ í–ˆë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë°°ê²½ì§€ì‹ì´ ì „í˜€ ì—†ëŠ” ë¶„ì´ ë¦¬ì„œì¹˜ ê³¼ì •ì„ ìˆ˜ë£Œí•˜ë ¤ë©´ ì ì–´ë„ ë§¤ì¼ 6ì‹œê°„ì”©ì€ ì¶”ê°€ê³µë¶€í•´ì•¼í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê³µë¶€ë°©ì‹ ì„ ìƒë‹˜ì´ ìˆëŠ”ê²Œ ì•„ë‹Œ, ê³µë¶€ìë£Œê°€ ì£¼ì–´ì§€ê³  ê°™ì´ ê³µë¶€í•˜ëŠ” ì‚¬ëŒë“¤ê³¼ í•¨ê»˜ ì•Œì•„ê°€ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ê³µë¶€ì˜ ë°©í–¥ì„ \"í¼ì‹¤\" ë¶„ë“¤ì´ ì¡ì•„ì£¼ê³ , ëª¨ë¥´ëŠ” ë¶€ë¶„ì„ í•´ì†Œí•´ì£¼ëŠ” ë°©ì‹ìœ¼ë¡œ ë„ì›€ì„ ì£¼ì‹­ë‹ˆë‹¤. ì´ê²Œ ë³´í†µ ìŠ¤í„°ë”” ê°™ì€ ê²½ìš° ë‚˜ë³´ë‹¤ ë” ì˜ì•„ëŠ” ì‚¬ëŒê³¼ í•´ì•¼ì§€ ì–»ëŠ”ê²Œ ë§ì„ ê²ƒì´ë¼ ìƒê°í•˜ëŠ”ë°, ê²½í—˜í•´ë³´ë‹ˆ ë°˜ëŒ€ì˜€ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. > ì™œ ì„ ìƒë‹˜ì´ ê°€ì¥ ë§ì´ ë°°ìš´ë‹¤ê³  í•˜ì–ì•„ìš”, ì €ëŠ” ë°°ê²½ì§€ì‹ì´ ìˆì–´ \"ì„ ìƒë‹˜\" ì—­í• ì„ ë§¡ì„ ë•Œê°€ ë§ì•˜ëŠ”ë° ì •ë§ ë§ì´ ë°°ì› ìŠµë‹ˆë‹¤. ì–´ëŠì •ë„ ì•Œë˜ ê°œë…ë“¤ì´ ì„¤ëª…ì„ í•˜ë©´ì„œ ì™„ì„±ë˜ê³¤ í–ˆìŠµë‹ˆë‹¤. íŠ¹íˆ ì•„ì— ëª¨ë¥´ë˜ ê°œë…ë“¤ë„ ì–´ë–»ê²Œ í•´ì•¼ ë¹ ë¥´ê²Œ ìŠµë“í•˜ê³  í•˜ë‚˜ì˜ ì§€ì‹ìœ¼ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ”ì§€ ë§ì´ ë°°ì› ìŠµë‹ˆë‹¤. ë˜ í† ì˜, í† ë¡ ì„ ì •ë§ ë§ì´ í–ˆëŠ”ë°, ì´ë¥¼ í†µí•´ ì œ ì§€ì‹ì„ ì ê²€í•  ìˆ˜ ìˆì—ˆê³  ì˜¤ë¥˜ê°€ ë‚œ ë¶€ë¶„ì„ ë§ì´ ì°¾ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì²˜ìŒì—ëŠ” ì–´ìƒ‰í–ˆë˜ ë°©ë²•ì¸ë°, ì •ë§ ë§Œì¡±í–ˆê³  ë§ì´ ë°°ìš¸ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. í”„ë¡œì íŠ¸ 6ê°œì›” ê³¼ì • ì¤‘ ë§ˆì§€ë§‰ 1~2ê°œì›”ì— ìµœì¢… í”„ë¡œì íŠ¸(ì•„ì´í í†¤)ë¥¼ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ê·¸ ì „ 4ê°œì›”ë™ì•ˆ ì§§ê²Œ í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆëŠ”ë° (í•˜ë£¨ or 3ì¼), ì´ ë•Œ ì—°ìŠµí•œê²Œ ìµœì¢… í”„ë¡œì íŠ¸ì— ë„ì›€ì´ ë§ì´ ëìŠµë‹ˆë‹¤. ì•„ì´í í†¤ì—ëŠ” ê¸°ì—…ì´ ì°¸ì—¬í•˜ê¸°ë„ í•˜ë©°, ì°¸ì—¬ê¸°ì—…ì´ ìˆì„ ê²½ìš° ì£¼ì œë¥¼ ë“¤ì–´ë³´ê³  ê´œì°®ë‹¤ê³  íŒë‹¨ë˜ë©´ ì°¸ì—¬í•´ ê¸°ì—…í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ì €ëŠ” í•´ë³´ê³  ì‹¶ì—ˆë˜ ì•„ì´ë””ì–´ê°€ ìˆì–´ì„œ ê·¸ê±¸ ë°œì œí–ˆê³ , ì„ ì •ì´ ëì§€ë§Œ ê¸°ì—…í”„ë¡œì íŠ¸ë¥¼ ì§„í–‰í–ˆìŠµë‹ˆë‹¤. ëª…í™•í•œ ê²°ê³¼ë¥¼ ë‚´ê³  ì‹¶ì—ˆê³ , ê¸°ì—…ì˜ ê´€ì ì´ ê¶ê¸ˆí–ˆìŠµë‹ˆë‹¤. ë˜ ì±„ìš© ê°€ëŠ¥ì„±ë„ ìˆì–´ì„œ ë§¤ë ¥ì ìœ¼ë¡œ ëŠê»´ì¡ŒìŠµë‹ˆë‹¤. ì±„ìš©ì— ëŒ€í•´ì„œëŠ” ê¸°ì—…ì†Œê°œë•Œ ê°™ì´ ì–¸ê¸‰í•´ì£¼ì‹­ë‹ˆë‹¤. (ë³´í†µ ë§˜ì— ë“¤ë©´ ì±„ìš©í•  ìˆ˜ë„ ìˆë‹¤ëŠ” ì‹ì…ë‹ˆë‹¤) ì•„ì´í ì—ì„œëŠ” ë©˜í† ë¥¼ ê° í”„ë¡œì íŠ¸ì— ë°°ì •í•´ì£¼ëŠ”ë°, ì´ ë¶€ë¶„ì´ ë„ì›€ì´ ë§ì´ ëìŠµë‹ˆë‹¤. ì„ íƒì˜ ê°ˆë¦¼ê¸¸ì—ì„œ ì¡°ì–¸ì„ ë§ì´ êµ¬í–ˆê³ , í”¼ë“œë°±ì„ ë§ì´ ë°›ì•˜ìŠµë‹ˆë‹¤. ë‹¤ë§Œ, ë©˜í† ë‹˜ì´ ëª¨ë‘ ì¢‹ì§€ëŠ” ì•Šì•˜ë˜ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ íŒ€ì—ì„œëŠ” ë˜ê²Œ ë¶ˆë§Œì¡±ìŠ¤ëŸ¬ì› ë‹¤ê³  ì´ì•¼ê¸°í•˜ë”ë¼êµ¬ìš”. ì•½ 2ê°œì›”ë™ì•ˆ ì£¼ì œë¥¼ ì •í•˜ê³ , ë¬¸ì œë¥¼ í•´ê²°í•´ë‚˜ê°€ë©´ì„œ ê²°ê³¼ë¥¼ ì™„ì„±í–ˆìŠµë‹ˆë‹¤. ì™œ í”„ë¡œì íŠ¸ ê²½í—˜ì´ ì¤‘ìš”í•˜ë‹¤ê³  ì´ì•¼ê¸°í•˜ëŠ”ì§€ ì •ë§ ë§ì´ ëŠê¼ˆê³ , í¬íŠ¸í´ë¦¬ì˜¤ì— í•˜ë‚˜ê°€ ì¶”ê°€ë˜ì–´ì„œ ì¢‹ì•˜ìŠµë‹ˆë‹¤. íšŒê³  ì‹œê°„ì´ ì°¸ ë¹ ë¥¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤. 6ê°œì›”ì´ ì§„ì§œ ë¹¨ë¦¬ ì§€ë‚˜ê°”ê³ , ê·¸ ê³¼ì •ì—ì„œ ë§ì´ ë°°ìš°ê³  ì¢‹ì€ ì¸ì—°ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ì•„ì´í ì— ë§Œì¡±í•˜ëƒê³  ë¬»ëŠ”ë‹¤ë©´, ì €ëŠ” ì§„ì§œ ëŒ€ë§Œì¡±ì…ë‹ˆë‹¤. ì •ë§ ì¢‹ì€ ë¶„ë“¤ì´ ê³„ì‹œê³ , ë§ì€ ë„ì›€ì„ ë°›ì•˜ìŠµë‹ˆë‹¤. ì•„ì´í ì˜ ì²˜ìŒ ëª©ì ì€ \"ë”¥ëŸ¬ë‹ì˜ ì›ë¦¬ë¥¼ ì´í•´í•˜ëŠ” ê²ƒ\" ì´ì—ˆëŠ”ë°, ëª©ì ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. ë‹¨ìˆœíˆ ì½”ë“œ ì¤‘ì‹¬ìœ¼ë¡œ ê²°ê³¼ë¥¼ ë‚´ëŠ”ê²Œ ì•„ë‹Œ, ê°œë…ì— ëŒ€í•´ í† ë¡ í•˜ë©´ì„œ ê°œë…ì„ ê¹Šì´ ì´í•´í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. 6ê°œì›”ì´ ì§€ë‚¬ê³ , ì´ ê³¼ì •ì—ì„œ ë°°ìš´ ê²ƒë“¤ì„ ì´ìš©í•´ ë¬¸ì œë¥¼ ì •ì˜í•˜ê³  í•´ê²°í•˜ë©´ì„œ ê¾¸ì¤€íˆ ì„±ì¥í•˜ë ¤ê³  í•©ë‹ˆë‹¤."
  },
  {
    "id": "template",
    "title": "ì—¬ê¸°ì— í¬ìŠ¤íŠ¸ ì œëª©ì„ ì…ë ¥í•˜ì„¸ìš”",
    "date": "2024-01-01",
    "excerpt": "í¬ìŠ¤íŠ¸ì— ëŒ€í•œ ê°„ë‹¨í•œ ì„¤ëª…ì„ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš” (ì„ íƒì‚¬í•­)",
    "category": "ì¹´í…Œê³ ë¦¬ëª…",
    "tags": [
      "íƒœê·¸1",
      "íƒœê·¸2",
      "íƒœê·¸3"
    ],
    "content": "í¬ìŠ¤íŠ¸ ì œëª© í¬ìŠ¤íŠ¸ì˜ ë‚´ìš©ì„ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”. ì†Œì œëª© ë” ì‘ì€ ì†Œì œëª© ì¼ë°˜ í…ìŠ¤íŠ¸ ë‚´ìš©ì…ë‹ˆë‹¤. > ì¸ìš©êµ¬ëŠ” ì´ë ‡ê²Œ ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. êµµì€ ê¸€ì”¨ì™€ ê¸°ìš¸ì„ ê¸€ì”¨ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª©ë¡ ì•„ì´í…œ 1 ëª©ë¡ ì•„ì´í…œ 2 ëª©ë¡ ì•„ì´í…œ 3 ë²ˆí˜¸ ëª©ë¡ 1 ë²ˆí˜¸ ëª©ë¡ 2 ë²ˆí˜¸ ëª©ë¡ 3 ``python ì½”ë“œ ë¸”ë¡ ì˜ˆì‹œ def hello_world(): print(\"Hello, World!\") `` !ì´ë¯¸ì§€ ì„¤ëª… --- ì¶”ê°€ ë‚´ìš©ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì— ì‘ì„±í•˜ì„¸ìš”."
  }
]