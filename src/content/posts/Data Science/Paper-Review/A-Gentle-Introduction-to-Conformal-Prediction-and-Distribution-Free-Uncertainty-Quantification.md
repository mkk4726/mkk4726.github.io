---
title: "A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification"
date: "2025-08-13"
excerpt: "Conformal Prediction 관련 Paper Review"
category: "Data Science"
tags: ["statistics", "prediction-interval", "uncertainty", "calibration", "paper-review"]
---


# Abstact
> Black-box machine learning models are now routinely used in high-risk settings, like medical diagnostics, which demand uncertainty quantification to avoid consequential model failures. 

되게 공감이 되는 부분이다. 모델 불확실성을 정량적으로 표현하는게 굉장히 중요함.

> Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty
sets/intervals for the predictions of such models.

**distribution-free** 의 특징을 가짐.

---

# 1. Conformal Prediction

<figure>
<img src="/post/DataScience/conformal_prediction/paper_figure1.png" alt="paper_figure1" width="100%" />
<figcaption>Figure 1: Prediction set examples on Imagenet. We show three progressively more difficult examples
of the class fox squirrel and the prediction sets (i.e., C(Xtest)) generated by conformal prediction.</figcaption>
</figure>

이미지별로 예측값에 대한 불확실성이 다른데, 이걸 어떻게 수치적으로 정량화해서  표현할 것인가.
불확실성을 정량화해서 표현할 수 있는, prediction interval을 계산하는 방법을 제시하고 있다. (식 1)

$$
1 - \alpha \leq \mathbb{P}(Y_{test} \in C(X_{test})) \leq 1 - \alpha + \frac{1}{n + 1} \tag{1}
$$

> In words, the probability that the prediction set contains the correct label is almost exactly 1 − α; we call
this property marginal coverage, since the probability is marginal (averaged) over the randomness in the
calibration and test points.

calibration step을 통해 prediction set을 생성.

## calibration step : 

<figure>
<img src="/post/DataScience/conformal_prediction/paper_figure2.png" alt="paper_figure2" width="100%" />
<figcaption>Figure 2: Illustration of conformal prediction with matching Python code.</figcaption>
</figure>


1. set the conformal score $s_i = 1 - \hat f(X_i)_{Y_i}$ : (예시) 1 - softmax output
   - **Score가 높을수록** 모델이 해당 클래스를 **잘못 예측**했다는 의미

2. define $\hat q$ to be the $[(n+1)(1 - \alpha)] / n$
   - empirical quantile of $\{s_1, ..., s_n\}$
   - $\lceil \cdot \rceil$: ceiling function (올림 함수)
   - 본질적으로는 $(1-\alpha)$ quantile이지만 작은 보정값 포함

3. creat a prediction set
   - 새로운 테스트 데이터 $X_{test}$에 대해:
   - $C(X_{test}) = \{y : \hat{f}(X_{test})_y \geq 1 - \hat{q}\}$
   - **충분히 높은 softmax 출력**을 가진 모든 클래스를 포함

## Remarks :

> Let us think about the interpretation of C

1. **Set-valued Function**
- **C**는 **set-valued function**입니다
- **Input**: image
- **Output**: set of classes (Figure 1 참조)

2. **Softmax Output 활용**
- 모델의 **softmax output**을 사용해서 집합을 생성
- 각 입력에 대해 **adaptively different output set**을 구성

3. **Uncertainty에 따른 Set Size 변화**
- **Model이 uncertain할 때**: set이 **커짐**
- **Image가 intrinsically hard할 때**: set이 **커짐**
- 이는 **desirable property**입니다!

4. **Set Size의 의미**
- **Set size** = **Model certainty indicator**
- Set이 클수록 → Model이 uncertain
- Set이 작을수록 → Model이 certain

5. **Interpretation**
- **C(X_test)**는 **"plausible classes that image X_test could be assigned to"**
- 즉, **가능성이 있는 classes를 모두 포함**

6. **Mathematical Guarantee**
- **C는 valid**: 수식 (1)을 만족
- **수식 (1)**: $\mathbb{P}(Y_{test} \in C(X_{test})) \geq 1-\alpha$

7. **Generalization**
- 이러한 **C의 properties**들은 **다른 ML problems**에도 자연스럽게 적용
- **Regression problems**에서도 동일한 원리 사용 가능

> However, we had no guarantee that the softmax outputs were any good; they may have been arbitrarily overfit or otherwise untrustworthy. 
> Therefore, instead of taking the softmax outputs at face value, we used the holdout set to adjust for their deficiencies.


**Softmax Output의 문제점**
  - **Softmax output**은 불확실성을 표현하는데 **heuristic하게 사용**됨
  - **엄밀한 확률**을 나타낼 수 없음
  - **Overfitting**이나 **untrustworthy**할 수 있음


**Holdout Set을 통한 보정**

- **Holdout Set의 역할:**
  - **n ≈ 500**개의 fresh data points
  - **Training 중에 모델이 보지 못한** 데이터
  - **모델 성능의 honest appraisal** 제공

- **Conformal Score 계산:**
  - **Conformal score** = $1 - \hat{f}(X_i)_{Y_i}$
  - **Model이 uncertain할 때** score가 커짐

**Quantile 기반 임계값 설정**
- **임계값 계산:**
  - $\hat{q} \approx (1-\alpha)$ quantile of scores
  - **α = 0.1**일 때: **90%의 ground truth softmax outputs**가 $1-\hat{q}$ level 이상임을 보장

**Test-time 적용:**
1. **새로운 이미지 X_test**의 softmax outputs 획득
2. **$1-\hat{q}$ 이상**인 모든 classes를 **prediction set C(X_test)** 에 수집
3. **새로운 true class Y_test**의 softmax output이 $1-\hat{q}$ 이상일 확률이 **최소 90%** 보장

---

## 1.1 Instructions for Conformal Prediction




---

# 2. Examples of Conformal Procedures


## 2.1 Classification with Adaptive Prediction Sets

> The previous method produces prediction sets with the smallest average size [6], but it tends to undercover hard subgroups and overcover easy ones. 
> Here we develop a different method called adaptive prediction sets (APS) that avoids this problem. 

구간의 두께를 X에 맞게 조절하는 방법 제시

<figure>
<img src="/post/DataScience/conformal_prediction/paper_figure3.png" alt="paper_figure3" width="100%" />
<figcaption>Figure 3: Python code for adaptive prediction sets.</figcaption>
</figure>


$$
C(x) = \{\pi_1(x), \ldots, \pi_k(x)\}, \text{ where } k = \sup\left\{k_0 : \sum_{j=1}^{k_0} \hat{f}(x)_{\pi_j(x)} < \hat{q}\right\} + 1. \tag{3}
$$

> APS는 "각 입력에 대해 1-alpha coverage를 정확히 만족하는 최소한의 prediction set"을 만드는 방법

1. Threshold 방식
   - 기존: 모든 클래스에 동일한 threshold 적용
   - APS: 각 입력 x마다 다른 threshold (1-alpha 만족할 때까지)
2. Coverage 보장
   - 기존: Marginal coverage만 보장 (전체 평균)
   - APS: 각 입력 x에 대해 개별적으로 1-alpha coverage 달성
3. Set Size
   - 기존: 모든 입력에 대해 비슷한 크기의 prediction set
   - APS: 입력별로 적응적으로 크기 조절

예시
- 쉬운 이미지: [0.8, 0.15, 0.05] → APS: [Class 1] (1개)
- 어려운 이미지: [0.4, 0.3, 0.2, 0.1] → APS: [Class 1, Class 2, Class 3] (3개)



### 기존 방법 : 

> 고정된 quantile threshold 사용
```python
threshold = np.quantile(softmax_scores, 1 - alpha)
prediction_set = [class for class in all_classes if softmax_score[class] >= threshold]
```

문제점:
- Undercover hard subgroups: 어려운 데이터는 너무 적은 클래스 포함 → coverage 부족
- Overcover easy subgroups: 쉬운 데이터는 너무 많은 클래스 포함 → 불필요하게 넓음

### APS (Adaptive Prediction Sets) : 

<figure>
<img src="/post/DataScience/conformal_prediction/paper_figure4.png" alt="paper_figure4" width="100%" />
<figcaption>Figure 4: A visualization of the adaptive prediction sets algorithm in Eq. (3). Classes are included from most to least likely until their cumulative softmax output exceeds the quantile.</figcaption>
</figure>

```python
# APS: 1-alpha를 만족할 때까지 greedy하게 추가
def build_adaptive_set(softmax_scores, alpha):
    sorted_classes = np.argsort(softmax_scores)[::-1]  # 내림차순 정렬
    cumulative_prob = 0
    prediction_set = []
    
    for class_idx in sorted_classes:
        cumulative_prob += softmax_scores[class_idx]
        prediction_set.append(class_idx)
        
        if cumulative_prob >= (1 - alpha):  # 1-alpha 만족하면 중단
            break
    
    return prediction_set
```

---

## 2.2 Conformalized Quantile Regression

<figure>
<img src="/post/DataScience/conformal_prediction/paper_figure5.png" alt="paper_figure5" width="100%" />
<figcaption>Figure 5: Python code for conformalized quantile regression.</figcaption>
</figure>

> However, because the fitted quantiles may be inaccurate, we will conformalize them. Python pseudocode for conformalized quantile regression is in Figure 5.

분류 모델과 마찬가지로 quantile regression이 정확하지 않으니, conforming 해준다.

$$
s(x, y) = \max\left\{ \hat{q}_{\alpha/2}(x) - y, y - \hat{q}_{1-\alpha/2}(x) \right\}
$$

> After computing the scores on our calibration set and setting $\hat{q} = \text{Quantile}(s_1, \ldots, s_n; \frac{\lceil(n+1)(1-\alpha)\rceil}{n})$, we can form valid prediction intervals by taking

$$
C(x) = \left[\hat{q}_{\alpha/2}(x) - \hat{q}, \hat{q}_{1-\alpha/2}(x) + \hat{q}\right]. \tag{4}
$$

<figure>
<img src="/post/DataScience/conformal_prediction/paper_figure6.png" alt="paper_figure6" width="100%" />
<figcaption>Figure 6: A visualization of the conformalized quantile regrssion algorithm in Eq. (4). We adjust the quantiles by the constant qˆ, picked during the calibration step.</figcaption>
</figure>

---

## 2.3 Conformalizing Scalar Uncertainty Estimates

### 2.3.1 The Estimated Standard Deviation

> As an alternative to quantile regression, our next example is a different way of constructing prediction sets for continuous y with a less rich but more common notion of heuristic uncertainty: an estimate of the standard deviation $\hat σ(x)$. 

기존의 uncertainty quantification 방법은 Y_test | X_test = x가 가우시안 분포 N(μ(x), σ(x))를 따른다고 가정하고, 모델 f̂(x)와 σ̂(x)를 학습시켜 각각 평균과 분산을 예측하는 방식입니다. 
이 방법은 PyTorch의 GaussianNLLLoss 같은 내장 손실 함수로 쉽게 구현할 수 있어 널리 사용되지만, 실제로는 Y_test | X_test가 가우시안이 아닌 경우가 많아서 σ̂(x)가 신뢰할 수 없을 수 있습니다. 
따라서 conformal prediction을 사용하여 이러한 heuristic한 uncertainty 개념을 f̂(x) ± q̂σ̂(x) 형태의 rigorous한 prediction interval로 변환할 수 있습니다

### 2.3.2 Other 1-D Uncertainty Estimates

---

# 3. Evaluating Conformal Prediction

이 내용을 깔끔하게 정리하면:

1. Evaluating adaptivity: 

평균 set size가 가장 작은 conformal prediction 절차가 반드시 최선은 아니라는 점을 명심해야 합니다. 
좋은 conformal prediction 절차는 쉬운 입력에서는 작은 set을, 어려운 입력에서는 큰 set을 제공하여 모델의 uncertainty를 충실히 반영합니다. 

이러한 적응성은 conformal prediction의 coverage guarantee에서 자동으로 보장되지 않지만, 실제 배포에서는 필수적입니다. 
우리는 적응성을 공식화하고, 그 결과를 탐구하며, 평가를 위한 실용적인 알고리즘을 제안할 것입니다.

2. Correctness checks: 

정확성 검사는 conformal prediction을 올바르게 구현했는지 테스트하는 데 도움이 됩니다. 
우리는 coverage가 Theorem 1을 만족하는지 경험적으로 확인할 것입니다. 

이 속성이 성립하는지 엄격하게 평가하려면 실제 데이터셋의 finite-sample 변동성을 신중하게 고려해야 합니다. 
우리는 coverage에서 1-α로부터의 편차를 관찰할 때 발생하는 benign fluctuations의 크기에 대한 명시적 공식을 개발합니다.

## 3.1 Evaluating Adaptivity

