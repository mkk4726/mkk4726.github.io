---
title: "자연어로 영화 포스터 검색하는 서비스 관련 아이디어 정리"
date: "2025-11-14"
description: "원하는 느낌의 포스터를 찾을 수 있도록 기능 개발하면 어떨까?"
category: "Data Science"
tags: ["CLIP", "Computer Vision", "Information Retrieval", "PyTorch", "Multimodal"]
---

# 아이디어 배경

미리디 캔버스와 같은 곳에서 자연어를 통해 원하는 프레젠테이션 탬플릿을 찾는 기능이 있던데, 나도 구현해볼만한지 않을까?
아마 태그를 달아서 이걸로 필터링을 하는 방식이거나, 텍스트와 이미지의 유사도를 임베딩으로 계산하는 방식.

이 중에서 임베딩에 기반한 검색 기능을 한번 구현해보자!

# 아이디어 정리

자연어로 원하는 영화 포스터를 찾는 검색 서비스를 만들려고 함.
예를 들어 "dark red horror movie poster with a single face"라고 검색하면, 실제로 그런 느낌의 포스터들을 찾아주는 시스템.

CLIP(Contrastive Language-Image Pre-training) 모델을 사용하면 텍스트와 이미지를 같은 embedding space에 매핑할 수 있어서, 자연어 쿼리와 이미지 간의 유사도를 직접 계산할 수 있음

# 시스템 구조

전체 시스템은 크게 두 단계로 나뉜다.

## Offline 단계 (데이터 전처리)

모든 영화 포스터 이미지를 미리 인코딩해서 embedding vector로 변환.
영화 개수가 약 8만개니까 시간이 꽤 걸림.

이 벡터들을 FAISS 같은 벡터 검색 엔진에 저장해두면, 실시간 검색 시 빠르게 유사한 이미지를 찾을 수 있음.

## Online 단계 (실시간 검색)

사용자가 입력한 자연어 쿼리를 텍스트 embedding으로 변환하고, 미리 구축한 인덱스에서 가장 유사한 포스터들을 찾아 반환한다.


---

# CLIP 모델 사용법

## 설치

```bash
pip install torch torchvision
pip install transformers pillow
```

## 모델 로딩 및 임베딩 생성

```python
import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel

# GPU 사용 가능 여부 확인
device = "cuda" if torch.cuda.is_available() else "cpu"

# 모델 로드
model = CLIPModel.from_pretrained("openai/clip-vit-large-patch14").to(device)
processor = CLIPProcessor.from_pretrained("openai/clip-vit-large-patch14")

def encode_image(image_path: str):
    """이미지 파일을 받아 CLIP 이미지 임베딩을 반환"""
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        image_features = model.get_image_features(**inputs)
    # Normalize (코사인 유사도용)
    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
    return image_features.cpu().numpy()

def encode_text(text: str):
    """텍스트를 받아 CLIP 텍스트 임베딩을 반환"""
    inputs = processor(text=[text], return_tensors="pt").to(device)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs)
    
    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
    return text_features.cpu().numpy()

# 테스트
if __name__ == "__main__":
    img_emb = encode_image("sample_poster.jpg")
    txt_emb = encode_text("a blue sci-fi poster with spaceship")
    print(img_emb.shape, txt_emb.shape)
```

이 코드에서 핵심은 embedding을 normalize하는 부분이다. L2 normalization을 적용하면 코사인 유사도 계산이 단순한 내적(dot product)으로 간소화된다.

## 유사도 계산

```python
import numpy as np

def cosine_similarity(a, b):
    return np.dot(a, b.T).item()

sim = cosine_similarity(txt_emb, img_emb)
print("similarity:", sim)
```

Normalized embedding끼리의 내적은 코사인 유사도와 동일하다. 값의 범위는 -1에서 1 사이이며, 1에 가까울수록 유사하다는 의미다.

# Top-K 검색 구현

여러 포스터 중에서 쿼리와 가장 유사한 상위 K개를 찾는 코드는 다음과 같다.

```python
import glob

# 포스터 폴더에서 모든 이미지 경로 가져오기
poster_paths = glob.glob("posters/*.jpg")

# 모든 포스터 인코딩
image_embeddings = []
for p in poster_paths:
    emb = encode_image(p)
    image_embeddings.append((p, emb))
    
query = "dark red horror movie poster with a single face"
query_emb = encode_text(query)

# 유사도 계산
scores = []
for path, emb in image_embeddings:
    sim = cosine_similarity(query_emb, emb)
    scores.append((path, sim))

# Top-K 반환
scores.sort(key=lambda x: x[1], reverse=True)
top_k = scores[:10]

for p, s in top_k:
    print(f"{p}: {s:.4f}")
```

이 방식은 간단하지만, 포스터가 수천 개 이상이면 매번 전체를 순회하는 것이 비효율적이다. 실전에서는 FAISS 같은 approximate nearest neighbor 검색 라이브러리를 사용하는 것이 좋다.

---

# 실전 프로젝트 적용 방안

## 데이터 전처리 단계 (Offline)

1. 모든 포스터 이미지를 `encode_image()`로 변환
2. Embedding을 numpy 배열로 저장 (`.npy` 파일)
3. FAISS 인덱스 구축 및 저장

이 단계는 한 번만 실행하면 되고, 새로운 포스터가 추가될 때만 업데이트하면 된다.

## 검색 단계 (Online)

1. 사용자의 자연어 쿼리를 `encode_text()`로 변환
2. FAISS 인덱스에서 가장 유사한 K개의 포스터 검색
3. 검색 결과를 사용자에게 반환

검색 시간은 포스터 수가 많아도 millisecond 단위로 빠르게 처리할 수 있다.

---

# 한국어 쿼리도 성능이 괜찮으려나?

기본적으로 사용하는 대상은 한국인, 한국어 쿼리를 생성할텐데 모델이 잘 작동할까?

한국어로 검색할 때 가장 먼저 고려해야 할 점은 기본 CLIP 계열(openai/clip-vit-*)이 영어 중심 데이터로 학습됐다는 사실이다. 
한국어를 입력해도 Zero-shot으로 어느 정도 동작은 하지만, 표현 뉘앙스를 세밀하게 반영하지 못해 영어 대비 정확도가 낮을 때가 많다. 
예를 들어 "네온사인이 많은 사이버펑크 느낌"처럼 복합적인 표현은 내부적으로 영어 표현으로 치환되며 의미가 다소 흐려진다.

이 문제를 완화하려면 멀티링구얼 학습이 강화된 최신 모델을 선택하는 것이 좋다. 
실제 서비스 수준의 품질을 확보하고 싶다면 Google이 공개한 SigLIP 계열이 가장 안정적이다. 

`google/siglip-base-patch16-224-multilingual` 모델은 한국어를 포함해 100개 이상의 언어를 지원하며, 
영화 포스터처럼 시각적 요소가 복잡한 이미지에서도 영어와 비영어권 쿼리 간 품질 격차가 작다. 

이외에 LAION에서 배포한 M-CLIP(`M-CLIP/XLM-Roberta-Large-Vit-B-32`)이나 국내 커뮤니티가 공개한 Korean CLIP 모델들도 사용 가능하지만, 도메인 특화된 데이터가 아니라면 SigLIP보다 일관된 품질을 내기는 어려울 것 같다.

openai CLIP은 영어 쿼리에는 여전히 최고 수준의 정확도를 내지만, 한국어에서는 중간 정도 성능에 그친다고 한다.

SigLIP multilingual은 영어와 한국어 모두에서 실사용이 가능할 만큼 안정적인 결과를 제공했고, M-CLIP은 영어에서는 다소 떨어지나 한국어에서는 준수한 편이라고 한다.
한국어 전용 CLIP들은 특정 데이터셋에서는 강점을 보이지만, 범용 검색에서는 편차가 있다고 한다.

-> 확인이 필요

추가적인 정밀도를 원한다면 언어 전처리 전략을 결합하는 것이 효과적이라고 한다.
가장 간단한 방법은 자연어 쿼리를 LLM이나 번역기로 영어 문장으로 바꾼 뒤 텍스트 인코더에 넣는 것이다. 
한국어 입력을 그대로 SigLIP에 넣는 것보다 "A movie poster with red lighting and a dark thriller atmosphere"처럼 번역된 문장을 사용하면 유사도 상위 결과가 한 단계 더 정교해질 것이라고 기대할 수 있다. 

또 다른 방법은 포스터 캡션을 영어로 자동 생성해 메타데이터로 저장해 두는 것.
 BLIP2 같은 모델로 이미지 캡션을 만들고 BM25와 CLIP을 혼합한 hybrid ranking을 구성하면, 한국어 쿼리를 번역한 뒤에도 텍스트 기반 검색과 벡터 검색의 장점을 동시에 취할 수 있다.

SigLIP 멀티링구얼 모델을 사용할 때 코드 변경은 거의 없다. 아래는 앞서 작성한 함수와 동일한 구조로 SigLIP을 적용한 예시다.

```python
from transformers import AutoProcessor, AutoModel
import torch
from PIL import Image

device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "google/siglip-base-patch16-224-multilingual"
model = AutoModel.from_pretrained(model_name).to(device)
processor = AutoProcessor.from_pretrained(model_name)

def encode_text_kr(text: str):
    inputs = processor(text=[text], return_tensors="pt").to(device)
    with torch.no_grad():
        emb = model.get_text_features(**inputs)
    return emb / emb.norm(dim=-1, keepdim=True)

def encode_image_siglip(image_path: str):
    image = Image.open(image_path).convert("RGB")
    inputs = processor(images=image, return_tensors="pt").to(device)
    with torch.no_grad():
        emb = model.get_image_features(**inputs)
    return emb / emb.norm(dim=-1, keepdim=True)
```

실제 프로덕션에서는 SigLIP을 기본값으로 사용하고, 쿼리를 영어로 번역한 버전과 원본 한국어 버전을 동시에 넣어본 뒤 더 높은 점수를 선택하는 식으로도 정확도를 높일 수 있다. 검색 품질을 더 끌어올리고 싶다면 BLIP2 기반 설명 텍스트를 미리 생성하거나, hybrid ranking을 도입해 BM25와 벡터 검색 결과를 재조합하는 파이프라인을 구축하면 된다.

---

# 확장 가능성

이 기본 구조에서 다양한 기능을 추가할 수 있다.

**BLIP2 기반 캡션 생성**: 포스터 이미지에서 자동으로 설명을 생성해서, 텍스트 기반 검색의 정확도를 높일 수 있다. CLIP embedding과 함께 사용하면 multimodal 검색이 가능하다.

**FastAPI 서버 구축**: RESTful API로 검색 기능을 제공하면, 다양한 클라이언트에서 접근할 수 있다. 간단한 POST 요청으로 쿼리를 보내고 결과를 받는 구조다.

**Next.js 프론트엔드**: 사용자 친화적인 UI를 만들어서 검색 결과를 시각적으로 보여줄 수 있다. 포스터 이미지를 그리드로 배치하고, hover 시 영화 정보를 표시하는 등의 인터랙션을 추가할 수 있다.

**필터링 기능**: 장르, 연도, 평점 등의 메타데이터를 활용해 검색 결과를 필터링할 수 있다. Embedding 기반 검색과 전통적인 필터링을 결합하면 더 정확한 결과를 얻을 수 있다.

