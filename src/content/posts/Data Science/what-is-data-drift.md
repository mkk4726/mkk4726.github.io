---
title: "Data Driftë€?"
date: "2025-08-10"
excerpt: "Data Driftì˜ ê°œë…ê³¼ í™œìš©ë˜ëŠ” ì‚¬ë¡€ì— ëŒ€í•œ ì •ë¦¬"
category: "Data Science"
tags: ["data-drift", "machine-learning", "statistics", "data-engineering"]
---

# ì°¸ê³ í•œ ê¸€
- 1: [ë§í¬ë“œì¸ - Machine Learning Community (Moderated)](https://www.linkedin.com/feed/update/urn:li:activity:7359233207621361666/)

---

# How to choose Data Drift metrics in ML Production (ì°¸ì¡° 1)?

- **Jensenâ€“Shannon(JS) Distance**
  - **ì¥ì **: symmetricì ì´ê³  ê°’ì´ 0~1ë¡œ **bounded**ë¼ í•´ì„ì´ ì‰½ê³  threshold ì„¤ì •ì´ ì•ˆì •ì . noiseÂ·ê²½ë¯¸í•œ ë³€ë™ì— **robust**.
  - **ë‹¨ì **: **computational cost**ê°€ ìƒëŒ€ì ìœ¼ë¡œ í¼(íŠ¹íˆ high-dimensional).
  - **ì–¸ì œ ì“°ë‚˜**: í•´ì„ ìš©ì´ì„±ê³¼ **stable monitoring**ì´ ì¤‘ìš”í•  ë•Œ.

- **Kullbackâ€“Leibler(KL) Divergence**
  - **ì¥ì **: distributionì˜ **subtle differences**ê¹Œì§€ sensitiveí•˜ê²Œ ê°ì§€(early warningì— ìœ ë¦¬).
  - **ë‹¨ì **: outlierÂ·ì‘ì€ probability massì— **sensitive**í•´ spike/false alarm ê°€ëŠ¥, support set mismatch ì‹œ ë°œì‚°.
  - **ì–¸ì œ ì“°ë‚˜**: **early detection**ì´ ìµœìš°ì„ ì´ê±°ë‚˜ ë†’ì€ sensitivityê°€ í•„ìš”í•œ ì‹œìŠ¤í…œ.

- **Wasserstein Distance**
  - **ì¥ì **: í° distribution ë³€í™”ë„ **smoothly tracking**, referenceÂ·current distributionì´ **ê²¹ì¹˜ì§€ ì•Šì•„ë„** ë™ì‘.
  - **ë‹¨ì **: **computational cost í¼**(large-scale/real-time, high-dimensionalì—ì„œ íŠ¹íˆ ë¶€ë‹´).
  - **ì–¸ì œ ì“°ë‚˜**: **large/structural changesì˜ stable tracking**ì´ í•„ìš”í•  ë•Œ.

- **í•œ ì¤„ ê°€ì´ë“œ**
  - **interpretabilityÂ·threshold stability**: JS
  - **early detection of subtle changes**: KL
  - **smooth and robust tracking**: Wasserstein


<figure>
<img src="/post/DataScience/DataDrift_plot_1.gif" alt="Data Drift Plot" />
<figcaption>ê·¸ë¦¼ 1. Data Drift Plot</figcaption>
</figure>

---

# ì§€í‘œë³„ íŒŒì´ì¬ êµ¬í˜„ ì˜ˆì‹œ (JS/KL/Wasserstein)
ì—°ì†í˜• featureì˜ distributionì„ ë™ì¼í•œ êµ¬ê°„ìœ¼ë¡œ discretizeí•œ ë’¤, KL/JSë¥¼ ê³„ì‚°í•˜ê³  Wasserstein distanceë¥¼ ì§ì ‘ ê³„ì‚°í•˜ëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œì…ë‹ˆë‹¤.

```python
import numpy as np
from scipy.stats import entropy, wasserstein_distance

def _normalize_hist(values: np.ndarray, bins: np.ndarray, epsilon: float = 1e-8) -> np.ndarray:
    counts, _ = np.histogram(values, bins=bins)
    probs = counts.astype(float)
    probs = probs / max(probs.sum(), 1.0)
    probs = np.clip(probs, epsilon, None)
    probs = probs / probs.sum()
    return probs

def kl_divergence_from_samples(reference: np.ndarray, current: np.ndarray, num_bins: int = 30, base: int = 2) -> float:
    """KL(P||Q): ê¸°ì¤€(reference)=P, í˜„ì¬(current)=Q"""
    bins = np.histogram_bin_edges(reference, bins=num_bins)
    p = _normalize_hist(reference, bins)
    q = _normalize_hist(current, bins)
    return float(entropy(p, qk=q, base=base))

def js_distance_from_samples(reference: np.ndarray, current: np.ndarray, num_bins: int = 30, base: int = 2) -> float:
    """JS distance: sqrt(JS divergence), 0~1 ë²”ìœ„(base=2)"""
    bins = np.histogram_bin_edges(reference, bins=num_bins)
    p = _normalize_hist(reference, bins)
    q = _normalize_hist(current, bins)
    m = 0.5 * (p + q)
    js_div = 0.5 * entropy(p, qk=m, base=base) + 0.5 * entropy(q, qk=m, base=base)
    return float(np.sqrt(js_div))

def wasserstein_from_samples(reference: np.ndarray, current: np.ndarray) -> float:
    return float(wasserstein_distance(reference, current))

# ì˜ˆì‹œ ë°ì´í„°
ref = np.random.normal(0.0, 1.0, size=10_000)
cur = np.random.normal(0.3, 1.2, size=2_000)

metrics = {
    "KL(P||Q)": kl_divergence_from_samples(ref, cur),
    "JS distance": js_distance_from_samples(ref, cur),
    "Wasserstein": wasserstein_from_samples(ref, cur),
}
print(metrics)
```

ì°¸ê³ : ë²”ì£¼í˜• featureëŠ” ê° categoryì˜ relative frequencyë¥¼ probabilityë¡œ normalizeí•˜ì—¬ KL/JSë¥¼ ê³„ì‚°í•˜ë©´ ë©ë‹ˆë‹¤(ë™ì¼í•œ category ìˆœì„œì™€ smoothing í•„ìš”).

## ë°ì´í„° ë“œë¦¬í”„íŠ¸ ì§€í‘œ ë¹„êµí‘œ

| Metric | ê°’ ë²”ìœ„ | symmetry | distribution overlap í•„ìš” | sensitivity(subtle changes) | ì¥ì  | ë‹¨ì  | computational cost | ê¶Œì¥ ì‚¬ìš© |
|---|---|---|---|---|---|---|---|---|
| JS distance | 0~1 | symmetric | ë¶ˆí•„ìš” | ì¤‘ê°„ | interpretabilityÂ·threshold ì„¤ì • ìš©ì´, noise robust | high-dimensionalì—ì„œ computational cost ì¦ê°€ | ì¤‘ | stable monitoring/threshold ìš´ì˜ |
| KL divergence | 0~âˆ | asymmetric | ë¶€ë¶„ overlap ê¶Œì¥ | ë†’ìŒ | subtle changes early detection | outlier/sparse probabilityì— sensitive, false alarm ê°€ëŠ¥ | ì¤‘ | early warningì´ ì¤‘ìš”í•œ ì‹œìŠ¤í…œ |
| Wasserstein | â‰¥0 | symmetric | ë¶ˆí•„ìš” | ì¤‘ê°„ | large/structural changesë„ smoothly tracking, interpretation ì§ê´€ì  | large-scale/real-timeÂ·high-dimensionalì—ì„œ cost í¼ | ë†’ìŒ | robust change tracking, overlap ì ì€ distribution |

---

# ì§€í‘œ ìƒì„¸ ì„¤ëª…

## Kullbackâ€“Leibler(KL) Divergence

relative entropy, I-divergence.

> A simple interpretation of the KL divergence of P from Q is the expected excess surprisal from using Q as a model instead of P when the actual distribution is P.

PëŒ€ì‹  Që¥¼ ì¼ì„ ë•Œ ì •ë³´ëŸ‰ì˜ ë³€í™”ì •ë„ê°€ KL divergence.
$$
D_{KL}(P||Q) = \sum_{x \in X} p(x) \log \frac{p(x)}{q(x)} = H(P, Q) - H(P) \tag{3}
$$

distanceì˜ ì†ì„±ì„ ë§Œì¡±í•˜ì§€ ëª»í•´ì„œ distanceë¼ê³  í‘œí˜„í•  ìˆ˜ëŠ” ì—†ë‹¤.
squared distancesì˜ ì†ì„±ì€ ë§Œì¡±í•œë‹¤ê³  í•œë‹¤. 

- ì§ê´€: ì§„ì§œê°€ Pì¸ë° Që¡œ codingí•˜ë ¤ê³  í•  ë•Œ â€œì¶”ê°€ë¡œ ë‚´ì•¼ í•˜ëŠ” information costâ€. directionalityì´ ì¤‘ìš”í•´ KL(P||Q)ì™€ KL(Q||P)ëŠ” ë‹¤ë¥¸ ê°’ì„ ì¤€ë‹¤.
- ìˆ˜ì‹
$$
KL(P\,\|\,Q) = \sum\limits_{x} p(x)\,\log\frac{p(x)}{q(x)}\quad(\text{ì—°ì†í˜•ì€ ì ë¶„})
$$
- íŠ¹ì§•: asymmetric, lower bound 0Â·upper bound ì—†ìŒ. Q(x)=0ì´ë©´ì„œ P(x)>0ì´ë©´ ë°œì‚°(infinity). sparse probabilityÂ·outlierì— ë§¤ìš° sensitive.
- ê³„ì‚°ë²•: histogram/KDEë¡œ p,q ì¶”ì • í›„ epsilon smoothing(ì˜ˆ: 1e-6). ìš´ì˜ì—ì„œëŠ” KL(P\|\|Q), KL(Q\|\|P) ë‘˜ ë‹¤ ë³´ê±°ë‚˜ symmetricization(KL sym) í˜¹ì€ JSë¡œ ëŒ€ì²´.
- threshold íŒíŠ¸: spikeê°€ ì¦ìœ¼ë¯€ë¡œ moving averageÂ·quantile ê¸°ë°˜ ê²½ë³´, permutation(label shuffle) nullë¡œ p-value ì¶”ì • ê¶Œì¥.
- ì‚¬ìš©ì²˜: early ë¯¸ì„¸ ë³€í™” íƒì§€, sensitiveë„ ìµœìš°ì„  ê²½ë³´ ì‹œìŠ¤í…œ, ë¦¬ìŠ¤í¬ê°€ í° ì˜ì—­ì˜ ì„¸ë°€ monitoring.
- í”í•œ í•¨ì •: sample ì ì„ ë•Œ unstable, bin ì„ íƒ/bandwidth sensitive, log ìˆ˜ì¹˜unstable â†’ smoothing/clippingÂ·normalization í•„ìš”.




---


## Jensenâ€“Shannon(JS) Divergence

information radius(IRAD) or total divergence

> It is based on the Kullback-Leibler divergence, with some notable (and useful) differences, including that it is symmetric and it always has a finite value.

square root of Jensen-Shannon divergence == Jensen-Shannon distance

- ì§ê´€: ë‘ distributionì˜ â€œì¤‘ê°„ distributionâ€ Mê³¼ì˜ information differenceë¥¼ í‰ê· í•´ ì¸¡ì •í•˜ê³ , square rootì„ ì·¨í•´ ì‹¤ì œ distanceë¡œ ë§Œë“  ê°’. symmetricì´ê³  0~1 ë²”ìœ„ë¼ thresholdì„ ì •í•˜ê¸° ì‰½ë‹¤.
- ìˆ˜ì‹
$$
JS(P, Q) = \tfrac{1}{2}\,KL(P\,\|\,M) + \tfrac{1}{2}\,KL(Q\,\|\,M)\\
M = \tfrac{1}{2}(P + Q)\\
JS\_\text{dist}(P, Q) = \sqrt{JS(P, Q)}\quad(\text{log base}=2\Rightarrow 0\le JS\_\text{dist}\le 1)
$$
- íŠ¹ì§•: symmetric, bounded(0~1), support set mismatchì—ë„ finite. rare eventì— over-sensitiveí•˜ì§€ ì•Šì•„ ìš´ì˜ monitoringì— ì•ˆì •ì .
- ê³„ì‚°ë²•: ì—°ì†í˜•ì€ ê³µí†µ bin ë˜ëŠ” KDEë¡œ í™•ë¥ ì¶”ì • í›„ ê³„ì‚°, ë²”ì£¼í˜•ì€ ë²”ì£¼ ì§‘í•©ì„ í•©ì§‘í•©ìœ¼ë¡œ ì •ë ¬í•´ smoothing(e.g., 1e-6) í›„ ê³„ì‚°. í‘œë³¸ ì ìœ¼ë©´ ë¶„ì‚°â†‘ â†’ ìœˆë„ìš° í¬ê¸° í™•ëŒ€.
- threshold íŒíŠ¸: ê¸°ì¤€ ê¸°ê°„ì—ì„œ distributionë¥¼ ë¶€íŠ¸ìŠ¤íŠ¸ë©í•´ ê²½í—˜ì  distributionë¥¼ ë§Œë“  ë’¤ ìƒìœ„ quantileë¡œ ê²½ë³´ì„  ì„¤ì •(ì¡°ì§ë³„ ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ê¶Œì¥).
- ì‚¬ìš©ì²˜: í•´ì„ ìš©ì´Â·ì•ˆì •ì  ìŠ¤ì¼€ì¼ì´ í•„ìš”í•œ ëŒ€ì‹œë³´ë“œ, ë‹¤ìˆ˜ í”¼ì²˜ì˜ ì¼ê´€ëœ threshold ìš´ì˜.
- í”í•œ í•¨ì •: bin ê°œìˆ˜/ê²½ê³„, KDE bandwidthì— sensitive. ë²”ì£¼ í¬ë°•ë„ ë†’ì„ ë•Œ ê°’ì´ ê³¼ì†Œí‰ê°€ë  ìˆ˜ ìˆìŒ â†’ smoothing/ë³‘í•© ê³ ë ¤.


---

## Wasserstein Distance (Earth Mover's Distance, 1-Wasserstein)
- ì§ê´€: í•œ distributionì˜ ì§ˆëŸ‰ì„ ë‹¤ë¥¸ distributionë¡œ ì˜®ê¸¸ ë•Œ í•„ìš”í•œ â€œìµœì†Œ ì‘ì—…ëŸ‰â€. ê°’ì˜ ë‹¨ìœ„ê°€ ì› ë³€ìˆ˜ì™€ ê°™ì•„ í•´ì„ì´ ì§ê´€ì ì´ë‹¤.
- ìˆ˜ì‹(1ì°¨ì›)
$$
W\_1(P, Q) = \int\limits_{0}^{1} \big|F\_P^{-1}(u) - F\_Q^{-1}(u)\big|\,du
$$
- íŠ¹ì§•: symmetric, distribution ê²¹ì¹¨ì´ ì—†ì–´ë„ ì •ì˜ ê°€ëŠ¥, support set ê²½ê³„ì— ê°•í•¨. ìœ„ì¹˜Â·ìŠ¤ì¼€ì¼ ì´ë™ì„ ë¶€ë“œëŸ½ê²Œ ë°˜ì˜.
- ê³„ì‚°ë²•: 1DëŠ” `scipy.stats.wasserstein_distance`ë¡œ ì •ë ¬ ê¸°ë°˜ O(n log n). ë‹¤ë³€ëŸ‰ì€ í‘œì¤€í™” í›„ Sinkhorn(ì—”íŠ¸ë¡œí”¼ normalization) ë“±ìœ¼ë¡œ ê·¼ì‚¬(POT ë“± ë¼ì´ë¸ŒëŸ¬ë¦¬).
- threshold íŒíŠ¸: ì› ë‹¨ìœ„ ê·¸ëŒ€ë¡œ í•´ì„í•˜ê±°ë‚˜ IQR/í‘œì¤€í¸ì°¨ë¡œ normalizationí•´ ë¬´ë‹¨ìœ„ distanceë¡œ ë¹„êµ. KPIì™€ ì§ì ‘ ì—°ê²°í•´ threshold ì„¸íŒ… ìš©ì´.
- ì‚¬ìš©ì²˜: í°/êµ¬ì¡°ì  ì´ë™ ì¶”ì , ê¸°ì¤€Â·í˜„ì¬ distributionê°€ ê±°ì˜ ê²¹ì¹˜ì§€ ì•ŠëŠ” ìƒí™©, ë¹„ì¦ˆë‹ˆìŠ¤ ë‹¨ìœ„ í•´ì„ì´ ì¤‘ìš”í•œ ê²½ìš°.
- í”í•œ í•¨ì •: ê³ ì°¨ì› ê³„ì‚°ëŸ‰ ë¶€ë‹´Â·ë©”ëª¨ë¦¬ ì‚¬ìš© ì¦ê°€, í”¼ì²˜ ìŠ¤ì¼€ì¼ mismatch ì‹œ ì™œê³¡ â†’ í‘œì¤€í™” í•„ìˆ˜. ë‹¤ë³€ëŸ‰ OTëŠ” êµ¬í˜„Â·íŠœë‹ ë‚œì´ë„ ë†’ìŒ.


ì§ê´€ì  ì´í•´
- ğŸšš "í™ì„ ì˜®ê¸°ëŠ” ë¹„ìš©" ê°œë…
- í•œ ë¶„í¬ì˜ "ì§ˆëŸ‰"ì„ ë‹¤ë¥¸ ë¶„í¬ë¡œ ì˜®ê¸°ëŠ” ìµœì†Œ ì‘ì—…ëŸ‰ì„ ì˜ë¯¸í•©ë‹ˆë‹¤
- ë§ˆì¹˜ ê±´ì„¤ í˜„ì¥ì—ì„œ í™ì„ í•œ ê³³ì—ì„œ ë‹¤ë¥¸ ê³³ìœ¼ë¡œ ì˜®ê¸°ëŠ” ê²ƒê³¼ ë¹„ìŠ·í•©ë‹ˆë‹¤
- ê° ìœ„ì¹˜ì—ì„œ í•„ìš”í•œ í™ì˜ ì–‘ê³¼ ì˜®ê²¨ì•¼ í•  ê±°ë¦¬ë¥¼ ê³ ë ¤í•˜ì—¬ ì´ ì‘ì—…ëŸ‰ì„ ê³„ì‚°í•©ë‹ˆë‹¤

ê°„ë‹¨í•œ ì˜ˆì‹œ
- ë¶„í¬ P: [0, 0, 1, 0, 0]  (ì¤‘ì•™ì—ë§Œ ì§ˆëŸ‰)
- ë¶„í¬ Q: [0, 0, 0, 1, 0]  (ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ ì´ë™)
- Wasserstein Distance = 1 (í•œ ì¹¸ ì˜®ê¸°ë©´ ë¨)
- KL divergence = âˆ (ê²¹ì¹˜ëŠ” ë¶€ë¶„ì´ ì—†ìŒ)

ë°ì´í„° ë“œë¦¬í”„íŠ¸ì—ì„œì˜ í™œìš©
- ë°ì´í„° ë“œë¦¬í”„íŠ¸ë¥¼ ê°ì§€í•  ë•Œ:
  - ì‹œê°„ì— ë”°ë¥¸ ë¶„í¬ ë³€í™”ë¥¼ ì •ëŸ‰ì ìœ¼ë¡œ ì¸¡ì •
  - ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì˜ ì›ì¸ ë¶„ì„
  - ë°ì´í„° í’ˆì§ˆ ëª¨ë‹ˆí„°ë§ì— í™œìš©
- ì´ ë°©ë²•ì€ íŠ¹íˆ ì—°ì†í˜• ë³€ìˆ˜ë‚˜ ìˆœì„œê°€ ìˆëŠ” ë²”ì£¼í˜• ë³€ìˆ˜ì—ì„œ ë¶„í¬ ë³€í™”ë¥¼ ì¸¡ì •í•  ë•Œ ë§¤ìš° ìœ ìš©í•©ë‹ˆë‹¤.

---


