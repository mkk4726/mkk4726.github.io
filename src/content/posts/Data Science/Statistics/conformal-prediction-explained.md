---
title: "Conformal Prediction: ë¶„í¬ ê°€ì • ì—†ì´ ìœ í•œ ìƒ˜í”Œ coverageë¥¼ ë³´ì¥í•˜ëŠ” ë°©ë²•"
date: "2025-08-13"
excerpt: "í†µê³„ì  ê°€ì • ì—†ì´ë„ finite-sample coverageë¥¼ ë³´ì¥í•˜ëŠ” conformal predictionì˜ ì›ë¦¬ì™€ ì‹¤ë¬´ ì ìš©ë²•"
category: "Data Science"
tags: ["statistics", "prediction-interval", "uncertainty", "calibration"]
---

ì°¸ê³ ìë£Œ
- 1 : [ë¸”ë¡œê·¸ - Conformal Predictionìœ¼ë¡œ ëª¨ë¸ì˜ ë¶ˆí™•ì‹¤ì„± ê³„ì‚°í•˜ê¸°](https://pizzathiefz.github.io/posts/introduction-to-conformal-prediction/)
- 2 : [Paper - Conformal Prediction: A Gentle Introduction](https://arxiv.org/abs/2107.07511)
- 3 : [ë¸”ë¡œê·¸ - Conformal Prediction](https://ddangchani.github.io/Conformal-Prediction/#how-to-make-a-prediction-set)

[Paper Review - 2](/posts/Data%20Science/Paper-Review/A-Gentle-Introduction-to-Conformal-Prediction-and-Distribution-Free-Uncertainty-Quantification/)


---

## Conformal Predictionì´ë€?

**Conformal prediction**ì€ **ë¶„í¬ ê°€ì • ì—†ì´ë„ finite-sample coverageë¥¼ ë³´ì¥**í•˜ëŠ” prediction intervalì„ ë§Œë“œëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### í•µì‹¬ íŠ¹ì§•
- **ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”**: ë°ì´í„°ê°€ ì–´ë–¤ ë¶„í¬ë¥¼ ë”°ë¥´ë“  ìƒê´€ì—†ìŒ
- **Finite-sample coverage**: ìœ í•œí•œ ìƒ˜í”Œì—ì„œë„ ëª©í‘œ coverage ë³´ì¥
- **ëª¨ë¸ agnostic**: ì–´ë–¤ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ê³¼ë„ ì‚¬ìš© ê°€ëŠ¥
- **Calibration ìë™í™”**: ë³„ë„ ì¡°ì • ì—†ì´ë„ coverage ë³´ì¥

## ê¸°ë³¸ ì›ë¦¬

### Exchangeability ê°€ì •
Conformal predictionì˜ í•µì‹¬ ê°€ì •ì€ **exchangeability**ì…ë‹ˆë‹¤:

> ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„° $(x_{n+1}, y_{n+1})$ì™€ ê¸°ì¡´ ë°ì´í„° $(x_1, y_1), ..., (x_n, y_n)$ì´ exchangeableí•˜ë‹¤.

**Exchangeability**: ë°ì´í„°ì˜ ìˆœì„œë¥¼ ë°”ê¿”ë„ í™•ë¥  ë¶„í¬ê°€ ë™ì¼

### í•µì‹¬ ì•„ì´ë””ì–´
1. **Nonconformity score** ê³„ì‚°: ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì¸¡ì •
2. **Calibration set**ì—ì„œ ë¶„ìœ„ìˆ˜ ê³„ì‚°: ëª©í‘œ coverageì— ë§ëŠ” ì„ê³„ê°’ ë„ì¶œ
3. **ìƒˆë¡œìš´ ì˜ˆì¸¡**ì— ì ìš©: ê³„ì‚°ëœ ì„ê³„ê°’ìœ¼ë¡œ prediction interval êµ¬ì„±

## Split Conformal Prediction

ê°€ì¥ ì‹¤ìš©ì ì´ê³  ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.

### ë‹¨ê³„ë³„ ê³¼ì •

**1ë‹¨ê³„: ë°ì´í„° ë¶„í• **
```
ì „ì²´ ë°ì´í„° â†’ Train Set + Calibration Set
```

**2ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ**
- Train setìœ¼ë¡œ ì˜ˆì¸¡ ëª¨ë¸ $\hat{f}$ í•™ìŠµ

**3ë‹¨ê³„: Nonconformity score ê³„ì‚°**
- Calibration setì—ì„œ ê° ë°ì´í„°ì˜ nonconformity score ê³„ì‚°
- ëŒ€ì¹­ PIì˜ ê²½ìš°: $r_i = |y_i - \hat{f}(x_i)|$

**4ë‹¨ê³„: ì„ê³„ê°’ ê³„ì‚°**
- $\hat{Q} = (1-\alpha)(1+1/n_{cal})$-quantile of $\{r_i\}$
- $n_{cal}$: calibration set í¬ê¸°

**5ë‹¨ê³„: Prediction Interval êµ¬ì„±**
- ìƒˆë¡œìš´ ì…ë ¥ $x$ì— ëŒ€í•´: $[\hat{f}(x) - \hat{Q}, \hat{f}(x) + \hat{Q}]$

### Python êµ¬í˜„ ì˜ˆì‹œ

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor

def split_conformal_prediction(X, y, alpha=0.1, test_size=0.2):
    # 1. ë°ì´í„° ë¶„í• 
    X_train, X_cal, y_train, y_cal = train_test_split(
        X, y, test_size=test_size, random_state=42
    )
    
    # 2. ëª¨ë¸ í•™ìŠµ
    model = RandomForestRegressor(random_state=42)
    model.fit(X_train, y_train)
    
    # 3. Calibration setì—ì„œ ì˜ˆì¸¡
    y_cal_pred = model.predict(X_cal)
    
    # 4. Nonconformity scores ê³„ì‚°
    residuals = np.abs(y_cal - y_cal_pred)
    
    # 5. ì„ê³„ê°’ ê³„ì‚°
    n_cal = len(y_cal)
    quantile_level = (1 - alpha) * (1 + 1/n_cal)
    threshold = np.quantile(residuals, quantile_level)
    
    return model, threshold

# ì‚¬ìš© ì˜ˆì‹œ
model, threshold = split_conformal_prediction(X, y, alpha=0.1)

# ìƒˆë¡œìš´ ì˜ˆì¸¡
y_pred = model.predict(X_new)
pi_lower = y_pred - threshold
pi_upper = y_pred + threshold
```

## Conformalized Quantile Regression (CQR)

Quantile regressionê³¼ conformal predictionì„ ê²°í•©í•œ ë°©ë²•ì…ë‹ˆë‹¤.

### CQR ê³¼ì •

**1ë‹¨ê³„: Quantile ëª¨ë¸ í•™ìŠµ**
- $\tau = \alpha/2$ (í•˜í•œ)
- $\tau = 1 - \alpha/2$ (ìƒí•œ)

**2ë‹¨ê³„: Nonconformity score ê³„ì‚°**
$$
e_i = \max\big(\hat{q}_{\alpha/2}(x_i) - y_i, y_i - \hat{q}_{1-\alpha/2}(x_i), 0\big)
$$

**3ë‹¨ê³„: ì„ê³„ê°’ ê³„ì‚°**
- $\hat{Q} = (1-\alpha)(1+1/n_{cal})$-quantile of $\{e_i\}$

**4ë‹¨ê³„: ìµœì¢… PI êµ¬ì„±**
$$
[\hat{q}_{\alpha/2}(x) - \hat{Q}, \hat{q}_{1-\alpha/2}(x) + \hat{Q}]
$$

### CQRì˜ ì¥ì 
- **Heteroscedasticity ëŒ€ì‘**: ë¶„ì‚°ì´ ì¼ì •í•˜ì§€ ì•Šì•„ë„ ì í•©
- **Quantile ì •ë³´ í™œìš©**: ë” ì •í™•í•œ PI êµ¬ì„±
- **Coverage ë³´ì¥**: Finite-sample marginal coverage

## CQR (Conformalized Quantile Regression) êµ¬í˜„

CQRì€ quantile regressionì„ ì‚¬ìš©í•˜ì—¬ ë” ì •í™•í•œ prediction intervalsë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

```python
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split

class ConformalizedQuantileRegression:
    def __init__(self, alpha=0.1):
        self.alpha = alpha
        self.lower_model = None
        self.upper_model = None
        self.tau = None
    
    def fit(self, X_train, y_train, X_cal, y_cal):
        """
        CQR ëª¨ë¸ í•™ìŠµ ë° calibration (Romano et al., 2019)
        - í•˜í•œ/ìƒí•œ quantile ëª¨ë¸ í•™ìŠµ
        - calibration setìœ¼ë¡œ ì „ì—­ ì˜¤í”„ì…‹ tau ê³„ì‚°
        """
        from sklearn.ensemble import GradientBoostingRegressor
        # 1) Quantile models: lower (Î±/2), upper (1-Î±/2)
        q_low = self.alpha / 2.0
        q_high = 1.0 - self.alpha / 2.0
        self.lower_model = GradientBoostingRegressor(loss="quantile", alpha=q_low, random_state=42)
        self.upper_model = GradientBoostingRegressor(loss="quantile", alpha=q_high, random_state=42)
        self.lower_model.fit(X_train, y_train)
        self.upper_model.fit(X_train, y_train)
        
        # 2) Calibration nonconformity scores
        lower_cal = self.lower_model.predict(X_cal)
        upper_cal = self.upper_model.predict(X_cal)
        # ì ìˆ˜ëŠ” interval ë°”ê¹¥ìœ¼ë¡œ ë‚˜ê°„ ì •ë„(ìŒìˆ˜ëŠ” 0ìœ¼ë¡œ ì ˆë‹¨)
        scores = np.maximum.reduce([
            lower_cal - y_cal,
            y_cal - upper_cal,
            np.zeros_like(y_cal)
        ])
        
        # 3) Finite-sample quantile â†’ tau (ì „ì—­ ì˜¤í”„ì…‹)
        n_cal = len(scores)
        quantile_level = (1 - self.alpha) * (1 + 1 / n_cal)
        self.tau = np.quantile(scores, quantile_level)
        return self
    
    def predict(self, X):
        """
        ìµœì¢… Prediction Interval ìƒì„±: [q_low(X) - tau, q_high(X) + tau]
        """
        ql = self.lower_model.predict(X)
        qh = self.upper_model.predict(X)
        # Quantile crossing ë°©ì§€: í•˜í•œ/ìƒí•œ ì •ë ¬ í›„ ì˜¤í”„ì…‹ ì ìš©
        q_low_aligned = np.minimum(ql, qh)
        q_high_aligned = np.maximum(ql, qh)
        lower = q_low_aligned - self.tau
        upper = q_high_aligned + self.tau
        return lower, upper
    
    def get_coverage(self, X_test, y_test):
        """Test setì—ì„œ coverage ê³„ì‚°"""
        lower_bounds, upper_bounds = self.predict(X_test)
        
        # ì‹¤ì œê°’ì´ prediction interval ì•ˆì— ìˆëŠ” ë¹„ìœ¨
        coverage = np.mean(
            (y_test >= lower_bounds) & (y_test <= upper_bounds)
        )
        
        return coverage

# ì‚¬ìš© ì˜ˆì‹œ
def demonstrate_cqr():
    # ê°€ìƒ ë°ì´í„° ìƒì„± (heteroscedastic)
    np.random.seed(42)
    n_samples = 1000
    
    X = np.random.uniform(0, 10, n_samples).reshape(-1, 1)
    # Heteroscedastic noise: Xê°€ í´ìˆ˜ë¡ noiseë„ ì»¤ì§
    noise = np.random.normal(0, 0.1 + 0.05 * X.flatten(), n_samples)
    y = 2 * X.flatten() + noise
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_temp, y_train, y_temp = train_test_split(
        X, y, test_size=0.4, random_state=42
    )
    X_cal, X_test, y_cal, y_test = train_test_split(
        X_temp, y_temp, test_size=0.5, random_state=42
    )
    
    # CQR ëª¨ë¸ í•™ìŠµ
    cqr = ConformalizedQuantileRegression(alpha=0.1)
    cqr.fit(X_train, y_train, X_cal, y_cal)
    
    # Prediction intervals ìƒì„±
    lower_bounds, upper_bounds = cqr.predict(X_test)
    
    # Coverage í™•ì¸
    coverage = cqr.get_coverage(X_test, y_test)
    print(f"CQR Coverage: {coverage:.3f}")
    
    # Interval width í†µê³„
    interval_widths = upper_bounds - lower_bounds
    print(f"Average interval width: {np.mean(interval_widths):.3f}")
    print(f"Interval width std: {np.std(interval_widths):.3f}")
    
    return cqr, X_test, y_test, lower_bounds, upper_bounds

# ì‹¤í–‰
if __name__ == "__main__":
    cqr_model, X_test, y_test, lower_bounds, upper_bounds = demonstrate_cqr()
```

## CQRì˜ í•µì‹¬ íŠ¹ì§•

### 1. **Quantile-based Approach**
- Lower quantile (Î±/2)ì™€ upper quantile (1-Î±/2)ì„ ì§ì ‘ ëª¨ë¸ë§
- ê° ì…ë ¥ Xì— ëŒ€í•´ conditional quantiles í•™ìŠµ

### 2. **Conformal Scores**
- `max(lower_pred - y, y - upper_pred)` í˜•íƒœ
- ì‹¤ì œê°’ì´ prediction interval ë°–ì— ìˆëŠ” ì •ë„ë¥¼ ì¸¡ì •

### 3. **Adaptive Interval Width**
- **Quantile ëª¨ë¸**: Xì— ë”°ë¼ ë‹¤ë¥¸ lower/upper bounds
- **Adaptive Conformalization**: Xì™€ ìœ ì‚¬í•œ calibration pointsë¡œ local threshold ê³„ì‚°
- **ê²°ê³¼**: Xì— ë”°ë¼ ë³€í•˜ëŠ” interval width + local coverage guarantee

### 4. **Heteroscedasticity ì²˜ë¦¬**
- **Quantile ëª¨ë¸**ì´ Xì— ë”°ë¥¸ noise ë³€í™”ë¥¼ í•™ìŠµ
- **Local threshold**: K-nearest neighborsë¡œ Xë³„ë¡œ ë‹¤ë¥¸ conformalization
- **CRê³¼ì˜ ì°¨ì´**: ê³ ì •ëœ widthê°€ ì•„ë‹Œ **fully adaptive approach**

ì´ êµ¬í˜„ì€ **Random Forest**ë¥¼ quantile regressionì— ì‚¬ìš©í–ˆì§€ë§Œ, **Neural Networks**ë‚˜ **Gradient Boosting** ë“± ë‹¤ë¥¸ ëª¨ë¸ë¡œë„ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤.

## ì‹¤ë¬´ ì ìš© íŒ

### 1. ë°ì´í„° ë¶„í•  ì „ëµ
- **Train/Calibration ë¹„ìœ¨**: ë³´í†µ 80:20 ë˜ëŠ” 70:30
- **Stratified sampling**: ë¶„ë¥˜ ë¬¸ì œì—ì„œ í´ë˜ìŠ¤ ë¹„ìœ¨ ìœ ì§€
- **Time series**: ì‹œê°„ ìˆœì„œ ê³ ë ¤í•œ ë¶„í• 

### 2. Nonconformity Score ì„ íƒ
- **íšŒê·€**: $|y - \hat{f}(x)|$ (ì ˆëŒ“ê°’)
- **ë¶„ë¥˜**: $1 - \hat{p}_y(x)$ (ì˜ˆì¸¡ í™•ë¥ ì˜ ë³´ì™„)
- **ë¹„ëŒ€ì¹­**: $y - \hat{f}(x)$ (ë°©í–¥ì„± ê³ ë ¤)

### 3. Coverage vs Width ê· í˜•
- **ë†’ì€ coverage**: ë” ë„“ì€ PI
- **ì¢ì€ PI**: ë‚®ì€ coverage
- **ì‹¤ë¬´ì  trade-off**: ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­ ê³ ë ¤

## ì¥ì ê³¼ í•œê³„

### ì¥ì 
- **ë¶„í¬ ê°€ì • ë¶ˆí•„ìš”**: ì–´ë–¤ ë°ì´í„°ë“  ì‚¬ìš© ê°€ëŠ¥
- **Finite-sample ë³´ì¥**: ìœ í•œí•œ ìƒ˜í”Œì—ì„œë„ coverage ë³´ì¥
- **ëª¨ë¸ ë…ë¦½ì **: ì–´ë–¤ ML ëª¨ë¸ê³¼ë„ ê²°í•© ê°€ëŠ¥
- **Calibration ìë™**: ë³„ë„ ì¡°ì • ë¶ˆí•„ìš”

### í•œê³„
- **Marginal coverageë§Œ ë³´ì¥**: Conditional coverageëŠ” ë³´ì¥í•˜ì§€ ì•ŠìŒ
- **ë°ì´í„° ë¶„í•  í•„ìš”**: í•™ìŠµ ë°ì´í„° ì¼ë¶€ë¥¼ calibrationì— ì‚¬ìš©
- **Computational cost**: ì¶”ê°€ ê³„ì‚° ë¹„ìš©
- **Exchangeability ê°€ì •**: ì‹œê°„ ìˆœì„œê°€ ì¤‘ìš”í•œ ë°ì´í„°ì—ëŠ” ë¶€ì í•©

## ìµœì‹  ë°œì „ ë™í–¥

### 1. Adaptive Conformal Prediction
- ì˜¨ë¼ì¸ í•™ìŠµ í™˜ê²½ì—ì„œ coverage ìœ ì§€
- Concept driftì— ì ì‘

### 2. Conformal Prediction for Time Series
- ì‹œê³„ì—´ ë°ì´í„°ì˜ íŠ¹ì„± ê³ ë ¤
- Autocorrelation ì²˜ë¦¬

### 3. Multi-output Conformal Prediction
- ë‹¤ì¤‘ ì¶œë ¥ ë³€ìˆ˜ ë™ì‹œ ì²˜ë¦¬
- ì¶œë ¥ ê°„ ìƒê´€ê´€ê³„ ê³ ë ¤

### 4. Conformal Prediction with Deep Learning
- ì‹ ê²½ë§ ëª¨ë¸ê³¼ì˜ ê²°í•©
- Uncertainty quantification

## ì‹¤ë¬´ ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì „ í™•ì¸ì‚¬í•­
- [ ] ë°ì´í„°ì˜ exchangeability ê°€ì • ê²€í† 
- [ ] ì ì ˆí•œ ë°ì´í„° ë¶„í•  ì „ëµ ìˆ˜ë¦½
- [ ] Nonconformity score í•¨ìˆ˜ ì„ íƒ
- [ ] ëª©í‘œ coverage level ê²°ì •

### êµ¬í˜„ í›„ ê²€ì¦ì‚¬í•­
- [ ] Calibration plotìœ¼ë¡œ coverage í™•ì¸
- [ ] PI widthì˜ ì ì ˆì„± ê²€í† 
- [ ] Out-of-distribution ë°ì´í„°ì—ì„œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
- [ ] ê³„ì‚° ë¹„ìš©ê³¼ ì„±ëŠ¥ì˜ ê· í˜•ì  í™•ì¸

## ê²°ë¡ 

Conformal predictionì€ **ë¶„í¬ ê°€ì • ì—†ì´ë„ reliableí•œ prediction interval**ì„ ì œê³µí•˜ëŠ” ê°•ë ¥í•œ ë„êµ¬ì…ë‹ˆë‹¤.

**ì£¼ìš” í™œìš© í¬ì¸íŠ¸:**
- **Uncertainty quantification**ì´ ì¤‘ìš”í•œ ì‹¤ë¬´ ë¬¸ì œ
- **ë¶„í¬ ê°€ì •**ì„ í•˜ê¸° ì–´ë ¤ìš´ ìƒí™©
- **Finite-sample coverage** ë³´ì¥ì´ í•„ìš”í•œ ê²½ìš°
- **ê¸°ì¡´ ML ëª¨ë¸**ì— uncertainty ì •ë³´ ì¶”ê°€

ë‹¤ë§Œ, **marginal coverageë§Œ ë³´ì¥**í•œë‹¤ëŠ” í•œê³„ë¥¼ ì´í•´í•˜ê³ , **ë°ì´í„° ë¶„í• **ê³¼ **computational cost**ë¥¼ ê³ ë ¤í•˜ì—¬ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.




## Coverageë€?

**Coverage**ëŠ” prediction intervalì´ ì‹¤ì œ ê°’ì„ í¬í•¨í•˜ëŠ” ë¹„ìœ¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

**ì˜ˆì‹œ:**
- 90% prediction intervalì„ ë§Œë“¤ì—ˆë‹¤ë©´
- 100ë²ˆì˜ ì˜ˆì¸¡ ì¤‘ 90ë²ˆì€ ì‹¤ì œ ê°’ì´ êµ¬ê°„ ì•ˆì— ë“¤ì–´ê°€ì•¼ í•¨
- ì´ê²Œ **90% coverage**ì…ë‹ˆë‹¤

## Marginal vs Conditional Coverage

Marginalì€ "ë‹¤ë¥¸ ë³€ìˆ˜ë¥¼ ëª¨ë‘ í•©ì‚°(ë˜ëŠ” ì ë¶„)í•´ì„œ ì œê±°í•œ í›„ ë‚¨ì€ ê²ƒ"ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.

### Marginal Coverage
- **ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ í‰ê·  coverage**
- **ëª¨ë“  ì…ë ¥ê°’ $X$ì— ëŒ€í•´ í‰ê· ì ìœ¼ë¡œ** ëª©í‘œ coverage ë‹¬ì„±
- **ê°œë³„ $X$ ê°’ì—ì„œëŠ”** coverageê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ

### Conditional Coverage
- **ê° ì…ë ¥ê°’ $X$ì— ëŒ€í•´ ê°œë³„ì ìœ¼ë¡œ** ëª©í‘œ coverage ë‹¬ì„±
- **ëª¨ë“  $X$ì—ì„œ ë™ì¼í•œ coverage** ë³´ì¥

## êµ¬ì²´ì  ì˜ˆì‹œ

**Marginal Coverage (90%):**
```
X=1ì¼ ë•Œ: 95% coverage (ì‹¤ì œ 95ë²ˆ/100ë²ˆ í¬í•¨)
X=2ì¼ ë•Œ: 85% coverage (ì‹¤ì œ 85ë²ˆ/100ë²ˆ í¬í•¨)
X=3ì¼ ë•Œ: 90% coverage (ì‹¤ì œ 90ë²ˆ/100ë²ˆ í¬í•¨)
â†’ í‰ê· : 90% coverage âœ…
```

**Conditional Coverage (90%):**
```
X=1ì¼ ë•Œ: 90% coverage (ì •í™•íˆ 90ë²ˆ/100ë²ˆ í¬í•¨)
X=2ì¼ ë•Œ: 90% coverage (ì •í™•íˆ 90ë²ˆ/100ë²ˆ í¬í•¨)  
X=3ì¼ ë•Œ: 90% coverage (ì •í™•íˆ 90ë²ˆ/100ë²ˆ í¬í•¨)
â†’ ëª¨ë“  Xì—ì„œ 90% coverage âœ…
```

## ì™œ Marginal Coverageë§Œ ë³´ì¥í•˜ëŠ”ê°€?

**Conformal predictionì˜ í•œê³„:**
- **Marginal coverageëŠ” ë³´ì¥** âœ…
- **Conditional coverageëŠ” ë³´ì¥í•˜ì§€ ì•ŠìŒ** âŒ

**ì´ìœ :**
- ë°ì´í„° ë¶„í• ì„ í†µí•´ ì „ì²´ì ì¸ coverageë§Œ ì¡°ì •
- ê°œë³„ $X$ ê°’ë³„ë¡œëŠ” coverageê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ
- ë” ì •í™•í•œ conditional coverageë¥¼ ìœ„í•´ì„œëŠ” ì¶”ê°€ì ì¸ ë°©ë²• í•„ìš”

**ì‹¤ë¬´ì  ì˜ë¯¸:**
- ì „ì²´ì ìœ¼ë¡œëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” êµ¬ê°„
- í•˜ì§€ë§Œ íŠ¹ì • ì…ë ¥ê°’ì—ì„œëŠ” ë„ˆë¬´ ë„“ê±°ë‚˜ ì¢ì„ ìˆ˜ ìˆìŒ
- ì´ëŠ” conformal predictionì˜ ê·¼ë³¸ì ì¸ í•œê³„ì…ë‹ˆë‹¤



# calibration set êµ¬ì„± í›„ unconformal scoreì—ì„œ ì¶”ì¶œí•˜ëŠ” ê³¼ì • ì¶”ê°€ ì„¤ëª…

## 1. ìƒí™© ì„¤ì •

* ìš°ë¦¬ëŠ” ì´ë¯¸ **ë¶„ìœ„ìˆ˜ íšŒê·€**ë¡œ
  $\hat{q}_{lo}(x)$ (ì˜ˆ: 10% ë¶„ìœ„ìˆ˜),
  $\hat{q}_{hi}(x)$ (ì˜ˆ: 90% ë¶„ìœ„ìˆ˜)
  ì˜ˆì¸¡ê°’ì„ ì–»ì—ˆìŠµë‹ˆë‹¤.
* ì´ ì˜ˆì¸¡ êµ¬ê°„ $[\hat{q}_{lo}(x), \hat{q}_{hi}(x)]$ì´ ì‹¤ì œë¡œëŠ” **80% ì»¤ë²„ë¦¬ì§€ë¥¼ ëª» ì§€í‚¬ ìˆ˜ ìˆìŒ** â†’ â€œë„ˆë¬´ ì¢ê±°ë‚˜, ë„ˆë¬´ ë„“ìŒâ€.

ê·¸ë˜ì„œ **Conformal**ì´ â€œì•ˆì „ë§ˆì§„â€ì„ ë”í•´ì„œ, ì»¤ë²„ë¦¬ì§€ë¥¼ ë°ì´í„° ê¸°ë°˜ìœ¼ë¡œ ë³´ì •í•©ë‹ˆë‹¤.

---

## 2. ì™œ nonconformity scoreë¥¼ ì €ë ‡ê²Œ ì •ì˜í•˜ë‚˜?

ì •ì˜:

$$
s_i = \max\{ \hat{q}_{lo}(x_i) - y_i,\; y_i - \hat{q}_{hi}(x_i),\; 0 \}
$$

ì¦‰, **ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì„¸íŠ¸ì—ì„œ ê° ì ì´ ì˜ˆì¸¡ êµ¬ê°„ì„ ë²—ì–´ë‚œ ì •ë„**ë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤.

* ë§Œì•½ $y_i$ê°€ êµ¬ê°„ ì•ˆì— ìˆë‹¤ë©´?
  $\hat{q}_{lo}(x_i) \le y_i \le \hat{q}_{hi}(x_i)$
  â†’ ë‘ í•­ ëª¨ë‘ ìŒìˆ˜ or 0 â†’ $s_i = 0$
  (ì¦‰, ì˜ ì»¤ë²„í•œ ê²½ìš° â€œë²Œì  ì—†ìŒâ€).

* ë§Œì•½ $y_i$ê°€ êµ¬ê°„ ì•„ë˜ìª½ì— ìˆë‹¤ë©´?
  $y_i < \hat{q}_{lo}(x_i)$
  â†’ ì²« ë²ˆì§¸ í•­ $\hat{q}_{lo}(x_i)-y_i > 0$
  â†’ êµ¬ê°„ë³´ë‹¤ ì–¼ë§ˆë‚˜ â€œë°‘ìœ¼ë¡œ ë²—ì–´ë‚¬ëŠ”ì§€â€ê°€ ì ìˆ˜.

* ë§Œì•½ $y_i$ê°€ êµ¬ê°„ ìœ„ìª½ì— ìˆë‹¤ë©´?
  $y_i > \hat{q}_{hi}(x_i)$
  â†’ ë‘ ë²ˆì§¸ í•­ $y_i - \hat{q}_{hi}(x_i) > 0$
  â†’ êµ¬ê°„ë³´ë‹¤ ì–¼ë§ˆë‚˜ â€œìœ„ë¡œ ë²—ì–´ë‚¬ëŠ”ì§€â€ê°€ ì ìˆ˜.

â†’ ê²°êµ­ $s_i$ëŠ” â€œì´ ì ì´ êµ¬ê°„ì—ì„œ ë²—ì–´ë‚œ ê±°ë¦¬(insideë©´ 0)â€ì…ë‹ˆë‹¤.

---

## 3. ë¶„ìœ„ìˆ˜ $\hat{q}_\alpha$ ë½‘ëŠ” ì´ìœ 

ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ì„¸íŠ¸ì—ì„œ ëª¨ë“  ì ìˆ˜ $\{s_i\}$ë¥¼ ëª¨ì•„ë†“ê³ , ê·¸ ì¤‘ **ìƒìœ„ $(1-\alpha)$-ë¶„ìœ„ìˆ˜**ë¥¼ ì°¾ìŠµë‹ˆë‹¤.

* ì˜ˆ: $\alpha=0.2$ (80% ì»¤ë²„ë¦¬ì§€ ì›í•¨).
* ì ìˆ˜ë“¤ì„ ì •ë ¬í–ˆì„ ë•Œ, ìœ„ì—ì„œ 80% ì§€ì ì— í•´ë‹¹í•˜ëŠ” ê°’ì´ $\hat{q}_\alpha$.
  â†’ â€œ20% ì •ë„ëŠ” ì´ë§Œí¼ ë²—ì–´ë‚˜ë”ë¼â€ëŠ” ì•ˆì „ë§ˆì§„.

---

## 4. ìµœì¢… ë³´ì •ëœ êµ¬ê°„

ëª¨ë“  ìƒˆë¡œìš´ $x$ì— ëŒ€í•´

$$
[\hat{q}_{lo}(x) - \hat{q}_\alpha,\;\; \hat{q}_{hi}(x) + \hat{q}_\alpha]
$$

* ì›ë˜ êµ¬ê°„ì— **ì¢Œìš°ë¡œ $\hat{q}_\alpha$ë§Œí¼ í™•ì¥**
* ê·¸ëŸ¬ë©´ (ë¶„í¬ ë¶ˆë³€ ê°€ì • í•˜) ìœ í•œ í‘œë³¸ì—ì„œë„ **80% ì´ìƒ ì»¤ë²„ë¦¬ì§€ê°€ ë³´ì¥**ë¨.

---

## 5. ë¹„ìœ 

* ì›ë˜ ëª¨ë¸ êµ¬ê°„ = â€œì˜ˆìƒí•œ ìš°ì‚° í¬ê¸°â€
* nonconformity score = â€œì‹¤ì œ ë¹„ê°€ ìš°ì‚° ë°”ê¹¥ìœ¼ë¡œ íŠ€ì–´ë‚˜ì˜¨ ê±°ë¦¬â€
* ê·¸ ê±°ë¦¬ë“¤ì˜ ìƒìœ„ ë¶„ìœ„ìˆ˜ = â€œì•ˆì „ ë§ˆì§„â€
* ìµœì¢… êµ¬ê°„ = â€œìš°ì‚° í¬ê¸° + ì—¬ìœ  ë§ˆì§„â€ â†’ ì§„ì§œë¡œ ë¹„ë¥¼ 80% ë§‰ì•„ëƒ„.

---

ğŸ‘‰ í•µì‹¬ ìš”ì•½

* $s_i$: ìº˜ë¦¬ë¸Œë ˆì´ì…˜ ë°ì´í„°ì—ì„œ **ì–¼ë§ˆë‚˜ êµ¬ê°„ì—ì„œ ë²—ì–´ë‚¬ëŠ”ê°€**
* $\hat{q}_\alpha$: ê·¸ ë²—ì–´ë‚œ ì •ë„ ì¤‘ â€œìƒìœ„ 20% ì»·ì˜¤í”„â€
* ìµœì¢… êµ¬ê°„: ì›ë˜ êµ¬ê°„ì— ì´ ë§ˆì§„ì„ ë”í•´ **ì»¤ë²„ë¦¬ì§€ë¥¼ ë³´ì¥**

---


# ê´€ë ¨ ì½”ë“œ

```python
import pandas as pd
import numpy as np
import lightgbm as lgb
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_error, mean_squared_error

# -------------------------
# 1) ë°ì´í„° ì¤€ë¹„
# -------------------------
# df ê°€ ì´ë¯¸ ë©”ëª¨ë¦¬ì— ìˆë‹¤ê³  ê°€ì •
X = df[["Cas_acd_endo", "Cas_clr", "Cas_ata", "size", "age"]].copy()
X["size"] = X["size"].astype(float)
y = df["meas_vault"].copy()

# -------------------------
# 2) Train / Calib / Test ë¶„í•  (ì˜ˆ: 60/20/20)
# -------------------------
X_train_full, X_test, y_train_full, y_test = train_test_split(
    X, y, test_size=0.20, random_state=42
)
X_train, X_calib, y_train, y_calib = train_test_split(
    X_train_full, y_train_full, test_size=0.25, random_state=42
)  # 0.25 of 0.8 = 0.2  -> ìµœì¢… 60/20/20

# -------------------------
# 3) ë¶„ìœ„ìˆ˜ ëª¨ë¸ í•™ìŠµ (10, 25, 50, 75, 90)
#    - LightGBMì€ ê° ë¶„ìœ„ìˆ˜ë§ˆë‹¤ ë³„ë„ ëª¨ë¸
# -------------------------
quantiles = [0.1, 0.25, 0.5, 0.75, 0.9]
models = {}

for q in quantiles:
    train_data = lgb.Dataset(X_train, label=y_train)
    params = {
        "objective": "quantile",
        "alpha": q,
        "metric": "quantile",
        "boosting_type": "gbdt",
        "learning_rate": 0.1,
        "num_leaves": 31,
        "verbose": -1,
        "seed": 42,
    }
    # ì£¼ì˜: lgb.trainì€ num_boost_roundë¡œ íŠ¸ë¦¬ ê°œìˆ˜ ì§€ì •
    model = lgb.train(params, train_data, num_boost_round=200)
    models[q] = model

# -------------------------
# 4) ì˜ˆì¸¡(ìº˜ë¦¬ë¸Œë ˆì´ì…˜/í…ŒìŠ¤íŠ¸)
# -------------------------
def predict_quantiles(models, X_any, qs=quantiles):
    out = {}
    for q in qs:
        out[f"q{int(q*100)}"] = models[q].predict(X_any)
    df_pred = pd.DataFrame(out, index=X_any.index)
    # (ê¶Œì¥) êµì°¨ ë°©ì§€: q10 <= q50 <= q90 í˜•íƒœê°€ ë³´ì¥ë˜ë„ë¡ ì •ë ¬
    order = [f"q{int(q*100)}" for q in qs]
    arr = df_pred[order].to_numpy()
    arr_sorted = np.sort(arr, axis=1)   # ì˜¤ë¦„ì°¨ìˆœìœ¼ë¡œ ì •ë ¬
    df_pred[order] = arr_sorted
    return df_pred

pred_calib = predict_quantiles(models, X_calib)
pred_test  = predict_quantiles(models, X_test)

# -------------------------
# 5) ì›ë˜(ë¹„ë³´ì •) êµ¬ê°„ ì„±ëŠ¥ í‰ê°€
# -------------------------
print("=== Original Quantile Regression Results (no conformal) ===")
for q in quantiles:
    col = f"q{int(q*100)}"
    pred = pred_test[col]
    quantile_loss = np.mean(np.where(y_test >= pred, q*(y_test - pred), (1-q)*(pred - y_test)))
    mae = mean_absolute_error(y_test, pred)
    rmse = np.sqrt(mean_squared_error(y_test, pred))
    print(f"Q{int(q*100)}: Quantile Loss={quantile_loss:.4f}, MAE={mae:.4f}, RMSE={rmse:.4f}")

# ë¹„ë³´ì • 80% êµ¬ê°„ ì»¤ë²„ë¦¬ì§€
lower_raw = pred_test["q10"]
upper_raw = pred_test["q90"]
coverage_raw = np.mean((y_test >= lower_raw) & (y_test <= upper_raw))
print(f"\n[no conformal] 80% interval coverage: {coverage_raw:.4f}")

# -------------------------
# 6) Split Conformal (CQR) ë³´ì •
#    - alpha=0.2 => 80% êµ¬ê°„ ëª©í‘œ
#    - nonconformity: max(q_lo - y, y - q_hi, 0)
# -------------------------
alpha = 0.20
q_lo_name = "q10"
q_hi_name = "q90"

q_lo_cal = pred_calib[q_lo_name].to_numpy()
q_hi_cal = pred_calib[q_hi_name].to_numpy()
y_cal = y_calib.to_numpy()

scores = np.maximum(q_lo_cal - y_cal, y_cal - q_hi_cal)
scores = np.maximum(scores, 0.0)  # ìŒìˆ˜ ë°©ì§€(ì´ë¡ ìƒ í•„ìš”ì—†ì§€ë§Œ ì•ˆì „)

# ìœ í•œí‘œë³¸ ë³´ì •(â€œhigherâ€ ë°©ì‹). numpy ë²„ì „ì— ë”°ë¼ method ì¸ì ì´ë¦„ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ.
try:
    q_hat = np.quantile(scores, 1 - alpha, method="higher")
except TypeError:
    # êµ¬ë²„ì „ numpy í˜¸í™˜
    q_hat = np.quantile(scores, 1 - alpha, interpolation="higher")

# -------------------------
# 7) ë³´ì • êµ¬ê°„ ìƒì„± ë° ì»¤ë²„ë¦¬ì§€ í‰ê°€
# -------------------------
q_lo_test = pred_test[q_lo_name].to_numpy()
q_hi_test = pred_test[q_hi_name].to_numpy()

lower_cqr = q_lo_test - q_hat
upper_cqr = q_hi_test + q_hat

coverage_cqr = np.mean((y_test.to_numpy() >= lower_cqr) & (y_test.to_numpy() <= upper_cqr))

print(f"[conformal CQR] 80% interval coverage: {coverage_cqr:.4f}")
print(f"Conformal adjustment (q_hat) = {q_hat:.4f}")

# -------------------------
# 8) ê²°ê³¼ DataFrame ì •ë¦¬ (ì›/ë³´ì • êµ¬ê°„)
# -------------------------
results_df = pd.DataFrame({
    "y_true": y_test,
    "q10_raw": lower_raw,
    "q50": pred_test["q50"],
    "q90_raw": upper_raw,
    "low_cqr": lower_cqr,
    "up_cqr": upper_cqr
}, index=y_test.index)

# í•„ìš”ì‹œ í™•ì¸
# results_df.head()
```

