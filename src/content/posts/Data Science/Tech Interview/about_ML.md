---
title: "ML 관련 인터뷰 준비"
date: "2025-09-08"
excerpt: ""
category: "Data Science"
tags: ["Interview"]
---

참고자료
- 1: [ai 테크 인터뷰](https://github.com/boost-devs/ai-tech-interview?tab=readme-ov-file)

---

### 알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)

regression task -> RMSE, MAE 등이 있겠고 
classification task -> recall, precision, ROC-AUC 등이 있겠다.


먼저 regression task의 경우, RMSE와 MAE가 가장 많이 사용됩니다. RMSE는 Root Mean Square Error로, 예측값과 실제값의 차이를 제곱해서 평균을 구한 후 제곱근을 취한 값입니다. 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. MAE는 Mean Absolute Error로, 절댓값 차이의 평균을 구하는데, 이상치에 덜 민감합니다. 그래서 이상치가 많은 데이터에서는 MAE가 더 적절할 수 있어요.

classification task에서는 상황에 따라 다양한 metric을 사용합니다. 가장 기본적인 accuracy는 전체 예측 중 맞춘 비율인데, 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. 

precision은 양성으로 예측한 것 중 실제로 양성인 비율이고, recall은 실제 양성 중 올바르게 찾아낸 비율입니다. precision이 중요한 경우는 스팸 메일 분류처럼 false positive 비용이 클 때이고, recall이 중요한 경우는 의료 진단처럼 false negative 비용이 클 때입니다. 

F1-score는 precision과 recall의 조화평균으로, 두 지표를 균형있게 고려하고 싶을 때 사용해요. ROC-AUC는 ROC 곡선 아래 면적을 의미하는데, 다양한 threshold에서의 모델 성능을 종합적으로 평가할 수 있어서 이진 분류에서 많이 사용됩니다.

실제 프로젝트에서는 비즈니스 요구사항과 데이터 특성을 고려해서 적절한 metric을 선택하는 것이 중요합니다.


### 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?

과적합을 피하기 위해서. lasso, ridge처럼 모델의 가중치를 loss에 포함시켜서 과적합 되지 않도록 유도한다.
tree계열에서도 마찬가지로 정규화를 사용함.

정규화는 주로 과적합을 방지하기 위해 사용합니다. 모델이 훈련 데이터에만 너무 잘 맞춰져서 새로운 데이터에 대한 일반화 성능이 떨어지는 것을 막기 위해서죠.

정규화 방법으로는 크게 두 가지가 있습니다. 첫 번째는 L1 정규화, 즉 Lasso입니다. 이는 가중치의 절댓값의 합을 loss function에 추가하는 방식으로, 불필요한 feature들을 0으로 만들어서 자동으로 feature selection 효과를 가져옵니다. 

두 번째는 L2 정규화, 즉 Ridge입니다. 이는 가중치의 제곱합을 loss function에 추가하는 방식으로, 가중치들을 0에 가깝게 만들어서 모델을 단순화시킵니다. Lasso와 달리 feature selection은 하지 않고 모든 feature를 유지하면서 가중치만 줄여줍니다.

Elastic Net은 L1과 L2를 결합한 방식이고, Dropout은 딥러닝에서 랜덤하게 일부 뉴런을 비활성화시키는 방법입니다.

Tree 계열 모델에서도 정규화를 사용하는데, 예를 들어 XGBoost에서는 max_depth, min_child_weight, gamma 등의 파라미터로 모델 복잡도를 제어하고, Random Forest에서는 max_depth나 min_samples_split 같은 파라미터로 과적합을 방지합니다.

실제로는 데이터의 크기, feature의 개수, 모델의 복잡도 등을 고려해서 적절한 정규화 방법을 선택하는 것이 중요합니다.


### Local Minima와 Global Minimum에 대해 설명해주세요.

지역적 최솟값과 전역적 최솟값을 의미하며, 최솟값이 부분적으로 최소인지 전체에서 최소인지를 구분.
global minima는 local minima 중 가장 작은 값.


네, Local Minima와 Global Minimum은 최적화 문제에서 중요한 개념입니다.

Local Minima는 지역적 최솟값으로, 특정 구간에서 가장 작은 값을 의미합니다. 즉, 그 점 주변의 작은 영역에서는 더 작은 값이 없지만, 전체 함수를 봤을 때는 더 작은 값이 존재할 수 있어요.

Global Minimum은 전역적 최솟값으로, 전체 함수 영역에서 가장 작은 값을 의미합니다. 모든 가능한 점들 중에서 가장 작은 값을 가지는 지점이죠.

간단한 예로 설명하면, 산맥을 생각해보시면 됩니다. 여러 개의 골짜기가 있는데, 각 골짜기 바닥이 Local Minima이고, 그 중에서 가장 낮은 골짜기가 Global Minimum입니다.

머신러닝에서 이 문제가 중요한 이유는, Gradient Descent 같은 최적화 알고리즘이 Local Minima에 빠져서 Global Minimum을 찾지 못할 수 있기 때문입니다. 특히 딥러닝에서는 수많은 Local Minima가 존재하는데, 다행히도 대부분의 Local Minima들이 비슷한 성능을 보인다는 연구 결과도 있습니다.

이를 해결하기 위해 다양한 기법들을 사용하는데, momentum, learning rate scheduling, 여러 초기값으로 시작하기, 또는 simulated annealing 같은 방법들이 있습니다.



### 차원의 저주에 대해 설명해주세요.

차원이 많이 질수록 sparse해진다는 것.

차원의 저주는 차원이 증가할수록 발생하는 여러 문제들을 통칭하는 개념입니다.

가장 핵심적인 문제는 **데이터의 희소성(sparsity)**입니다. 차원이 늘어날수록 같은 데이터 개수로는 공간을 채우기 어려워져요. 예를 들어, 1차원에서는 10개의 점으로 선을 채울 수 있지만, 10차원에서는 같은 10개의 점으로는 공간의 극히 일부분만 커버하게 됩니다.

이로 인해 몇 가지 구체적인 문제들이 발생합니다. 첫째, **근접성의 의미가 사라집니다**. 고차원에서는 모든 점들이 거의 비슷한 거리에 있게 되어서, "가까운" 점과 "먼" 점을 구분하기 어려워져요.

둘째, **과적합이 쉽게 발생**합니다. 차원이 많을수록 모델이 복잡해지고, 같은 데이터 개수로는 충분한 학습이 어려워집니다.

셋째, **계산 복잡도가 기하급수적으로 증가**합니다. 차원이 하나씩 늘어날 때마다 필요한 계산량이 폭발적으로 증가해요.

실제로는 차원 축소 기법들로 이 문제를 해결합니다. PCA, t-SNE, UMAP 같은 방법들로 중요한 정보는 유지하면서 차원을 줄이거나, feature selection을 통해 불필요한 차원을 제거하는 방식으로 접근합니다.




### dimension reduction기법으로 보통 어떤 것들이 있나요?

차원 축소 기법은 크게 선형과 비선형 방법으로 나뉩니다. 

가장 기본적이고 널리 사용되는 방법은 **PCA(Principal Component Analysis)**입니다. PCA는 데이터의 분산이 가장 큰 방향으로 새로운 축을 찾아 차원을 축소하는 선형 방법으로, 주성분을 통해 원본 데이터의 정보를 최대한 보존하면서도 해석이 용이하고 계산이 빠릅니다. 주로 이미지 압축, 노이즈 제거, 시각화 등에 사용됩니다.

선형 방법 중에는 **LDA(Linear Discriminant Analysis)**도 중요한데, 이는 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 지도학습 방법으로 분류 성능 향상에 특화되어 있습니다. 또한 **Factor Analysis**는 관찰된 변수들을 잠재 변수로 설명하는 방법으로 심리학이나 사회과학 분야에서 주로 사용됩니다.

비선형 방법으로는 **t-SNE**가 가장 유명합니다. t-SNE는 고차원 데이터를 2D나 3D로 시각화하는 데 특화되어 있으며, 지역적 구조를 잘 보존하기 때문에 클러스터링 결과 시각화에 매우 효과적입니다. 다만 전역적 구조는 보존하지 못하고 하이퍼파라미터에 민감하다는 단점이 있습니다.

**UMAP**은 t-SNE의 단점을 보완한 방법으로, t-SNE보다 빠르면서도 전역적 구조를 어느 정도 보존합니다. 지역적과 전역적 구조의 균형을 잘 맞춰서 대용량 데이터에도 적용 가능합니다.

**Autoencoder**는 신경망 기반의 차원 축소 방법으로, 인코더-디코더 구조를 통해 데이터를 압축하고 복원합니다. 비선형 관계를 잘 학습할 수 있어서 딥러닝과 결합하여 사용되는 경우가 많습니다.

기타 방법들로는 **Isomap**이 있는데, 이는 지오데식 거리를 사용하는 매니폴드 학습의 대표적인 방법입니다. **LLE(Locally Linear Embedding)**는 지역적으로 선형인 관계를 가정하는 방법으로 작은 데이터셋에서 효과적입니다.

실제로 어떤 방법을 선택할지는 목적에 따라 다릅니다. 시각화가 목적이라면 t-SNE나 UMAP을, 전처리가 목적이라면 PCA나 Autoencoder를, 분류 성능 향상이 목적이라면 LDA를 사용하는 것이 좋습니다. 대용량 데이터를 다룬다면 UMAP이나 PCA를, 해석 가능성이 중요하다면 PCA나 Factor Analysis를 선택하는 것이 적절합니다.





### PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?

PCA가 이 세 가지 역할을 모두 할 수 있는 이유는 PCA의 핵심 동작 원리에 있습니다. PCA는 데이터의 분산이 가장 큰 방향, 즉 주성분(principal component)을 찾아서 새로운 좌표계를 만드는 기법인데, 이 과정에서 자연스럽게 세 가지 효과가 나타납니다.

먼저 차원 축소 측면에서는, PCA가 원본 데이터의 분산을 가장 잘 설명하는 몇 개의 주성분만 선택하기 때문입니다. 예를 들어 100차원 데이터에서 분산의 95%를 설명하는 10개의 주성분만 선택하면, 100차원을 10차원으로 줄이면서도 중요한 정보는 대부분 보존할 수 있어요.

데이터 압축 측면에서는, 주성분을 통해 원본 데이터를 재구성할 수 있기 때문입니다. 원본 데이터를 주성분들의 선형결합으로 표현하고, 다시 원래 공간으로 복원할 수 있어요. 물론 일부 정보 손실은 있지만, 압축률에 비해 품질 손실이 적어서 이미지 압축 등에 활용됩니다.

노이즈 제거 측면에서는, 노이즈가 보통 작은 분산을 가지기 때문입니다. PCA는 분산이 큰 방향, 즉 신호가 강한 방향을 우선적으로 선택하고, 분산이 작은 방향, 즉 노이즈가 많은 방향은 제거하게 됩니다. 따라서 주성분만 사용해서 데이터를 재구성하면 자연스럽게 노이즈가 제거된 깨끗한 데이터를 얻을 수 있어요.

이 세 가지 효과는 모두 PCA가 "분산이 큰 방향을 우선적으로 선택한다"는 하나의 원리에서 나오는 결과입니다. 그래서 PCA는 단순한 차원 축소 도구가 아니라, 데이터의 본질적인 구조를 찾아내는 강력한 기법이라고 할 수 있습니다.






### LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?

이 세 기법들은 모두 행렬 분해를 기반으로 하는 차원 축소 기법들이지만, 각각 다른 목적과 특성을 가지고 있습니다.

먼저 **SVD(Singular Value Decomposition)**는 가장 기본이 되는 행렬 분해 기법입니다. 임의의 행렬을 세 개의 행렬의 곱으로 분해하는데, A = UΣV^T 형태로 나타납니다. 여기서 U와 V는 직교행렬이고, Σ는 대각행렬입니다. SVD는 선형대수학의 핵심 도구로, 데이터의 내재적 구조를 찾아내는 데 사용됩니다.

**LSA(Latent Semantic Analysis)**는 SVD를 텍스트 마이닝에 특화해서 적용한 기법입니다. 문서-단어 행렬을 SVD로 분해해서 잠재 의미 공간을 찾아내는 방법이에요. 예를 들어 "자동차"와 "차량"이라는 서로 다른 단어가 같은 문서에서 자주 나타나면, LSA는 이들이 비슷한 의미를 가진다는 것을 학습할 수 있습니다. 이렇게 해서 단어의 동의어나 유의어 관계를 자동으로 찾아내고, 문서의 주제를 파악하는 데 활용됩니다.

**LDA(Linear Discriminant Analysis)**는 앞의 두 기법과는 다른 목적을 가진 지도학습 기법입니다. 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 방향으로 차원을 축소하는 방법이에요. 분류 성능 향상에 특화되어 있어서, 같은 차원 축소를 하더라도 분류에 유리한 방향으로 축을 선택합니다.

이 세 기법의 관계를 정리하면, SVD는 수학적 기반이 되는 행렬 분해 기법이고, LSA는 SVD를 텍스트 분석에 적용한 비지도학습 방법이며, LDA는 분류 성능 향상을 위한 지도학습 방법입니다. 모두 차원 축소라는 공통점이 있지만, LSA는 의미적 유사성을 찾는 데, LDA는 분류 성능을 높이는 데 초점을 맞춘다는 차이점이 있습니다.

실제로는 텍스트 분석에서는 LSA를, 분류 문제에서는 LDA를, 그리고 일반적인 행렬 분해나 노이즈 제거에는 SVD를 직접 사용하는 경우가 많습니다.






### Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?

고등학생에게 Markov Chain을 설명할 때는 일상생활의 구체적인 예시를 사용하는 것이 가장 효과적입니다. 가장 좋은 예시는 **날씨 예측**입니다.

"오늘 비가 오면 내일도 비가 올 확률이 60%, 맑을 확률이 40%이고, 오늘 맑으면 내일 맑을 확률이 70%, 비가 올 확률이 30%다"라고 설명해보세요. 이때 중요한 점은 **과거의 모든 날씨 기록이 아니라 오늘의 날씨만이 내일 날씨를 결정한다**는 것입니다.

이것이 바로 Markov Chain의 핵심인 "현재 상태가 다음 상태를 결정한다"는 Markov 성질입니다. 과거의 모든 정보를 기억할 필요 없이, 현재 상태만 알면 미래를 예측할 수 있다는 것이죠.

다른 좋은 예시로는 **게임의 이동**이 있습니다. 보드게임에서 현재 위치에서만 다음 위치가 결정되고, 어떻게 그 위치에 도달했는지는 중요하지 않다는 점을 설명하면 됩니다. 이런 식으로 일상생활의 친숙한 예시를 통해 Markov Chain의 개념을 자연스럽게 이해시킬 수 있습니다.


### 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?

텍스트에서 주제를 추출하는 작업은 **토픽 모델링(Topic Modeling)**이라고 하며, 여러 단계의 체계적인 접근이 필요합니다.

먼저 **데이터 전처리** 단계에서 텍스트를 정제합니다. 불용어(stopwords) 제거, 어간 추출(stemming), 소문자 변환 등을 통해 노이즈를 줄이고, 토큰화를 통해 단어 단위로 분리합니다. 이때 특수문자나 숫자 처리도 고려해야 합니다.

다음으로 **문서-단어 행렬(Document-Term Matrix)**을 생성합니다. TF-IDF나 단순 빈도 기반으로 각 문서에서 단어의 중요도를 계산하여 수치화합니다.

주제 추출의 핵심 방법으로는 **LDA(Latent Dirichlet Allocation)**가 가장 널리 사용됩니다. LDA는 각 문서가 여러 주제의 혼합으로 구성되어 있고, 각 주제는 단어들의 확률 분포로 표현된다고 가정합니다. 이를 통해 문서의 주제 분포와 주제별 단어 분포를 동시에 학습합니다.

대안으로는 **LSA(Latent Semantic Analysis)**나 **NMF(Non-negative Matrix Factorization)**도 사용할 수 있습니다. LSA는 SVD를 활용한 선형 방법이고, NMF는 음수가 아닌 행렬 분해를 통해 주제를 찾는 방법입니다.

최근에는 **BERT 기반의 토픽 모델링**도 주목받고 있습니다. 사전 훈련된 언어 모델을 활용하여 더 정교한 의미적 유사성을 반영한 주제 추출이 가능합니다.

실제 적용 시에는 주제 개수 설정, 하이퍼파라미터 튜닝, 결과 해석의 어려움 등을 고려해야 하며, 도메인 지식과 함께 결과를 검증하는 것이 중요합니다.
### SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

SVM이 차원을 확장시키는 이유는 **선형 분리가 불가능한 데이터를 고차원 공간에서 선형 분리가 가능하도록 만들기 위해서**입니다. 이를 **커널 트릭(Kernel Trick)**이라고 합니다.

예를 들어, 2차원에서 원형으로 분포된 데이터는 직선으로 분리할 수 없습니다. 하지만 3차원으로 올려서 z = x² + y² 같은 변환을 하면, 3차원에서 평면으로 분리할 수 있게 됩니다. SVM은 이런 아이디어를 극한까지 활용해서, 데이터를 무한 차원의 공간으로 보내서도 선형 분리가 가능하도록 만듭니다.

핵심은 실제로 고차원 공간으로 변환하지 않고도, 커널 함수를 통해 고차원에서의 내적을 계산할 수 있다는 점입니다. RBF 커널, 다항식 커널 등이 대표적인 예시입니다.

SVM이 좋은 이유는 여러 가지가 있습니다. 첫째, **고차원 데이터에서도 효과적**입니다. 차원이 높아져도 성능이 크게 떨어지지 않아요. 둘째, **메모리 효율적**입니다. 서포트 벡터만 저장하면 되므로 전체 데이터를 기억할 필요가 없습니다. 셋째, **과적합에 강합니다**. 마진을 최대화하는 원리로 인해 일반화 성능이 좋습니다. 넷째, **수학적으로 잘 정립**되어 있어서 이론적 근거가 탄탄합니다.

또한 **이상치에 강하고**, **비선형 관계도 잘 학습**할 수 있으며, **작은 데이터셋에서도 좋은 성능**을 보입니다. 다만 대용량 데이터에서는 계산 비용이 높고, 하이퍼파라미터 튜닝이 중요하다는 단점도 있습니다.
### 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.

나이브 베이즈가 오래된 기법이라고 해서 무시할 수 없는 여러 가지 강력한 장점들이 있습니다.

가장 큰 장점은 **훈련 속도가 매우 빠르다**는 점입니다. 복잡한 최적화 과정 없이 단순히 확률만 계산하면 되므로, 대용량 데이터에서도 실시간으로 학습이 가능합니다. 이는 스트리밍 데이터나 실시간 분류가 필요한 상황에서 큰 장점이 됩니다.

**메모리 효율성**도 뛰어납니다. 모델 파라미터가 매우 적어서 저장 공간을 거의 차지하지 않으며, 모바일 환경이나 임베디드 시스템에서도 쉽게 배포할 수 있습니다.

**작은 데이터셋에서도 잘 동작**합니다. 다른 복잡한 모델들이 과적합에 빠지는 상황에서도 나이브 베이즈는 안정적인 성능을 보여줍니다. 특히 텍스트 분류에서는 여전히 매우 경쟁력 있는 성능을 보입니다.

**해석 가능성**이 뛰어납니다. 각 feature가 클래스에 미치는 영향을 확률로 직접 해석할 수 있어서, 비즈니스 의사결정에 도움이 됩니다. 스팸 필터에서 어떤 단어가 스팸 확률을 높이는지 바로 알 수 있는 것처럼 말이죠.

**이상치에 강하고**, **feature 간 상관관계가 있어도 어느 정도 견딜 수 있으며**, **다중 클래스 분류에서도 자연스럽게 확장**됩니다. 또한 **온라인 학습**이 가능해서 새로운 데이터가 들어올 때마다 점진적으로 모델을 업데이트할 수 있습니다.

특히 **텍스트 분류, 스팸 필터링, 감정 분석** 같은 도메인에서는 여전히 최고 수준의 성능을 보이며, **베이스라인 모델**로서의 역할도 훌륭합니다. 복잡한 모델을 구현하기 전에 나이브 베이즈로 시작해서 성능을 확인해보는 것이 좋은 접근 방식입니다.
### 회귀 / 분류시 알맞은 metric은 무엇일까?

회귀와 분류 문제에서는 각각의 특성에 맞는 서로 다른 metric을 사용해야 합니다.

**회귀 문제**에서는 주로 **RMSE(Root Mean Square Error)**와 **MAE(Mean Absolute Error)**를 사용합니다. RMSE는 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. 반면 MAE는 이상치에 덜 민감해서 이상치가 많은 데이터에서는 더 적절할 수 있습니다. **R²(결정계수)**는 모델이 데이터의 분산을 얼마나 잘 설명하는지를 보여주는 지표로, 0과 1 사이의 값을 가지며 1에 가까울수록 좋습니다.

**분류 문제**에서는 상황에 따라 다양한 metric을 선택해야 합니다. **Accuracy**는 가장 직관적이지만 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. **Precision**은 양성으로 예측한 것 중 실제로 양성인 비율이고, **Recall**은 실제 양성 중 올바르게 찾아낸 비율입니다. **F1-score**는 이 둘의 조화평균으로 균형을 맞춘 지표입니다.

**ROC-AUC**는 다양한 threshold에서의 성능을 종합적으로 평가할 수 있어서 이진 분류에서 널리 사용됩니다. **PR-AUC**는 클래스 불균형이 심할 때 ROC-AUC보다 더 적절한 지표가 될 수 있습니다.

다중 클래스 분류에서는 **Macro/Micro F1-score**를 사용하며, **Confusion Matrix**를 통해 각 클래스별 성능을 자세히 분석할 수 있습니다.

실제로는 **비즈니스 요구사항**을 고려해서 metric을 선택해야 합니다. 의료 진단에서는 Recall이 중요하고, 스팸 필터에서는 Precision이 중요하죠. 또한 **데이터의 특성**과 **클래스 분포**도 고려해야 합니다.
### Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.

Association Rule은 장바구니 분석에서 "A를 사는 사람이 B도 산다"는 규칙을 찾는 기법으로, 세 가지 핵심 지표가 있습니다.

**Support(지지도)**는 전체 거래 중에서 해당 아이템들이 함께 나타나는 비율입니다. Support(A→B) = P(A∩B)로 계산되며, 이 규칙이 얼마나 자주 발생하는지를 보여줍니다. 예를 들어 전체 거래 1000건 중 빵과 우유가 함께 구매된 거래가 100건이라면, Support(빵→우유) = 0.1입니다. 너무 낮으면 의미 없는 규칙이고, 너무 높으면 당연한 규칙이 될 수 있어요.

**Confidence(신뢰도)**는 A를 구매한 사람 중에서 B도 구매한 비율입니다. Confidence(A→B) = P(B|A) = P(A∩B)/P(A)로 계산됩니다. 빵을 산 사람 200명 중 100명이 우유도 샀다면, Confidence(빵→우유) = 0.5입니다. 이는 A를 구매했을 때 B를 구매할 확률을 의미하므로, 추천 시스템에서 중요한 지표가 됩니다.

**Lift(향상도)**는 A와 B가 독립적일 때 대비 얼마나 더 자주 함께 나타나는지를 보여줍니다. Lift(A→B) = P(B|A)/P(B) = Confidence(A→B)/P(B)로 계산됩니다. Lift가 1보다 크면 양의 상관관계, 1이면 독립, 1보다 작으면 음의 상관관계를 의미합니다. 예를 들어 Lift가 2라면, A를 구매했을 때 B를 구매할 확률이 일반적인 경우보다 2배 높다는 뜻입니다.

실제로는 이 세 지표를 모두 고려해서 규칙을 평가합니다. Support는 빈도를, Confidence는 예측 정확도를, Lift는 독립성 대비 향상 정도를 측정하므로, 각각 다른 관점에서 규칙의 유용성을 평가할 수 있습니다.
### 최적화 기법중 Newton's Method와 Gradient Descent 방법에 대해 알고 있나요?

Gradient Descent와 Newton's Method는 모두 함수의 최솟값을 찾는 최적화 기법이지만, 접근 방식과 특성이 다릅니다.

**Gradient Descent**는 가장 기본적인 최적화 방법으로, 함수의 기울기(gradient) 방향으로 조금씩 이동하면서 최솟값을 찾습니다. 수식으로는 θ_{t+1} = θ_t - α∇f(θ_t)로 표현되며, 여기서 α는 학습률(learning rate)입니다. 기울기가 가파른 곳에서는 크게 이동하고, 기울기가 완만한 곳에서는 작게 이동하는 직관적인 방법이에요.

장점은 구현이 간단하고 메모리 효율적이며, 대용량 데이터에도 적용 가능하다는 점입니다. 하지만 학습률 설정이 중요하고, 수렴 속도가 느리며, 골짜기나 평지에서 비효율적으로 움직일 수 있습니다.

**Newton's Method**는 2차 도함수(헤시안 행렬)까지 사용해서 더 정확한 최적화를 수행합니다. 수식으로는 θ_{t+1} = θ_t - H^{-1}∇f(θ_t)로 표현되며, 여기서 H는 헤시안 행렬입니다. 1차 도함수뿐만 아니라 2차 도함수 정보까지 활용해서 곡률을 고려한 더 정확한 방향으로 이동합니다.

장점은 수렴 속도가 매우 빠르고(2차 수렴), 골짜기나 평지에서도 효율적으로 움직인다는 점입니다. 하지만 헤시안 행렬을 계산하고 역행렬을 구해야 하므로 계산 비용이 매우 높고, 메모리 사용량도 많습니다. 또한 헤시안이 양정치가 아닌 경우 문제가 될 수 있어요.

실제로는 **Quasi-Newton 방법들**(BFGS, L-BFGS)이 두 방법의 장점을 결합한 하이브리드 접근법으로 널리 사용됩니다. Newton's Method의 빠른 수렴 속도는 유지하면서도 헤시안을 직접 계산하지 않고 근사하는 방식으로 계산 비용을 줄입니다.
### 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

머신러닝과 통계학은 모두 데이터에서 패턴을 찾는 학문이지만, 접근 방식과 목적에서 차이가 있습니다.

**통계학적 접근**은 **해석 가능성과 추론**에 중점을 둡니다. 가설을 세우고, 모델의 가정을 검증하며, p-value나 신뢰구간을 통해 통계적 유의성을 평가합니다. 모델의 파라미터가 실제 현상을 어떻게 설명하는지에 관심이 많고, 인과관계 추론에 강점이 있습니다. 또한 표본에서 모집단으로의 일반화 가능성을 엄밀하게 다룹니다.

**머신러닝적 접근**은 **예측 성능**에 중점을 둡니다. 복잡한 패턴을 찾아내고, 새로운 데이터에 대한 예측 정확도를 최대화하는 것이 목표입니다. 모델이 블랙박스여도 예측이 정확하면 괜찮다고 보는 경향이 있어요. 또한 대용량 데이터와 고차원 문제에 특화되어 있습니다.

**데이터 크기**에 대한 관점도 다릅니다. 통계학은 전통적으로 작은 표본에서도 유의미한 결론을 도출하는 데 집중했지만, 머신러닝은 빅데이터 환경에서 복잡한 패턴을 학습하는 데 특화되어 있습니다.

**모델 복잡도**에 대한 접근도 다릅니다. 통계학은 간단하고 해석 가능한 모델을 선호하는 반면, 머신러닝은 성능 향상을 위해 복잡한 모델도 주저하지 않습니다.

하지만 최근에는 이 경계가 모호해지고 있습니다. 통계학에서도 예측 성능을 중시하는 경향이 생겼고, 머신러닝에서도 해석 가능성에 대한 관심이 높아지고 있어요. 실제로는 두 접근법을 적절히 조합해서 사용하는 것이 가장 효과적입니다.
### 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?

전통적인 인공신경망은 여러 가지 근본적인 문제점들로 인해 한계에 부딪혔습니다.

가장 큰 문제는 **차원의 저주(Curse of Dimensionality)**였습니다. 입력 차원이 늘어날수록 필요한 파라미터 수가 기하급수적으로 증가하고, 학습에 필요한 데이터도 폭발적으로 늘어났습니다. 이로 인해 고차원 데이터에서는 실용적이지 못했습니다.

**기울기 소실 문제(Vanishing Gradient Problem)**도 심각했습니다. 역전파 과정에서 기울기가 계층을 거치면서 점점 작아져서, 깊은 네트워크에서는 앞쪽 층의 가중치가 거의 업데이트되지 않았습니다. 이로 인해 깊은 네트워크를 학습시키는 것이 거의 불가능했어요.

**과적합 문제**도 만성적이었습니다. 복잡한 네트워크는 훈련 데이터에 너무 잘 맞춰져서 일반화 성능이 떨어졌고, 당시에는 효과적인 정규화 기법이 부족했습니다.

**지역 최솟값(Local Minima)**에 빠지는 문제도 있었습니다. 복잡한 손실 함수에서 전역 최솟값을 찾지 못하고 지역 최솟값에 갇혀서 성능이 제한되었습니다.

**계산 자원의 한계**도 큰 문제였습니다. 당시의 하드웨어로는 복잡한 신경망을 학습시키기에 부족했고, 병렬 처리 기술도 제한적이었습니다.

**초기화 문제**도 있었습니다. 가중치 초기화가 잘못되면 학습이 전혀 진행되지 않거나 매우 느려졌습니다.

이러한 문제들로 인해 전통적인 신경망은 1990년대 후반부터 2000년대 초반까지 "AI 겨울"을 겪게 되었고, 대신 SVM이나 Random Forest 같은 다른 기법들이 주목받게 되었습니다.
### 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?

현재 딥러닝의 혁신은 여러 요소들이 복합적으로 작용한 결과라고 생각합니다.

가장 근본적인 변화는 **데이터와 컴퓨팅 파워의 폭발적 증가**입니다. 인터넷과 모바일 기기의 보급으로 엄청난 양의 데이터가 생성되고, GPU와 클라우드 컴퓨팅의 발전으로 이를 처리할 수 있는 환경이 마련되었습니다. 이는 딥러닝이 필요로 하는 "빅데이터 + 고성능 컴퓨팅" 환경을 제공했습니다.

**알고리즘적 혁신**도 핵심입니다. ReLU 활성화 함수가 기울기 소실 문제를 크게 완화했고, Dropout, Batch Normalization 같은 정규화 기법들이 과적합을 효과적으로 방지했습니다. 또한 Xavier/He 초기화, Adam 옵티마이저 등이 학습 안정성을 크게 향상시켰습니다.

**아키텍처 혁신**도 중요한 역할을 했습니다. CNN은 이미지 처리에서, RNN/LSTM은 시퀀스 데이터에서, Transformer는 자연어 처리에서 각각 혁신을 일으켰습니다. 특히 Attention 메커니즘은 모델이 중요한 부분에 집중할 수 있게 해주었습니다.

**사전 훈련(Pre-training)과 전이 학습(Transfer Learning)**의 개념도 혁신적이었습니다. 대용량 데이터로 사전 훈련한 모델을 특정 태스크에 맞게 미세 조정하는 방식으로, 적은 데이터로도 좋은 성능을 얻을 수 있게 되었습니다.

**오픈소스 생태계**의 발전도 빼놓을 수 없습니다. TensorFlow, PyTorch 같은 프레임워크가 연구자들의 접근성을 높였고, GitHub를 통한 지식 공유가 혁신 속도를 가속화했습니다.

하지만 가장 근본적인 혁신은 **"end-to-end 학습"**이라는 패러다임 변화라고 생각합니다. 기존에는 feature engineering이 핵심이었지만, 이제는 원시 데이터부터 최종 결과까지 모든 과정을 신경망이 자동으로 학습하는 방식으로 바뀌었습니다. 이는 인간의 직관과 경험에 의존하던 부분을 데이터와 알고리즘이 대체한 혁신입니다.
### ROC 커브에 대해 설명해주실 수 있으신가요?

ROC 커브(Receiver Operating Characteristic Curve)는 이진 분류 모델의 성능을 평가하는 중요한 시각화 도구입니다.

ROC 커브는 **True Positive Rate(TPR)**와 **False Positive Rate(FPR)**의 관계를 그래프로 나타낸 것입니다. TPR은 실제 양성 중 올바르게 양성으로 예측한 비율(민감도, Sensitivity)이고, FPR은 실제 음성 중 잘못 양성으로 예측한 비율(1-특이도)입니다.

X축은 FPR, Y축은 TPR로 설정하며, 그래프는 (0,0)에서 (1,1)까지의 곡선으로 나타납니다. 분류 threshold를 변화시키면서 각각의 TPR과 FPR을 계산해서 점들을 연결하면 ROC 커브가 완성됩니다.

**완벽한 분류기**는 (0,1) 지점에 위치하며, 이는 FPR=0, TPR=1, 즉 모든 양성을 올바르게 찾아내면서 음성은 하나도 잘못 분류하지 않는다는 의미입니다.

**무작위 분류기**는 대각선(0,0)에서 (1,1)을 잇는 직선으로 나타나며, 이는 동전 던지기와 같은 수준의 성능을 의미합니다.

**ROC-AUC(Area Under Curve)**는 ROC 커브 아래 면적을 의미하며, 0과 1 사이의 값을 가집니다. 1에 가까울수록 좋은 성능이고, 0.5는 무작위 분류기와 같은 성능을 의미합니다.

ROC 커브의 장점은 **클래스 불균형에 상대적으로 덜 민감**하다는 점입니다. 또한 **threshold에 관계없이 모델의 전반적인 성능**을 평가할 수 있어서, 다양한 threshold에서의 성능을 종합적으로 비교할 수 있습니다.

하지만 클래스 불균형이 매우 심한 경우에는 **PR 커브(Precision-Recall Curve)**가 더 적절할 수 있습니다. ROC 커브는 FPR을 사용하기 때문에 음성 클래스가 매우 많을 때는 실제 성능을 과대평가할 수 있기 때문입니다.

### 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?

100대의 서버 환경에서는 Random Forest가 인공신경망보다 훨씬 유리한 여러 가지 이유가 있습니다.

가장 큰 이유는 **병렬 처리의 효율성**입니다. Random Forest의 각 트리는 독립적으로 학습할 수 있어서, 100대 서버에 트리를 분산시켜서 동시에 학습할 수 있습니다. 이는 학습 시간을 크게 단축시킬 수 있어요. 반면 신경망은 순차적인 역전파 과정 때문에 완전한 병렬화가 어렵습니다.

**메모리 효율성**도 중요한 장점입니다. Random Forest는 각 트리가 상대적으로 작고, 전체 모델을 메모리에 로드할 필요가 없어서 서버당 메모리 사용량이 적습니다. 신경망은 전체 모델을 메모리에 올려야 하므로 메모리 요구사항이 높습니다.

**안정성과 견고성**도 Random Forest의 강점입니다. 일부 서버가 다운되거나 네트워크 문제가 발생해도, 남은 서버들로 학습을 계속할 수 있습니다. 각 트리가 독립적이기 때문에 부분적인 실패가 전체 모델에 치명적이지 않아요.

**하이퍼파라미터 튜닝의 용이성**도 있습니다. Random Forest는 상대적으로 튜닝할 파라미터가 적고, 각 파라미터의 효과가 직관적이어서 분산 환경에서도 쉽게 최적화할 수 있습니다.

**해석 가능성**도 실무에서 중요한 장점입니다. Random Forest는 feature importance를 제공하고, 각 트리의 의사결정 과정을 추적할 수 있어서 비즈니스 의사결정에 도움이 됩니다.

**학습 속도**도 빠릅니다. Random Forest는 각 트리를 독립적으로 학습하므로 전체 학습 시간이 단축되고, 조기 종료나 증분 학습도 쉽게 구현할 수 있습니다.

물론 신경망이 더 좋은 성능을 보일 수 있는 복잡한 패턴이 있다면 신경망을 선택해야 하지만, 대부분의 경우 Random Forest가 분산 환경에서 더 실용적이고 효율적인 선택이 됩니다.
### K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

K-means의 가장 큰 의미론적 단점은 **클러스터 개수를 미리 정해야 한다**는 점입니다. 실제 데이터에서는 몇 개의 클러스터가 있는지 알 수 없는 경우가 대부분인데, K-means는 이를 사전에 지정해야 합니다. 잘못된 K값을 선택하면 의미 없는 클러스터링 결과를 얻을 수 있어요.

**구형(원형) 클러스터만 잘 찾는다**는 한계도 있습니다. K-means는 유클리드 거리를 기반으로 하기 때문에, 타원형이나 비선형적으로 분포된 클러스터는 제대로 찾지 못합니다. 실제 데이터는 다양한 모양의 클러스터를 가질 수 있는데, 이에 대한 가정이 너무 제한적입니다.

**초기값에 민감하다**는 문제도 있습니다. 초기 중심점이 어떻게 설정되느냐에 따라 최종 결과가 크게 달라질 수 있어요. 특히 클러스터가 겹치거나 경계가 모호한 경우에는 더욱 그렇습니다.

**이상치에 취약하다**는 점도 의미론적 단점입니다. 이상치가 있으면 중심점이 그쪽으로 끌려가서 전체 클러스터링 결과가 왜곡될 수 있습니다. 실제 데이터에는 이상치가 흔한데, 이를 처리하는 메커니즘이 부족합니다.

**클러스터 크기가 균등하다고 가정**한다는 한계도 있습니다. 실제로는 클러스터마다 크기가 다를 수 있는데, K-means는 모든 클러스터가 비슷한 크기를 가진다고 가정합니다.

**연속적인 특성만 다룰 수 있다**는 제약도 있습니다. 범주형 데이터나 혼합형 데이터에는 직접 적용하기 어렵고, 전처리가 필요합니다.

이러한 의미론적 한계들 때문에 실제로는 K-means보다는 DBSCAN, Gaussian Mixture Model, 또는 계층적 클러스터링 같은 다른 방법들이 더 적합한 경우가 많습니다.
### L1, L2 정규화에 대해 설명해주세요.

L1과 L2 정규화는 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법으로, 손실 함수에 정규화 항을 추가하는 방식입니다.

**L1 정규화(Lasso)**는 가중치의 절댓값의 합을 손실 함수에 추가합니다. 수식으로는 Loss + λ∑|w_i|로 표현되며, 여기서 λ는 정규화 강도를 조절하는 하이퍼파라미터입니다. L1 정규화의 가장 큰 특징은 **feature selection 효과**가 있다는 점입니다. 불필요한 feature의 가중치를 0으로 만들어서 자동으로 feature selection을 수행합니다. 이는 모델을 단순화시키고 해석 가능성을 높입니다.

**L2 정규화(Ridge)**는 가중치의 제곱합을 손실 함수에 추가합니다. 수식으로는 Loss + λ∑w_i²로 표현됩니다. L2 정규화는 가중치들을 0에 가깝게 만들지만 완전히 0으로 만들지는 않습니다. 모든 feature를 유지하면서 가중치만 줄여서 모델의 복잡도를 제어합니다.

**기하학적 차이**도 있습니다. L1 정규화는 다이아몬드 모양의 제약 영역을 만들어서 축과 만나는 지점(가중치가 0이 되는 지점)에서 최적해를 찾는 경향이 있습니다. 반면 L2 정규화는 원형 제약 영역을 만들어서 모든 가중치를 비슷하게 줄이는 경향이 있습니다.

**계산적 차이**도 있습니다. L1 정규화는 절댓값 함수 때문에 미분이 불연속적이어서 최적화가 복잡하지만, L2 정규화는 미분 가능해서 최적화가 상대적으로 쉽습니다.

**Elastic Net**은 L1과 L2를 결합한 방법으로, 두 정규화의 장점을 모두 활용할 수 있습니다. 특히 feature가 많은 경우에 효과적입니다.

실제로는 데이터의 특성과 목적에 따라 선택해야 합니다. feature selection이 중요하다면 L1을, 모든 feature를 유지하면서 정규화하고 싶다면 L2를, 둘 다 고려하고 싶다면 Elastic Net을 사용하는 것이 좋습니다.

### Cross Validation은 무엇이고 어떻게 해야하나요?

Cross Validation(교차 검증)은 모델의 일반화 성능을 평가하고 과적합을 방지하기 위한 기법으로, 데이터를 여러 번 나누어서 반복적으로 학습과 검증을 수행하는 방법입니다.

가장 기본적인 **K-Fold Cross Validation**은 데이터를 K개의 폴드로 나누고, 각 폴드를 한 번씩 검증 데이터로 사용하면서 나머지 K-1개 폴드로 학습을 수행합니다. 예를 들어 5-Fold CV라면 데이터를 5개로 나누고, 5번의 학습-검증 과정을 거쳐서 5개의 성능 점수를 얻습니다. 최종 성능은 이 5개 점수의 평균과 표준편차로 평가합니다.

**Stratified K-Fold**는 클래스 비율을 유지하면서 폴드를 나누는 방법으로, 분류 문제에서 클래스 불균형이 있을 때 유용합니다. 각 폴드가 원본 데이터와 비슷한 클래스 분포를 가지도록 보장합니다.

**Leave-One-Out Cross Validation(LOOCV)**은 K를 데이터 개수와 같게 설정하는 극단적인 경우로, 각 샘플을 하나씩 검증 데이터로 사용합니다. 계산 비용이 높지만 작은 데이터셋에서는 유용합니다.

**Time Series Cross Validation**은 시계열 데이터에 특화된 방법으로, 시간 순서를 고려해서 과거 데이터로 학습하고 미래 데이터로 검증합니다. 일반적인 K-Fold는 시계열 데이터에 부적절할 수 있어요.

Cross Validation의 장점은 **더 안정적인 성능 평가**가 가능하다는 점입니다. 단일 train-test split보다 훨씬 신뢰할 수 있는 성능 추정을 제공합니다. 또한 **하이퍼파라미터 튜닝**에도 활용할 수 있어서, 다양한 파라미터 조합을 체계적으로 평가할 수 있습니다.

하지만 **계산 비용이 높다**는 단점이 있고, **데이터가 적을 때는 각 폴드의 크기가 작아져서** 성능 평가가 불안정할 수 있습니다. 또한 **시계열 데이터나 공간적 상관관계가 있는 데이터**에서는 특별한 주의가 필요합니다.

실제로는 데이터의 크기와 특성, 그리고 계산 자원을 고려해서 적절한 Cross Validation 방법을 선택해야 합니다.

### XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

XGBoost(eXtreme Gradient Boosting)는 그래디언트 부스팅의 확장된 버전으로, 캐글에서 압도적인 인기를 얻고 있는 모델입니다.

XGBoost가 캐글에서 유명한 이유는 여러 가지가 있습니다. 가장 큰 이유는 **뛰어난 성능**입니다. 대부분의 테이블 데이터에서 최고 수준의 성능을 보여주며, 특히 구조화된 데이터에서 딥러닝보다도 더 좋은 결과를 내는 경우가 많습니다.

**속도와 효율성**도 큰 장점입니다. 병렬 처리와 분산 컴퓨팅을 지원해서 대용량 데이터도 빠르게 처리할 수 있고, 메모리 사용량도 최적화되어 있습니다. 캐글의 대용량 데이터셋에서도 실용적으로 사용할 수 있어요.

**과적합 방지 기능**이 뛰어납니다. 정규화 항, 조기 종료, 드롭아웃 등 다양한 과적합 방지 기법이 내장되어 있어서, 복잡한 모델임에도 불구하고 일반화 성능이 좋습니다.

**유연성**도 큰 장점입니다. 회귀, 분류, 랭킹 등 다양한 태스크에 적용할 수 있고, 커스텀 손실 함수와 평가 지표도 쉽게 정의할 수 있습니다. 또한 결측값 처리, 범주형 변수 처리 등도 자동으로 해줍니다.

**하이퍼파라미터 튜닝**이 상대적으로 쉽습니다. 많은 파라미터가 있지만 각각의 효과가 직관적이고, 그리드 서치나 베이지안 최적화로 쉽게 튜닝할 수 있어요.

**해석 가능성**도 제공합니다. Feature importance를 통해 어떤 변수가 중요한지 알 수 있고, SHAP 같은 도구와 결합하면 더 자세한 해석도 가능합니다.

**안정성**도 뛰어납니다. 다양한 데이터 타입과 분포에서 일관되게 좋은 성능을 보여주며, 이상치에도 상대적으로 강합니다.

캐글에서는 "XGBoost로 시작해서 성능이 좋으면 그대로 사용하고, 더 개선이 필요하면 딥러닝이나 다른 기법을 시도한다"는 전략이 일반적입니다. 실제로 많은 캐글 우승 솔루션에서 XGBoost가 핵심 모델로 사용되었습니다.

### 앙상블 방법엔 어떤 것들이 있나요?

앙상블 방법은 여러 모델의 예측을 결합해서 더 좋은 성능을 얻는 기법으로, 크게 세 가지 접근 방식이 있습니다.

**Bagging(Bootstrap Aggregating)**은 같은 알고리즘을 여러 번 학습시키되, 각각 다른 데이터 샘플을 사용하는 방법입니다. 대표적인 예가 **Random Forest**로, 여러 의사결정나무를 학습시키되 각각 다른 feature와 데이터 샘플을 사용합니다. Bagging은 분산을 줄여서 과적합을 방지하는 효과가 있어요.

**Boosting**은 순차적으로 모델을 학습시키되, 이전 모델이 잘못 예측한 샘플에 더 집중해서 다음 모델을 학습시키는 방법입니다. **AdaBoost**, **Gradient Boosting**, **XGBoost** 등이 대표적인 예시입니다. Boosting은 편향을 줄여서 전체적인 성능을 향상시키는 효과가 있습니다.

**Stacking**은 여러 다른 알고리즘의 예측을 메타 모델(meta-model)이 학습해서 최종 예측을 만드는 방법입니다. 예를 들어 Random Forest, SVM, Neural Network의 예측을 Logistic Regression이 학습해서 최종 결과를 만드는 방식이에요. Stacking은 모델의 다양성을 활용해서 더 정확한 예측을 할 수 있습니다.

**Voting**은 여러 모델의 예측을 단순히 투표나 평균으로 결합하는 방법입니다. **Hard Voting**은 다수결로 결정하고, **Soft Voting**은 확률의 평균을 사용합니다.

**Blending**은 Stacking과 비슷하지만, 메타 모델을 학습할 때 별도의 검증 데이터를 사용하는 방법입니다. Stacking보다 과적합 위험이 적지만 데이터 사용 효율성이 떨어집니다.

각 방법의 특징을 정리하면, Bagging은 과적합이 심한 모델에 효과적이고, Boosting은 약한 학습기에 효과적이며, Stacking은 서로 다른 특성을 가진 모델들을 결합할 때 효과적입니다. 실제로는 여러 방법을 조합해서 사용하는 경우가 많으며, 캐글 같은 대회에서는 앙상블이 우승의 핵심 전략이 되는 경우가 많습니다.

### feature vector란 무엇일까요?

Feature vector는 머신러닝에서 데이터의 특성을 수치로 표현한 벡터입니다. 쉽게 말해서 하나의 데이터 포인트를 숫자들의 리스트로 나타낸 것이에요.

예를 들어 사람의 정보를 feature vector로 표현한다면, [나이, 키, 몸무게, 소득] 같은 형태가 될 수 있습니다. 이때 [25, 170, 65, 3000]은 25세, 170cm, 65kg, 월소득 3000만원인 사람을 나타내는 feature vector입니다.

Feature vector의 각 원소는 **feature** 또는 **attribute**라고 하며, 이는 데이터의 한 가지 특성을 나타냅니다. 벡터의 차원은 feature의 개수와 같아서, 위 예시에서는 4차원 벡터가 됩니다.

**Feature Engineering**은 원시 데이터를 의미 있는 feature vector로 변환하는 과정입니다. 텍스트 데이터를 TF-IDF 벡터로 변환하거나, 이미지를 픽셀 값들의 벡터로 변환하는 것처럼 말이죠.

**Feature Selection**은 중요한 feature만 선택해서 차원을 줄이는 과정이고, **Feature Scaling**은 feature들의 스케일을 맞춰주는 전처리 과정입니다.

Feature vector는 머신러닝 알고리즘의 입력으로 사용되며, 모델은 이 벡터들을 학습해서 패턴을 찾아냅니다. 따라서 feature vector의 품질이 모델의 성능에 직접적인 영향을 미칩니다.

최근에는 딥러닝의 발전으로 **Representation Learning**이 주목받고 있는데, 이는 모델이 자동으로 좋은 feature vector를 학습하는 방법입니다. 하지만 여전히 도메인 지식을 활용한 feature engineering이 중요한 역할을 합니다.


### 좋은 모델의 정의는 무엇일까요?

좋은 모델의 정의는 상황과 목적에 따라 달라지지만, 일반적으로 고려해야 할 몇 가지 핵심 요소들이 있습니다.

가장 기본적인 요소는 **예측 정확도**입니다. 모델이 새로운 데이터에 대해 얼마나 정확하게 예측하는지가 가장 중요한 평가 기준이에요. 하지만 정확도만으로는 부족하고, **일반화 성능**이 중요합니다. 훈련 데이터에만 잘 맞는 것이 아니라, 보지 못한 새로운 데이터에서도 좋은 성능을 보여야 합니다.

**해석 가능성**도 현대 머신러닝에서 점점 더 중요해지고 있습니다. 특히 의료, 금융, 법률 같은 도메인에서는 모델이 왜 그런 예측을 했는지 설명할 수 있어야 합니다. 비즈니스 의사결정에 활용하려면 해석 가능성이 필수적이에요.

**안정성과 견고성**도 중요한 요소입니다. 이상치나 노이즈가 있어도 일관된 성능을 보여야 하고, 데이터 분포가 조금 바뀌어도 크게 성능이 떨어지지 않아야 합니다.

**효율성**도 고려해야 합니다. 학습 시간, 예측 시간, 메모리 사용량 등이 실용적인 수준이어야 합니다. 정확도가 아무리 높아도 실시간으로 예측할 수 없다면 실무에서 사용하기 어려워요.

**유지보수성**도 중요합니다. 모델을 업데이트하거나 수정하기 쉬워야 하고, 새로운 데이터가 들어와도 쉽게 재학습할 수 있어야 합니다.

**비용 효율성**도 고려해야 합니다. 모델 개발과 운영에 드는 비용 대비 얻는 이익이 합리적이어야 합니다.

하지만 가장 중요한 것은 **비즈니스 목표 달성**입니다. 기술적으로 완벽한 모델이라도 비즈니스 문제를 해결하지 못한다면 좋은 모델이 아닙니다. 따라서 도메인 전문가와의 협업을 통해 실제 문제를 정확히 정의하고, 그에 맞는 적절한 모델을 선택하는 것이 핵심입니다.

### 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?

일반적으로 50개의 작은 의사결정나무가 하나의 큰 의사결정나무보다 더 좋은 성능을 보입니다. 이는 **앙상블 효과** 때문입니다.

가장 큰 이유는 **분산 감소(Variance Reduction)**입니다. 하나의 큰 나무는 훈련 데이터에 과적합되기 쉽고, 작은 변화에도 민감하게 반응합니다. 하지만 50개의 작은 나무들의 평균을 내면 이런 변동성이 상쇄되어 더 안정적인 예측을 할 수 있어요.

**편향과 분산의 균형**도 더 좋습니다. 하나의 큰 나무는 편향은 낮지만 분산이 높은 반면, 50개의 작은 나무는 각각은 편향이 높을 수 있지만 분산이 낮아서 전체적으로는 더 좋은 성능을 보입니다.

**다양성(Diversity)**도 중요한 요소입니다. 50개의 나무가 서로 다른 패턴을 학습할 수 있어서, 하나의 나무가 놓친 패턴을 다른 나무가 보완할 수 있습니다. 이는 Random Forest에서 각 나무가 다른 feature와 데이터 샘플을 사용하는 것과 같은 원리입니다.

**견고성(Robustness)**도 뛰어납니다. 일부 나무가 잘못된 예측을 해도 다른 나무들이 이를 보완할 수 있어서, 이상치나 노이즈에 더 강합니다.

**병렬 처리**도 가능합니다. 50개의 나무를 동시에 학습할 수 있어서, 하나의 큰 나무를 학습하는 것보다 시간이 더 오래 걸리지 않을 수 있습니다.

하지만 **해석 가능성**은 떨어집니다. 하나의 큰 나무는 의사결정 과정을 쉽게 추적할 수 있지만, 50개 나무의 결합은 복잡해서 해석하기 어려워요.

**메모리 사용량**도 더 많을 수 있습니다. 50개의 나무를 모두 저장해야 하므로 하나의 큰 나무보다 메모리를 더 사용합니다.

전반적으로는 성능 면에서 50개의 작은 나무가 더 우수하지만, 해석 가능성이 중요한 상황이라면 하나의 큰 나무를 선택할 수도 있습니다. 실제로는 Random Forest나 Gradient Boosting 같은 앙상블 방법이 이런 아이디어를 체계적으로 구현한 것입니다.
### 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?

스팸 필터에서 로지스틱 회귀가 널리 사용되는 이유는 여러 가지 실용적인 장점들 때문입니다.

가장 큰 이유는 **해석 가능성**입니다. 로지스틱 회귀는 각 feature(단어)가 스팸 확률에 미치는 영향을 계수로 직접 해석할 수 있어요. 예를 들어 "무료"라는 단어의 계수가 2.5라면, 이 단어가 있을 때 스팸 확률이 2.5배 증가한다는 의미입니다. 이는 사용자에게 왜 이메일이 스팸으로 분류되었는지 설명할 수 있게 해줍니다.

**확률 출력**도 중요한 장점입니다. 로지스틱 회귀는 0과 1 사이의 확률값을 출력하므로, 스팸 확률이 0.8인 이메일과 0.95인 이메일을 구분할 수 있습니다. 이는 사용자가 임계값을 조정해서 민감도를 조절할 수 있게 해줍니다.

**학습 속도**가 빠릅니다. 텍스트 데이터는 보통 수십만 개의 단어로 구성된 고차원 벡터인데, 로지스틱 회귀는 이런 고차원 데이터도 빠르게 학습할 수 있습니다. 실시간으로 새로운 스팸 패턴을 학습해야 하는 환경에서 중요한 장점이에요.

**메모리 효율성**도 뛰어납니다. 모델 파라미터가 단순한 계수들이므로 저장 공간이 적고, 예측 시에도 빠르게 계산할 수 있습니다.

**안정성**도 좋습니다. 이상치나 노이즈에 상대적으로 강하고, 하이퍼파라미터 튜닝이 간단해서 안정적인 성능을 보장합니다.

**확장성**도 뛰어납니다. 새로운 feature를 쉽게 추가할 수 있고, 온라인 학습도 가능해서 새로운 스팸 패턴이 나타나면 점진적으로 모델을 업데이트할 수 있습니다.

**L1 정규화**와 잘 맞습니다. L1 정규화를 사용하면 불필요한 단어들의 계수를 0으로 만들어서 자동으로 feature selection이 되고, 모델이 더 간단해집니다.

물론 딥러닝 모델이 더 복잡한 패턴을 학습할 수 있지만, 스팸 필터링에서는 **해석 가능성과 실시간 처리**가 더 중요하기 때문에 로지스틱 회귀가 여전히 널리 사용됩니다. 또한 **베이스라인 모델**로서의 역할도 훌륭해서, 더 복잡한 모델을 구현하기 전에 로지스틱 회귀로 시작하는 것이 일반적입니다.
### OLS(ordinary least squre) regression의 공식은 무엇인가요?

OLS(Ordinary Least Squares) 회귀는 선형 회귀의 가장 기본적인 형태로, 잔차의 제곱합을 최소화하는 방법입니다.

**기본 모델**은 y = Xβ + ε 형태로 표현되며, 여기서 y는 종속변수 벡터, X는 독립변수 행렬, β는 회귀계수 벡터, ε는 오차 벡터입니다.

**목적 함수**는 잔차의 제곱합을 최소화하는 것입니다:

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 = \min_{\beta} ||y - X\beta||^2
$$

**정규 방정식(Normal Equation)**을 통해 해를 구할 수 있습니다:

$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$

이 공식은 X^T X가 역행렬을 가질 때 유효하며, 이는 X가 full rank일 때입니다.

**기하학적 해석**으로는, X의 열공간에서 y에 가장 가까운 점을 찾는 것으로 볼 수 있습니다. 이는 y를 X의 열공간에 정사영(projection)하는 것과 같습니다.

**가정사항**으로는 선형성, 독립성, 등분산성, 정규성이 있으며, 이 가정들이 만족될 때 OLS 추정량은 BLUE(Best Linear Unbiased Estimator)가 됩니다.

**잔차**는 e = y - Xβ̂로 계산되며, 잔차의 제곱합(RSS)은:

$$
RSS = e^T e = (y - X\hat{\beta})^T (y - X\hat{\beta})
$$

**결정계수(R²)**는:

$$
R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

여기서 TSS는 총 제곱합(Total Sum of Squares)입니다.

OLS는 수학적으로 우아하고 해석이 용이하지만, 다중공선성이나 이상치에 민감하다는 한계가 있습니다. 현대에는 Ridge, Lasso 같은 정규화된 회귀 방법들이 더 널리 사용되지만, OLS는 여전히 회귀 분석의 기초가 되는 중요한 방법입니다.