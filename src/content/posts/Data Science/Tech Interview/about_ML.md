---
title: "ML 관련 인터뷰 준비"
date: "2025-09-08"
excerpt: ""
category: "Data Science"
tags: ["Interview"]
---

참고자료
- 1: [ai 테크 인터뷰](https://github.com/boost-devs/ai-tech-interview?tab=readme-ov-file)

---

# 1. 알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)

regression task -> RMSE, MAE 등이 있겠고 
classification task -> recall, precision, ROC-AUC 등이 있겠다.


먼저 regression task의 경우, RMSE와 MAE가 가장 많이 사용됩니다. RMSE는 Root Mean Square Error로, 예측값과 실제값의 차이를 제곱해서 평균을 구한 후 제곱근을 취한 값입니다. 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. MAE는 Mean Absolute Error로, 절댓값 차이의 평균을 구하는데, 이상치에 덜 민감합니다. 그래서 이상치가 많은 데이터에서는 MAE가 더 적절할 수 있어요.

classification task에서는 상황에 따라 다양한 metric을 사용합니다. 가장 기본적인 accuracy는 전체 예측 중 맞춘 비율인데, 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. 

precision은 양성으로 예측한 것 중 실제로 양성인 비율이고, recall은 실제 양성 중 올바르게 찾아낸 비율입니다. precision이 중요한 경우는 스팸 메일 분류처럼 false positive 비용이 클 때이고, recall이 중요한 경우는 의료 진단처럼 false negative 비용이 클 때입니다. 

F1-score는 precision과 recall의 조화평균으로, 두 지표를 균형있게 고려하고 싶을 때 사용해요. ROC-AUC는 ROC 곡선 아래 면적을 의미하는데, 다양한 threshold에서의 모델 성능을 종합적으로 평가할 수 있어서 이진 분류에서 많이 사용됩니다.

실제 프로젝트에서는 비즈니스 요구사항과 데이터 특성을 고려해서 적절한 metric을 선택하는 것이 중요합니다.

---

# 2. 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?

과적합을 피하기 위해서. lasso, ridge처럼 모델의 가중치를 loss에 포함시켜서 과적합 되지 않도록 유도한다.
tree계열에서도 마찬가지로 정규화를 사용함.

정규화는 주로 과적합을 방지하기 위해 사용합니다. 모델이 훈련 데이터에만 너무 잘 맞춰져서 새로운 데이터에 대한 일반화 성능이 떨어지는 것을 막기 위해서죠.

정규화 방법으로는 크게 두 가지가 있습니다. 첫 번째는 L1 정규화, 즉 Lasso입니다. 이는 가중치의 절댓값의 합을 loss function에 추가하는 방식으로, 불필요한 feature들을 0으로 만들어서 자동으로 feature selection 효과를 가져옵니다. 

두 번째는 L2 정규화, 즉 Ridge입니다. 이는 가중치의 제곱합을 loss function에 추가하는 방식으로, 가중치들을 0에 가깝게 만들어서 모델을 단순화시킵니다. Lasso와 달리 feature selection은 하지 않고 모든 feature를 유지하면서 가중치만 줄여줍니다.

Elastic Net은 L1과 L2를 결합한 방식이고, Dropout은 딥러닝에서 랜덤하게 일부 뉴런을 비활성화시키는 방법입니다.

Tree 계열 모델에서도 정규화를 사용하는데, 예를 들어 XGBoost에서는 max_depth, min_child_weight, gamma 등의 파라미터로 모델 복잡도를 제어하고, Random Forest에서는 max_depth나 min_samples_split 같은 파라미터로 과적합을 방지합니다.

실제로는 데이터의 크기, feature의 개수, 모델의 복잡도 등을 고려해서 적절한 정규화 방법을 선택하는 것이 중요합니다.

---

# 3. Local Minima와 Global Minimum에 대해 설명해주세요.

지역적 최솟값과 전역적 최솟값을 의미하며, 최솟값이 부분적으로 최소인지 전체에서 최소인지를 구분.
global minima는 local minima 중 가장 작은 값.


네, Local Minima와 Global Minimum은 최적화 문제에서 중요한 개념입니다.

Local Minima는 지역적 최솟값으로, 특정 구간에서 가장 작은 값을 의미합니다. 즉, 그 점 주변의 작은 영역에서는 더 작은 값이 없지만, 전체 함수를 봤을 때는 더 작은 값이 존재할 수 있어요.

Global Minimum은 전역적 최솟값으로, 전체 함수 영역에서 가장 작은 값을 의미합니다. 모든 가능한 점들 중에서 가장 작은 값을 가지는 지점이죠.

간단한 예로 설명하면, 산맥을 생각해보시면 됩니다. 여러 개의 골짜기가 있는데, 각 골짜기 바닥이 Local Minima이고, 그 중에서 가장 낮은 골짜기가 Global Minimum입니다.

머신러닝에서 이 문제가 중요한 이유는, Gradient Descent 같은 최적화 알고리즘이 Local Minima에 빠져서 Global Minimum을 찾지 못할 수 있기 때문입니다. 특히 딥러닝에서는 수많은 Local Minima가 존재하는데, 다행히도 대부분의 Local Minima들이 비슷한 성능을 보인다는 연구 결과도 있습니다.

이를 해결하기 위해 다양한 기법들을 사용하는데, momentum, learning rate scheduling, 여러 초기값으로 시작하기, 또는 simulated annealing 같은 방법들이 있습니다.

---

# 4. 차원의 저주에 대해 설명해주세요.

차원이 많이 질수록 sparse해진다는 것.

차원의 저주는 차원이 증가할수록 발생하는 여러 문제들을 통칭하는 개념입니다.

가장 핵심적인 문제는 **데이터의 희소성(sparsity)**입니다. 차원이 늘어날수록 같은 데이터 개수로는 공간을 채우기 어려워져요. 예를 들어, 1차원에서는 10개의 점으로 선을 채울 수 있지만, 10차원에서는 같은 10개의 점으로는 공간의 극히 일부분만 커버하게 됩니다.

이로 인해 몇 가지 구체적인 문제들이 발생합니다. 첫째, **근접성의 의미가 사라집니다**. 고차원에서는 모든 점들이 거의 비슷한 거리에 있게 되어서, "가까운" 점과 "먼" 점을 구분하기 어려워져요.

둘째, **과적합이 쉽게 발생**합니다. 차원이 많을수록 모델이 복잡해지고, 같은 데이터 개수로는 충분한 학습이 어려워집니다.

셋째, **계산 복잡도가 기하급수적으로 증가**합니다. 차원이 하나씩 늘어날 때마다 필요한 계산량이 폭발적으로 증가해요.

실제로는 차원 축소 기법들로 이 문제를 해결합니다. PCA, t-SNE, UMAP 같은 방법들로 중요한 정보는 유지하면서 차원을 줄이거나, feature selection을 통해 불필요한 차원을 제거하는 방식으로 접근합니다.


---

# 5. dimension reduction기법으로 보통 어떤 것들이 있나요?

차원 축소 기법은 크게 선형과 비선형 방법으로 나뉩니다. 

가장 기본적이고 널리 사용되는 방법은 **PCA(Principal Component Analysis)**입니다. PCA는 데이터의 분산이 가장 큰 방향으로 새로운 축을 찾아 차원을 축소하는 선형 방법으로, 주성분을 통해 원본 데이터의 정보를 최대한 보존하면서도 해석이 용이하고 계산이 빠릅니다. 주로 이미지 압축, 노이즈 제거, 시각화 등에 사용됩니다.

선형 방법 중에는 **LDA(Linear Discriminant Analysis)**도 중요한데, 이는 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 지도학습 방법으로 분류 성능 향상에 특화되어 있습니다. 또한 **Factor Analysis**는 관찰된 변수들을 잠재 변수로 설명하는 방법으로 심리학이나 사회과학 분야에서 주로 사용됩니다.

비선형 방법으로는 **t-SNE**가 가장 유명합니다. t-SNE는 고차원 데이터를 2D나 3D로 시각화하는 데 특화되어 있으며, 지역적 구조를 잘 보존하기 때문에 클러스터링 결과 시각화에 매우 효과적입니다. 다만 전역적 구조는 보존하지 못하고 하이퍼파라미터에 민감하다는 단점이 있습니다.

**UMAP**은 t-SNE의 단점을 보완한 방법으로, t-SNE보다 빠르면서도 전역적 구조를 어느 정도 보존합니다. 지역적과 전역적 구조의 균형을 잘 맞춰서 대용량 데이터에도 적용 가능합니다.

**Autoencoder**는 신경망 기반의 차원 축소 방법으로, 인코더-디코더 구조를 통해 데이터를 압축하고 복원합니다. 비선형 관계를 잘 학습할 수 있어서 딥러닝과 결합하여 사용되는 경우가 많습니다.

기타 방법들로는 **Isomap**이 있는데, 이는 지오데식 거리를 사용하는 매니폴드 학습의 대표적인 방법입니다. **LLE(Locally Linear Embedding)**는 지역적으로 선형인 관계를 가정하는 방법으로 작은 데이터셋에서 효과적입니다.

실제로 어떤 방법을 선택할지는 목적에 따라 다릅니다. 시각화가 목적이라면 t-SNE나 UMAP을, 전처리가 목적이라면 PCA나 Autoencoder를, 분류 성능 향상이 목적이라면 LDA를 사용하는 것이 좋습니다. 대용량 데이터를 다룬다면 UMAP이나 PCA를, 해석 가능성이 중요하다면 PCA나 Factor Analysis를 선택하는 것이 적절합니다.


---

# 6. PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?

PCA가 이 세 가지 역할을 모두 할 수 있는 이유는 PCA의 핵심 동작 원리에 있습니다. PCA는 데이터의 분산이 가장 큰 방향, 즉 주성분(principal component)을 찾아서 새로운 좌표계를 만드는 기법인데, 이 과정에서 자연스럽게 세 가지 효과가 나타납니다.

먼저 차원 축소 측면에서는, PCA가 원본 데이터의 분산을 가장 잘 설명하는 몇 개의 주성분만 선택하기 때문입니다. 예를 들어 100차원 데이터에서 분산의 95%를 설명하는 10개의 주성분만 선택하면, 100차원을 10차원으로 줄이면서도 중요한 정보는 대부분 보존할 수 있어요.

데이터 압축 측면에서는, 주성분을 통해 원본 데이터를 재구성할 수 있기 때문입니다. 원본 데이터를 주성분들의 선형결합으로 표현하고, 다시 원래 공간으로 복원할 수 있어요. 물론 일부 정보 손실은 있지만, 압축률에 비해 품질 손실이 적어서 이미지 압축 등에 활용됩니다.

노이즈 제거 측면에서는, 노이즈가 보통 작은 분산을 가지기 때문입니다. PCA는 분산이 큰 방향, 즉 신호가 강한 방향을 우선적으로 선택하고, 분산이 작은 방향, 즉 노이즈가 많은 방향은 제거하게 됩니다. 따라서 주성분만 사용해서 데이터를 재구성하면 자연스럽게 노이즈가 제거된 깨끗한 데이터를 얻을 수 있어요.

이 세 가지 효과는 모두 PCA가 "분산이 큰 방향을 우선적으로 선택한다"는 하나의 원리에서 나오는 결과입니다. 그래서 PCA는 단순한 차원 축소 도구가 아니라, 데이터의 본질적인 구조를 찾아내는 강력한 기법이라고 할 수 있습니다.






### LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?

이 세 기법들은 모두 행렬 분해를 기반으로 하는 차원 축소 기법들이지만, 각각 다른 목적과 특성을 가지고 있습니다.

먼저 **SVD(Singular Value Decomposition)**는 가장 기본이 되는 행렬 분해 기법입니다. 임의의 행렬을 세 개의 행렬의 곱으로 분해하는데, A = UΣV^T 형태로 나타납니다. 여기서 U와 V는 직교행렬이고, Σ는 대각행렬입니다. SVD는 선형대수학의 핵심 도구로, 데이터의 내재적 구조를 찾아내는 데 사용됩니다.

**LSA(Latent Semantic Analysis)**는 SVD를 텍스트 마이닝에 특화해서 적용한 기법입니다. 문서-단어 행렬을 SVD로 분해해서 잠재 의미 공간을 찾아내는 방법이에요. 예를 들어 "자동차"와 "차량"이라는 서로 다른 단어가 같은 문서에서 자주 나타나면, LSA는 이들이 비슷한 의미를 가진다는 것을 학습할 수 있습니다. 이렇게 해서 단어의 동의어나 유의어 관계를 자동으로 찾아내고, 문서의 주제를 파악하는 데 활용됩니다.

**LDA(Linear Discriminant Analysis)**는 앞의 두 기법과는 다른 목적을 가진 지도학습 기법입니다. 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 방향으로 차원을 축소하는 방법이에요. 분류 성능 향상에 특화되어 있어서, 같은 차원 축소를 하더라도 분류에 유리한 방향으로 축을 선택합니다.

이 세 기법의 관계를 정리하면, SVD는 수학적 기반이 되는 행렬 분해 기법이고, LSA는 SVD를 텍스트 분석에 적용한 비지도학습 방법이며, LDA는 분류 성능 향상을 위한 지도학습 방법입니다. 모두 차원 축소라는 공통점이 있지만, LSA는 의미적 유사성을 찾는 데, LDA는 분류 성능을 높이는 데 초점을 맞춘다는 차이점이 있습니다.

실제로는 텍스트 분석에서는 LSA를, 분류 문제에서는 LDA를, 그리고 일반적인 행렬 분해나 노이즈 제거에는 SVD를 직접 사용하는 경우가 많습니다.

---

# 8. Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?

고등학생에게 Markov Chain을 설명할 때는 일상생활의 구체적인 예시를 사용하는 것이 가장 효과적입니다. 가장 좋은 예시는 **날씨 예측**입니다.

"오늘 비가 오면 내일도 비가 올 확률이 60%, 맑을 확률이 40%이고, 오늘 맑으면 내일 맑을 확률이 70%, 비가 올 확률이 30%다"라고 설명해보세요. 이때 중요한 점은 **과거의 모든 날씨 기록이 아니라 오늘의 날씨만이 내일 날씨를 결정한다**는 것입니다.

이것이 바로 Markov Chain의 핵심인 "현재 상태가 다음 상태를 결정한다"는 Markov 성질입니다. 과거의 모든 정보를 기억할 필요 없이, 현재 상태만 알면 미래를 예측할 수 있다는 것이죠.

다른 좋은 예시로는 **게임의 이동**이 있습니다. 보드게임에서 현재 위치에서만 다음 위치가 결정되고, 어떻게 그 위치에 도달했는지는 중요하지 않다는 점을 설명하면 됩니다. 이런 식으로 일상생활의 친숙한 예시를 통해 Markov Chain의 개념을 자연스럽게 이해시킬 수 있습니다.

---

# 9. 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?

텍스트에서 주제를 추출하는 작업은 **토픽 모델링(Topic Modeling)**이라고 하며, 여러 단계의 체계적인 접근이 필요합니다.

먼저 **데이터 전처리** 단계에서 텍스트를 정제합니다. 불용어(stopwords) 제거, 어간 추출(stemming), 소문자 변환 등을 통해 노이즈를 줄이고, 토큰화를 통해 단어 단위로 분리합니다. 이때 특수문자나 숫자 처리도 고려해야 합니다.

다음으로 **문서-단어 행렬(Document-Term Matrix)**을 생성합니다. TF-IDF나 단순 빈도 기반으로 각 문서에서 단어의 중요도를 계산하여 수치화합니다.

주제 추출의 핵심 방법으로는 **LDA(Latent Dirichlet Allocation)**가 가장 널리 사용됩니다. LDA는 각 문서가 여러 주제의 혼합으로 구성되어 있고, 각 주제는 단어들의 확률 분포로 표현된다고 가정합니다. 이를 통해 문서의 주제 분포와 주제별 단어 분포를 동시에 학습합니다.

대안으로는 **LSA(Latent Semantic Analysis)**나 **NMF(Non-negative Matrix Factorization)**도 사용할 수 있습니다. LSA는 SVD를 활용한 선형 방법이고, NMF는 음수가 아닌 행렬 분해를 통해 주제를 찾는 방법입니다.

최근에는 **BERT 기반의 토픽 모델링**도 주목받고 있습니다. 사전 훈련된 언어 모델을 활용하여 더 정교한 의미적 유사성을 반영한 주제 추출이 가능합니다.

실제 적용 시에는 주제 개수 설정, 하이퍼파라미터 튜닝, 결과 해석의 어려움 등을 고려해야 하며, 도메인 지식과 함께 결과를 검증하는 것이 중요합니다.

---

# 10. SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?

SVM이 차원을 확장시키는 이유는 **선형 분리가 불가능한 데이터를 고차원 공간에서 선형 분리가 가능하도록 만들기 위해서**입니다. 이를 **커널 트릭(Kernel Trick)**이라고 합니다.

예를 들어, 2차원에서 원형으로 분포된 데이터는 직선으로 분리할 수 없습니다. 하지만 3차원으로 올려서 z = x² + y² 같은 변환을 하면, 3차원에서 평면으로 분리할 수 있게 됩니다. SVM은 이런 아이디어를 극한까지 활용해서, 데이터를 무한 차원의 공간으로 보내서도 선형 분리가 가능하도록 만듭니다.

핵심은 실제로 고차원 공간으로 변환하지 않고도, 커널 함수를 통해 고차원에서의 내적을 계산할 수 있다는 점입니다. RBF 커널, 다항식 커널 등이 대표적인 예시입니다.

SVM이 좋은 이유는 여러 가지가 있습니다. 첫째, **고차원 데이터에서도 효과적**입니다. 차원이 높아져도 성능이 크게 떨어지지 않아요. 둘째, **메모리 효율적**입니다. 서포트 벡터만 저장하면 되므로 전체 데이터를 기억할 필요가 없습니다. 셋째, **과적합에 강합니다**. 마진을 최대화하는 원리로 인해 일반화 성능이 좋습니다. 넷째, **수학적으로 잘 정립**되어 있어서 이론적 근거가 탄탄합니다.

또한 **이상치에 강하고**, **비선형 관계도 잘 학습**할 수 있으며, **작은 데이터셋에서도 좋은 성능**을 보입니다. 다만 대용량 데이터에서는 계산 비용이 높고, 하이퍼파라미터 튜닝이 중요하다는 단점도 있습니다.

---

# 11. 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.

나이브 베이즈가 오래된 기법이라고 해서 무시할 수 없는 여러 가지 강력한 장점들이 있습니다.

가장 큰 장점은 **훈련 속도가 매우 빠르다**는 점입니다. 복잡한 최적화 과정 없이 단순히 확률만 계산하면 되므로, 대용량 데이터에서도 실시간으로 학습이 가능합니다. 이는 스트리밍 데이터나 실시간 분류가 필요한 상황에서 큰 장점이 됩니다.

**메모리 효율성**도 뛰어납니다. 모델 파라미터가 매우 적어서 저장 공간을 거의 차지하지 않으며, 모바일 환경이나 임베디드 시스템에서도 쉽게 배포할 수 있습니다.

**작은 데이터셋에서도 잘 동작**합니다. 다른 복잡한 모델들이 과적합에 빠지는 상황에서도 나이브 베이즈는 안정적인 성능을 보여줍니다. 특히 텍스트 분류에서는 여전히 매우 경쟁력 있는 성능을 보입니다.

**해석 가능성**이 뛰어납니다. 각 feature가 클래스에 미치는 영향을 확률로 직접 해석할 수 있어서, 비즈니스 의사결정에 도움이 됩니다. 스팸 필터에서 어떤 단어가 스팸 확률을 높이는지 바로 알 수 있는 것처럼 말이죠.

**이상치에 강하고**, **feature 간 상관관계가 있어도 어느 정도 견딜 수 있으며**, **다중 클래스 분류에서도 자연스럽게 확장**됩니다. 또한 **온라인 학습**이 가능해서 새로운 데이터가 들어올 때마다 점진적으로 모델을 업데이트할 수 있습니다.

특히 **텍스트 분류, 스팸 필터링, 감정 분석** 같은 도메인에서는 여전히 최고 수준의 성능을 보이며, **베이스라인 모델**로서의 역할도 훌륭합니다. 복잡한 모델을 구현하기 전에 나이브 베이즈로 시작해서 성능을 확인해보는 것이 좋은 접근 방식입니다.

---

# 12. 회귀 / 분류시 알맞은 metric은 무엇일까?

회귀와 분류 문제에서는 각각의 특성에 맞는 서로 다른 metric을 사용해야 합니다.

**회귀 문제**에서는 주로 **RMSE(Root Mean Square Error)**와 **MAE(Mean Absolute Error)**를 사용합니다. RMSE는 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. 반면 MAE는 이상치에 덜 민감해서 이상치가 많은 데이터에서는 더 적절할 수 있습니다. **R²(결정계수)**는 모델이 데이터의 분산을 얼마나 잘 설명하는지를 보여주는 지표로, 0과 1 사이의 값을 가지며 1에 가까울수록 좋습니다.

**분류 문제**에서는 상황에 따라 다양한 metric을 선택해야 합니다. **Accuracy**는 가장 직관적이지만 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. **Precision**은 양성으로 예측한 것 중 실제로 양성인 비율이고, **Recall**은 실제 양성 중 올바르게 찾아낸 비율입니다. **F1-score**는 이 둘의 조화평균으로 균형을 맞춘 지표입니다.

**ROC-AUC**는 다양한 threshold에서의 성능을 종합적으로 평가할 수 있어서 이진 분류에서 널리 사용됩니다. **PR-AUC**는 클래스 불균형이 심할 때 ROC-AUC보다 더 적절한 지표가 될 수 있습니다.

다중 클래스 분류에서는 **Macro/Micro F1-score**를 사용하며, **Confusion Matrix**를 통해 각 클래스별 성능을 자세히 분석할 수 있습니다.

실제로는 **비즈니스 요구사항**을 고려해서 metric을 선택해야 합니다. 의료 진단에서는 Recall이 중요하고, 스팸 필터에서는 Precision이 중요하죠. 또한 **데이터의 특성**과 **클래스 분포**도 고려해야 합니다.

---

# 13. Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.

Association Rule은 장바구니 분석에서 "A를 사는 사람이 B도 산다"는 규칙을 찾는 기법으로, 세 가지 핵심 지표가 있습니다.

**Support(지지도)**는 전체 거래 중에서 해당 아이템들이 함께 나타나는 비율입니다. Support(A→B) = P(A∩B)로 계산되며, 이 규칙이 얼마나 자주 발생하는지를 보여줍니다. 예를 들어 전체 거래 1000건 중 빵과 우유가 함께 구매된 거래가 100건이라면, Support(빵→우유) = 0.1입니다. 너무 낮으면 의미 없는 규칙이고, 너무 높으면 당연한 규칙이 될 수 있어요.

**Confidence(신뢰도)**는 A를 구매한 사람 중에서 B도 구매한 비율입니다. Confidence(A→B) = P(B|A) = P(A∩B)/P(A)로 계산됩니다. 빵을 산 사람 200명 중 100명이 우유도 샀다면, Confidence(빵→우유) = 0.5입니다. 이는 A를 구매했을 때 B를 구매할 확률을 의미하므로, 추천 시스템에서 중요한 지표가 됩니다.

**Lift(향상도)**는 A와 B가 독립적일 때 대비 얼마나 더 자주 함께 나타나는지를 보여줍니다. Lift(A→B) = P(B|A)/P(B) = Confidence(A→B)/P(B)로 계산됩니다. Lift가 1보다 크면 양의 상관관계, 1이면 독립, 1보다 작으면 음의 상관관계를 의미합니다. 예를 들어 Lift가 2라면, A를 구매했을 때 B를 구매할 확률이 일반적인 경우보다 2배 높다는 뜻입니다.

실제로는 이 세 지표를 모두 고려해서 규칙을 평가합니다. Support는 빈도를, Confidence는 예측 정확도를, Lift는 독립성 대비 향상 정도를 측정하므로, 각각 다른 관점에서 규칙의 유용성을 평가할 수 있습니다.

---

# 14. 최적화 기법중 Newton's Method와 Gradient Descent 방법에 대해 알고 있나요?

Gradient Descent와 Newton's Method는 모두 함수의 최솟값을 찾는 최적화 기법이지만, 접근 방식과 특성이 다릅니다.

**Gradient Descent**는 가장 기본적인 최적화 방법으로, 함수의 기울기(gradient) 방향으로 조금씩 이동하면서 최솟값을 찾습니다. 수식으로는 θ_{t+1} = θ_t - α∇f(θ_t)로 표현되며, 여기서 α는 학습률(learning rate)입니다. 기울기가 가파른 곳에서는 크게 이동하고, 기울기가 완만한 곳에서는 작게 이동하는 직관적인 방법이에요.

장점은 구현이 간단하고 메모리 효율적이며, 대용량 데이터에도 적용 가능하다는 점입니다. 하지만 학습률 설정이 중요하고, 수렴 속도가 느리며, 골짜기나 평지에서 비효율적으로 움직일 수 있습니다.

**Newton's Method**는 2차 도함수(헤시안 행렬)까지 사용해서 더 정확한 최적화를 수행합니다. 수식으로는 θ_{t+1} = θ_t - H^{-1}∇f(θ_t)로 표현되며, 여기서 H는 헤시안 행렬입니다. 1차 도함수뿐만 아니라 2차 도함수 정보까지 활용해서 곡률을 고려한 더 정확한 방향으로 이동합니다.

장점은 수렴 속도가 매우 빠르고(2차 수렴), 골짜기나 평지에서도 효율적으로 움직인다는 점입니다. 하지만 헤시안 행렬을 계산하고 역행렬을 구해야 하므로 계산 비용이 매우 높고, 메모리 사용량도 많습니다. 또한 헤시안이 양정치가 아닌 경우 문제가 될 수 있어요.

실제로는 **Quasi-Newton 방법들**(BFGS, L-BFGS)이 두 방법의 장점을 결합한 하이브리드 접근법으로 널리 사용됩니다. Newton's Method의 빠른 수렴 속도는 유지하면서도 헤시안을 직접 계산하지 않고 근사하는 방식으로 계산 비용을 줄입니다.

---

# 15. 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?

머신러닝과 통계학은 모두 데이터에서 패턴을 찾는 학문이지만, 접근 방식과 목적에서 차이가 있습니다.

**통계학적 접근**은 **해석 가능성과 추론**에 중점을 둡니다. 가설을 세우고, 모델의 가정을 검증하며, p-value나 신뢰구간을 통해 통계적 유의성을 평가합니다. 모델의 파라미터가 실제 현상을 어떻게 설명하는지에 관심이 많고, 인과관계 추론에 강점이 있습니다. 또한 표본에서 모집단으로의 일반화 가능성을 엄밀하게 다룹니다.

**머신러닝적 접근**은 **예측 성능**에 중점을 둡니다. 복잡한 패턴을 찾아내고, 새로운 데이터에 대한 예측 정확도를 최대화하는 것이 목표입니다. 모델이 블랙박스여도 예측이 정확하면 괜찮다고 보는 경향이 있어요. 또한 대용량 데이터와 고차원 문제에 특화되어 있습니다.

**데이터 크기**에 대한 관점도 다릅니다. 통계학은 전통적으로 작은 표본에서도 유의미한 결론을 도출하는 데 집중했지만, 머신러닝은 빅데이터 환경에서 복잡한 패턴을 학습하는 데 특화되어 있습니다.

**모델 복잡도**에 대한 접근도 다릅니다. 통계학은 간단하고 해석 가능한 모델을 선호하는 반면, 머신러닝은 성능 향상을 위해 복잡한 모델도 주저하지 않습니다.

하지만 최근에는 이 경계가 모호해지고 있습니다. 통계학에서도 예측 성능을 중시하는 경향이 생겼고, 머신러닝에서도 해석 가능성에 대한 관심이 높아지고 있어요. 실제로는 두 접근법을 적절히 조합해서 사용하는 것이 가장 효과적입니다.

---

# 16. 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?

전통적인 인공신경망은 여러 가지 근본적인 문제점들로 인해 한계에 부딪혔습니다.

가장 큰 문제는 **차원의 저주(Curse of Dimensionality)**였습니다. 입력 차원이 늘어날수록 필요한 파라미터 수가 기하급수적으로 증가하고, 학습에 필요한 데이터도 폭발적으로 늘어났습니다. 이로 인해 고차원 데이터에서는 실용적이지 못했습니다.

**기울기 소실 문제(Vanishing Gradient Problem)**도 심각했습니다. 역전파 과정에서 기울기가 계층을 거치면서 점점 작아져서, 깊은 네트워크에서는 앞쪽 층의 가중치가 거의 업데이트되지 않았습니다. 이로 인해 깊은 네트워크를 학습시키는 것이 거의 불가능했어요.

**과적합 문제**도 만성적이었습니다. 복잡한 네트워크는 훈련 데이터에 너무 잘 맞춰져서 일반화 성능이 떨어졌고, 당시에는 효과적인 정규화 기법이 부족했습니다.

**지역 최솟값(Local Minima)**에 빠지는 문제도 있었습니다. 복잡한 손실 함수에서 전역 최솟값을 찾지 못하고 지역 최솟값에 갇혀서 성능이 제한되었습니다.

**계산 자원의 한계**도 큰 문제였습니다. 당시의 하드웨어로는 복잡한 신경망을 학습시키기에 부족했고, 병렬 처리 기술도 제한적이었습니다.

**초기화 문제**도 있었습니다. 가중치 초기화가 잘못되면 학습이 전혀 진행되지 않거나 매우 느려졌습니다.

이러한 문제들로 인해 전통적인 신경망은 1990년대 후반부터 2000년대 초반까지 "AI 겨울"을 겪게 되었고, 대신 SVM이나 Random Forest 같은 다른 기법들이 주목받게 되었습니다.

---

# 17. 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?

현재 딥러닝의 혁신은 여러 요소들이 복합적으로 작용한 결과라고 생각합니다.

가장 근본적인 변화는 **데이터와 컴퓨팅 파워의 폭발적 증가**입니다. 인터넷과 모바일 기기의 보급으로 엄청난 양의 데이터가 생성되고, GPU와 클라우드 컴퓨팅의 발전으로 이를 처리할 수 있는 환경이 마련되었습니다. 이는 딥러닝이 필요로 하는 "빅데이터 + 고성능 컴퓨팅" 환경을 제공했습니다.

**알고리즘적 혁신**도 핵심입니다. ReLU 활성화 함수가 기울기 소실 문제를 크게 완화했고, Dropout, Batch Normalization 같은 정규화 기법들이 과적합을 효과적으로 방지했습니다. 또한 Xavier/He 초기화, Adam 옵티마이저 등이 학습 안정성을 크게 향상시켰습니다.

**아키텍처 혁신**도 중요한 역할을 했습니다. CNN은 이미지 처리에서, RNN/LSTM은 시퀀스 데이터에서, Transformer는 자연어 처리에서 각각 혁신을 일으켰습니다. 특히 Attention 메커니즘은 모델이 중요한 부분에 집중할 수 있게 해주었습니다.

**사전 훈련(Pre-training)과 전이 학습(Transfer Learning)**의 개념도 혁신적이었습니다. 대용량 데이터로 사전 훈련한 모델을 특정 태스크에 맞게 미세 조정하는 방식으로, 적은 데이터로도 좋은 성능을 얻을 수 있게 되었습니다.

**오픈소스 생태계**의 발전도 빼놓을 수 없습니다. TensorFlow, PyTorch 같은 프레임워크가 연구자들의 접근성을 높였고, GitHub를 통한 지식 공유가 혁신 속도를 가속화했습니다.

하지만 가장 근본적인 혁신은 **"end-to-end 학습"**이라는 패러다임 변화라고 생각합니다. 기존에는 feature engineering이 핵심이었지만, 이제는 원시 데이터부터 최종 결과까지 모든 과정을 신경망이 자동으로 학습하는 방식으로 바뀌었습니다. 이는 인간의 직관과 경험에 의존하던 부분을 데이터와 알고리즘이 대체한 혁신입니다.

---

# 18. ROC 커브에 대해 설명해주실 수 있으신가요?

ROC 커브(Receiver Operating Characteristic Curve)는 이진 분류 모델의 성능을 평가하는 중요한 시각화 도구입니다.

ROC 커브는 **True Positive Rate(TPR)**와 **False Positive Rate(FPR)**의 관계를 그래프로 나타낸 것입니다. TPR은 실제 양성 중 올바르게 양성으로 예측한 비율(민감도, Sensitivity)이고, FPR은 실제 음성 중 잘못 양성으로 예측한 비율(1-특이도)입니다.

X축은 FPR, Y축은 TPR로 설정하며, 그래프는 (0,0)에서 (1,1)까지의 곡선으로 나타납니다. 분류 threshold를 변화시키면서 각각의 TPR과 FPR을 계산해서 점들을 연결하면 ROC 커브가 완성됩니다.

**완벽한 분류기**는 (0,1) 지점에 위치하며, 이는 FPR=0, TPR=1, 즉 모든 양성을 올바르게 찾아내면서 음성은 하나도 잘못 분류하지 않는다는 의미입니다.

**무작위 분류기**는 대각선(0,0)에서 (1,1)을 잇는 직선으로 나타나며, 이는 동전 던지기와 같은 수준의 성능을 의미합니다.

**ROC-AUC(Area Under Curve)**는 ROC 커브 아래 면적을 의미하며, 0과 1 사이의 값을 가집니다. 1에 가까울수록 좋은 성능이고, 0.5는 무작위 분류기와 같은 성능을 의미합니다.

ROC 커브의 장점은 **클래스 불균형에 상대적으로 덜 민감**하다는 점입니다. 또한 **threshold에 관계없이 모델의 전반적인 성능**을 평가할 수 있어서, 다양한 threshold에서의 성능을 종합적으로 비교할 수 있습니다.

하지만 클래스 불균형이 매우 심한 경우에는 **PR 커브(Precision-Recall Curve)**가 더 적절할 수 있습니다. ROC 커브는 FPR을 사용하기 때문에 음성 클래스가 매우 많을 때는 실제 성능을 과대평가할 수 있기 때문입니다.

---

# 19. 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?

100대의 서버 환경에서는 Random Forest가 인공신경망보다 훨씬 유리한 여러 가지 이유가 있습니다.

가장 큰 이유는 **병렬 처리의 효율성**입니다. Random Forest의 각 트리는 독립적으로 학습할 수 있어서, 100대 서버에 트리를 분산시켜서 동시에 학습할 수 있습니다. 이는 학습 시간을 크게 단축시킬 수 있어요. 반면 신경망은 순차적인 역전파 과정 때문에 완전한 병렬화가 어렵습니다.

**메모리 효율성**도 중요한 장점입니다. Random Forest는 각 트리가 상대적으로 작고, 전체 모델을 메모리에 로드할 필요가 없어서 서버당 메모리 사용량이 적습니다. 신경망은 전체 모델을 메모리에 올려야 하므로 메모리 요구사항이 높습니다.

**안정성과 견고성**도 Random Forest의 강점입니다. 일부 서버가 다운되거나 네트워크 문제가 발생해도, 남은 서버들로 학습을 계속할 수 있습니다. 각 트리가 독립적이기 때문에 부분적인 실패가 전체 모델에 치명적이지 않아요.

**하이퍼파라미터 튜닝의 용이성**도 있습니다. Random Forest는 상대적으로 튜닝할 파라미터가 적고, 각 파라미터의 효과가 직관적이어서 분산 환경에서도 쉽게 최적화할 수 있습니다.

**해석 가능성**도 실무에서 중요한 장점입니다. Random Forest는 feature importance를 제공하고, 각 트리의 의사결정 과정을 추적할 수 있어서 비즈니스 의사결정에 도움이 됩니다.

**학습 속도**도 빠릅니다. Random Forest는 각 트리를 독립적으로 학습하므로 전체 학습 시간이 단축되고, 조기 종료나 증분 학습도 쉽게 구현할 수 있습니다.

물론 신경망이 더 좋은 성능을 보일 수 있는 복잡한 패턴이 있다면 신경망을 선택해야 하지만, 대부분의 경우 Random Forest가 분산 환경에서 더 실용적이고 효율적인 선택이 됩니다.

---

# 20. K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)

K-means의 가장 큰 의미론적 단점은 **클러스터 개수를 미리 정해야 한다**는 점입니다. 실제 데이터에서는 몇 개의 클러스터가 있는지 알 수 없는 경우가 대부분인데, K-means는 이를 사전에 지정해야 합니다. 잘못된 K값을 선택하면 의미 없는 클러스터링 결과를 얻을 수 있어요.

**구형(원형) 클러스터만 잘 찾는다**는 한계도 있습니다. K-means는 유클리드 거리를 기반으로 하기 때문에, 타원형이나 비선형적으로 분포된 클러스터는 제대로 찾지 못합니다. 실제 데이터는 다양한 모양의 클러스터를 가질 수 있는데, 이에 대한 가정이 너무 제한적입니다.

**초기값에 민감하다**는 문제도 있습니다. 초기 중심점이 어떻게 설정되느냐에 따라 최종 결과가 크게 달라질 수 있어요. 특히 클러스터가 겹치거나 경계가 모호한 경우에는 더욱 그렇습니다.

**이상치에 취약하다**는 점도 의미론적 단점입니다. 이상치가 있으면 중심점이 그쪽으로 끌려가서 전체 클러스터링 결과가 왜곡될 수 있습니다. 실제 데이터에는 이상치가 흔한데, 이를 처리하는 메커니즘이 부족합니다.

**클러스터 크기가 균등하다고 가정**한다는 한계도 있습니다. 실제로는 클러스터마다 크기가 다를 수 있는데, K-means는 모든 클러스터가 비슷한 크기를 가진다고 가정합니다.

**연속적인 특성만 다룰 수 있다**는 제약도 있습니다. 범주형 데이터나 혼합형 데이터에는 직접 적용하기 어렵고, 전처리가 필요합니다.

이러한 의미론적 한계들 때문에 실제로는 K-means보다는 DBSCAN, Gaussian Mixture Model, 또는 계층적 클러스터링 같은 다른 방법들이 더 적합한 경우가 많습니다.

---

# 21. L1, L2 정규화에 대해 설명해주세요.

L1과 L2 정규화는 과적합을 방지하고 모델의 일반화 성능을 향상시키는 기법으로, 손실 함수에 정규화 항을 추가하는 방식입니다.

**L1 정규화(Lasso)**는 가중치의 절댓값의 합을 손실 함수에 추가합니다. 수식으로는 Loss + λ∑|w_i|로 표현되며, 여기서 λ는 정규화 강도를 조절하는 하이퍼파라미터입니다. L1 정규화의 가장 큰 특징은 **feature selection 효과**가 있다는 점입니다. 불필요한 feature의 가중치를 0으로 만들어서 자동으로 feature selection을 수행합니다. 이는 모델을 단순화시키고 해석 가능성을 높입니다.

**L2 정규화(Ridge)**는 가중치의 제곱합을 손실 함수에 추가합니다. 수식으로는 Loss + λ∑w_i²로 표현됩니다. L2 정규화는 가중치들을 0에 가깝게 만들지만 완전히 0으로 만들지는 않습니다. 모든 feature를 유지하면서 가중치만 줄여서 모델의 복잡도를 제어합니다.

**기하학적 차이**도 있습니다. L1 정규화는 다이아몬드 모양의 제약 영역을 만들어서 축과 만나는 지점(가중치가 0이 되는 지점)에서 최적해를 찾는 경향이 있습니다. 반면 L2 정규화는 원형 제약 영역을 만들어서 모든 가중치를 비슷하게 줄이는 경향이 있습니다.

**계산적 차이**도 있습니다. L1 정규화는 절댓값 함수 때문에 미분이 불연속적이어서 최적화가 복잡하지만, L2 정규화는 미분 가능해서 최적화가 상대적으로 쉽습니다.

**Elastic Net**은 L1과 L2를 결합한 방법으로, 두 정규화의 장점을 모두 활용할 수 있습니다. 특히 feature가 많은 경우에 효과적입니다.

실제로는 데이터의 특성과 목적에 따라 선택해야 합니다. feature selection이 중요하다면 L1을, 모든 feature를 유지하면서 정규화하고 싶다면 L2를, 둘 다 고려하고 싶다면 Elastic Net을 사용하는 것이 좋습니다.

---

# 22. Cross Validation은 무엇이고 어떻게 해야하나요?

Cross Validation(교차 검증)은 모델의 일반화 성능을 평가하고 과적합을 방지하기 위한 기법으로, 데이터를 여러 번 나누어서 반복적으로 학습과 검증을 수행하는 방법입니다.

가장 기본적인 **K-Fold Cross Validation**은 데이터를 K개의 폴드로 나누고, 각 폴드를 한 번씩 검증 데이터로 사용하면서 나머지 K-1개 폴드로 학습을 수행합니다. 예를 들어 5-Fold CV라면 데이터를 5개로 나누고, 5번의 학습-검증 과정을 거쳐서 5개의 성능 점수를 얻습니다. 최종 성능은 이 5개 점수의 평균과 표준편차로 평가합니다.

**Stratified K-Fold**는 클래스 비율을 유지하면서 폴드를 나누는 방법으로, 분류 문제에서 클래스 불균형이 있을 때 유용합니다. 각 폴드가 원본 데이터와 비슷한 클래스 분포를 가지도록 보장합니다.

**Leave-One-Out Cross Validation(LOOCV)**은 K를 데이터 개수와 같게 설정하는 극단적인 경우로, 각 샘플을 하나씩 검증 데이터로 사용합니다. 계산 비용이 높지만 작은 데이터셋에서는 유용합니다.

**Time Series Cross Validation**은 시계열 데이터에 특화된 방법으로, 시간 순서를 고려해서 과거 데이터로 학습하고 미래 데이터로 검증합니다. 일반적인 K-Fold는 시계열 데이터에 부적절할 수 있어요.

Cross Validation의 장점은 **더 안정적인 성능 평가**가 가능하다는 점입니다. 단일 train-test split보다 훨씬 신뢰할 수 있는 성능 추정을 제공합니다. 또한 **하이퍼파라미터 튜닝**에도 활용할 수 있어서, 다양한 파라미터 조합을 체계적으로 평가할 수 있습니다.

하지만 **계산 비용이 높다**는 단점이 있고, **데이터가 적을 때는 각 폴드의 크기가 작아져서** 성능 평가가 불안정할 수 있습니다. 또한 **시계열 데이터나 공간적 상관관계가 있는 데이터**에서는 특별한 주의가 필요합니다.

실제로는 데이터의 크기와 특성, 그리고 계산 자원을 고려해서 적절한 Cross Validation 방법을 선택해야 합니다.

---

# 23. XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

XGBoost(eXtreme Gradient Boosting)는 그래디언트 부스팅의 확장된 버전으로, 캐글에서 압도적인 인기를 얻고 있는 모델입니다.

XGBoost가 캐글에서 유명한 이유는 여러 가지가 있습니다. 가장 큰 이유는 **뛰어난 성능**입니다. 대부분의 테이블 데이터에서 최고 수준의 성능을 보여주며, 특히 구조화된 데이터에서 딥러닝보다도 더 좋은 결과를 내는 경우가 많습니다.

**속도와 효율성**도 큰 장점입니다. 병렬 처리와 분산 컴퓨팅을 지원해서 대용량 데이터도 빠르게 처리할 수 있고, 메모리 사용량도 최적화되어 있습니다. 캐글의 대용량 데이터셋에서도 실용적으로 사용할 수 있어요.

**과적합 방지 기능**이 뛰어납니다. 정규화 항, 조기 종료, 드롭아웃 등 다양한 과적합 방지 기법이 내장되어 있어서, 복잡한 모델임에도 불구하고 일반화 성능이 좋습니다.

**유연성**도 큰 장점입니다. 회귀, 분류, 랭킹 등 다양한 태스크에 적용할 수 있고, 커스텀 손실 함수와 평가 지표도 쉽게 정의할 수 있습니다. 또한 결측값 처리, 범주형 변수 처리 등도 자동으로 해줍니다.

**하이퍼파라미터 튜닝**이 상대적으로 쉽습니다. 많은 파라미터가 있지만 각각의 효과가 직관적이고, 그리드 서치나 베이지안 최적화로 쉽게 튜닝할 수 있어요.

**해석 가능성**도 제공합니다. Feature importance를 통해 어떤 변수가 중요한지 알 수 있고, SHAP 같은 도구와 결합하면 더 자세한 해석도 가능합니다.

**안정성**도 뛰어납니다. 다양한 데이터 타입과 분포에서 일관되게 좋은 성능을 보여주며, 이상치에도 상대적으로 강합니다.

캐글에서는 "XGBoost로 시작해서 성능이 좋으면 그대로 사용하고, 더 개선이 필요하면 딥러닝이나 다른 기법을 시도한다"는 전략이 일반적입니다. 실제로 많은 캐글 우승 솔루션에서 XGBoost가 핵심 모델로 사용되었습니다.

---

# 24. 앙상블 방법엔 어떤 것들이 있나요?

앙상블 방법은 여러 모델의 예측을 결합해서 더 좋은 성능을 얻는 기법으로, 크게 세 가지 접근 방식이 있습니다.

**Bagging(Bootstrap Aggregating)**은 같은 알고리즘을 여러 번 학습시키되, 각각 다른 데이터 샘플을 사용하는 방법입니다. 대표적인 예가 **Random Forest**로, 여러 의사결정나무를 학습시키되 각각 다른 feature와 데이터 샘플을 사용합니다. Bagging은 분산을 줄여서 과적합을 방지하는 효과가 있어요.

**Boosting**은 순차적으로 모델을 학습시키되, 이전 모델이 잘못 예측한 샘플에 더 집중해서 다음 모델을 학습시키는 방법입니다. **AdaBoost**, **Gradient Boosting**, **XGBoost** 등이 대표적인 예시입니다. Boosting은 편향을 줄여서 전체적인 성능을 향상시키는 효과가 있습니다.

**Stacking**은 여러 다른 알고리즘의 예측을 메타 모델(meta-model)이 학습해서 최종 예측을 만드는 방법입니다. 예를 들어 Random Forest, SVM, Neural Network의 예측을 Logistic Regression이 학습해서 최종 결과를 만드는 방식이에요. Stacking은 모델의 다양성을 활용해서 더 정확한 예측을 할 수 있습니다.

**Voting**은 여러 모델의 예측을 단순히 투표나 평균으로 결합하는 방법입니다. **Hard Voting**은 다수결로 결정하고, **Soft Voting**은 확률의 평균을 사용합니다.

**Blending**은 Stacking과 비슷하지만, 메타 모델을 학습할 때 별도의 검증 데이터를 사용하는 방법입니다. Stacking보다 과적합 위험이 적지만 데이터 사용 효율성이 떨어집니다.

각 방법의 특징을 정리하면, Bagging은 과적합이 심한 모델에 효과적이고, Boosting은 약한 학습기에 효과적이며, Stacking은 서로 다른 특성을 가진 모델들을 결합할 때 효과적입니다. 실제로는 여러 방법을 조합해서 사용하는 경우가 많으며, 캐글 같은 대회에서는 앙상블이 우승의 핵심 전략이 되는 경우가 많습니다.

---

# 25. feature vector란 무엇일까요?

Feature vector는 머신러닝에서 데이터의 특성을 수치로 표현한 벡터입니다. 쉽게 말해서 하나의 데이터 포인트를 숫자들의 리스트로 나타낸 것이에요.

예를 들어 사람의 정보를 feature vector로 표현한다면, [나이, 키, 몸무게, 소득] 같은 형태가 될 수 있습니다. 이때 [25, 170, 65, 3000]은 25세, 170cm, 65kg, 월소득 3000만원인 사람을 나타내는 feature vector입니다.

Feature vector의 각 원소는 **feature** 또는 **attribute**라고 하며, 이는 데이터의 한 가지 특성을 나타냅니다. 벡터의 차원은 feature의 개수와 같아서, 위 예시에서는 4차원 벡터가 됩니다.

**Feature Engineering**은 원시 데이터를 의미 있는 feature vector로 변환하는 과정입니다. 텍스트 데이터를 TF-IDF 벡터로 변환하거나, 이미지를 픽셀 값들의 벡터로 변환하는 것처럼 말이죠.

**Feature Selection**은 중요한 feature만 선택해서 차원을 줄이는 과정이고, **Feature Scaling**은 feature들의 스케일을 맞춰주는 전처리 과정입니다.

Feature vector는 머신러닝 알고리즘의 입력으로 사용되며, 모델은 이 벡터들을 학습해서 패턴을 찾아냅니다. 따라서 feature vector의 품질이 모델의 성능에 직접적인 영향을 미칩니다.

최근에는 딥러닝의 발전으로 **Representation Learning**이 주목받고 있는데, 이는 모델이 자동으로 좋은 feature vector를 학습하는 방법입니다. 하지만 여전히 도메인 지식을 활용한 feature engineering이 중요한 역할을 합니다.

---

# 26. 좋은 모델의 정의는 무엇일까요?

좋은 모델의 정의는 상황과 목적에 따라 달라지지만, 일반적으로 고려해야 할 몇 가지 핵심 요소들이 있습니다.

가장 기본적인 요소는 **예측 정확도**입니다. 모델이 새로운 데이터에 대해 얼마나 정확하게 예측하는지가 가장 중요한 평가 기준이에요. 하지만 정확도만으로는 부족하고, **일반화 성능**이 중요합니다. 훈련 데이터에만 잘 맞는 것이 아니라, 보지 못한 새로운 데이터에서도 좋은 성능을 보여야 합니다.

**해석 가능성**도 현대 머신러닝에서 점점 더 중요해지고 있습니다. 특히 의료, 금융, 법률 같은 도메인에서는 모델이 왜 그런 예측을 했는지 설명할 수 있어야 합니다. 비즈니스 의사결정에 활용하려면 해석 가능성이 필수적이에요.

**안정성과 견고성**도 중요한 요소입니다. 이상치나 노이즈가 있어도 일관된 성능을 보여야 하고, 데이터 분포가 조금 바뀌어도 크게 성능이 떨어지지 않아야 합니다.

**효율성**도 고려해야 합니다. 학습 시간, 예측 시간, 메모리 사용량 등이 실용적인 수준이어야 합니다. 정확도가 아무리 높아도 실시간으로 예측할 수 없다면 실무에서 사용하기 어려워요.

**유지보수성**도 중요합니다. 모델을 업데이트하거나 수정하기 쉬워야 하고, 새로운 데이터가 들어와도 쉽게 재학습할 수 있어야 합니다.

**비용 효율성**도 고려해야 합니다. 모델 개발과 운영에 드는 비용 대비 얻는 이익이 합리적이어야 합니다.

하지만 가장 중요한 것은 **비즈니스 목표 달성**입니다. 기술적으로 완벽한 모델이라도 비즈니스 문제를 해결하지 못한다면 좋은 모델이 아닙니다. 따라서 도메인 전문가와의 협업을 통해 실제 문제를 정확히 정의하고, 그에 맞는 적절한 모델을 선택하는 것이 핵심입니다.

---

# 27. 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?

일반적으로 50개의 작은 의사결정나무가 하나의 큰 의사결정나무보다 더 좋은 성능을 보입니다. 이는 **앙상블 효과** 때문입니다.

가장 큰 이유는 **분산 감소(Variance Reduction)**입니다. 하나의 큰 나무는 훈련 데이터에 과적합되기 쉽고, 작은 변화에도 민감하게 반응합니다. 하지만 50개의 작은 나무들의 평균을 내면 이런 변동성이 상쇄되어 더 안정적인 예측을 할 수 있어요.

**편향과 분산의 균형**도 더 좋습니다. 하나의 큰 나무는 편향은 낮지만 분산이 높은 반면, 50개의 작은 나무는 각각은 편향이 높을 수 있지만 분산이 낮아서 전체적으로는 더 좋은 성능을 보입니다.

**다양성(Diversity)**도 중요한 요소입니다. 50개의 나무가 서로 다른 패턴을 학습할 수 있어서, 하나의 나무가 놓친 패턴을 다른 나무가 보완할 수 있습니다. 이는 Random Forest에서 각 나무가 다른 feature와 데이터 샘플을 사용하는 것과 같은 원리입니다.

**견고성(Robustness)**도 뛰어납니다. 일부 나무가 잘못된 예측을 해도 다른 나무들이 이를 보완할 수 있어서, 이상치나 노이즈에 더 강합니다.

**병렬 처리**도 가능합니다. 50개의 나무를 동시에 학습할 수 있어서, 하나의 큰 나무를 학습하는 것보다 시간이 더 오래 걸리지 않을 수 있습니다.

하지만 **해석 가능성**은 떨어집니다. 하나의 큰 나무는 의사결정 과정을 쉽게 추적할 수 있지만, 50개 나무의 결합은 복잡해서 해석하기 어려워요.

**메모리 사용량**도 더 많을 수 있습니다. 50개의 나무를 모두 저장해야 하므로 하나의 큰 나무보다 메모리를 더 사용합니다.

전반적으로는 성능 면에서 50개의 작은 나무가 더 우수하지만, 해석 가능성이 중요한 상황이라면 하나의 큰 나무를 선택할 수도 있습니다. 실제로는 Random Forest나 Gradient Boosting 같은 앙상블 방법이 이런 아이디어를 체계적으로 구현한 것입니다.

---

# 28. 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?

스팸 필터에서 로지스틱 회귀가 널리 사용되는 이유는 여러 가지 실용적인 장점들 때문입니다.

가장 큰 이유는 **해석 가능성**입니다. 로지스틱 회귀는 각 feature(단어)가 스팸 확률에 미치는 영향을 계수로 직접 해석할 수 있어요. 예를 들어 "무료"라는 단어의 계수가 2.5라면, 이 단어가 있을 때 스팸 확률이 2.5배 증가한다는 의미입니다. 이는 사용자에게 왜 이메일이 스팸으로 분류되었는지 설명할 수 있게 해줍니다.

**확률 출력**도 중요한 장점입니다. 로지스틱 회귀는 0과 1 사이의 확률값을 출력하므로, 스팸 확률이 0.8인 이메일과 0.95인 이메일을 구분할 수 있습니다. 이는 사용자가 임계값을 조정해서 민감도를 조절할 수 있게 해줍니다.

**학습 속도**가 빠릅니다. 텍스트 데이터는 보통 수십만 개의 단어로 구성된 고차원 벡터인데, 로지스틱 회귀는 이런 고차원 데이터도 빠르게 학습할 수 있습니다. 실시간으로 새로운 스팸 패턴을 학습해야 하는 환경에서 중요한 장점이에요.

**메모리 효율성**도 뛰어납니다. 모델 파라미터가 단순한 계수들이므로 저장 공간이 적고, 예측 시에도 빠르게 계산할 수 있습니다.

**안정성**도 좋습니다. 이상치나 노이즈에 상대적으로 강하고, 하이퍼파라미터 튜닝이 간단해서 안정적인 성능을 보장합니다.

**확장성**도 뛰어납니다. 새로운 feature를 쉽게 추가할 수 있고, 온라인 학습도 가능해서 새로운 스팸 패턴이 나타나면 점진적으로 모델을 업데이트할 수 있습니다.

**L1 정규화**와 잘 맞습니다. L1 정규화를 사용하면 불필요한 단어들의 계수를 0으로 만들어서 자동으로 feature selection이 되고, 모델이 더 간단해집니다.

물론 딥러닝 모델이 더 복잡한 패턴을 학습할 수 있지만, 스팸 필터링에서는 **해석 가능성과 실시간 처리**가 더 중요하기 때문에 로지스틱 회귀가 여전히 널리 사용됩니다. 또한 **베이스라인 모델**로서의 역할도 훌륭해서, 더 복잡한 모델을 구현하기 전에 로지스틱 회귀로 시작하는 것이 일반적입니다.

---

# 29. OLS(ordinary least squre) regression의 공식은 무엇인가요?

OLS(Ordinary Least Squares) 회귀는 선형 회귀의 가장 기본적인 형태로, 잔차의 제곱합을 최소화하는 방법입니다.

**기본 모델**은 y = Xβ + ε 형태로 표현되며, 여기서 y는 종속변수 벡터, X는 독립변수 행렬, β는 회귀계수 벡터, ε는 오차 벡터입니다.

**목적 함수**는 잔차의 제곱합을 최소화하는 것입니다:

$$
\min_{\beta} \sum_{i=1}^{n} (y_i - x_i^T\beta)^2 = \min_{\beta} ||y - X\beta||^2
$$

**정규 방정식(Normal Equation)**을 통해 해를 구할 수 있습니다:

$$
\hat{\beta} = (X^T X)^{-1} X^T y
$$

이 공식은 X^T X가 역행렬을 가질 때 유효하며, 이는 X가 full rank일 때입니다.

**기하학적 해석**으로는, X의 열공간에서 y에 가장 가까운 점을 찾는 것으로 볼 수 있습니다. 이는 y를 X의 열공간에 정사영(projection)하는 것과 같습니다.

**가정사항**으로는 선형성, 독립성, 등분산성, 정규성이 있으며, 이 가정들이 만족될 때 OLS 추정량은 BLUE(Best Linear Unbiased Estimator)가 됩니다.

**잔차**는 e = y - Xβ̂로 계산되며, 잔차의 제곱합(RSS)은:

$$
RSS = e^T e = (y - X\hat{\beta})^T (y - X\hat{\beta})
$$

**결정계수(R²)**는:

$$
R^2 = 1 - \frac{RSS}{TSS} = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y}_i)^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}
$$

여기서 TSS는 총 제곱합(Total Sum of Squares)입니다.

OLS는 수학적으로 우아하고 해석이 용이하지만, 다중공선성이나 이상치에 민감하다는 한계가 있습니다. 현대에는 Ridge, Lasso 같은 정규화된 회귀 방법들이 더 널리 사용되지만, OLS는 여전히 회귀 분석의 기초가 되는 중요한 방법입니다.

---

# 30. Transformer에 대해 설명해주세요.

Transformer는 2017년 "Attention is All You Need" 논문에서 제안된 혁신적인 아키텍처로, 자연어 처리 분야에 혁명을 일으킨 모델입니다.

Transformer의 핵심 구조는 **Encoder와 Decoder**로 이루어져 있습니다. 인코더는 입력 문장을 받아서 문장 속 각 단어를 벡터로 변환하고, 문장 전체의 의미와 순서를 학습합니다. 디코더는 인코더가 생성한 벡터 표현을 받아서 다음 단어를 예측하는 방식으로 학습이 진행됩니다.

가장 혁신적인 부분은 **Self-Attention 메커니즘**입니다. 이는 문장 내 각 단어가 다른 모든 단어들과의 관계를 계산해서, 문맥에 따라 단어의 의미를 동적으로 조정합니다. 예를 들어 "은행"이라는 단어가 "돈"과 함께 나오면 금융 기관을, "강"과 함께 나오면 물가를 의미하도록 학습할 수 있습니다.

**Multi-Head Attention**은 여러 개의 Attention을 병렬로 사용해서 다양한 관점에서 문맥을 이해할 수 있게 해줍니다. 각 헤드는 서로 다른 패턴을 학습해서 더 풍부한 표현을 만들어냅니다.

**Positional Encoding**은 단어의 순서 정보를 모델에 제공합니다. Attention은 순서를 고려하지 않기 때문에, 위치 정보를 별도로 인코딩해서 추가해줍니다.

Transformer의 장점은 **병렬 처리가 가능**하다는 점입니다. RNN과 달리 순차적으로 처리할 필요가 없어서 학습 속도가 매우 빠릅니다. 또한 **장거리 의존성**을 잘 학습할 수 있어서, 긴 문장에서도 멀리 떨어진 단어 간의 관계를 잘 파악합니다.

Transformer는 BERT, GPT, T5 등 현대 자연어 처리 모델의 기반이 되었으며, 최근에는 Vision Transformer로 컴퓨터 비전 분야에서도 활용되고 있습니다.

---

# 31. 학습 과정에서 Optimizer의 역할은 무엇인가요?

Optimizer는 머신러닝 학습 과정에서 손실 함수를 최소화하기 위해 모델의 파라미터를 업데이트하는 알고리즘입니다.

가장 기본적인 개념은 **Delta(학습률)**에서 시작됩니다. 초창기 기계학습에서는 단순히 기울기에 학습률을 곱해서 파라미터를 업데이트했습니다. 하지만 이 방법은 **Local Minimum**에 빠지거나, 수렴 속도가 느리거나, 진동하는 문제가 있었습니다.

이러한 문제를 해결하기 위해 다양한 Optimizer들이 개발되었습니다. **Momentum**은 이전 업데이트 방향을 관성처럼 유지해서 Local Minimum을 벗어나고 수렴 속도를 높입니다. 공이 언덕을 굴러 내려가는 것처럼 움직여서 작은 골짜기는 넘어갈 수 있습니다.

**AdaGrad**는 각 파라미터마다 다른 학습률을 적용합니다. 자주 업데이트된 파라미터는 학습률을 줄이고, 드물게 업데이트된 파라미터는 학습률을 유지해서 희소한 데이터에 효과적입니다.

**RMSprop**는 AdaGrad의 단점을 보완해서 학습률이 너무 빨리 감소하는 문제를 해결했습니다. 최근 기울기들의 이동 평균을 사용해서 적응적으로 학습률을 조정합니다.

**Adam(Adaptive Moment Estimation)**은 현재 가장 널리 사용되는 Optimizer로, Momentum과 RMSprop의 장점을 결합한 방법입니다. 1차 모멘트(평균)와 2차 모멘트(분산)를 모두 추정해서 각 파라미터마다 적응적으로 학습률을 조정합니다. 대부분의 상황에서 안정적이고 빠른 수렴을 보여주기 때문에 기본 선택으로 많이 사용됩니다.

최근에는 **AdamW**(Adam with Weight Decay)나 **RAdam**(Rectified Adam) 같은 개선된 버전들도 나오고 있으며, 각각 정규화나 초기 학습 안정성을 향상시킵니다.

---

# 32. Regularization은 무엇인가요?

Regularization은 오버피팅을 방지하기 위한 기술로, 모델이 훈련 데이터에만 과도하게 맞춰지는 것을 막아서 일반화 성능을 향상시킵니다.

기계 학습에는 모든 데이터를 완벽하게 학습하면 결국 그 데이터에 대해서는 오버피팅이 된다는 알려진 문제가 있습니다. 이를 해결하기 위해 다양한 Regularization 기법들이 개발되었습니다.

**L1 Penalty(Lasso)**는 가중치의 절댓값의 합을 손실 함수에 추가합니다. 이는 불필요한 가중치를 0으로 만들어서 자동으로 feature selection 효과를 가져오며, 모델을 단순화시킵니다.

**L2 Penalty(Ridge)**는 가중치의 제곱합을 손실 함수에 추가합니다. L2 정규화는 가중치들을 0에 가깝게 만들지만 완전히 0으로 만들지는 않습니다. 모든 feature를 유지하면서 가중치를 조금씩 작게 만들어서 모델의 복잡도를 제어합니다. 이는 가중치가 너무 커지는 것을 방지해서 모델이 특정 feature에 과도하게 의존하지 않도록 합니다.

**Dropout**은 딥러닝에서 많이 사용되는 정규화 기법으로, 학습 과정에서 랜덤하게 일부 뉴런을 비활성화시킵니다. 보통은 dropout 비율에 따라 이항분포 함수(불규칙 동전 던지기)를 적용하게 됩니다. 예를 들어 dropout rate가 0.5라면 각 뉴런이 50% 확률로 비활성화됩니다. 이는 모델이 특정 뉴런에 의존하지 않고 여러 뉴런의 조합으로 학습하도록 강제해서 일반화 성능을 향상시킵니다.

**Batch Normalization**은 각 미니배치의 평균과 분산을 정규화해서 학습을 안정화시키고 정규화 효과도 가져옵니다. 내부 공변량 변화(Internal Covariate Shift)를 줄여서 더 높은 학습률을 사용할 수 있게 해줍니다.

**Early Stopping**은 검증 데이터의 성능이 더 이상 향상되지 않을 때 학습을 조기에 종료하는 방법입니다. 모델이 훈련 데이터에 과적합되기 전에 학습을 멈춰서 일반화 성능을 유지합니다.

**Data Augmentation**도 정규화 효과가 있는데, 훈련 데이터를 인위적으로 증가시켜서 모델이 다양한 변형에 강건하도록 만듭니다.

실제로는 여러 정규화 기법을 조합해서 사용하는 경우가 많으며, 데이터의 크기와 모델의 복잡도에 따라 적절한 정규화 강도를 조절하는 것이 중요합니다.

---

# 33. Collaborative Filtering에 대해 설명해주세요.

Collaborative Filtering은 추천 시스템에서 널리 사용되는 기법으로, 사용자와 아이템 간의 상호작용 이력을 학습해서 아직 상호작용하지 않은 관계에 대한 선호를 예측하는 알고리즘입니다.

핵심 아이디어는 **"비슷한 취향을 가진 사람들은 미래에도 비슷한 것을 좋아할 것이다"**는 가정입니다. 예를 들어 A와 B가 영화 10개를 비슷하게 평가했다면, A가 좋아한 11번째 영화를 B도 좋아할 가능성이 높다는 것입니다.

**User-based Collaborative Filtering**은 비슷한 사용자들을 찾아서 그들의 선호를 참고합니다. 나와 취향이 비슷한 사람들이 좋아한 아이템을 추천하는 방식입니다.

**Item-based Collaborative Filtering**은 비슷한 아이템들을 찾아서 추천합니다. 내가 좋아한 아이템과 유사한 다른 아이템을 추천하는 방식으로, 일반적으로 User-based보다 안정적이고 확장성이 좋습니다.

**Matrix Factorization**은 현대적인 Collaborative Filtering의 핵심 기법입니다. 사용자-아이템 행렬을 두 개의 저차원 행렬로 분해해서, 사용자와 아이템을 잠재 요인(latent factor) 공간으로 표현합니다. 넷플릭스 프라이즈에서 큰 성공을 거둔 방법이에요.

**ALS(Alternating Least Squares)**는 Matrix Factorization을 최적화하는 효율적인 알고리즘으로, 사용자 행렬과 아이템 행렬을 번갈아가며 업데이트해서 손실을 최소화합니다. 대용량 데이터에서도 효과적으로 작동합니다.

**Factorization Machine**은 Matrix Factorization을 일반화한 방법으로, 추가적인 feature들도 함께 활용할 수 있습니다. 사용자의 나이, 성별, 아이템의 카테고리 등 부가 정보를 모델에 통합할 수 있어요.

최근에는 **Deep Learning 방식**도 많이 사용됩니다. Neural Collaborative Filtering, Autoencoders, Variational Autoencoders 등이 복잡한 비선형 패턴을 학습할 수 있어서 전통적인 방법보다 더 좋은 성능을 보이는 경우가 많습니다.

Collaborative Filtering의 주요 문제로는 **Cold Start 문제**(새로운 사용자나 아이템에 대한 추천이 어려움)와 **Sparsity 문제**(대부분의 사용자-아이템 조합에 대한 정보가 없음)가 있습니다. 이를 해결하기 위해 Content-based Filtering과 결합한 Hybrid 방법들도 많이 사용됩니다.

---

# 34. Maximum Likelihood Estimator(MLE)에 대해 설명해주세요.

Maximum Likelihood Estimator(최대우도추정량)는 관측된 데이터를 가장 잘 설명하는 모수(parameter)를 찾는 통계적 추정 방법입니다.

기본 아이디어는 **"실제로 관측된 데이터가 발생할 확률을 최대화하는 모수를 찾자"**는 것입니다. 역으로 생각하면, 이미 일어난 사건(sample)을 가지고 그 사건을 가장 잘 설명하는 확률 분포의 모수 값을 추정하는 방법론입니다.

구체적인 과정을 설명하면, 먼저 관측된 데이터가 특정 확률 분포(예: 정규분포, 이항분포)를 따른다고 가정합니다. 그 다음 **우도 함수(Likelihood Function)**를 정의하는데, 이는 모수가 주어졌을 때 관측된 데이터가 나올 확률입니다:

$$
L(\theta | x_1, x_2, ..., x_n) = P(x_1, x_2, ..., x_n | \theta) = \prod_{i=1}^{n} P(x_i | \theta)
$$

실제로는 곱셈을 덧셈으로 바꾸기 위해 **로그 우도 함수(Log-Likelihood)**를 사용합니다:

$$
\log L(\theta) = \sum_{i=1}^{n} \log P(x_i | \theta)
$$

이 로그 우도 함수를 최대화하는 모수 θ를 찾기 위해 미분을 사용합니다. 로그 우도를 모수에 대해 미분하고 0으로 놓으면 최댓값을 찾을 수 있습니다:

$$
\frac{\partial \log L(\theta)}{\partial \theta} = 0
$$

예를 들어 동전을 10번 던져서 앞면이 7번 나왔다면, 이항분포를 가정하고 MLE를 적용하면 앞면이 나올 확률 p는 7/10 = 0.7로 추정됩니다.

MLE의 장점은 **일관성(Consistency)**과 **점근적 효율성(Asymptotic Efficiency)**을 가진다는 점입니다. 샘플 크기가 커질수록 참값에 수렴하고, 분산이 가장 작은 추정량이 됩니다.

하지만 **편향(Bias)**이 있을 수 있고, **과적합**에 취약하다는 단점도 있습니다. 이를 보완하기 위해 **MAP(Maximum A Posteriori)**나 **베이지안 추정** 같은 방법들이 사용되기도 합니다.

로지스틱 회귀, 신경망 등 많은 머신러닝 모델들이 실제로는 MLE를 기반으로 학습됩니다. 손실 함수를 최소화하는 것이 로그 우도를 최대화하는 것과 동일한 경우가 많기 때문입니다.

---

# 35. PCA의 동작 원리를 설명해주세요.

PCA(Principal Component Analysis)는 데이터의 분산이 가장 큰 방향을 찾아서 차원을 축소하는 기법입니다.

PCA의 핵심 아이디어는 **가장 큰 분산을 가진 방향을 첫 번째 주성분(principal component)으로 선택하고, 그 다음으로 큰 분산을 가지면서 첫 번째 주성분과 직교하는 방향을 두 번째 주성분으로 선택**하는 것입니다. 이 과정을 순차적으로 반복해서 모든 주성분을 찾습니다.

수학적으로는 **공분산 행렬의 고유벡터(eigenvector)**를 찾는 과정입니다. 각 고유벡터가 주성분의 방향을 나타내고, 고유값(eigenvalue)이 그 방향의 분산 크기를 나타냅니다. 고유값이 큰 순서대로 고유벡터를 선택하면, 데이터의 분산을 최대한 보존하면서 차원을 축소할 수 있습니다.

구체적인 단계는 다음과 같습니다. 먼저 **데이터를 중심화(centering)**해서 평균을 0으로 만듭니다. 그 다음 **공분산 행렬**을 계산하고, 공분산 행렬의 **고유값과 고유벡터를 계산**합니다. 고유값이 큰 순서대로 k개의 고유벡터를 선택해서, 원본 데이터를 이 k개의 주성분에 투영하면 차원 축소가 완료됩니다.

PCA의 장점은 **선형 변환**이어서 계산이 빠르고, **해석이 용이**하며, **노이즈 제거** 효과가 있다는 점입니다. 분산이 작은 방향(노이즈가 많은 방향)을 제거하므로 자연스럽게 노이즈가 제거됩니다.

하지만 **비선형 관계를 잡지 못하고**, **이상치에 민감하며**, **스케일에 영향을 받는다**는 단점도 있습니다. 따라서 실제로는 데이터를 표준화(standardization)한 후 PCA를 적용하는 것이 일반적입니다.

**Kernel PCA**는 비선형 관계를 다룰 수 있는 확장 버전이고, **Incremental PCA**는 대용량 데이터를 배치로 처리할 수 있는 버전입니다. 또한 **Sparse PCA**는 해석 가능성을 높이기 위해 주성분에 희소성을 부여하는 방법입니다.

PCA는 시각화, 전처리, 노이즈 제거, 특징 추출 등 다양한 목적으로 활용되며, 데이터 과학에서 가장 기본적이면서도 강력한 도구 중 하나입니다.

---

# 36. Supervised Learning과 Unsupervised Learning의 차이점은 무엇인가요?

Supervised Learning(지도학습)과 Unsupervised Learning(비지도학습)은 머신러닝의 두 가지 주요 패러다임으로, 데이터에 레이블이 있는지 여부에 따라 구분됩니다.

**Supervised Learning**은 입력 데이터와 그에 대응하는 정답(레이블)이 모두 주어진 상태에서 학습하는 방법입니다. 모델은 입력과 출력의 관계를 학습해서, 새로운 입력이 주어졌을 때 올바른 출력을 예측할 수 있도록 훈련됩니다. 선생님이 정답을 알려주면서 가르치는 것과 비슷해서 "지도학습"이라고 부릅니다.

Supervised Learning은 크게 두 가지 유형으로 나뉩니다. **Classification(분류)**는 이산적인 클래스를 예측하는 문제로, 스팸 메일 필터링, 이미지 인식, 질병 진단 등이 대표적인 예시입니다. 예를 들어 "이 이메일이 스팸인가 정상인가?"를 판단하는 것입니다. **Regression(회귀)**는 연속적인 값을 예측하는 문제로, 주택 가격 예측, 주식 가격 예측, 기온 예측 등이 여기에 해당합니다.

대표적인 Supervised Learning 알고리즘으로는 Linear Regression, Logistic Regression, Decision Tree, Random Forest, SVM, Neural Networks 등이 있습니다. 이들은 모두 레이블이 있는 훈련 데이터를 통해 입력과 출력의 관계를 학습합니다.

**Unsupervised Learning**은 레이블이 없는 데이터에서 숨겨진 패턴이나 구조를 찾아내는 방법입니다. 정답 없이 데이터 자체의 특성과 관계를 발견하는 것으로, 선생님 없이 스스로 배우는 것과 비슷해서 "비지도학습"이라고 부릅니다.

Unsupervised Learning의 주요 유형으로는 **Clustering(군집화)**이 있습니다. 비슷한 특성을 가진 데이터들을 그룹으로 묶는 것으로, 고객 세분화, 이미지 압축, 유전자 분석 등에 사용됩니다. K-means, DBSCAN, Hierarchical Clustering 등이 대표적인 클러스터링 알고리즘입니다.

**Dimensionality Reduction(차원 축소)**도 중요한 비지도학습 기법입니다. PCA, t-SNE, Autoencoder 등을 통해 고차원 데이터를 저차원으로 변환하면서 중요한 정보는 보존합니다. 이는 시각화, 전처리, 노이즈 제거 등에 활용됩니다.

**Anomaly Detection(이상치 탐지)**은 정상 패턴과 다른 비정상적인 데이터를 찾아내는 기법으로, 사기 탐지, 네트워크 침입 탐지, 품질 관리 등에 사용됩니다.

**Association Rule Learning**은 데이터 간의 연관 관계를 찾는 방법으로, 장바구니 분석이나 추천 시스템에 활용됩니다.

두 접근법의 주요 차이점을 정리하면, Supervised Learning은 명확한 목표가 있고 성능을 정량적으로 평가하기 쉽지만 레이블링 비용이 높습니다. 반면 Unsupervised Learning은 레이블 없이도 데이터의 구조를 발견할 수 있지만, 결과의 해석이 주관적일 수 있고 성능 평가가 어렵습니다.

실제로는 **Semi-Supervised Learning**(준지도학습)이나 **Self-Supervised Learning**(자기지도학습) 같은 중간 형태도 있습니다. Semi-Supervised Learning은 소량의 레이블 데이터와 대량의 비레이블 데이터를 함께 사용하고, Self-Supervised Learning은 데이터 자체에서 레이블을 자동으로 생성해서 학습합니다. BERT나 GPT 같은 대규모 언어 모델이 Self-Supervised Learning의 대표적인 예시입니다.

또한 **Reinforcement Learning**(강화학습)이라는 제3의 패러다임도 있는데, 이는 환경과 상호작용하면서 보상을 최대화하는 방향으로 학습하는 방법으로, 게임 AI, 로봇 제어, 자율주행 등에 활용됩니다.

---

# 37. Bias-Variance Trade-off를 설명해주세요.

모델의 일반화 오차는 편향(Bias), 분산(Variance), 그리고 데이터에 내재된 잡음으로 분해됩니다. 핵심은 복잡도를 높이면 편향은 줄지만 분산이 커지고, 복잡도를 낮추면 분산은 줄지만 편향이 커지는 상충 관계입니다.

$$
\mathbb{E}\big[(y - \hat f(x))^2\big] = \underbrace{\big(\mathbb{E}[\hat f(x)] - f(x)\big)^2}_{\text{Bias}^2} + \underbrace{\mathbb{V}[\hat f(x)]}_{\text{Variance}} + \underbrace{\sigma^2}_{\text{Irreducible noise}}
$$

편향이 큰 모델은 일관되게 빗나가는 경향이 있고(과소적합), 분산이 큰 모델은 데이터 샘플에 민감하게 출렁입니다(과적합). 교차검증으로 복잡도(정규화 강도, 트리 깊이, 특성 수 등)를 조정해 두 항을 균형 있게 최소화하는 것이 실무적인 해법입니다. 조기 종료, 정규화(L1/L2), 앙상블(Bagging은 분산 감소, Boosting은 편향 감소)도 효과적인 수단입니다.

---

# 38. 모델 캘리브레이션(Model Calibration)은 왜 중요하고 어떻게 하나요?

캘리브레이션은 모델의 확률 출력이 실제 빈도와 얼마나 일치하는지 측정하고 바로잡는 과정입니다. 예를 들어 0.8 확률로 양성이라 한 표본 중 실제로 약 80%가 양성이면 잘 캘리브레이트된 것입니다. 분류 임계값 튜닝, 위험 기반 의사결정, 랭킹 후 재점수화에서 필수입니다.

평가에는 신뢰도 다이어그램(구간별 예측 확률 vs 실제 빈도), Brier score, ECE(농축 오차) 등을 씁니다. 보정 기법으로는 로지스틱 회귀를 쓰는 Platt scaling, 비모수적 Isotonic regression, 다중 클래스의 Temperature scaling이 대표적입니다. 보정은 반드시 홀드아웃/검증 세트에서 수행하고, 데이터가 바뀌면 주기적으로 다시 보정해야 합니다.

---

# 39. EM 알고리즘과 Gaussian Mixture Model(GMM)을 설명해주세요.

EM은 잠재 변수(latent variable)가 있는 확률 모델의 최대우도 추정을 반복적으로 수행하는 절차입니다. 현재 파라미터로 잠재 변수의 분포를 추정(E-step), 그 기대값을 고정하고 파라미터를 최적화(M-step)합니다. 수렴할 때까지 E–M을 반복합니다.

GMM은 데이터를 여러 개의 가우시안 성분이 섞여 생성된다고 가정합니다. E-step에서 각 점이 각 성분에 속할 책임도(responsibility)를 계산하고, M-step에서 책임도로 가중된 평균, 공분산, 혼합 계수를 갱신합니다. 지역 최적점에 민감하므로 K-means 초기화, 여러 초기값 재시도, 공분산 정규화가 중요합니다. 성분 수 선택에는 BIC/AIC를 흔히 사용합니다.

---

# 40. 클래스 불균형(Class Imbalance)은 어떻게 다루나요?

먼저 적절한 평가 지표를 택합니다. Accuracy 대신 PR-AUC, ROC-AUC, Macro F1, Recall@Precision, 비용 민감 손실 등을 사용합니다. 데이터 수준 기법으로는 소수 클래스 Oversampling, 다수 클래스 Undersampling, SMOTE/ADASYN 같은 합성 샘플 생성이 있습니다. 알고리즘 수준으로는 클래스 가중치, 임계값 조정, Focal loss가 효과적입니다. 분할은 반드시 Stratified K-Fold로 하고, 리샘플링은 학습 세트 내부에서만 수행해 누수를 막습니다.

---

# 41. Feature Scaling/Normalization은 언제, 어떻게 하나요?

거리 기반/기울기 기반 알고리즘(SVM, k-NN, 로지스틱/선형 회귀, 신경망)에서는 스케일 정규화가 필수입니다. Tree 기반 모델은 대체로 스케일에 둔감합니다. 표준화(Standardization, 평균 0·표준편차 1), 정규화(Min–Max), 이상치에 강한 Robust scaler가 대표적입니다. 로그 변환은 치우친 분포에 유용합니다. 모든 전처리는 학습 세트로만 적합(fit)하고, 변환(transform)은 검증/테스트에 적용해 데이터 누수를 방지합니다.

---

# 42. 데이터 누수(Data Leakage)는 무엇이며 어떻게 방지하나요?

누수는 학습 시점에 사용할 수 없는 정보가 모델에 흘러들어가 평가가 과대추정되는 현상입니다. 전처리를 전체 데이터로 먼저 수행(스케일링/특징 선택/임퓨테이션), 시계열에서 미래 정보 포함, 라벨 누출되는 파생 변수, 교차 집계 시 그룹 누락 등이 전형적 사례입니다. 방지하려면 시간/그룹을 보존한 분할을 먼저 수행하고, 파이프라인으로 각 Fold 안에서만 적합·변환을 수행합니다. 목표 인코딩은 누수 방지를 위한 K-fold 평균 또는 카테고리-평활화를 사용합니다.

---

# 43. 결측치(Missing Data)는 어떻게 처리하나요?

결측 메커니즘(MCAR/MAR/MNAR)을 먼저 가정하고 진단합니다. 단순 임퓨테이션(평균/중앙/최빈), KNN 임퓨테이션, 다중 임퓨테이션(MICE), 모델 기반 임퓨테이션 등을 사용합니다. 결측 여부 플래그를 함께 추가하면 도움이 될 때가 많습니다. 모든 임퓨터는 학습 세트로만 적합하고 검증/테스트에는 변환만 적용해 누수를 피합니다. 트리 계열은 결측에 비교적 관대하지만 일관된 처리 규칙을 유지해야 합니다.

---

# 44. 하이퍼파라미터 튜닝은 어떻게 설계하나요?

Grid search는 직관적이지만 차원이 늘면 비효율적입니다. Random search는 중요한 하이퍼파라미터에 더 빨리 도달하는 경향이 있어 실무 기본값으로 유용합니다. Bayesian optimization(TPE, Gaussian Process), Hyperband/ASHA 같은 예산 기반 방법은 탐색 효율을 높입니다. Stratified K-Fold 교차검증, 조기 종료, 탐색 공간의 합리적 스케일(로그 스케일) 설정, Nested CV로의 성능 추정 바이어스 제거가 모범 사례입니다.

---

# 45. 모델 설명가능성(Explainability)은 어떻게 확보하나요?

전역 수준에서는 Permutation importance, Feature importance, Partial Dependence Plot(PDP), Accumulated Local Effects(ALE)를 사용하고, 개별 예측 수준에서는 LIME과 SHAP이 대표적입니다. SHAP은 일관성 보장을 제공해 상호작용 해석에도 유리합니다. 상관된 특성에서는 중요도가 분산될 수 있고, 해석은 인과가 아니라 연관임을 명확히 해야 합니다. 규제/공정성 맥락에서는 그룹별 지표와 편향 감사를 병행합니다.

---

# 46. 운영 배포 후 모니터링과 드리프트 관리는 어떻게 하나요?

데이터 드리프트(입력 분포 변화), 컨셉 드리프트(입력–출력 관계 변화), 라벨 드리프트를 구분해 감시합니다. 특징 분포의 PSI/KL 거리를 모니터링하고, 예측 확률의 캘리브레이션, 지연 라벨을 고려한 성능 추적, 이상치 비율을 관찰합니다. Canary/Shadow 배포, 슬라이싱 지표, 재학습 트리거와 롤백 기준, 모델/데이터 버전 관리, 재현 가능한 파이프라인과 피처 스토어가 실무의 핵심입니다.

---

# 47. 오프라인 지표와 온라인 A/B 테스트는 어떻게 연결하나요?

오프라인에서는 교차검증으로 ROC-AUC, PR-AUC, F1 등으로 필터링하고, 온라인에서는 비즈니스 KPI(전환율, 유지율, 수익)로 검증합니다. 가설·지표·효과 크기를 선정하고 검정력/표본 크기를 설계합니다. SRM 점검, 중도 조회(피킹) 방지, 다중 실험 보정, 노벨티/학습 효과, 이질성 분석을 포함합니다. 순위·추천 문제에서는 역확률 가중(IPS) 등 반사실적 평가가 필요할 수 있습니다. 오프라인–온라인 간 불일치를 줄이려면 피처/분포 정합과 캘리브레이션을 점검합니다.

---

# 48. Batch Normalization과 Layer Normalization의 차이는 무엇인가요?

Batch Normalization은 미니배치 차원에서 정규화를 수행합니다. 각 특성에 대해 배치 내 샘플들의 평균과 분산을 계산해서 정규화하고, 학습 가능한 스케일과 시프트 파라미터로 표현력을 유지합니다. 이는 내부 공변량 변화를 줄여서 더 높은 학습률을 사용할 수 있게 하고, 학습을 안정화시키며 정규화 효과도 제공합니다.

하지만 Batch Normalization은 배치 크기에 민감하고, 작은 배치에서는 통계량 추정이 불안정해집니다. 또한 추론 시에는 학습 중 계산된 이동 평균 통계량을 사용해야 하므로 학습과 추론의 동작이 다릅니다. RNN이나 시퀀스 모델에서는 시간 축의 길이가 가변적이어서 적용이 어렵습니다.

Layer Normalization은 각 샘플 내에서 모든 특성에 대해 정규화를 수행합니다. 배치 크기와 무관하게 작동하므로 RNN, Transformer 같은 시퀀스 모델에 적합하고, 추론 시에도 학습과 동일한 방식으로 동작합니다. Transformer에서는 Layer Normalization이 표준이 되었고, BERT, GPT 같은 대규모 언어 모델에서도 사용됩니다.

Instance Normalization은 각 샘플의 각 채널마다 정규화하는 방식으로 스타일 전이 같은 이미지 생성 작업에 효과적이고, Group Normalization은 채널을 그룹으로 나눠서 정규화하는 중간 형태로 배치 크기가 작을 때 유용합니다.

---

# 49. 추천 시스템의 평가 지표는 무엇이 있나요?

추천 시스템은 랭킹 품질과 사용자 만족도를 모두 고려해야 합니다. 정확도 기반 지표로는 **Precision@K**와 **Recall@K**가 기본입니다. 상위 K개 추천 중 관련 아이템 비율과, 전체 관련 아이템 중 상위 K개에 포함된 비율을 측정합니다.

**MAP(Mean Average Precision)**은 각 관련 아이템 위치에서의 precision을 평균내서 순위를 고려합니다. **NDCG(Normalized Discounted Cumulative Gain)**는 관련도 점수와 위치를 모두 반영하는 표준 지표로, 상위에 더 관련 있는 아이템이 올수록 높은 점수를 줍니다. 로그 할인으로 하위 순위의 영향을 감소시킵니다.

$$
\text{DCG@K} = \sum_{i=1}^{K} \frac{rel_i}{\log_2(i+1)}, \quad \text{NDCG@K} = \frac{\text{DCG@K}}{\text{IDCG@K}}
$$

**MRR(Mean Reciprocal Rank)**은 첫 번째 관련 아이템의 순위 역수 평균으로, 검색이나 추천에서 간단하면서도 유용합니다. **Hit Rate@K**는 상위 K개에 관련 아이템이 하나라도 있는 비율입니다.

실제로는 다양성(Coverage, Diversity), 새로움(Novelty), 의외성(Serendipity), 공정성(Fairness) 같은 품질 지표도 함께 고려해야 합니다. 비즈니스 지표로는 CTR, 전환율, 세션 길이, 재방문율 등을 추적하고, A/B 테스트로 온라인 성능을 검증합니다.

---

# 50. Collaborative Filtering의 Cold Start 문제는 어떻게 해결하나요?

Cold Start는 새로운 사용자나 아이템에 대한 상호작용 이력이 없어서 추천이 어려운 문제입니다. 여러 전략을 조합해서 해결합니다.

**Content-based Filtering**과 결합한 하이브리드 접근이 가장 효과적입니다. 아이템 메타데이터(장르, 태그, 설명)나 사용자 프로필(인구통계, 관심사)을 활용해서 초기 추천을 제공합니다. **Factorization Machine**이나 **Deep Learning 모델**은 협업 필터링 신호와 콘텐츠 특성을 자연스럽게 통합할 수 있습니다.

새 사용자에게는 인기도 기반 추천, 다양한 카테고리의 대표 아이템 제시, 온보딩 설문 등으로 초기 선호를 빠르게 수집합니다. 새 아이템은 전문가 큐레이션, 유사 아이템 기반 추천, Explore-Exploit 전략(Thompson Sampling, Upper Confidence Bound)으로 노출을 확보합니다.

**Transfer Learning**으로 다른 도메인이나 관련 작업에서 학습한 표현을 활용할 수 있고, **Meta-Learning**은 소량의 상호작용으로 빠르게 적응하는 능력을 학습합니다. 소셜 네트워크 정보가 있다면 친구나 유사 사용자의 선호를 활용하는 것도 효과적입니다.

---

# 51. 시계열 데이터의 교차 검증은 어떻게 하나요?

일반 K-Fold는 시간 순서를 무시하므로 미래 정보가 누수되어 부적절합니다. 시계열에서는 **Time Series Split** 또는 **Walk-Forward Validation**을 사용합니다.

가장 단순한 방법은 데이터를 시간 순서대로 train-validation-test로 분할하는 것입니다. 하지만 이는 검증 세트가 하나뿐이라 분산이 크고, 데이터 크기가 작을 때 비효율적입니다.

**Rolling Window** 방식은 고정 크기의 학습 윈도우를 시간 순서대로 이동시키면서 여러 fold를 만듭니다. 각 fold에서 학습 세트는 과거 N개 기간, 검증 세트는 그 다음 M개 기간을 사용합니다. 계절성이 있다면 윈도우 크기를 계절 주기의 배수로 설정하는 것이 좋습니다.

**Expanding Window**는 학습 세트를 누적해서 사용합니다. 첫 fold는 처음 N개 기간으로 학습하고, 두 번째 fold는 처음 N+M개 기간으로 학습하는 식입니다. 더 많은 데이터로 학습할 수 있지만, 오래된 데이터가 관련성이 낮아질 수 있습니다.

**Blocked Cross-Validation**은 시간 블록을 만들어서 각 블록을 순차적으로 검증 세트로 사용합니다. Gap을 두어서 학습과 검증 사이에 버퍼 기간을 설정하면 단기 자기상관의 영향을 줄일 수 있습니다.

시계열 분할 시에는 계절성, 트렌드, 구조적 변화를 고려해야 하고, 학습 기간이 검증보다 충분히 길어야 하며, 최종 테스트는 가장 최근 기간으로 설정해 실제 배포 상황을 반영해야 합니다.

---

# 52. Autoencoder는 어떻게 활용되나요?

Autoencoder는 입력을 압축된 잠재 표현으로 인코딩하고 다시 복원하도록 학습하는 비지도 신경망입니다. 인코더가 차원을 축소하고, 디코더가 복원하며, 복원 오차를 최소화하는 방향으로 학습됩니다.

**차원 축소**에서는 PCA의 비선형 확장으로 사용되며, 중간 층의 잠재 표현을 특징으로 추출할 수 있습니다. **이상치 탐지**에서는 정상 데이터로만 학습한 후, 복원 오차가 큰 샘플을 이상치로 판단합니다. 신용카드 사기 탐지, 제조 결함 검사, 네트워크 침입 탐지 등에 활용됩니다.

**Denoising Autoencoder**는 입력에 노이즈를 추가하고 원본을 복원하도록 학습해서 강건한 특징을 학습합니다. **Variational Autoencoder(VAE)**는 잠재 공간을 확률 분포로 모델링해서 새로운 샘플을 생성할 수 있으며, 이미지 생성, 분자 설계, 데이터 증강에 사용됩니다.

**추천 시스템**에서는 사용자-아이템 행렬을 복원하도록 학습해서 협업 필터링을 수행합니다. **전처리**로는 고차원 데이터를 압축해서 다운스트림 모델의 입력으로 사용하고, **표현 학습**으로는 라벨이 없는 대량의 데이터에서 유용한 특징을 미리 학습한 후 전이 학습에 활용합니다.

---

# 53. Gradient Boosting의 변형들(XGBoost, LightGBM, CatBoost)의 차이는 무엇인가요?

세 모델 모두 Gradient Boosting의 효율성과 성능을 개선한 구현체지만 각기 다른 최적화 전략을 사용합니다.

**XGBoost**는 정규화 항을 추가한 목적 함수, 2차 테일러 근사를 사용한 최적화, 분할 찾기의 근사 알고리즘, 희소성 인식 처리, 병렬 처리와 캐시 최적화로 속도를 크게 향상시켰습니다. Level-wise 트리 성장 방식을 사용하고, 정규화와 조기 종료로 과적합을 방지합니다. 범용성이 뛰어나서 캐글에서 가장 널리 사용됩니다.

**LightGBM**은 Leaf-wise 트리 성장으로 더 깊고 비대칭적인 트리를 만들어 정확도를 높이고, Gradient-based One-Side Sampling(GOSS)로 기울기가 큰 샘플에 집중하며, Exclusive Feature Bundling(EFB)로 상호 배타적인 특성을 묶어서 차원을 줄입니다. 대용량 데이터와 고차원 특성에서 XGBoost보다 훨씬 빠르지만, 작은 데이터에서는 과적합 위험이 있습니다.

**CatBoost**는 범주형 변수를 자동으로 처리하는 Ordered Target Statistics로 목표 누수를 방지하고, Symmetric 트리 구조로 예측 속도를 최적화하며, Ordered Boosting으로 예측 편향을 줄입니다. 범주형 특성이 많은 데이터에서 전처리 없이 바로 사용할 수 있어 편리하고, 하이퍼파라미터 튜닝에 덜 민감합니다.

실무에서는 데이터 크기(대용량이면 LightGBM), 범주형 특성 비율(많으면 CatBoost), 안정성과 범용성(XGBoost), 학습 속도 요구사항을 고려해서 선택합니다. 앙상블로 세 모델을 결합하면 더 좋은 성능을 얻을 수 있습니다.

---

# 54. Transfer Learning과 Fine-tuning은 어떻게 다르고 언제 사용하나요?

Transfer Learning은 한 작업에서 학습한 지식을 다른 작업에 전이하는 일반적인 접근법입니다. 대용량 데이터셋(ImageNet, Wikipedia)으로 사전 훈련된 모델을 가져와서 새로운 작업에 활용하는 방식입니다.

가장 단순한 방법은 **Feature Extraction**입니다. 사전 훈련된 모델의 가중치를 동결하고 마지막 층만 새로운 작업에 맞춰 학습시킵니다. 데이터가 매우 적거나, 새 작업이 사전 훈련 작업과 유사할 때 효과적입니다. 계산 비용이 낮고 과적합 위험이 적습니다.

**Fine-tuning**은 사전 훈련된 가중치를 초기값으로 사용하되, 전체 또는 일부 층을 새로운 데이터로 재학습시킵니다. 보통 낮은 학습률을 사용해서 사전 학습된 표현을 크게 훼손하지 않으면서 새 작업에 맞춥니다. 상위 층부터 점진적으로 해동하는(progressive unfreezing) 전략이 효과적입니다.

데이터 크기와 유사도에 따라 전략을 선택합니다. 새 데이터가 적고 유사하면 마지막 층만 학습, 새 데이터가 적고 다르면 더 많은 층을 학습하되 정규화 강화, 새 데이터가 많고 유사하면 전체를 낮은 학습률로 fine-tune, 새 데이터가 많고 다르면 사전 훈련 모델을 단순 초기화로 사용합니다.

**Domain Adaptation**은 소스와 타겟 도메인 간 분포 차이를 명시적으로 다루고, **Few-shot Learning**은 극소량 데이터에서 빠르게 적응하는 메타 학습 기법입니다. **BERT, GPT, Vision Transformer** 같은 대규모 사전 훈련 모델은 fine-tuning으로 다양한 다운스트림 작업에서 최고 성능을 달성했습니다.

---

# 55. Batch Size가 모델 학습에 미치는 영향은 무엇인가요?

Batch size는 학습 속도, 메모리 사용량, 일반화 성능에 모두 영향을 미치는 중요한 하이퍼파라미터입니다.

**큰 배치**는 기울기 추정이 안정적이고 분산이 낮아서 수렴이 부드럽습니다. GPU 병렬 처리를 효율적으로 활용해서 epoch당 시간이 단축되고, Batch Normalization의 통계량 추정이 정확해집니다. 하지만 메모리 요구량이 크고, 날카로운(sharp) 최솟값에 수렴하는 경향이 있어 일반화 성능이 떨어질 수 있으며, 더 많은 epoch가 필요해서 전체 학습 시간이 길어질 수 있습니다.

**작은 배치**는 기울기에 노이즈가 많아서 날카로운 최솟값을 벗어나 평평한(flat) 최솟값을 찾는 경향이 있어 일반화 성능이 좋습니다. 정규화 효과가 있고, 메모리 효율적이며, 파라미터 업데이트가 빈번해서 빠르게 수렴할 수 있습니다. 하지만 학습이 불안정하고, Batch Normalization이 부정확해지며, GPU 활용률이 낮아집니다.

실무적으로는 하드웨어 메모리 한계 내에서 가능한 큰 배치를 사용하되, 일반화 성능을 유지하기 위해 **학습률 스케일링**(Linear scaling rule: 배치 크기가 k배 증가하면 학습률도 k배 증가), **Warmup**(초기에 작은 학습률로 시작해서 점진적으로 증가), **Layer-wise Adaptive Rate Scaling(LARS)**나 **LAMB** 같은 대규모 배치 최적화 기법을 사용합니다.

일반적인 시작점은 32-256 사이이며, 작업과 모델에 따라 조정합니다. **Gradient Accumulation**을 사용하면 메모리 제약 하에서도 큰 effective batch size를 구현할 수 있습니다.

---

# 56. Learning Rate Scheduling은 왜 필요하고 어떤 방법이 있나요?

고정 학습률은 초기에는 너무 느리게 수렴하고, 후기에는 최적점 주변에서 진동하는 문제가 있습니다. Learning rate scheduling은 학습 과정에 따라 학습률을 조정해서 빠른 수렴과 안정적인 최적화를 모두 달성합니다.

**Step Decay**는 정해진 epoch마다 학습률을 일정 비율(예: 0.1배)로 감소시킵니다. 간단하지만 효과적이며, 구간 설정이 중요합니다. **Exponential Decay**는 매 step마다 지수적으로 감소시켜 부드러운 곡선을 만듭니다.

**Cosine Annealing**은 코사인 함수로 학습률을 부드럽게 감소시킵니다. 주기적으로 재시작하는 Cosine Annealing with Warm Restarts(SGDR)는 여러 local minima를 탐색할 수 있습니다:

$$
\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{T_{cur}}{T_{max}}\pi))
$$

**ReduceLROnPlateau**는 검증 성능이 개선되지 않으면 학습률을 감소시키는 적응적 방법입니다. 자동으로 조정되어 편리하지만 노이즈에 민감할 수 있습니다.

**Warmup**은 초기에 작은 학습률로 시작해서 점진적으로 증가시킨 후 스케줄링을 시작합니다. 큰 배치나 Transformer 같은 복잡한 모델에서 학습 안정성을 크게 향상시킵니다. 대부분의 현대 모델에서 표준적으로 사용됩니다.

**Cyclical Learning Rate**는 학습률을 주기적으로 증가·감소시켜서 saddle point를 벗어나고 더 나은 최적점을 찾습니다. **One Cycle Policy**는 단일 사이클로 빠르게 학습하는 방법으로, 최고 학습률까지 증가 후 감소시키며 momentum을 반대로 조정합니다.

실무에서는 Warmup + Cosine Annealing이나 Warmup + Step Decay를 많이 사용하며, Adaptive optimizer(Adam)와 결합하면 더 안정적입니다.

---

# 57. 모델 압축(Model Compression) 기법에는 어떤 것이 있나요?

대규모 모델을 경량화해서 모바일이나 엣지 디바이스에 배포하거나 추론 속도를 높이기 위해 모델 압축을 사용합니다.

**Quantization(양자화)**는 가중치와 활성화를 저정밀도(FP16, INT8)로 변환합니다. Post-Training Quantization은 학습 후 변환해서 간단하지만 정확도 손실이 있고, Quantization-Aware Training은 학습 중 양자화 오차를 시뮬레이션해서 정확도를 보존합니다. 4-8배 압축과 2-4배 속도 향상을 달성하면서 정확도 손실을 1% 미만으로 유지할 수 있습니다.

**Pruning(가지치기)**는 중요하지 않은 가중치나 뉴런을 제거합니다. Unstructured pruning은 개별 가중치를 0으로 만들어 희소성을 높이고, Structured pruning은 전체 필터나 채널을 제거해서 실제 속도 향상을 얻습니다. Magnitude-based, gradient-based 기준으로 중요도를 평가하며, iterative pruning으로 점진적으로 제거하면서 재학습합니다. Lottery Ticket Hypothesis는 초기화가 좋은 작은 서브네트워크가 존재한다는 흥미로운 발견입니다.

**Knowledge Distillation**은 큰 teacher 모델의 지식을 작은 student 모델로 전이합니다. Student는 hard label 대신 teacher의 soft probability를 학습해서 더 풍부한 정보를 얻습니다. Teacher의 중간 표현도 매칭하면 더 효과적입니다. BERT의 DistilBERT, MobileBERT 등이 대표적 사례입니다.

**Low-Rank Factorization**은 가중치 행렬을 저차원 행렬의 곱으로 분해해서 파라미터를 줄입니다. **Neural Architecture Search(NAS)**는 효율적인 아키텍처를 자동으로 찾으며, **Efficient Architectures**(MobileNet, EfficientNet, TinyBERT)는 처음부터 경량화를 고려해 설계됩니다.

실무에서는 Quantization + Pruning을 결합하거나, Distillation으로 학습한 후 Quantization을 적용하는 multi-stage 압축이 효과적입니다. 목표 플랫폼의 하드웨어 지원(TensorRT, CoreML, ONNX Runtime)을 고려해야 합니다.

---

# 58. Ensemble 전략을 실무에서 어떻게 구성하나요?

Ensemble은 여러 모델의 예측을 결합해서 개별 모델보다 나은 성능과 안정성을 얻는 강력한 기법입니다. 효과적인 앙상블은 다양성과 정확도의 균형이 핵심입니다.

**모델 다양성 확보**가 가장 중요합니다. 서로 다른 알고리즘(Tree-based, Neural Network, Linear model), 다른 특성 부분집합, 다른 하이퍼파라미터, 다른 데이터 샘플(Bagging, Bootstrap)을 사용해서 각 모델이 다른 패턴을 학습하도록 합니다. 상관도가 낮을수록 앙상블 효과가 큽니다.

**가중 평균**은 검증 성능에 비례하거나, 최적화로 가중치를 학습합니다. **Voting**은 Hard voting(다수결)과 Soft voting(확률 평균)으로 나뉩니다. **Stacking**은 1차 모델들의 예측을 메타 모델이 학습하며, 2-3단계 계층 구조로 구성합니다. Out-of-fold 예측을 사용해 누수를 방지해야 합니다.

**Blending**은 홀드아웃 세트로 메타 모델을 학습하는 간단한 변형입니다. **Snapshot Ensemble**은 단일 모델을 주기적으로 저장해서 앙상블하는 효율적 방법입니다.

실무에서는 5-10개 다양한 모델로 시작하고, 상관도가 낮고 성능이 좋은 모델들을 선택합니다. 계산 비용과 성능 향상을 고려해서 앙상블 크기를 결정하며, 보통 10-20개를 초과하면 수익 체감이 큽니다. **Greedy forward selection**으로 모델을 하나씩 추가하면서 검증 성능을 모니터링합니다.

캐글에서는 수십 개 모델의 다층 스태킹이 표준이지만, 프로덕션에서는 추론 비용과 유지보수를 고려해 3-5개 모델의 단순 평균이나 2단계 스태킹을 많이 사용합니다.

---

# 59. Feature Engineering에서 도메인 지식을 어떻게 활용하나요?

Feature engineering은 원시 데이터를 모델이 학습하기 좋은 형태로 변환하는 과정이며, 도메인 지식이 가장 큰 차이를 만드는 영역입니다.

**시간 특성**에서는 날짜를 연/월/일/요일/시간으로 분해하고, 주말/공휴일 플래그, 월초/월말, 분기, 계절을 추가합니다. 금융에서는 결산 시즌, 리테일에서는 블랙프라이데이 같은 도메인 특정 이벤트를 인코딩합니다. Lag 특성, 이동 평균, 차분으로 시계열 패턴을 포착합니다.

**지리 정보**는 위경도를 거리, 클러스터, 행정구역으로 변환하고, 인근 POI(관심 지점) 개수, 인구 밀도, 소득 수준 같은 외부 데이터를 결합합니다. 리테일이라면 경쟁사 매장과의 거리, 교통 접근성을 추가할 수 있습니다.

**비율과 상호작용**은 도메인 직관을 반영합니다. 신용 평가에서 부채/소득 비율, 리테일에서 재구매율, 광고에서 CTR(클릭/노출)은 개별 값보다 강력합니다. 카테고리 간 상호작용(지역×시간대, 상품×고객군)도 유용합니다.

**집계 특성**은 사용자/상품/지역 단위로 통계량을 계산합니다. 평균, 표준편차, 최소/최대, 분위수, 트렌드를 시간 윈도우별로 만들고, 목표 인코딩으로 과거 타겟 평균을 특성화합니다. 누수 방지를 위해 시간 순서와 교차검증을 신중히 다뤄야 합니다.

**텍스트와 카테고리**는 빈도, TF-IDF, 임베딩으로 변환하고, 카테고리 조합이나 희귀 범주 묶기를 시도합니다. 도메인에서 중요한 키워드(법률 문서의 조항, 의료 기록의 증상)를 명시적으로 추출합니다.

**정규화와 변환**도 도메인 지식을 반영합니다. 우편향 분포는 로그 변환, 주기성은 sin/cos 인코딩, 이상치는 윈저화나 분위수 변환을 적용합니다.

실무에서는 도메인 전문가와 협업해서 비즈니스 규칙을 특성으로 구현하고, EDA로 패턴을 발견하며, 반복적으로 특성을 추가·제거·수정합니다. AutoML이나 딥러닝이 특성 추출을 자동화하지만, 잘 설계된 도메인 특성은 여전히 큰 성능 향상을 가져옵니다.

---

# 60. 인과 추론(Causal Inference)과 예측 모델의 차이는 무엇인가요?

예측 모델은 "X가 주어졌을 때 Y는 무엇인가?"를 답하지만, 인과 추론은 "X를 변화시키면 Y가 어떻게 바뀌는가?"를 답합니다. 예측은 상관관계로 충분하지만, 의사결정에는 인과관계가 필요합니다.

**예측 모델**은 P(Y|X)를 추정하며, 관찰된 패턴을 학습합니다. 고차원 특성, 복잡한 비선형성, 대량 데이터를 효과적으로 다루지만, 개입(intervention)이나 반사실(counterfactual) 질문에는 답할 수 없습니다. "이 환자가 아플 확률은?"에는 답하지만 "이 치료가 환자를 낫게 할까?"에는 답하지 못합니다.

**인과 추론**은 P(Y|do(X))를 추정하며, X를 강제로 설정했을 때의 효과를 평가합니다. Randomized Controlled Trial(RCT)이 황금 표준이지만, 관찰 데이터에서는 교란 변수(confounder)를 통제해야 합니다. **Potential Outcomes Framework**(Rubin), **Structural Causal Models**(Pearl), **DAG(방향성 비순환 그래프)**로 인과 구조를 명시합니다.

주요 기법으로는 **성향 점수(Propensity Score)** 매칭/가중/층화, **Inverse Probability Weighting(IPW)**, **Doubly Robust** 추정, **Instrumental Variables**, **Difference-in-Differences**, **Regression Discontinuity** 등이 있습니다. 최근에는 머신러닝을 결합한 **Causal Forest**, **Meta-learners**(S-learner, T-learner, X-learner), **DoWhy**, **EconML** 같은 프레임워크가 발전하고 있습니다.

실무에서는 A/B 테스트로 인과 효과를 직접 측정하거나, 관찰 데이터에서 준실험 설계를 찾습니다. 예측 모델로 성향 점수나 결과를 추정하되, 인과 가정(무혼란성, 양성, 일관성)을 검증하고 민감도 분석을 수행해야 합니다. 마케팅 캠페인 효과, 정책 영향 평가, 개인화 치료 추천 등에 필수적입니다.

---

# 61. CTR(Click-Through Rate) 예측 모델은 어떻게 설계하나요?

CTR 예측은 광고 산업의 핵심 문제로, 사용자가 광고를 클릭할 확률을 예측해서 광고 랭킹과 입찰 최적화에 사용됩니다. 이진 분류 문제지만 극심한 클래스 불균형(CTR 보통 0.1-3%)과 실시간 추론 요구사항이 특징입니다.

**Feature Engineering**이 매우 중요합니다. 사용자 특성(인구통계, 관심사, 과거 행동 이력), 광고 특성(카테고리, 크리에이티브 요소, 랜딩 페이지), 컨텍스트 특성(시간, 요일, 기기, 위치, 페이지 컨텐츠), 교차 특성(사용자×광고, 사용자×시간, 광고×위치)을 체계적으로 설계합니다. 특히 ID 특성(user_id, ad_id, advertiser_id)의 임베딩이 핵심입니다.

**모델 아키텍처**로는 전통적으로 Logistic Regression이 해석 가능성과 속도로 많이 쓰였지만, 현대에는 **GBDT + LR**, **Factorization Machine(FM)**, **Field-aware FM(FFM)**이 표준입니다. 딥러닝에서는 **Wide & Deep**, **DeepFM**, **xDeepFM**, **Deep & Cross Network(DCN)** 같은 아키텍처가 저차원 특성과 고차원 교차를 동시에 학습합니다.

**Calibration**이 필수적입니다. 예측 확률이 실제 CTR과 일치해야 예산 계획과 입찰 전략이 정확해집니다. Platt scaling이나 Isotonic regression으로 보정하고, 주기적으로 재보정합니다.

**평가 지표**로는 AUC(랭킹 품질), Log-Loss(확률 정확도), Calibration 지표를 함께 봅니다. 실제로는 온라인 A/B 테스트에서 실제 CTR, 수익(Revenue), 사용자 경험 지표로 최종 검증합니다.

**운영 고려사항**으로는 실시간 추론(수 ms 이내), 데이터 신선도 유지를 위한 온라인 학습이나 빈번한 재학습, 새 광고/사용자의 Cold Start 처리, Feature drift 모니터링이 중요합니다. Negative Sampling으로 학습 효율을 높이고, Importance Sampling으로 편향을 보정합니다.

---

# 62. CVR(Conversion Rate) 예측과 CTR 예측의 차이는 무엇인가요?

CVR 예측은 클릭한 사용자가 전환(구매, 가입 등)할 확률을 예측하는 문제로, CTR 예측과는 다른 도전 과제가 있습니다.

가장 큰 차이는 **Sample Selection Bias**입니다. CVR 모델은 클릭한 샘플만으로 학습되므로, 전체 노출에 대한 전환율을 정확히 예측하지 못할 수 있습니다. 클릭하는 사용자는 이미 어느 정도 관심이 있는 편향된 집단이기 때문입니다.

**데이터 희소성**도 문제입니다. CTR은 0.1-3%지만 CVR은 그보다 훨씬 낮아서(보통 1-10%) 양성 샘플이 극히 적고, 새 광고에 대한 전환 데이터가 매우 부족합니다.

**Delayed Conversion**도 고려해야 합니다. 클릭과 전환 사이에 시간차가 있고(몇 시간~몇 주), 이 기간에 attribution window를 어떻게 설정하느냐가 중요합니다. 학습 데이터 구성 시 충분한 observation window를 두어야 합니다.

이를 해결하기 위한 모델들이 있습니다. **ESMM(Entire Space Multi-Task Model)**은 CTR과 CVR을 동시에 학습하되, CVR 예측에 전체 노출 데이터를 사용해 selection bias를 완화합니다. pCTR × pCVR = pCTCVR 관계를 이용합니다.

$$
p(conversion|impression) = p(click|impression) \times p(conversion|click)
$$

**Multi-Task Learning**으로 클릭, 전환, 체류 시간, 장바구니 추가 등 여러 목표를 함께 학습하면 데이터 효율성과 일반화가 향상됩니다. Shared-bottom, MMoE(Multi-gate Mixture-of-Experts), PLE(Progressive Layered Extraction) 같은 아키텍처를 사용합니다.

**Transfer Learning**으로 유사 카테고리나 제품에서 학습한 표현을 활용하거나, Pre-training으로 대규모 CTR 데이터로 먼저 학습한 후 CVR에 fine-tuning할 수 있습니다.

실무에서는 pCTR과 pCVR을 곱해서 eCPM(expected Cost Per Mille)이나 expected revenue를 계산하고, 이를 기준으로 광고를 랭킹합니다. 각 단계의 캘리브레이션이 정확해야 전체 수익 예측이 신뢰할 만합니다.

---

# 63. 광고 경매와 입찰 전략(Bidding Strategy)을 어떻게 설계하나요?

광고 경매는 제한된 광고 슬롯에 여러 광고주가 경쟁하는 메커니즘으로, 대부분 **Second-Price Auction**(Vickrey Auction)이나 **Generalized Second-Price(GSP)** 방식을 사용합니다. 최고 입찰자가 이기지만, 두 번째로 높은 입찰가를 지불하는 구조로 진실된 가치 입찰을 유도합니다.

**랭킹 함수**는 단순 입찰가가 아니라 **bid × quality_score** 형태입니다. Quality score는 예상 CTR, 광고 품질, 사용자 경험을 반영해서 플랫폼 수익과 사용자 만족을 동시에 최적화합니다:

$$
\text{Ad Rank} = \text{Bid} \times \text{pCTR} \times \text{Quality Factor}
$$

**입찰 전략**은 광고주의 목표에 따라 달라집니다. **CPC(Cost Per Click)** 목표라면 클릭당 원하는 비용 이하로 유지하도록 입찰을 조정하고, **CPA(Cost Per Acquisition)** 목표라면 전환당 비용을 목표치에 맞춥니다. **ROAS(Return on Ad Spend)** 목표는 광고 수익/비용 비율을 최대화합니다.

**자동 입찰**은 머신러닝으로 실시간 입찰가를 결정합니다. pCTR과 pCVR을 예측해서 expected value를 계산하고, 목표 CPA나 ROAS를 만족하도록 입찰가를 역산합니다:

$$
\text{Optimal Bid} = \text{Target CPA} \times p(\text{conversion}|\text{impression})
$$

**예산 관리(Budget Pacing)**도 중요합니다. 하루 예산을 시간대별로 분배해서 조기 소진을 방지하고, 높은 가치 기회에 더 많이 투자합니다. **PID 제어기**나 **강화학습**으로 실시간 페이싱을 조정합니다.

**실시간 입찰(Real-Time Bidding, RTB)**에서는 100ms 이내에 입찰 결정을 내려야 하므로, 경량 모델, 캐싱, Feature 사전 계산이 필수입니다. 모델 서빙 인프라와 latency 최적화가 핵심입니다.

**Multi-Armed Bandit**이나 **Thompson Sampling**으로 Explore-Exploit 균형을 맞춰서 새 광고나 타겟의 성능을 빠르게 학습하면서도 검증된 전략을 활용합니다.

실무에서는 시뮬레이션으로 입찰 전략을 테스트하고, A/B 테스트로 실제 성과를 검증하며, 경매 역학(경쟁자 행동)과 시장 변화에 적응하는 피드백 루프를 구축합니다.

---

# 64. Attribution 모델링(어느 터치포인트에 전환을 귀속시킬 것인가)은 어떻게 하나요?

Attribution은 사용자가 여러 광고 접점을 거쳐 전환할 때, 각 터치포인트에 얼마만큼의 크레딧을 줄지 결정하는 문제입니다. 광고 효과 측정과 예산 배분에 결정적인 영향을 미칩니다.

**Rule-based Attribution**은 간단하지만 제한적입니다. **Last-Click**은 전환 직전 광고에 100% 귀속하는 가장 단순한 방법이지만, 초기 인지 단계의 기여를 무시합니다. **First-Click**은 첫 접점에, **Linear**는 모든 터치포인트에 균등 배분, **Time-Decay**는 전환에 가까울수록 높은 가중치를 주는 방식입니다.

**Data-Driven Attribution**은 실제 전환 데이터에서 각 채널의 증분 효과를 학습합니다. **Shapley Value**는 게임 이론에서 유래한 방법으로, 모든 가능한 터치포인트 조합에서 각 채널의 marginal contribution을 계산합니다. 공정한 크레딧 배분을 보장하지만 계산 비용이 높아서 근사 방법을 씁니다.

**Markov Chain** 모델은 사용자 여정을 상태 전이로 모델링합니다. 특정 채널을 제거했을 때 전환 확률이 얼마나 감소하는지를 각 채널의 기여도로 측정합니다. 채널 간 전이 패턴을 학습해서 고객 여정을 더 정교하게 표현합니다.

**Survival Analysis**나 **Hazard Model**은 시간까지 고려해서 각 터치포인트가 전환 확률을 얼마나 높이는지 추정합니다. Cox Proportional Hazards Model이 대표적입니다.

딥러닝 기반으로는 **Recurrent Neural Network**나 **Transformer**로 시퀀스를 모델링하고, Attention 메커니즘으로 각 터치포인트의 중요도를 학습할 수 있습니다.

**실무적 고려사항**으로는 attribution window(클릭 후 7일, 뷰 후 1일 등) 설정, Cross-device 추적(모바일-데스크톱), View-through vs Click-through 구분, Privacy 규제(쿠키 제한, GDPR) 대응이 있습니다.

**Incrementality Testing**으로 attribution 모델을 검증합니다. 특정 채널을 켜고 끄는 실험(Ghost Ads, PSA)으로 진짜 증분 효과를 측정하고, attribution 모델이 이를 정확히 반영하는지 확인합니다.

실무에서는 비즈니스 목표에 따라 여러 attribution 모델을 병행하고, 채널 최적화와 전략 수립에는 data-driven attribution을, 실시간 입찰에는 last-click 같은 단순 모델을 사용하는 등 맥락에 맞게 선택합니다.

---

# 65. Multi-Objective Optimization을 광고에서 어떻게 적용하나요?

광고 시스템은 클릭, 전환, 수익, 사용자 경험, 광고 다양성 등 여러 목표를 동시에 최적화해야 합니다. 이들은 종종 상충하기 때문에 trade-off를 잘 관리하는 것이 핵심입니다.

**Weighted Sum** 접근은 각 목표에 가중치를 주고 선형 결합하는 가장 단순한 방법입니다:

$$
\text{Score} = w_1 \cdot \text{pCTR} + w_2 \cdot \text{pCVR} + w_3 \cdot \text{EngagementScore}
$$

가중치를 비즈니스 가치에 맞춰 설정하고(예: 전환당 $10, 클릭당 $0.5), A/B 테스트로 최적 가중치를 찾습니다. 하지만 목표 간 스케일 차이가 크고, 단순 선형 결합은 복잡한 trade-off를 표현하기 어렵습니다.

**Multi-Task Learning**은 여러 목표를 공유 표현에서 동시에 학습합니다. **Hard parameter sharing**은 하위 층을 공유하고 각 목표마다 별도 출력 층을 두는 방식이고, **Soft parameter sharing**은 각 목표별 네트워크를 두되 정규화로 가깝게 유지합니다.

**MMoE(Multi-gate Mixture-of-Experts)**는 여러 expert 네트워크를 두고, 각 목표마다 gating 네트워크가 expert를 선택적으로 조합합니다. 목표 간 상관이 낮거나 충돌할 때 효과적입니다:

$$
y_k = \sum_{i=1}^{n} g_k^{(i)}(x) \cdot f_i(x)
$$

**PLE(Progressive Layered Extraction)**는 공유 expert와 목표별 전용 expert를 계층적으로 구성해서 공유와 분리를 더 세밀하게 조정합니다.

**Pareto Optimization**은 한 목표를 개선하면서 다른 목표를 악화시키지 않는 Pareto frontier를 찾습니다. 여러 모델을 학습해서 trade-off curve를 그리고, 비즈니스 요구에 맞는 지점을 선택합니다.

**제약 최적화**로는 주 목표를 최대화하되 다른 목표들을 제약으로 두는 방식입니다. 예를 들어 수익을 최대화하되 CTR이 특정 값 이상, 광고 다양성 지수가 일정 수준 이상 유지되도록 합니다.

**실무 예시**로 광고 랭킹에서는 pCTR × pCVR × bid로 수익을 최대화하되, diversity penalty로 같은 광고주 연속 노출을 방지하고, quality threshold로 낮은 품질 광고를 걸러내며, user fatigue model로 같은 광고 반복을 억제합니다.

**피드 랭킹**에서는 engagement(클릭, 좋아요)와 time-spent를 함께 최적화하되, 클릭베이트는 패널티를 주고, 컨텐츠 다양성과 신선도를 보상합니다.

평가는 각 목표별 개별 지표와 전체 비즈니스 KPI(DAU, 수익, 사용자 만족도)를 모두 추적하고, 온라인 A/B 테스트로 실제 영향을 검증합니다.

---

# 66. Explore-Exploit Trade-off를 광고에서 어떻게 다루나요?

새 광고나 타겟의 성능을 학습하면서(Explore) 동시에 검증된 전략으로 수익을 최대화(Exploit)해야 하는 딜레마입니다. 충분한 탐색 없이는 더 나은 기회를 놓치지만, 과도한 탐색은 단기 성과를 희생합니다.

**ε-Greedy**는 가장 단순한 방법으로, ε 확률로 랜덤 선택(explore), 1-ε 확률로 최선 선택(exploit)을 합니다. 간단하지만 불확실성을 고려하지 않고, 명확히 나쁜 옵션도 계속 탐색합니다.

**Upper Confidence Bound(UCB)**는 각 옵션의 예상 보상에 불확실성(표준편차)을 더해서 선택합니다:

$$
\text{UCB}_i = \bar{X}_i + c\sqrt{\frac{\ln N}{n_i}}
$$

샘플이 적은 옵션은 불확실성이 커서 더 탐색되고, 충분히 시도한 옵션은 실제 성능에 수렴합니다. 이론적으로 regret 상한이 보장됩니다.

**Thompson Sampling**은 베이지안 접근으로, 각 옵션의 성능 분포에서 샘플링하고 가장 높은 값을 선택합니다. 불확실성이 클수록 분포가 넓어서 자연스럽게 탐색 확률이 높아집니다. 실무에서 성능이 우수하고 구현도 간단합니다:

```python
for arm in arms:
    theta_arm = sample_from_posterior(alpha[arm], beta[arm])
selected_arm = argmax(theta_arm)
```

**Contextual Bandits**는 컨텍스트(사용자 특성, 시간 등)를 고려해서 각 상황에 맞는 최적 행동을 학습합니다. **LinUCB**, **Neural Bandit**, **Deep Contextual Bandits** 등이 있으며, 광고 개인화에 적합합니다.

**실무 적용**으로 **새 광고 Cold Start**에서는 초기 몇 시간 동안 강제 노출로 빠르게 성능을 측정하고, Thompson Sampling으로 점진적으로 노출을 조정합니다. **A/B 테스트 동적 할당**으로 성능이 좋은 variant에 더 많은 트래픽을 보내서 regret을 줄일 수 있습니다.

**예산 제약**이 있으면 Knapsack Bandit이나 Budgeted Thompson Sampling을 사용합니다. **지연 피드백**이 있으면(전환은 나중에 발생) 중간 신호(CTR)를 보상 대리로 쓰거나, Bayesian 업데이트를 지연시킵니다.

**실전 팁**으로는 exploit에 편향되도록 초기 prior를 설정(낙관적 초기화), 명확히 나쁜 옵션은 조기 중단, 계절성이나 트렌드 변화에는 시간 감쇠나 주기적 리셋을 적용합니다. 또한 완전 랜덤 트래픽 일부를 유지해서 distribution shift를 감지합니다.

---

# 67. Lookalike Modeling(유사 고객 확장)은 어떻게 동작하나요?

Lookalike modeling은 고가치 고객(전환한 사용자, VIP 고객)과 유사한 특성을 가진 새로운 잠재 고객을 찾는 기법으로, 타겟 오디언스를 효과적으로 확장합니다.

**기본 접근**은 전환 사용자를 양성, 나머지를 음성으로 하는 이진 분류 모델을 학습하는 것입니다. 로지스틱 회귀, GBDT, 신경망으로 전환 확률을 예측하고, 상위 확률을 가진 사용자를 lookalike 오디언스로 선정합니다.

**Feature Engineering**이 핵심입니다. 인구통계(나이, 성별, 지역), 행동 특성(방문 페이지, 관심 카테고리, 앱 사용 패턴), 구매 이력, 사용 기기, 시간대 패턴 등을 종합합니다. 시드 고객 집단의 특성 분포를 분석해서 차별적 특성을 발견합니다.

**Embedding 기반** 방법은 사용자를 저차원 벡터 공간에 임베딩하고, 시드 사용자와의 거리나 유사도로 lookalike를 찾습니다. User2Vec, Graph Embedding(DeepWalk, Node2Vec)으로 소셜 그래프나 행동 그래프에서 표현을 학습하면 풍부한 유사성을 포착할 수 있습니다.

**Transfer Learning**으로 대규모 사용자 데이터에서 사전 학습한 표현을 활용하고, 특정 시드 집단에 fine-tuning합니다. 시드 사이즈가 작을 때 효과적입니다.

**Collaborative Filtering** 관점에서는 전환한 사용자들과 유사한 아이템을 소비한 사용자를 찾거나, 유사한 사용자가 전환한 광고주를 찾는 방식입니다.

**확장 정도 조절**이 중요합니다. 너무 좁으면(1% lookalike) 시드와 매우 유사하지만 규모가 작고, 너무 넓으면(10% lookalike) 규모는 크지만 유사도가 떨어집니다. 여러 확장 비율을 테스트해서 CPA와 도달 범위를 균형있게 맞춥니다.

**Cold Start** 문제로 시드가 너무 작으면 패턴 학습이 어렵습니다. 최소 수백~수천 개 시드를 확보하고, 유사 캠페인이나 카테고리에서 transfer하거나, rule-based heuristic으로 초기 확장 후 점진적으로 학습합니다.

**Negative sampling**도 고려합니다. 전체 비전환 사용자를 음성으로 쓰면 극심한 불균형이므로, 시드와 어느 정도 유사하지만 전환하지 않은 사용자를 hard negative로 샘플링하면 경계를 더 정교하게 학습합니다.

**Privacy 측면**에서는 개인 식별 정보를 제외하고, 집계 수준 특성으로 모델링하며, Differential Privacy 같은 기법으로 개별 사용자 정보를 보호합니다. GDPR, CCPA 등 규제 준수가 필수적입니다.

실무에서는 lookalike 모델로 타겟팅하고, A/B 테스트로 broad targeting 대비 CPA, ROAS 개선을 검증합니다. 시드 품질(고가치 고객)이 결과에 결정적이므로, 올바른 시드 정의가 중요합니다.

---

# 68. 광고 피로(Ad Fatigue)와 빈도 제한(Frequency Capping)은 어떻게 관리하나요?

같은 광고를 반복 노출하면 초기에는 효과가 있지만 점차 CTR이 떨어지고 사용자 경험이 나빠지는 현상이 광고 피로입니다. 효과적인 빈도 관리가 광고 성과와 사용자 만족을 모두 높입니다.

**빈도 제한(Frequency Capping)**은 일정 기간 동안 같은 사용자에게 특정 광고를 최대 몇 번까지만 보여줄지 제한하는 정책입니다. 예를 들어 "하루 3회", "주간 10회" 같은 규칙을 설정합니다. 너무 낮으면 도달이 부족하고, 너무 높으면 피로가 발생하므로 최적값을 찾아야 합니다.

**동적 빈도 제한**은 고정값 대신 광고, 사용자, 캠페인 특성에 따라 적응적으로 조정합니다. 브랜드 인지 캠페인은 높은 빈도가 필요하지만, 전환 유도는 낮은 빈도로 충분할 수 있습니다. 프리미엄 크리에이티브나 관심 있는 사용자는 더 높은 빈도를 허용합니다.

**Fatigue Modeling**로 노출 횟수와 CTR/CVR 관계를 학습합니다. 로지스틱 곡선이나 지수 감쇠로 피로 효과를 모델링하고, 한계 효용(marginal utility)을 추정합니다:

$$
\text{pCTR}(n) = \text{pCTR}_0 \times e^{-\lambda n}
$$

여기서 n은 누적 노출 횟수, λ는 피로 계수입니다. 사용자별, 광고별로 이 파라미터를 학습하고, marginal pCTR이 임계값 아래로 떨어지면 노출을 중단합니다.

**Recency**도 중요합니다. 마지막 노출 후 충분한 시간이 지나면 효과가 회복될 수 있으므로(24시간, 7일 등), 시간 감쇠를 고려한 노출 이력 관리가 필요합니다.

**크리에이티브 로테이션**으로 여러 버전을 번갈아 보여주면 피로를 줄일 수 있습니다. 같은 메시지를 다른 이미지, 문구, 포맷으로 표현해서 신선함을 유지합니다.

**사용자 피드백**을 활용합니다. "광고 숨기기", "관심 없음" 같은 명시적 피드백이나, 빠른 스크롤, 낮은 CTR 같은 암묵적 신호로 피로를 감지하고 노출을 조절합니다.

**예산 효율성** 관점에서 피로한 사용자에게 계속 노출하는 것은 낭비이므로, 같은 예산으로 새로운 사용자에게 노출하거나 다른 광고로 전환하는 것이 효율적입니다.

**측정**은 노출 빈도별 성과 곡선을 그려서 최적 빈도를 찾고, 빈도 제한이 있는/없는 그룹을 A/B 테스트로 비교합니다. Brand lift survey로 고빈도 노출의 브랜드 인지 효과를 측정할 수도 있습니다.

실무에서는 캠페인 목표(인지 vs 전환), 크리에이티브 품질, 타겟 오디언스 크기를 고려해서 빈도 전략을 설계하고, 실시간 모니터링으로 조정합니다.

---

# 69. 광고 시스템의 Feature Store와 Online/Offline Feature는 어떻게 설계하나요?

광고 ML 시스템은 수천 개의 feature를 실시간으로 계산하고 서빙해야 하므로, Feature Store가 핵심 인프라입니다. Offline(학습), Nearline(주기적 업데이트), Online(실시간) feature를 일관되게 관리해야 합니다.

**Offline Features**는 배치로 계산되는 feature로, 과거 장기 통계가 대표적입니다. 사용자의 30일 평균 CTR, 광고의 전체 전환율, 카테고리별 인기도 등입니다. Hive, Spark로 매일 또는 매시간 계산하고, Parquet이나 Delta Lake 같은 포맷으로 저장합니다. Feature Backfill로 과거 학습 데이터를 재구성하고, Point-in-time correctness를 보장해서 미래 정보 누수를 방지합니다.

**Online Features**는 실시간 추론 시 필요한 feature로, 컨텍스트(요청 시간, 기기, 위치), 최근 사용자 행동(최근 1시간 활동), 실시간 광고 상태(남은 예산, 현재 입찰가) 등입니다. Redis, Cassandra 같은 저지연 Key-Value store에서 수 ms 내에 조회합니다.

**Feature Serving**은 모델 추론 시 필요한 모든 feature를 빠르게 조합해서 제공합니다. User ID, Ad ID로 각각의 feature를 병렬로 조회하고, 교차 feature는 실시간 계산하거나 사전 계산합니다. Feast, Tecton, Hopsworks 같은 Feature Store 플랫폼이 이를 추상화합니다.

**Train-Serve Skew** 방지가 중요합니다. 학습과 추론에서 feature 계산 로직이 달라지면 성능이 크게 떨어집니다. Feature 정의를 코드로 중앙화하고(Python DSL, SQL), 같은 로직을 배치(PySpark)와 온라인(Python service)에서 재사용합니다. Integration test로 offline-online feature 일치를 검증합니다.

**Feature Freshness**는 trade-off입니다. 실시간 feature는 최신이지만 계산 비용이 높고, 배치 feature는 저렴하지만 지연이 있습니다. 중요도에 따라 streaming(Flink, Kafka Streams)으로 near-realtime 업데이트하거나, micro-batch(5-15분)로 준실시간 갱신합니다.

**Feature 유형별 전략**으로 **ID embedding**은 offline 학습하고 online serving은 embedding lookup으로 빠르게, **집계 feature**는 streaming aggregation으로 실시간 갱신, **교차 feature**는 조합 폭발을 피하려고 상위 K개만 materialization합니다.

**Cold Start** 처리로 새 사용자/광고는 feature가 없으므로 default value나 global average를 사용하고, 첫 몇 번 노출로 빠르게 개인화된 feature를 구축합니다.

**모니터링**으로 feature 분포 drift, missing value 비율, 계산 지연(lag), feature importance 변화를 추적합니다. Anomaly detection으로 feature 파이프라인 장애를 조기 발견합니다.

**버저닝**도 필수입니다. Feature 정의 변경 시 하위 호환성을 유지하고, 모델과 feature 버전을 함께 관리해서 재현 가능한 실험과 롤백을 보장합니다.

실무에서는 시작은 간단히(PostgreSQL + Python service)하고, 규모가 커지면 Feature Store 플랫폼으로 마이그레이션합니다. Feature engineering pipeline, model training, serving이 유기적으로 연결된 End-to-end MLOps 구축이 목표입니다.

---

# 70. 광고 ML 모델의 A/B 테스트와 배포 전략은 어떻게 설계하나요?

광고 시스템은 수익에 직접 영향을 미치므로 신중한 실험과 단계적 배포가 필수적입니다.

**Offline Evaluation**로 먼저 필터링합니다. 과거 로그 데이터로 replay하거나, 교차검증으로 AUC, Log-Loss, Calibration을 평가합니다. 하지만 offline 지표와 online 성과가 불일치할 수 있어서(selection bias, feedback loop) 온라인 검증이 필수입니다.

**A/B 테스트 설계**에서는 사용자 또는 경매 단위로 무작위 분할합니다. 사용자 단위가 일반적이지만, 광고주별 효과를 보려면 광고 또는 캠페인 단위 분할도 고려합니다. Stratification으로 중요 세그먼트(heavy user, 지역)의 균형을 맞춥니다.

**지표 선정**이 핵심입니다. Primary metric(광고주 수익, 플랫폼 수익), Secondary metrics(CTR, CVR, CPC, ROAS), Guardrail metrics(사용자 참여도, 이탈률, 유기적 컨텐츠 소비)를 모두 추적합니다. 단기 수익은 올랐지만 사용자 만족도가 떨어지면 장기적으로 손해일 수 있습니다.

**샘플 크기와 실험 기간**은 MDE(Minimum Detectable Effect)와 검정력으로 설계합니다. 광고는 노이즈가 크고 요일/시간대 효과가 있어서 최소 1주일, 보통 2-4주 실험합니다. SRM(Sample Ratio Mismatch) 체크로 무작위 배정 이상을 감지합니다.

**Interleaving**은 A/B보다 민감한 방법으로, 같은 사용자에게 두 모델의 광고를 섞어서 보여주고 상대적 성과를 비교합니다. Counterfactual evaluation으로 로그 데이터에서 새 정책의 효과를 추정할 수도 있습니다(IPS, Doubly Robust).

**점진적 배포(Canary, Ramp-up)**로 위험을 관리합니다. 먼저 1% 트래픽에 배포해서 시스템 안정성(latency, error rate)을 확인하고, 이상이 없으면 5% → 25% → 50% → 100%로 단계적으로 확대합니다. 각 단계에서 모니터링하고, 이상 시 자동 롤백합니다.

**Shadow Mode**는 새 모델이 예측은 하지만 실제 서빙에는 사용하지 않고, 기존 모델과 비교만 합니다. 프로덕션 트래픽으로 안전하게 검증하고, 분포 차이나 edge case를 발견합니다.

**Multi-Armed Bandit**으로 동적 트래픽 할당을 할 수도 있습니다. 성능이 좋은 variant에 더 많은 트래픽을 보내서 regret을 줄이지만, 인과 추론이 복잡해지고 명확한 winner 선언이 어려워집니다.

**Long-term Effect**를 위해 Switchback test(시간 단위 on/off 반복)나 Geo experiment(지역별 배정)로 장기 효과를 측정합니다. Novelty effect(초기에만 좋음)나 Primacy effect(학습 기간 필요)를 고려해야 합니다.

**실험 간 간섭**도 문제입니다. 여러 팀이 동시에 실험하면 상호작용이 발생할 수 있어서, 실험 레이어링(독립적 차원), 실험 조율(충돌 회피), 또는 다중 비교 보정(Bonferroni, FDR)이 필요합니다.

**배포 체크리스트**로 성능 벤치마크(TPS, p99 latency), 리소스 사용량(CPU, 메모리), Backward compatibility, 롤백 계획, Runbook 문서화를 확인합니다.

실무에서는 데이터 과학자, ML 엔지니어, 제품 매니저가 협업해서 실험을 설계하고, 결과를 해석하며, trade-off를 논의해서 배포를 결정합니다. 실험 문화와 인프라가 빠른 반복과 혁신의 토대입니다.

---