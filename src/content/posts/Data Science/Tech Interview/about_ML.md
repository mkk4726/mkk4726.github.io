---
title: "ML 관련 인터뷰 준비"
date: "2025-09-08"
excerpt: ""
category: "Data Science"
tags: ["Interview"]
---

참고자료
- 1: [ai 테크 인터뷰](https://github.com/boost-devs/ai-tech-interview?tab=readme-ov-file)

---

### 알고 있는 metric에 대해 설명해주세요. (ex. RMSE, MAE, recall, precision ...)

regression task -> RMSE, MAE 등이 있겠고 
classification task -> recall, precision, ROC-AUC 등이 있겠다.


먼저 regression task의 경우, RMSE와 MAE가 가장 많이 사용됩니다. RMSE는 Root Mean Square Error로, 예측값과 실제값의 차이를 제곱해서 평균을 구한 후 제곱근을 취한 값입니다. 큰 오차에 더 민감하게 반응하기 때문에 이상치가 있을 때 더 큰 페널티를 주는 특징이 있어요. MAE는 Mean Absolute Error로, 절댓값 차이의 평균을 구하는데, 이상치에 덜 민감합니다. 그래서 이상치가 많은 데이터에서는 MAE가 더 적절할 수 있어요.

classification task에서는 상황에 따라 다양한 metric을 사용합니다. 가장 기본적인 accuracy는 전체 예측 중 맞춘 비율인데, 클래스 불균형이 심한 데이터에서는 부적절할 수 있어요. 

precision은 양성으로 예측한 것 중 실제로 양성인 비율이고, recall은 실제 양성 중 올바르게 찾아낸 비율입니다. precision이 중요한 경우는 스팸 메일 분류처럼 false positive 비용이 클 때이고, recall이 중요한 경우는 의료 진단처럼 false negative 비용이 클 때입니다. 

F1-score는 precision과 recall의 조화평균으로, 두 지표를 균형있게 고려하고 싶을 때 사용해요. ROC-AUC는 ROC 곡선 아래 면적을 의미하는데, 다양한 threshold에서의 모델 성능을 종합적으로 평가할 수 있어서 이진 분류에서 많이 사용됩니다.

실제 프로젝트에서는 비즈니스 요구사항과 데이터 특성을 고려해서 적절한 metric을 선택하는 것이 중요합니다.


### 정규화를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?

과적합을 피하기 위해서. lasso, ridge처럼 모델의 가중치를 loss에 포함시켜서 과적합 되지 않도록 유도한다.
tree계열에서도 마찬가지로 정규화를 사용함.

정규화는 주로 과적합을 방지하기 위해 사용합니다. 모델이 훈련 데이터에만 너무 잘 맞춰져서 새로운 데이터에 대한 일반화 성능이 떨어지는 것을 막기 위해서죠.

정규화 방법으로는 크게 두 가지가 있습니다. 첫 번째는 L1 정규화, 즉 Lasso입니다. 이는 가중치의 절댓값의 합을 loss function에 추가하는 방식으로, 불필요한 feature들을 0으로 만들어서 자동으로 feature selection 효과를 가져옵니다. 

두 번째는 L2 정규화, 즉 Ridge입니다. 이는 가중치의 제곱합을 loss function에 추가하는 방식으로, 가중치들을 0에 가깝게 만들어서 모델을 단순화시킵니다. Lasso와 달리 feature selection은 하지 않고 모든 feature를 유지하면서 가중치만 줄여줍니다.

Elastic Net은 L1과 L2를 결합한 방식이고, Dropout은 딥러닝에서 랜덤하게 일부 뉴런을 비활성화시키는 방법입니다.

Tree 계열 모델에서도 정규화를 사용하는데, 예를 들어 XGBoost에서는 max_depth, min_child_weight, gamma 등의 파라미터로 모델 복잡도를 제어하고, Random Forest에서는 max_depth나 min_samples_split 같은 파라미터로 과적합을 방지합니다.

실제로는 데이터의 크기, feature의 개수, 모델의 복잡도 등을 고려해서 적절한 정규화 방법을 선택하는 것이 중요합니다.


### Local Minima와 Global Minimum에 대해 설명해주세요.

지역적 최솟값과 전역적 최솟값을 의미하며, 최솟값이 부분적으로 최소인지 전체에서 최소인지를 구분.
global minima는 local minima 중 가장 작은 값.


네, Local Minima와 Global Minimum은 최적화 문제에서 중요한 개념입니다.

Local Minima는 지역적 최솟값으로, 특정 구간에서 가장 작은 값을 의미합니다. 즉, 그 점 주변의 작은 영역에서는 더 작은 값이 없지만, 전체 함수를 봤을 때는 더 작은 값이 존재할 수 있어요.

Global Minimum은 전역적 최솟값으로, 전체 함수 영역에서 가장 작은 값을 의미합니다. 모든 가능한 점들 중에서 가장 작은 값을 가지는 지점이죠.

간단한 예로 설명하면, 산맥을 생각해보시면 됩니다. 여러 개의 골짜기가 있는데, 각 골짜기 바닥이 Local Minima이고, 그 중에서 가장 낮은 골짜기가 Global Minimum입니다.

머신러닝에서 이 문제가 중요한 이유는, Gradient Descent 같은 최적화 알고리즘이 Local Minima에 빠져서 Global Minimum을 찾지 못할 수 있기 때문입니다. 특히 딥러닝에서는 수많은 Local Minima가 존재하는데, 다행히도 대부분의 Local Minima들이 비슷한 성능을 보인다는 연구 결과도 있습니다.

이를 해결하기 위해 다양한 기법들을 사용하는데, momentum, learning rate scheduling, 여러 초기값으로 시작하기, 또는 simulated annealing 같은 방법들이 있습니다.



### 차원의 저주에 대해 설명해주세요.

차원이 많이 질수록 sparse해진다는 것.

차원의 저주는 차원이 증가할수록 발생하는 여러 문제들을 통칭하는 개념입니다.

가장 핵심적인 문제는 **데이터의 희소성(sparsity)**입니다. 차원이 늘어날수록 같은 데이터 개수로는 공간을 채우기 어려워져요. 예를 들어, 1차원에서는 10개의 점으로 선을 채울 수 있지만, 10차원에서는 같은 10개의 점으로는 공간의 극히 일부분만 커버하게 됩니다.

이로 인해 몇 가지 구체적인 문제들이 발생합니다. 첫째, **근접성의 의미가 사라집니다**. 고차원에서는 모든 점들이 거의 비슷한 거리에 있게 되어서, "가까운" 점과 "먼" 점을 구분하기 어려워져요.

둘째, **과적합이 쉽게 발생**합니다. 차원이 많을수록 모델이 복잡해지고, 같은 데이터 개수로는 충분한 학습이 어려워집니다.

셋째, **계산 복잡도가 기하급수적으로 증가**합니다. 차원이 하나씩 늘어날 때마다 필요한 계산량이 폭발적으로 증가해요.

실제로는 차원 축소 기법들로 이 문제를 해결합니다. PCA, t-SNE, UMAP 같은 방법들로 중요한 정보는 유지하면서 차원을 줄이거나, feature selection을 통해 불필요한 차원을 제거하는 방식으로 접근합니다.




### dimension reduction기법으로 보통 어떤 것들이 있나요?

차원 축소 기법은 크게 선형과 비선형 방법으로 나뉩니다. 

가장 기본적이고 널리 사용되는 방법은 **PCA(Principal Component Analysis)**입니다. PCA는 데이터의 분산이 가장 큰 방향으로 새로운 축을 찾아 차원을 축소하는 선형 방법으로, 주성분을 통해 원본 데이터의 정보를 최대한 보존하면서도 해석이 용이하고 계산이 빠릅니다. 주로 이미지 압축, 노이즈 제거, 시각화 등에 사용됩니다.

선형 방법 중에는 **LDA(Linear Discriminant Analysis)**도 중요한데, 이는 클래스 간 분산을 최대화하고 클래스 내 분산을 최소화하는 지도학습 방법으로 분류 성능 향상에 특화되어 있습니다. 또한 **Factor Analysis**는 관찰된 변수들을 잠재 변수로 설명하는 방법으로 심리학이나 사회과학 분야에서 주로 사용됩니다.

비선형 방법으로는 **t-SNE**가 가장 유명합니다. t-SNE는 고차원 데이터를 2D나 3D로 시각화하는 데 특화되어 있으며, 지역적 구조를 잘 보존하기 때문에 클러스터링 결과 시각화에 매우 효과적입니다. 다만 전역적 구조는 보존하지 못하고 하이퍼파라미터에 민감하다는 단점이 있습니다.

**UMAP**은 t-SNE의 단점을 보완한 방법으로, t-SNE보다 빠르면서도 전역적 구조를 어느 정도 보존합니다. 지역적과 전역적 구조의 균형을 잘 맞춰서 대용량 데이터에도 적용 가능합니다.

**Autoencoder**는 신경망 기반의 차원 축소 방법으로, 인코더-디코더 구조를 통해 데이터를 압축하고 복원합니다. 비선형 관계를 잘 학습할 수 있어서 딥러닝과 결합하여 사용되는 경우가 많습니다.

기타 방법들로는 **Isomap**이 있는데, 이는 지오데식 거리를 사용하는 매니폴드 학습의 대표적인 방법입니다. **LLE(Locally Linear Embedding)**는 지역적으로 선형인 관계를 가정하는 방법으로 작은 데이터셋에서 효과적입니다.

실제로 어떤 방법을 선택할지는 목적에 따라 다릅니다. 시각화가 목적이라면 t-SNE나 UMAP을, 전처리가 목적이라면 PCA나 Autoencoder를, 분류 성능 향상이 목적이라면 LDA를 사용하는 것이 좋습니다. 대용량 데이터를 다룬다면 UMAP이나 PCA를, 해석 가능성이 중요하다면 PCA나 Factor Analysis를 선택하는 것이 적절합니다.





### PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?

### LSA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요?

### Markov Chain을 고등학생에게 설명하려면 어떤 방식이 제일 좋을까요?


### 텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?


### SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? SVM은 왜 좋을까요?


### 다른 좋은 머신 러닝 대비, 오래된 기법인 나이브 베이즈(naive bayes)의 장점을 옹호해보세요.


### 회귀 / 분류시 알맞은 metric은 무엇일까?


### Association Rule의 Support, Confidence, Lift에 대해 설명해주세요.


### 최적화 기법중 Newton’s Method와 Gradient Descent 방법에 대해 알고 있나요?


### 머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?


### 인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?


### 지금 나오고 있는 deep learning 계열의 혁신의 근간은 무엇이라고 생각하시나요?


### ROC 커브에 대해 설명해주실 수 있으신가요?

### 여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야하는 이유는 뭘까요?


### K-means의 대표적 의미론적 단점은 무엇인가요? (계산량 많다는것 말고)


### L1, L2 정규화에 대해 설명해주세요.

### Cross Validation은 무엇이고 어떻게 해야하나요?

### XGBoost을 아시나요? 왜 이 모델이 캐글에서 유명할까요?

### 앙상블 방법엔 어떤 것들이 있나요?

### feature vector란 무엇일까요?


### 좋은 모델의 정의는 무엇일까요?

### 50개의 작은 의사결정 나무는 큰 의사결정 나무보다 괜찮을까요? 왜 그렇게 생각하나요?


### 스팸 필터에 로지스틱 리그레션을 많이 사용하는 이유는 무엇일까요?


### OLS(ordinary least squre) regression의 공식은 무엇인가요?