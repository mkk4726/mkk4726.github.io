---
title: "Double Robustness: ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì™„ì „ ì •ë³µ"
date: "2025-07-17"
excerpt: "Double Robustnessì˜ ì´ë¡ ì  ê¸°ë°˜ë¶€í„° ì‹¤ìš©ì  ì‘ìš©ê¹Œì§€ ì™„ì „ í•´ë¶€"
category: "Causal Inference"
tags: ["Double Robustness", "Causal Inference", "Theory", "R-learner"]
---

# Double Robustness: ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì™„ì „ ì •ë³µ

**Double Robustness**(ì´ì¤‘ ê°•ê±´ì„±)ëŠ” í˜„ëŒ€ ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ ê°œë… ì¤‘ í•˜ë‚˜ë¡œ, R-learnerì™€ ê°™ì€ ìµœì‹  ë°©ë²•ë¡ ì˜ ì´ë¡ ì  ê¸°ë°˜ì´ ë©ë‹ˆë‹¤. ì´ í¬ìŠ¤íŠ¸ì—ì„œëŠ” Double Robustnessì˜ ëª¨ë“  ì¸¡ë©´ì„ ìì„¸íˆ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤.

# 1. Double Robustnessë€ ë¬´ì—‡ì¸ê°€?

## 1.1 ì§ê´€ì  ì´í•´

**Double Robustness**ëŠ” **"ë‘ ê°€ì§€ ëª¨ë¸ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ í¸í–¥ ì—†ëŠ” ì¶”ì •ì´ ê°€ëŠ¥í•œ ì„±ì§ˆ"**ì…ë‹ˆë‹¤.

### **ê°„ë‹¨í•œ ë¹„ìœ **
- **ìë¬¼ì‡  ë‘ ê°œ**: ë¬¸ì„ ì—´ë ¤ë©´ ë‘˜ ì¤‘ í•˜ë‚˜ë§Œ ì—´ë©´ ë¨
- **ë°±ì—… ì‹œìŠ¤í…œ**: ì£¼ ì‹œìŠ¤í…œì´ ì‹¤íŒ¨í•´ë„ ë³´ì¡° ì‹œìŠ¤í…œì´ ì‘ë™
- **ì´ì¤‘ ë³´í—˜**: í•˜ë‚˜ê°€ ì‹¤íŒ¨í•´ë„ ë‹¤ë¥¸ í•˜ë‚˜ê°€ ë³´ì¥

## 1.2 ì¸ê³¼ì¶”ë¡ ì—ì„œì˜ ì˜ë¯¸

ì¸ê³¼ì¶”ë¡ ì—ì„œ ìš°ë¦¬ê°€ ì¶”ì •í•´ì•¼ í•˜ëŠ” ë‘ ê°€ì§€ í•µì‹¬ ìš”ì†Œ:

1. **Outcome Model** (ê²°ê³¼ ëª¨ë¸): $\mu(x, w) = \mathbb{E}[Y | X = x, W = w]$
2. **Propensity Score Model** (ì²˜ì¹˜ í™•ë¥  ëª¨ë¸): $e(x) = \text{Pr}(W = 1 | X = x)$

**Double Robustness**: ì´ ë‘˜ ì¤‘ **í•˜ë‚˜ë§Œ ì •í™•í•´ë„** ì²˜ì¹˜íš¨ê³¼ë¥¼ í¸í–¥ ì—†ì´ ì¶”ì •í•  ìˆ˜ ìˆìŒ!

# 2. ìˆ˜í•™ì  ì´ë¡ : í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜

## 2.1 Potential Outcomes Framework

**ê¸°ë³¸ ì„¤ì •**:
- **ì ì¬ê²°ê³¼**: $Y_i(0), Y_i(1)$ (ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œ/ë°›ì•˜ì„ ë•Œì˜ ê²°ê³¼)
- **ê´€ì°°ê²°ê³¼**: $Y_i = Y_i(W_i)$ 
- **ê°œë³„ ì²˜ì¹˜íš¨ê³¼**: $\tau_i = Y_i(1) - Y_i(0)$
- **í‰ê·  ì²˜ì¹˜íš¨ê³¼**: $\tau = \mathbb{E}[Y(1) - Y(0)]$

**ê·¼ë³¸ì  ë¬¸ì œ**: ê°™ì€ ê°œì¸ì— ëŒ€í•´ $Y_i(0)$ê³¼ $Y_i(1)$ì„ ë™ì‹œì— ê´€ì°°í•  ìˆ˜ ì—†ìŒ!

## 2.2 Unconfoundedness Assumption

$$\{Y(0), Y(1)\} \perp \!\!\! \perp W \mid X$$

**ì˜ë¯¸**: ê³µë³€ëŸ‰ $X$ë¥¼ í†µì œí•˜ë©´ ì²˜ì¹˜ í• ë‹¹ì´ ë¬´ì‘ìœ„ì™€ ê°™ìŒ

ì´ ê°€ì • í•˜ì—ì„œ ë‹¤ìŒì´ ì„±ë¦½:
$$\mathbb{E}[Y(w) | X] = \mathbb{E}[Y | X, W = w] = \mu(X, w)$$

## 2.3 Double Robustnessì˜ ìˆ˜í•™ì  í‘œí˜„

### **í•µì‹¬ í•­ë“±ì‹**

Average Treatment Effect (ATE)ì— ëŒ€í•œ doubly robust ì¶”ì •ëŸ‰:

$$\hat{\tau}_{DR} = \frac{1}{n} \sum_{i=1}^n \left[ \frac{W_i Y_i}{\hat{e}(X_i)} - \frac{(1-W_i) Y_i}{1-\hat{e}(X_i)} + \frac{W_i - \hat{e}(X_i)}{\hat{e}(X_i)(1-\hat{e}(X_i))} \left\{ \hat{\mu}(X_i, 1) - \hat{\mu}(X_i, 0) \right\} \right]$$

### **ë” ê°„ë‹¨í•œ í˜•íƒœ**

AIPW (Augmented Inverse Propensity Weighting) ì¶”ì •ëŸ‰:

$$\hat{\tau}_{AIPW} = \frac{1}{n} \sum_{i=1}^n \left[ \hat{\mu}(X_i, 1) - \hat{\mu}(X_i, 0) + \frac{W_i}{\hat{e}(X_i)} \{Y_i - \hat{\mu}(X_i, 1)\} - \frac{1-W_i}{1-\hat{e}(X_i)} \{Y_i - \hat{\mu}(X_i, 0)\} \right]$$

## 2.4 ì™œ "Double Robust"ì¸ê°€?

### **í•µì‹¬ ì •ë¦¬**

**Theorem**: ë‹¤ìŒ ì¡°ê±´ ì¤‘ **í•˜ë‚˜ë§Œ** ë§Œì¡±í•˜ë©´ $\mathbb{E}[\hat{\tau}_{AIPW}] = \tau$:

1. **Outcome modelì´ ì •í™•**: $\hat{\mu}(x, w) = \mu^*(x, w)$
2. **Propensity modelì´ ì •í™•**: $\hat{e}(x) = e^*(x)$

### **ì¦ëª…ì˜ í•µì‹¬ ì•„ì´ë””ì–´**

AIPW ì¶”ì •ëŸ‰ì„ ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$\hat{\tau}_{AIPW} = \underbrace{\frac{1}{n} \sum_{i=1}^n \{\hat{\mu}(X_i, 1) - \hat{\mu}(X_i, 0)\}}_{\text{Outcome-based}} + \underbrace{\frac{1}{n} \sum_{i=1}^n \left[\frac{W_i}{\hat{e}(X_i)} - \frac{1-W_i}{1-\hat{e}(X_i)}\right] \{Y_i - \hat{\mu}(X_i, W_i)\}}_{\text{IPW correction}}$$

**ì¼€ì´ìŠ¤ 1**: Outcome modelì´ ì •í™•í•œ ê²½ìš°
- ì²« ë²ˆì§¸ í•­ì´ ì •í™•í•œ $\tau$ ì œê³µ
- ë‘ ë²ˆì§¸ í•­ì˜ ê¸°ëŒ“ê°’ì´ 0 (ì”ì°¨ì˜ ê°€ì¤‘í‰ê· )

**ì¼€ì´ìŠ¤ 2**: Propensity modelì´ ì •í™•í•œ ê²½ìš°  
- ë‘ ë²ˆì§¸ í•­ì´ IPW ì¶”ì •ëŸ‰ìœ¼ë¡œ ì •í™•í•œ $\tau$ ì œê³µ
- ì²« ë²ˆì§¸ í•­ì˜ ì˜¤ì°¨ê°€ ë‘ ë²ˆì§¸ í•­ì— ì˜í•´ ìƒì‡„

# 3. ì§ê´€ì  ì´í•´: ì™œ ì‘ë™í•˜ëŠ”ê°€?

## 3.1 Visual Intuition

```
ì§„ì‹¤í•œ ì²˜ì¹˜íš¨ê³¼: Ï„* = 5

ì‹œë‚˜ë¦¬ì˜¤ 1: Outcome Model ì •í™•, Propensity Model ë¶€ì •í™•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Outcome Model   â”‚    â”‚ Propensity Modelâ”‚
â”‚ âœ… Ï„Ì‚ = 5.0     â”‚ +  â”‚ âŒ í¸í–¥ ìˆìŒ    â”‚ = Ï„Ì‚ = 5.0 âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì‹œë‚˜ë¦¬ì˜¤ 2: Outcome Model ë¶€ì •í™•, Propensity Model ì •í™•  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Outcome Model   â”‚    â”‚ Propensity Modelâ”‚
â”‚ âŒ í¸í–¥ ìˆìŒ    â”‚ +  â”‚ âœ… ì •í™•í•¨       â”‚ = Ï„Ì‚ = 5.0 âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ì‹œë‚˜ë¦¬ì˜¤ 3: ë‘˜ ë‹¤ ì •í™•
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Outcome Model   â”‚    â”‚ Propensity Modelâ”‚
â”‚ âœ… Ï„Ì‚ = 5.0     â”‚ +  â”‚ âœ… ì •í™•í•¨       â”‚ = Ï„Ì‚ = 5.0 âœ…
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 3.2 ë³´ìƒ ë©”ì»¤ë‹ˆì¦˜ (Compensation Mechanism)

### **Outcome Modelì´ ì •í™•í•œ ê²½ìš°**
```python
# Outcome modelì´ ì™„ë²½í•˜ë©´
mu_hat(x, 1) - mu_hat(x, 0) â‰ˆ Ï„*

# IPW correction termì€
E[W/e_hat(X) - (1-W)/(1-e_hat(X))] * {Y - mu_hat(X,W)} â‰ˆ 0
# (ì”ì°¨ì˜ ê°€ì¤‘í‰ê· ì€ 0ì— ê°€ê¹Œì›€)
```

### **Propensity Modelì´ ì •í™•í•œ ê²½ìš°**
```python
# Propensity modelì´ ì™„ë²½í•˜ë©´ IPWê°€ ì •í™•í•œ ì¶”ì • ì œê³µ
IPW_term = E[W*Y/e*(X) - (1-W)*Y/(1-e*(X))] = Ï„*

# Outcome modelì˜ ì˜¤ì°¨ëŠ” IPW correctionì— ì˜í•´ ìƒì‡„ë¨
```

## 3.3 ì‹¤ì œ ë°ì´í„° ì˜ˆì‹œ

### **ì‹œë®¬ë ˆì´ì…˜ ì„¤ì •**
```python
n = 1000
X = np.random.normal(0, 1, (n, 2))
e_true = expit(X[:, 0])  # true propensity
W = np.random.binomial(1, e_true)

# True outcome model: complex nonlinear
Y_0 = X[:, 0]**2 + X[:, 1] + noise
Y_1 = Y_0 + 2 + X[:, 0]  # treatment effect = 2 + X[:, 0]
Y = W * Y_1 + (1 - W) * Y_0

true_ATE = 2 + np.mean(X[:, 0])  # â‰ˆ 2.0
```

### **ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²°ê³¼**
```python
# Scenario 1: Good outcome model, bad propensity model
mu_hat_good = fit_complex_model(Y, X, W)  # RÂ² = 0.95
e_hat_bad = 0.5  # constant (wrong!)

AIPW_1 = compute_AIPW(Y, W, X, mu_hat_good, e_hat_bad)
# Result: 2.03 (ê±°ì˜ ì •í™•! âœ…)

# Scenario 2: Bad outcome model, good propensity model  
mu_hat_bad = fit_linear_model(Y, X, W)  # RÂ² = 0.3 (underfit)
e_hat_good = fit_logistic_model(W, X)  # very accurate

AIPW_2 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_good)
# Result: 1.97 (ì—¬ì „íˆ ì •í™•! âœ…)

# Scenario 3: Both models bad
AIPW_3 = compute_AIPW(Y, W, X, mu_hat_bad, e_hat_bad)
# Result: 1.2 (í¸í–¥ë¨ âŒ)
```

# 4. Double Robustnessì˜ ì¥ì ê³¼ í•œê³„

## 4.1 í•µì‹¬ ì¥ì 

### **1. ê²¬ê³ ì„± (Robustness)**
- **ëª¨ë¸ ì˜¤ì§€ì •ì— ëŒ€í•œ ë³´í—˜**: í•˜ë‚˜ê°€ í‹€ë ¤ë„ ì•ˆì „
- **ì‹¤ë¬´ì  ì•ˆì •ì„±**: ì™„ë²½í•œ ëª¨ë¸ë§ì´ ì–´ë ¤ìš´ í˜„ì‹¤ì—ì„œ ìœ ìš©

### **2. íš¨ìœ¨ì„± (Efficiency)**
- **ë‘˜ ë‹¤ ì •í™•í•˜ë©´ ìµœê³  íš¨ìœ¨ì„±**: ìµœì†Œ ë¶„ì‚° ë‹¬ì„±
- **Semiparametric efficiency bound** ë‹¬ì„±

### **3. ìœ ì—°ì„± (Flexibility)**
- **ì„œë¡œ ë‹¤ë¥¸ ë°©ë²• ì¡°í•©**: ê° ëª¨ë¸ì— ìµœì í™”ëœ ë°©ë²• ì‚¬ìš© ê°€ëŠ¥
- **ê¸°ê³„í•™ìŠµ ë°©ë²• í™œìš©**: ë³µì¡í•œ ëª¨ë¸ë„ ì‚¬ìš© ê°€ëŠ¥

## 4.2 í•œê³„ì 

### **1. ë‘˜ ë‹¤ í‹€ë¦¬ë©´ í¸í–¥**
```python
# ìµœì•…ì˜ ì‹œë‚˜ë¦¬ì˜¤
if outcome_model_wrong and propensity_model_wrong:
    bias = f(error_outcome, error_propensity)  # í¸í–¥ ë°œìƒ
```

### **2. ë¶„ì‚° ì¦ê°€ ê°€ëŠ¥ì„±**
- **IPW termì˜ ê·¹ë‹¨ê°’**: propensity scoreê°€ 0 ë˜ëŠ” 1ì— ê°€ê¹Œìš°ë©´ ë¶„ì‚° ê¸‰ì¦
- **Overlap ì¡°ê±´ í•„ìš”**: $0 < e(x) < 1$ for all $x$

### **3. ì¶”ì • ë³µì¡ì„±**
- **ë‘ ëª¨ë¸ ëª¨ë‘ ì¶”ì •**: ê³„ì‚° ë¶€ë‹´ ì¦ê°€
- **êµì°¨ê²€ì¦ ë³µì¡ì„±**: ê° ëª¨ë¸ì˜ íŠœë‹ì´ ë³µì¡

# 5. R-learnerì™€ì˜ ì—°ê²°

## 5.1 Robinson's Transformationê³¼ Double Robustness

R-learnerëŠ” Robinson's transformationì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

$$Y_i - m^*(X_i) = \{W_i - e^*(X_i)\} \tau^*(X_i) + \varepsilon_i$$

ì´ë¥¼ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´:

$$Y_i = m^*(X_i) + \{W_i - e^*(X_i)\} \tau^*(X_i) + \varepsilon_i$$

### **Double Robust Structure**

R-loss í•¨ìˆ˜:
$$\hat{L}_n\{\tau(\cdot)\} = \frac{1}{n} \sum_{i=1}^n \left[ \left\{Y_i - \hat{m}^{(-q(i))}(X_i)\right\} - \left\{W_i - \hat{e}^{(-q(i))}(X_i)\right\} \tau(X_i) \right]^2$$

**í•µì‹¬**: ì´ ì†ì‹¤í•¨ìˆ˜ë„ double robust ì„±ì§ˆì„ ê°€ì§‘ë‹ˆë‹¤!

- **$\hat{m}$ì´ ì •í™•**í•˜ë©´ ì²« ë²ˆì§¸ í•­ì´ ì •í™•í•œ ì”ì°¨ ì œê³µ
- **$\hat{e}$ê°€ ì •í™•**í•˜ë©´ ë‘ ë²ˆì§¸ í•­ì´ ì •í™•í•œ ê°€ì¤‘ì¹˜ ì œê³µ

## 5.2 ì™œ R-learnerê°€ Quasi-Oracleì¸ê°€?

```python
# Oracle knows m*(x) and e*(x)
oracle_loss = E[(Y - m*(X) - {W - e*(X)}*Ï„*(X))Â²] 

# R-learner uses estimates
r_loss = E[(Y - mÌ‚(X) - {W - Ãª(X)}*Ï„Ì‚(X))Â²]

# Double robustness ensures:
if rate(mÌ‚ - m*) * rate(Ãª - e*) â†’ 0 faster than oracle_rate:
    rate(R-learner) â‰ˆ oracle_rate  # ğŸ¯
```

# 6. ì‹¤ìš©ì  êµ¬í˜„ ê°€ì´ë“œ

## 6.1 Python Implementation

```python
import numpy as np
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_predict

def doubly_robust_ate(Y, W, X, outcome_model=None, propensity_model=None):
    """
    Doubly Robust ATE ì¶”ì •
    
    Parameters:
    -----------
    Y : array-like, shape (n,)
        Outcome variable
    W : array-like, shape (n,)  
        Treatment indicator (0 or 1)
    X : array-like, shape (n, p)
        Covariates
    outcome_model : sklearn estimator
        Outcome regression model
    propensity_model : sklearn estimator  
        Propensity score model
    
    Returns:
    --------
    ate_estimate : float
        Doubly robust ATE estimate
    """
    
    n = len(Y)
    
    # Default models
    if outcome_model is None:
        outcome_model = RandomForestRegressor(n_estimators=100, random_state=42)
    if propensity_model is None:
        propensity_model = LogisticRegression(random_state=42)
    
    # Cross-fitting for outcome model
    mu_1_hat = np.zeros(n)
    mu_0_hat = np.zeros(n)
    
    # Estimate E[Y|X,W=1]
    idx_1 = W == 1
    if np.sum(idx_1) > 0:
        mu_1_hat[idx_1] = cross_val_predict(
            outcome_model, X[idx_1], Y[idx_1], cv=5
        )
        outcome_model.fit(X[idx_1], Y[idx_1])
        mu_1_hat[~idx_1] = outcome_model.predict(X[~idx_1])
    
    # Estimate E[Y|X,W=0]  
    idx_0 = W == 0
    if np.sum(idx_0) > 0:
        mu_0_hat[idx_0] = cross_val_predict(
            outcome_model, X[idx_0], Y[idx_0], cv=5
        )
        outcome_model.fit(X[idx_0], Y[idx_0])
        mu_0_hat[~idx_0] = outcome_model.predict(X[~idx_0])
    
    # Cross-fitting for propensity score
    e_hat = cross_val_predict(
        propensity_model, X, W, cv=5, method='predict_proba'
    )[:, 1]
    
    # Clip propensity scores to avoid extreme weights
    e_hat = np.clip(e_hat, 0.01, 0.99)
    
    # AIPW estimator
    aipw_components = (
        (mu_1_hat - mu_0_hat) +  # Outcome-based estimate
        W * (Y - mu_1_hat) / e_hat -  # IPW correction for treated
        (1 - W) * (Y - mu_0_hat) / (1 - e_hat)  # IPW correction for control
    )
    
    ate_estimate = np.mean(aipw_components)
    
    # Standard error (using influence function)
    influence_func = aipw_components - ate_estimate
    se_estimate = np.std(influence_func) / np.sqrt(n)
    
    return {
        'ate': ate_estimate,
        'se': se_estimate,
        'ci_lower': ate_estimate - 1.96 * se_estimate,
        'ci_upper': ate_estimate + 1.96 * se_estimate
    }

# Usage example
def simulate_data(n=1000):
    """Generate simulation data"""
    X = np.random.normal(0, 1, (n, 3))
    
    # True propensity score (logistic)
    e_true = 1 / (1 + np.exp(-(X[:, 0] + 0.5 * X[:, 1])))
    W = np.random.binomial(1, e_true)
    
    # True outcome model (nonlinear)
    Y_0 = X[:, 0]**2 + X[:, 1] + 0.5 * X[:, 2] + np.random.normal(0, 0.5, n)
    Y_1 = Y_0 + 2 + X[:, 0]  # Heterogeneous treatment effect
    Y = W * Y_1 + (1 - W) * Y_0
    
    return Y, W, X, 2.0  # True ATE â‰ˆ 2.0

# Run example
Y, W, X, true_ate = simulate_data()
result = doubly_robust_ate(Y, W, X)

print(f"True ATE: {true_ate:.3f}")
print(f"DR Estimate: {result['ate']:.3f}")
print(f"95% CI: [{result['ci_lower']:.3f}, {result['ci_upper']:.3f}]")
```

## 6.2 Best Practices

### **1. ëª¨ë¸ ì„ íƒ ê°€ì´ë“œë¼ì¸**

```python
# Outcome Model
# - High flexibility for complex relationships
outcome_models = {
    'linear': LinearRegression(),
    'rf': RandomForestRegressor(n_estimators=200),
    'xgb': XGBRegressor(),
    'nn': MLPRegressor(hidden_layer_sizes=(100, 50))
}

# Propensity Model  
# - Focus on overlap and calibration
propensity_models = {
    'logistic': LogisticRegression(C=1.0),
    'rf': RandomForestClassifier(n_estimators=200),
    'calibrated': CalibratedClassifierCV(LogisticRegression())
}
```

### **2. ì§„ë‹¨ ë„êµ¬**

```python
def diagnose_overlap(e_hat, W):
    """Check overlap assumption"""
    print(f"Propensity score range: [{e_hat.min():.3f}, {e_hat.max():.3f}]")
    print(f"Extreme scores (< 0.1 or > 0.9): {np.mean((e_hat < 0.1) | (e_hat > 0.9)):.1%}")
    
    # Plot distributions
    import matplotlib.pyplot as plt
    plt.figure(figsize=(10, 4))
    
    plt.subplot(1, 2, 1)
    plt.hist(e_hat[W==0], alpha=0.7, label='Control', bins=30)
    plt.hist(e_hat[W==1], alpha=0.7, label='Treated', bins=30)
    plt.xlabel('Propensity Score')
    plt.ylabel('Frequency')
    plt.legend()
    plt.title('Overlap Check')
    
    plt.subplot(1, 2, 2)
    plt.boxplot([e_hat[W==0], e_hat[W==1]], labels=['Control', 'Treated'])
    plt.ylabel('Propensity Score')
    plt.title('Distribution by Treatment')
    
    plt.tight_layout()
    plt.show()

def check_balance(X, W, e_hat):
    """Check covariate balance after weighting"""
    weights_1 = W / e_hat
    weights_0 = (1 - W) / (1 - e_hat)
    
    for j in range(X.shape[1]):
        mean_1 = np.average(X[W==1, j], weights=weights_1[W==1])
        mean_0 = np.average(X[W==0, j], weights=weights_0[W==0])
        print(f"Variable {j}: Weighted difference = {mean_1 - mean_0:.4f}")
```

# 7. ìµœì‹  ì—°êµ¬ ë™í–¥ê³¼ í™•ì¥

## 7.1 Machine Learningê³¼ì˜ ê²°í•©

### **1. Targeted Maximum Likelihood Estimation (TMLE)**
- **One-step correction**: ì´ˆê¸° ì¶”ì •ê°’ì„ ì—…ë°ì´íŠ¸
- **Cross-validation**: ìµœì  ëª¨ë¸ ì„ íƒ

### **2. Causal Random Forests**
- **Honest splitting**: í¸í–¥ ì—†ëŠ” ì¶”ì •
- **Local centering**: Double robustness ë³´ì¥

### **3. Neural Network Approaches**
- **Representation learning**: ê³µí†µ íŠ¹ì„± í•™ìŠµ
- **Domain adaptation**: Treatment/control domain ì •ë ¬

## 7.2 ê³ ì°¨ì› ë°ì´í„°ì—ì„œì˜ í™•ì¥

### **Debiased Machine Learning (DML)**
```python
# Neyman orthogonality + Cross-fitting
def debiased_ml_ate(Y, W, X):
    # Step 1: Estimate nuisance functions
    theta_0 = estimate_initial(Y, W, X)
    
    # Step 2: Compute orthogonal score
    psi = compute_orthogonal_score(Y, W, X, theta_0)
    
    # Step 3: Solve orthogonal equation
    theta_final = solve_orthogonal_equation(psi)
    
    return theta_final
```

## 7.3 ì—°ì† ì²˜ì¹˜ì™€ ë‹¤ì¤‘ ì²˜ì¹˜

### **Continuous Treatment**
```python
# GPS (Generalized Propensity Score) + Outcome Model
def continuous_dr(Y, T, X):
    # Estimate treatment density: f(T|X)
    gps_hat = estimate_gps(T, X)
    
    # Estimate dose-response: E[Y|T,X]
    outcome_hat = estimate_outcome(Y, T, X)
    
    # Double robust estimand
    dr_estimate = compute_dr_continuous(Y, T, X, gps_hat, outcome_hat)
    
    return dr_estimate
```

# 8. ê²°ë¡ : Double Robustnessì˜ ì˜ì˜

## 8.1 ì´ë¡ ì  ê¸°ì—¬

1. **í¸í–¥ ì—†ëŠ” ì¶”ì •ì˜ ë³´ì¥**: ëª¨ë¸ ì˜¤ì§€ì •ì—ë„ robust
2. **íš¨ìœ¨ì„±**: ìµœì  ì¡°ê±´ì—ì„œ semiparametric efficiency bound ë‹¬ì„±
3. **ìœ ì—°ì„±**: ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ê³¼ ê²°í•© ê°€ëŠ¥

## 8.2 ì‹¤ìš©ì  ê°€ì¹˜

1. **í˜„ì‹¤ì  í•´ê²°ì±…**: ì™„ë²½í•œ ëª¨ë¸ë§ì´ ì–´ë ¤ìš´ í˜„ì‹¤ì—ì„œ ì•ˆì „ë§ ì œê³µ
2. **ë°©ë²•ë¡  ë°œì „ì˜ ê¸°ë°˜**: R-learner, TMLE, DML ë“±ì˜ ì´ë¡ ì  í† ëŒ€
3. **ì‚°ì—… ì‘ìš©**: A/B í…ŒìŠ¤íŠ¸, ê°œì¸í™”, ì •ì±… í‰ê°€ ë“±ì—ì„œ ë„ë¦¬ í™œìš©

## 8.3 ë¯¸ë˜ ì „ë§

Double RobustnessëŠ” ì•ìœ¼ë¡œë„ ì¸ê³¼ì¶”ë¡  ë°©ë²•ë¡  ë°œì „ì˜ í•µì‹¬ ì›ë¦¬ë¡œ ì‘ìš©í•  ê²ƒì…ë‹ˆë‹¤:

- **ë”¥ëŸ¬ë‹ê³¼ì˜ ê²°í•©**: í‘œí˜„ í•™ìŠµ ê¸°ë°˜ double robust ë°©ë²•
- **ì‹¤ì‹œê°„ ì¶”ë¡ **: ì˜¨ë¼ì¸ í•™ìŠµì—ì„œì˜ double robustness
- **ë³µì¡í•œ ì²˜ì¹˜ êµ¬ì¡°**: ë„¤íŠ¸ì›Œí¬, ì‹œê³„ì—´ì—ì„œì˜ í™•ì¥

Double Robustnessë¥¼ ì´í•´í•˜ëŠ” ê²ƒì€ í˜„ëŒ€ ì¸ê³¼ì¶”ë¡ ì˜ í•µì‹¬ì„ íŒŒì•…í•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ê°œë…ì„ ë°”íƒ•ìœ¼ë¡œ ë” robustí•˜ê³  íš¨ìœ¨ì ì¸ ì¸ê³¼ ì¶”ë¡  ë°©ë²•ë“¤ì´ ê³„ì† ë°œì „í•  ê²ƒì…ë‹ˆë‹¤. ğŸ¯

---

**ì°¸ê³ ë¬¸í—Œ**:
- Robins, J.M., Rotnitzky, A., & Zhao, L.P. (1994). Estimation of regression coefficients when some regressors are not always observed.
- Bang, H. & Robins, J.M. (2005). Doubly robust estimation in missing data and causal inference models.
- Chernozhukov, V., et al. (2018). Double/debiased machine learning for treatment and structural parameters.
- Kennedy, E.H. (2020). Towards optimal doubly robust estimation of heterogeneous causal effects. 