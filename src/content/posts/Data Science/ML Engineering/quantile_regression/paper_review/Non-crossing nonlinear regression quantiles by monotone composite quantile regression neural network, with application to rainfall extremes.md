---
title: "Non-crossing nonlinear regression quantiles by monotone composite quantile regression neural network, with application to rainfall extremes"
date: "2025-10-21"
excerpt: "í•œì¤„ ìš”ì•½ : "
category: "Machine Learning"
tags: ["Quantile Regression", "paper review"]
---

- [paper link](https://link.springer.com/article/10.1007/s00477-018-1573-6)

ë…¼ë¬¸ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ëŠ” ì—°ìŠµë„ ê°™ì´


# ê°œë… ìš”ì•½, ìŠ¤ì¼€ì¹˜


---

# 1ë‹¨ê³„. êµ¬ì¡°ë¶€í„° ì¡ëŠ” â€˜ìŠ¤ì¼ˆë ˆí†¤ ë¦¬ë”©â€™ (5~10ë¶„)
> ğŸ“ëª©í‘œ: ì´ ë…¼ë¬¸ì´ ë‚˜ë‘ ê´€ë ¨ ìˆëŠ”ì§€ íŒë‹¨

---

## Abstract
- ë¬¸ì œ ì •ì˜ + í•µì‹¬ ê¸°ì—¬ + ë¹„êµ ëŒ€ìƒì´ ë‚˜ì˜¤ëŠ” ë¶€ë¶„.
- ì—¬ê¸°ì„œ â€œì´ ë…¼ë¬¸ì´ í•´ê²°í•˜ë ¤ëŠ” Pain Pointâ€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´.

ë…¼ë¬¸ì—ì„œëŠ” quantile crossing problemì„ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì œì‹œí•˜ê³  ìˆë‹¤.

> These estimates are prone to â€œquantile crossingâ€, where regression predictions for different quantile probabilities do not increase as probability increases.

> As a remedy, this study introduces a novel nonlinear quantile regression model, the monotone composite quantile regression neural network (MCQRNN)
1. simultaneously estimates multiple non-crossing, nonlinear conditional quantile functions
2. allows for optional monotonicity, positivity/non-negativity, and generalized additive model constraints
3. can be adapted to estimate standard least-squares regression and non-crossing expectile regression functions

MCQRNNì„ ì œì‹œ. ì´ì— ëŒ€í•œ ì„¤ëª….

>  In comparison to standard QRNN models, the ability of the MCQRNN model to incorporate these constraints, in addition to non-crossing, leads to more robust and realistic estimates of extreme rainfall.

QRNN ëª¨ë¸ ëŒ€ë¹„ ì‹¤ì œ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì¡°ê±´ë“¤ë„ í¬í•¨ì‹œì¼œì„œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.

---

## Figure
- ëª¨ë¸ êµ¬ì¡°ë‚˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì£¼ëŠ” ë„ì‹ì´ ëŒ€ë¶€ë¶„ í•µì‹¬ ìš”ì•½ì„.
- â€œì…ë ¥â€“ì²˜ë¦¬â€“ì¶œë ¥â€“í‰ê°€â€ íë¦„ì„ ë¨¸ë¦¿ì†ì— ê·¸ë ¤ë‘¬.

<figure>
  <img src="/post/PaperReview/MCQRNN/figure1.png" alt="MCQRNN" />
  <figcaption>Fig. 1</figcaption>
</figure>
- ê·¸ë¦¼ ì„¤ëª… ì •ë¦¬:
  - (a), (c): ì¼ë°˜ QRNN(Quantile Regression Neural Network) ê²°ê³¼  
  - (b), (d): MCQRNN(Monotone Composite QRNN) ê²°ê³¼  
  - ê° ê·¸ë˜í”„ì˜ ê²€ì€ ì ì€ synthetic data (Eq. 15: a, b / Eq. 16: c, d)ë¥¼ ì˜ë¯¸  
  - ë¬´ì§€ê°œìƒ‰ ê³¡ì„ ë“¤ì€ ì•„ë˜ì—ì„œ ìœ„ë¡œ ê°ê° ë¶„ìœ„ìˆ˜ s = 0.1, 0.2, ..., 0.9 (ì´ 9ê°œ)ë¥¼ ë‚˜íƒ€ëƒ„  
  - íšŒìƒ‰ ì‹¤ì„ : ì‹¤ì œ(ground truth) ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ í•¨ìˆ˜  
- ì£¼ìš” í¬ì¸íŠ¸:  
  - MCQRNNì€ quantile crossing ì—†ì´, ë” ë¶€ë“œëŸ½ê³  í˜„ì‹¤ì ì¸ ë¶„ìœ„ìˆ˜ ê³¡ì„ ì„ ì‚°ì¶œí•¨

MCQRNNì´ ì¡°ê¸ˆ ë” ì•ˆì •ì ì´ë‹¤. 

<figure>
  <img src="/post/PaperReview/MCQRNN/figure2.png" alt="MCQRNN" />
  <figcaption>Fig. 2</figcaption>
</figure>

- Fig. 1b, dì™€ ìœ ì‚¬í•œ ì‹¤í—˜ ê²°ê³¼ì´ì§€ë§Œ,  
  - (a) : MCQRNNì— positivity constraint(ì¶œë ¥ ì–‘ìˆ˜ ì œì•½)ë¥¼ ì¶”ê°€í•¨  
  - (b) : positivity + monotonicity constraint(ì¶œë ¥ ì–‘ìˆ˜ ë° ë‹¨ì¡°ì„± ì œì•½) ëª¨ë‘ ì¶”ê°€  
- (c), (d) : Fig. 1b, dì—ì„œ 0.1, 0.5, 0.9-quantileì— ëŒ€í•´ parametric bootstrap(500íšŒ)ë¡œ ì¶”ì •í•œ 95% ì‹ ë¢°êµ¬ê°„(Confidence Interval) ê²°ê³¼ í‘œì‹œ  

---

## Conclusion
- ì–´ë–¤ ì ì„ â€˜ìƒˆë¡­ê²Œ í–ˆë‹¤â€™ + â€˜í–¥í›„ ê³¼ì œâ€™ë¡œ ì •ë¦¬í•´ë‘ëŠ” ë¶€ë¶„.
- Abstractê³¼ ê²°ë¡ ì„ ë¹„êµí•˜ë©´ â€œì§„ì§œ ì–»ì€ ê±´ ë­”ê°€â€ë¥¼ íŒŒì•… ê°€ëŠ¥.

> MCQRNN is the first neural network-based quantile regression model that guarantees non-crossing of regression quantiles.

> Given its close relationship to composite QR models, MCQRNN is first evaluated using the Monte Carlo simulation experiments adopted by Xu et al. (2017) to demonstrate the CQRNN model

ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ robust í•˜ë‹¤.

- MCQRNNì€ ê¸°ì¡´ MLP, QRNN, CQRNN ëª¨ë¸ ëŒ€ë¹„ íŠ¹íˆ ì˜¤ì°¨ ë¶„í¬ê°€ ë¹„ì •ê·œ(non-normal)ì¼ ë•Œ ë”ìš± ê²¬ê³ í•˜ë‹¤.
- ìºë‚˜ë‹¤ ê°•ìš°ëŸ‰(IDF ê³¡ì„ ) ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì‹¤ì œ ì„±ëŠ¥ì„ í‰ê°€í•¨.
- ë‹¤ì–‘í•œ í­í’ ì§€ì† ì‹œê°„ê³¼ ì¬í˜„ê¸°ê°„ì— ëŒ€í•´ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê³µìœ í•˜ì—¬ ê³¼ì í•©ì— ê°•í•˜ë‹¤(cross-validation result).
- ë‹¨ì¡° ì œì•½(monotonicity constraint) ì ìš©ì´ ê°€ëŠ¥í•˜ì—¬, ê°•ìš° ê°•ë„ê°€ ë°œìƒ ë¹ˆë„ ë° ì§€ì† ì‹œê°„ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ìì—°ìŠ¤ëŸ½ê²Œ ì¦ê°€í•˜ëŠ” ì‹¤ì œ íŠ¹ì„±ì„ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.
- ê³„ì¸¡ì†Œê°€ ì—†ëŠ” ì§€ì—­ì—ì„œë„ ê·¹í•œ ê°•ìš°ì˜ í˜„ì‹¤ì ì´ê³  ì‹ ë¢°ì„± ìˆëŠ” ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤.

---

## 1ë¬¸ì¥ ìš”ì•½ ì‹œë„
- ì˜ˆ: â€œì´ ë…¼ë¬¸ì€ ê¸°ì¡´ GBDT ê¸°ë°˜ CTR ì˜ˆì¸¡ì˜ calibration ë¬¸ì œë¥¼ NN ê¸°ë°˜ìœ¼ë¡œ í•´ê²°í•¨.â€
- ì´ í•œ ë¬¸ì¥ì´ ì•ˆ ë‚˜ì˜¤ë©´ â†’ ì•„ì§ ìš”ì•½í•  ìˆ˜ì¤€ìœ¼ë¡œ ì´í•´ê°€ ì•ˆ ëœ ê²ƒ.

QRNNì— ë‹¤ì–‘í•œ ì œì•½ì¡°ê±´ì„ ì¶”ê°€í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆê³ , ì´ë¥¼ í†µí•´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.


---

# 2ë‹¨ê³„. â€˜êµ¬ì¡°ì  ë¦¬ë”©â€™ â€” í•µì‹¬ë§Œ ê¹Šê²Œ (20~40ë¶„)
> ğŸ“ëª©í‘œ: í•µì‹¬ ì•„ì´ë””ì–´Â·ëª¨ë¸Â·ì‹¤í—˜ êµ¬ì¡°ë¥¼ ë¹ ë¥´ê²Œ ì¬êµ¬ì„±í•˜ê¸°

---

## Introduction
- â€œì™œ ì´ ë¬¸ì œì¸ê°€?â€ â€” ë¬¸ì œì˜ ì¤‘ìš”ì„±
- â€œê¸°ì¡´ ë°©ë²•ì˜ í•œê³„â€ â€” baseline ì •ë¦¬
- â€œìš°ë¦¬ì˜ ê¸°ì—¬â€ â€” bullet pointë¡œ ì„¸ ì¤„ ì •ë¦¬

to provide estimates of predictive uncertinity in forcast ìœ„í•´ quantile regressionì„ ì§„í–‰í•´ì™”ìŒ.

> However, given finite samples, this flexibility can lead to â€˜â€˜quantile crossingâ€™â€™ where, for some values of the covariates, quantile regression predictions do not increase with the specified quantile probability tau.

>  As Ouali et al. (2016) state, â€˜â€˜crossing quantile regression is a serious modeling problem that may lead to an invalid response distributionâ€™â€™.
  
ê³ ì „ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ , "quantile crossing problem".

> Three main approaches have been used to solve the quantile crossing problem: post-processing, stepwise estimation, and simultaneous estimation. 

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 3ê°€ì§€ ë°©ë²•ì„ ì‹œë„í•´ì™”ìŒ.

1. post-processing : ê°•ì œë¡œ ìˆœì„œ ì •ë ¬ ì‹œí‚¤ëŠ” ê²ƒ
2. stepwise estimation : ì´ì „ì— ì¶”ì •ëœ ë¶„í¬ì„ ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì•½ ì¡°ê±´ì„ ê±¸ì–´ê°€ë©° ìˆœì°¨ì ìœ¼ë¡œ íšŒê·€ì„ ì„ í•™ìŠµí•¨. (ì´ì „ ë¶„í¬ì„ ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ê³„ë¥¼ ë‚˜ëˆ  êµì°¨ë¥¼ ë°©ì§€í•˜ë©° í•™ìŠµ)
3. simultaneous estimation : ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ íšŒê·€ì‹ì„ ë™ì‹œì— ì¶”ì •í•˜ë©´ì„œ, íŒŒë¼ë¯¸í„° ìµœì í™”ì— ì¶”ê°€ ì œì•½ì¡°ê±´ì„ ë„£ì–´ ë¶„ìœ„ìˆ˜ ê°„ êµì°¨ë¥¼ ë°©ì§€í•¨ (Takeuchi et al. 2006; Bondell et al. 2010; Liu and Wu 2011; Bang et al. 2016).  
í•œì¤„ ìš”ì•½: ëª¨ë“  ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ í•œ ë²ˆì— í•™ìŠµ + êµì°¨ ë°©ì§€ ì œì•½ ì¶”ê°€

> Unlike sequential estimation, simultaneous estimation is attractive because it does not depend on the order in which quantiles are estimated. Furthermore, fitting for multiple values of tau simultaneously allows one to â€˜â€˜borrow strengthâ€™â€™ across regression quantiles and improve overall model performance (Bang et al. 2016).

ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ë¥¼ í•œë²ˆì— í•™ìŠµí•˜ëŠ” ê±´ ì œì•½ì¡°ê±´ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ê²ƒ ì™¸ì—ë„ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì„ ì¤Œ.

> For a flexible nonlinear model like a neural network, imposing extra constraints, for example as informed by process knowledge, can be useful for narrowing the overall search space of potential nonlinearities.

íŠ¹íˆ non-linear í•  ë•Œ ê°€ì¥ ë„ì›€ì„ ì¤Œ. êµ‰ì¥íˆ ë§ì€ ê³µê°„ì„ íƒìƒ‰í•˜ëŠ” non-linear + constraintëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ê²ƒì´ë¼ ê¸°ëŒ€ ë¨.

- Muggeo et al. (2013): ì„±ì¥ ê³¡ì„ ì´ ë‚˜ì´ì— ë”°ë¼ ë‹¨ì¡° ì¦ê°€í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ë“¤ì–´, non-crossing ì œì•½ì— ë”í•´ monotonicity(ë‹¨ì¡°ì„±) ì œì•½ ì¡°ê±´ì„ ì¶”ê°€í•¨.
- Roth et al. (2015): ë¹„ì„ í˜• ë‹¨ì¡° ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ í™œìš©í•´ ê°•ìš° ê·¹ê°’(rainfall extremes)ì—ì„œ ê°ì†Œ ë˜ëŠ” ì¦ê°€í•˜ëŠ” ì¶”ì„¸(ë‹¨ì¡° ì¶”ì„¸)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì„¤ëª…í•¨.
- Takeuchi et al. (2006): ì»¤ë„ ê¸°ë°˜ ë¹„ëª¨ìˆ˜(nonparametric) ë°©ì‹ì˜ ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ ì œì•ˆí–ˆê³ , ì´ ëª¨ë¸ì€ SVMê³¼ ìœ ì‚¬í•œ êµ¬ì¡°ì— non-crossing, monotonicity(ë‹¨ì¡°ì„±) ì œì•½ì„ ëª¨ë‘ ì ìš©í•¨. ì¶”ê°€ì ìœ¼ë¡œ ì–‘ìˆ˜ì„±(positivity), ê°€ì‚°ì„±(additivity) ë“± ë‹¤ì–‘í•œ ì œì•½ì¡°ê±´ ì ìš© ë°©ë²•ë„ ì œì‹œí•¨.

> However, standard implementations of the kernel quantile regression model (e.g., Karatzoglou et al. 2004; Hofmeister 2017) are computationally costly, with complexity that is cubic in the number of samples, and do not explicitly implement the proposed constraints.

ì´ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” í•µì‹¬. ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ë¹„ì‹¸ë‹¤ëŠ” í•œê³„ì ì„ ê°€ì§€ê³  ìˆë‹¤. 

> As an alternative, this study introduces an efficient, flexible nonlinear quantile regression model, the monotone composite quantile regression neural network (MCQRNN), that:
1. simultaneously estimates multiple non-crossing quantile functions
2. allows for optional monotonicity, positivity/nonnegativity, and additivity constraints, as well as fine-grained control on the degree of non-additivity
3. can be modified to estimate standard least-squares regression and non-crossing expectile regression functions

ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì€ ì•„ë˜ì™€ ê°™ì€ ê¸°ì¡´ neural network/íšŒê·€ ëª¨ë¸ ìš”ì†Œë“¤ì„ ê²°í•©í•´ì„œ í•˜ë‚˜ì˜ í†µí•© í”„ë ˆì„ì›Œí¬ë¡œ êµ¬í˜„í•¨:

| ì°¸ê³  ëª¨ë¸                   | í•µì‹¬ ì•„ì´ë””ì–´                         |
|-----------------------------|--------------------------------------|
| **QRNN** (White 1992; Taylor 2000; Cannon 2011)      | í‘œì¤€ quantile regression NN êµ¬ì¡°         |
| **Monotone MLP** (Zhang & Zhang 1999; Lang 2005; Minin et al. 2010) | ë‹¨ì¡°ì„± ì œì•½ ì¶”ê°€                       |
| **Composite QRNN (CQRNN)** (Xu et al. 2017)         | ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ ë™ì‹œ ì¶”ì •                    |
| **Expectile Regression NN** (Jiang et al. 2017)     | expectile íšŒê·€ í™•ì¥                     |
| **Generalized Additive NN** (Potts 1999)            | ê°€ì‚°ì„± ë“± ì¶”ê°€ ì œì•½ ì ìš©                |

MCQRNNì€ ë³¸ ë…¼ë¬¸ ê¸°ì¤€ ìµœì´ˆë¡œ, ì‹ ê²½ë§ ê¸°ë°˜ì—ì„œ **ë¹„êµì°¨(non-crossing)** ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ ë³´ì¥í•˜ëŠ” ëª¨ë¸ì„.

MCQRNN ëª¨ë¸ ê°œë°œ íë¦„ê³¼ ë…¼ë¬¸ì˜ êµ¬ì„±ì€ ì•„ë˜ì™€ ê°™ë‹¤.

- **Sect. 2**: MMLP â†’ MQRNN â†’ ìµœì¢… MCQRNN ìˆœì„œë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¥í•¨.  
  - ë‹¨ì¡°ì„±(monotonicity), ì–‘ìˆ˜ì„±/ë¹„ìŒìˆ˜ì„±(positivity/non-negativity), ê°€ì‚°ì„±(generalized additive) ì œì•½ì„ ì†ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨.
  - ì¡°ê±´ë¶€ tau-ë¶„ìœ„ìˆ˜(quantile) ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ë„ ì œì‹œ.

- **Sect. 3**: Monte Carlo ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ MCQRNNê³¼ ê¸°ì¡´ MLP, QRNN, CQRNN ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦.
  - ë¹„êµ ì‹¤í—˜ì—ëŠ” Xu et al. (2017)ì˜ 3ê°€ì§€ í•¨ìˆ˜ì™€ ì˜¤ì°¨ ë¶„í¬ ì¡°í•©ì„ ì‚¬ìš©.

- **Sect. 4**: ì‹¤ì œ ìºë‚˜ë‹¤ ê¸°í›„ ë°ì´í„°(ì—°ê°„ ìµœëŒ€ ê°•ìˆ˜ëŸ‰)ë¡œ MCQRNN ì ìš© ì‚¬ë¡€ ì œì‹œ.
  - ê´€ì¸¡ì†Œ ë¯¸ì„¤ì¹˜ ì§€ì—­ì˜ IDF ê³¡ì„ (Intensity-Duration-Frequency curve) ì˜ˆì¸¡ ë¬¸ì œì— í™œìš©.
  - IDF ê³¡ì„ ì€ ê·¹í•œ ê°•ìˆ˜(Intensity)ì˜ ë¹ˆë„(Occurrence frequency) ë° ì§€ì†ì‹œê°„(Duration)ê³¼ì˜ ê´€ê³„ë¥¼ ìš”ì•½í•˜ë©°, ê·¹í•œ ê°•ìˆ˜ëŸ‰ì€ ë¹„ìŒìˆ˜ì´ê³ , ë°œìƒí™•ë¥ ì´ ë‚®ê±°ë‚˜(ì¦‰, ì¬í˜„ì£¼ê¸°ê°€ ê¸¸ê±°ë‚˜) ì§€ì†ì‹œê°„ì´ ì§§ì„ìˆ˜ë¡ ê°•ë„ê°€ ë†’ì•„ì§€ëŠ” â€˜ë‹¨ì¡° ì¦ê°€â€™ ì„±ì§ˆì„ ë‚˜íƒ€ëƒ„.
  - ë”°ë¼ì„œ ë‹¨ì¡°ì„±, ë¹„ìŒìˆ˜ì„± ì œì•½ì´ íŠ¹íˆ ì¤‘ìš”.
  - MCQRNN ê¸°ë°˜ IDF ê³¡ì„ ì€ ê° ë¦¬í„´í”¼ë¦¬ì–´ë“œ/ì§€ì†ì‹œê°„ë³„ QRNNì„ ë³„ë„ë¡œ ì í•©ì‹œí‚¤ëŠ” ê¸°ì¡´ ë°©ì‹(Ouali & Cannon, 2018)ê³¼ ë¹„êµ ë¶„ì„.

- **Sect. 5**: ê²°ë¡  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥ ì œì‹œ.


---

## Method
- ë„ì‹(fig)ì„ ë”°ë¼ê°€ë©´ì„œ ìˆ˜ì‹ì€ ê±´ë„ˆë›°ë˜,
  - ê° blockì´ í•˜ëŠ” ì—­í• ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìš”ì•½: (ì˜ˆ: Encoder â†’ user/item embedding, Decoder â†’ CTR prediction).
  - â€œê¸°ì¡´ ëŒ€ë¹„ ë°”ë€ ì â€ì„ ë§ˆí‚¹í•´. (attention ì¶”ê°€, loss ë³€ê²½ ë“±)

### Modeling framework

#### Monotone multi-layer perceptron (MMLP)

MMLPëŠ” ì¼ë¶€ ì…ë ¥ ë³€ìˆ˜ì— ëŒ€í•´ ë‹¨ì¡°ì„±(monotonicity)ì„ ë³´ì¥í•˜ëŠ” neural networkë‹¤. ì…ë ¥ ë³€ìˆ˜ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤:
- $M$: ë‹¨ì¡°ì„±ì„ ì›í•˜ëŠ” ë³€ìˆ˜ë“¤ (monotone variables)
- $I$: ë‹¨ì¡°ì„± ì œì•½ì´ ì—†ëŠ” ë³€ìˆ˜ë“¤ (ignore variables)

**Hidden layer:**

$$
h_j(t) = f\left( \sum_{m \in M} x_m(t) \exp(W_{mj}^{(h)}) + \sum_{i \in I} x_i(t) W_{ij}^{(h)} + b_j^{(h)} \right )
$$

ì—¬ê¸°ì„œ:
- $h_j(t)$: $j$ë²ˆì§¸ hidden unitì˜ ì¶œë ¥
- $f$: ë¹„ì„ í˜• activation function (ì˜ˆ: sigmoid, tanh)
- $\exp(W_{mj}^{(h)})$: ë‹¨ì¡° ë³€ìˆ˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ëŠ” exponentialì„ ì·¨í•´ í•­ìƒ ì–‘ìˆ˜ë¥¼ ë³´ì¥
- $W_{ij}^{(h)}$: ì¼ë°˜ ë³€ìˆ˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ (ì œì•½ ì—†ìŒ)
- $b_j^{(h)}$: bias term

**Output layer:**

$$
g(t) = \sum_{j=1}^{J} h_j(t) \exp(W_j^{(o)}) + b^{(o)}
$$

ì—¬ê¸°ì„œ:
- $g(t)$: ìµœì¢… ì¶œë ¥
- $\exp(W_j^{(o)})$: hidden layerì—ì„œ outputìœ¼ë¡œ ê°€ëŠ” ê°€ì¤‘ì¹˜ë„ exponentialì„ ì·¨í•´ ì–‘ìˆ˜ ë³´ì¥
- $b^{(o)}$: output bias
- $J$: hidden unitì˜ ê°œìˆ˜

í•µì‹¬ì€ ë‹¨ì¡°ì„±ì„ ì›í•˜ëŠ” ê²½ë¡œì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ì— $\exp(\cdot)$ë¥¼ ì ìš©í•˜ì—¬ ì–‘ìˆ˜ë¡œ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë‹¨ì¡° ë³€ìˆ˜ $x_m$ì´ ì¦ê°€í•  ë•Œ ì¶œë ¥ $g(t)$ë„ ë°˜ë“œì‹œ ì¦ê°€í•˜ê²Œ ëœë‹¤ (activation function $f$ê°€ ë¹„ê°ì†Œ í•¨ìˆ˜ì¼ ë•Œ).

#### Monotone quantile regression neural network (MQRNN)

MQRNNì€ MMLPì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, loss functionì„ quantile regressionìš© pinball lossë¡œ ë³€ê²½í•œ ëª¨ë¸ì´ë‹¤.

**Pinball loss (Check loss):**

$$
\rho_\tau(u) = u(\tau - \mathbb{I}_{u < 0})
$$

ì—¬ê¸°ì„œ:
- $u = y - g(t)$: ì˜ˆì¸¡ ì˜¤ì°¨ (ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)
- $\tau \in (0, 1)$: ëª©í‘œ ë¶„ìœ„ìˆ˜ (ì˜ˆ: 0.5ëŠ” ì¤‘ì•™ê°’)
- $\mathbb{I}_{u < 0}$: indicator function (u < 0ì´ë©´ 1, ì•„ë‹ˆë©´ 0)

Pinball lossëŠ” ë¹„ëŒ€ì¹­ì (asymmetric)ì¸ ì†ì‹¤ í•¨ìˆ˜ë¡œ:
- $u \geq 0$ (ê³¼ì†Œì¶”ì •)ì¼ ë•Œ: $\tau \cdot u$
- $u < 0$ (ê³¼ëŒ€ì¶”ì •)ì¼ ë•Œ: $(\tau - 1) \cdot u = -(1-\tau) \cdot u$

ì˜ˆë¥¼ ë“¤ì–´ $\tau = 0.9$ì¸ ê²½ìš°:
- ê³¼ì†Œì¶”ì • ì‹œ í˜ë„í‹°: $0.9 \times u$ (í° í˜ë„í‹°)
- ê³¼ëŒ€ì¶”ì • ì‹œ í˜ë„í‹°: $0.1 \times |u|$ (ì‘ì€ í˜ë„í‹°)

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ $\tau$-ë¶„ìœ„ìˆ˜ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤.

**Training objective:**

$$
\min_{W, b} \sum_{t=1}^{T} \rho_\tau(y(t) - g(t))
$$

MQRNNì€ MMLPì˜ ë‹¨ì¡°ì„± ì œì•½ì„ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì • ë¶„ìœ„ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ë¥¼ ë™ì‹œì— ì¶”ì •í•˜ì§€ëŠ” ëª»í•˜ë©°, ê° $\tau$ë§ˆë‹¤ ë³„ë„ë¡œ í•™ìŠµí•´ì•¼ í•œë‹¤. ì´ ê²½ìš° quantile crossingì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

**Huber-norm approximation:**

Pinball lossì˜ ë¬¸ì œì ì€ ì›ì ($u = 0$)ì—ì„œ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥(non-differentiable)í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. Gradient-based optimizationì„ ìœ„í•´ì„œëŠ” smooth approximationì´ í•„ìš”í•˜ë‹¤.

Chen (2007)ê³¼ Cannon (2011)ì„ ë”°ë¼, Huber-norm ë²„ì „ìœ¼ë¡œ pinball lossë¥¼ ê·¼ì‚¬í•œë‹¤:

$$
\rho_\tau^{(A)}(e) = 
\begin{cases}
\tau \cdot u(e) & \text{if } e \geq 0 \\
(\tau - 1) \cdot u(e) & \text{if } e < 0
\end{cases}
$$

ì—¬ê¸°ì„œ Huber functionì€:

$$
u(e) = 
\begin{cases}
\frac{e^2}{2\alpha} & \text{if } 0 \leq |e| \leq \alpha \\
|e| - \frac{\alpha}{2} & \text{if } |e| > \alpha
\end{cases}
$$

Huber functionì˜ íŠ¹ì§•:
- $|e| \leq \alpha$ êµ¬ê°„: squared error ($e^2/2\alpha$) ì‚¬ìš© â†’ ì›ì ì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥
- $|e| > \alpha$ êµ¬ê°„: absolute error ($|e| - \alpha/2$) ì‚¬ìš© â†’ quantile regressionì˜ íŠ¹ì„± ìœ ì§€
- $\alpha \to 0$ì¼ ë•Œ: ì •í™•í•œ pinball lossë¡œ ìˆ˜ë ´
- $\alpha$ëŠ” hyperparameterë¡œ, smoothnessì™€ ì •í™•ë„ ì‚¬ì´ì˜ trade-offë¥¼ ì¡°ì ˆ

ì´ approximationì„ ì‚¬ìš©í•˜ë©´ gradient descentë¡œ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.








---


## Experiment
- ì–´ë–¤ ë°ì´í„°ì…‹, ë¹„êµ ëª¨ë¸, ì§€í‘œ, í–¥ìƒ ì •ë„ì¸ê°€
- í‘œ 1ê°œ, ê·¸ë¦¼ 1ê°œë§Œ ì„ íƒí•´ì„œ ìˆ«ì ë©”ëª¨
- ë‚˜ì¤‘ì— â€œì´ ë…¼ë¬¸ì€ ê¸°ì¡´ ëŒ€ë¹„ ~% í–¥ìƒâ€ ì´ëŸ° ì‹ìœ¼ë¡œ ë°”ë¡œ ì¸ìš© ê°€ëŠ¥í•˜ê²Œ

---

## í•œ ì¤„ ìš”ì•½ ê°±ì‹ 
- (ë¬¸ì œ) â€” (í•µì‹¬ ì•„ì´ë””ì–´) â€” (ê²°ê³¼) êµ¬ì¡°ë¡œ ë‹¤ì‹œ ìš”ì•½
- ì˜ˆ: â€œImbalanced CTR ë°ì´í„°ì—ì„œ feature interactionì˜ ê³¼ì í•©ì„ ì™„í™”í•˜ê¸° ìœ„í•´ regularized cross-networkë¥¼ ì œì•ˆí–ˆê³ , AUC +0.7% í–¥ìƒ.

---


# 3ë‹¨ê³„. â€˜ë…¼ë¦¬ì  ë¦¬ë”©â€™ â€” ì •ë§ ì“¸ ë…¼ë¬¸ë§Œ (1~2ì‹œê°„)
> ğŸ“ëª©í‘œ: ì´ë¡ ì  ê·¼ê±°ì™€ ì¬í˜„ ê°€ëŠ¥ì„±ì„ ì™„ì „íˆ ì´í•´

---

## ìˆ˜ì‹/ê°€ì • ì •ë¦¬
- ì´ Lossê°€ ì‹¤ì œë¡œ convexì¸ì§€, regularizerì˜ ì˜ë¯¸, gradient ê³„ì‚° êµ¬ì¡° ë“± â€˜ì´ë¡ ì  ì •ë‹¹ì„±â€™ í™•ì¸.

---

## Discussion / Ablation / Limitation
- ì—°êµ¬ìë“¤ì´ ì¸ì •í•œ í•œê³„ì™€ future workëŠ” ë„¤ê°€ í›„ì† í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¡œ ì¨ë¨¹ì„ í¬ì¸íŠ¸.

---

## Reference Jump
- ì¸ìš©ëœ í•µì‹¬ 2~3ê°œ ë…¼ë¬¸ì„ ë°”ë¡œ ì²´í¬í•´ â€œê³„ë³´â€ íŒŒì•….
- ì´ê²Œ â€˜ë¦¬ì„œì¹˜ íŠ¸ë¦¬(tree)â€™ë¥¼ í˜•ì„±í•¨.