---
title: "MSE Lossë¥¼ ë” ê¹Šê²Œ ì´í•´í•´ë³´ê¸°"
date: "2025-11-28"
excerpt: "ì™œ ê¸°ë³¸ì ìœ¼ë¡œ MSE lossë¥¼ ì‚¬ìš©í• ê¹Œ?"
category: "Data Science"
tags: ["ê¸°ë³¸ê¸°", "MSE", "MAE", "huber"]
---

regression modelì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ mse lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì™œ ê·¸ëŸ°ê±¸ê¹Œ?
ë‚˜ ê¸°ë³¸ê¸° ì§„ì§œ ë¶€ì¡±í•˜êµ¬ë‚˜...ã…ã…

ê°œë… í•œë²ˆ ì •ë¦¬í•´ë³´ì.

## ë“¤ì–´ê°€ë©°

Regression ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ MSE lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì™œ ê·¸ëŸ´ê¹Œìš”? ë‹¨ìˆœíˆ "ì„±ëŠ¥ì´ ì˜ ë‚˜ì™€ì„œ"ê°€ ì•„ë‹ˆë¼, **ì´ë¡ ì  ë°°ê²½**ì´ ìˆìŠµë‹ˆë‹¤.

> **ğŸ’¡ í•µì‹¬ ê°œë…**  
> MSE lossëŠ” **error termì„ Gaussianìœ¼ë¡œ ê°€ì •**í•˜ê³  **Maximum Likelihood Estimation (MLE)** ê´€ì ì—ì„œ ìì—°ìŠ¤ëŸ½ê²Œ ë„ì¶œë©ë‹ˆë‹¤. ì´ ê°€ì •ì˜ ë°°ê²½ì—ëŠ” **ì¤‘ì‹¬ê·¹í•œì •ë¦¬(Central Limit Theorem, CLT)**ê°€ ìˆìŠµë‹ˆë‹¤.

> error termì„ Gaussianìœ¼ë¡œ ê°€ì •í•˜ê³  maximum likelihood estimation ê´€ì ì—ì„œ í•´ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
> ê·¸ ê°€ì •ì˜ ë°°ê²½ì—ëŠ” ì¤‘ì‹¬ê·¹í•œì •ë¦¬(central limit theorem)ê°€ ìˆìŠµë‹ˆë‹¤. 
> linear regressionê³¼ ê°™ì€ ìƒê°ì˜ íë¦„ì„ deep networksì—ì„œë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒê¹Œì§€ ì´í•´í•˜ë©´, deep networksì—ì„œë„ regression ë¬¸ì œë¥¼ í’€ ë•Œ MSE lossë¥¼ ì“°ëŠ” ê²Œ ìì—°ìŠ¤ëŸ¬ì›Œì§‘ë‹ˆë‹¤. 
> ê·¸ë¦¬ê³ , MSE lossì™¸ì— ë‹¤ë¥¸ lossë¥¼ ì“°ë ¤ë©´ ì¶©ë¶„í•œ rationaleì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤.

---

## 1. MSE Lossì˜ ì´ë¡ ì  ë°°ê²½

### 1.1 ì¤‘ì‹¬ê·¹í•œì •ë¦¬(Central Limit Theorem)

ì¸¡ì • ì˜¤ì°¨ëŠ” ì¼ë°˜ì ìœ¼ë¡œ **ì—¬ëŸ¬ ì‘ì€ ë…ë¦½ì ì¸ ìš”ì¸ë“¤ì˜ í•©**ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤:

```
ì˜¤ì°¨ = ì„¼ì„œ ë…¸ì´ì¦ˆ + í™˜ê²½ ë³€í™” + ì¸¡ì • íƒ€ì´ë° + ...
```

ì¤‘ì‹¬ê·¹í•œì •ë¦¬ì— ë”°ë¥´ë©´:

> **ğŸ“Œ ì°¸ê³ **  
> ë…ë¦½ì ì¸ í™•ë¥ ë³€ìˆ˜ë“¤ì˜ í•©ì€ í‘œë³¸ í¬ê¸°ê°€ ì¶©ë¶„íˆ í¬ë©´ **ì •ê·œë¶„í¬(Gaussian distribution)**ì— ê·¼ì‚¬í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ:
- ì˜¤ì°¨í•­ Îµ ~ N(0, ÏƒÂ²)ë¡œ ê°€ì •í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì 
- ì´ëŠ” ë‹¨ìˆœí•œ í¸ì˜ê°€ ì•„ë‹ˆë¼ **í†µê³„í•™ì  ê·¼ê±°**ê°€ ìˆëŠ” ê°€ì •

### 1.2 Maximum Likelihood Estimation (MLE)

íšŒê·€ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
y = f(x; Î¸) + Îµ,  Îµ ~ N(0, ÏƒÂ²)
```

ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™•ë¥  ëª¨ë¸ë¡œ í•´ì„ë©ë‹ˆë‹¤:

```
p(y | x, Î¸) = N(f(x; Î¸), ÏƒÂ²)
```

**Likelihood function**:

```
L(Î¸) = âˆáµ¢ p(yáµ¢ | xáµ¢, Î¸) = âˆáµ¢ (1/âˆš(2Ï€ÏƒÂ²)) exp(-(yáµ¢ - f(xáµ¢; Î¸))Â²/(2ÏƒÂ²))
```

**Log-likelihood**:

```
log L(Î¸) = Î£áµ¢ [-1/2 log(2Ï€ÏƒÂ²) - (yáµ¢ - f(xáµ¢; Î¸))Â²/(2ÏƒÂ²)]
```

**MLEëŠ” log-likelihoodë¥¼ ìµœëŒ€í™”**í•˜ëŠ”ë°, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ë©ë‹ˆë‹¤:

```
maximize log L(Î¸) âŸº minimize Î£áµ¢ (yáµ¢ - f(xáµ¢; Î¸))Â²
```

> **ğŸ’¡ í•µì‹¬ ê°œë…**  
> **MSE lossëŠ” Gaussian noise ê°€ì • í•˜ì—ì„œ MLEì˜ ê²°ê³¼**ì…ë‹ˆë‹¤. ì¦‰, MSEë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ = ë°ì´í„°ì˜ likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

### 1.3 MSEëŠ” ì¡°ê±´ë¶€ í‰ê· ì„ ì¶”ì •

MSE lossì˜ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ íŠ¹ì„±:

```
argmin E[(y - Å·)Â²] = E[y | x]  (ì¡°ê±´ë¶€ í‰ê· )
```

ì¦‰, **MSE lossë¥¼ ìµœì†Œí™”í•˜ë©´ ì¡°ê±´ë¶€ í‰ê· (conditional mean)**ì„ ì¶”ì •í•˜ê²Œ ë©ë‹ˆë‹¤.

---

## 2. Loss Function ë¹„êµ

### 2.1 ìˆ˜í•™ì  ì •ì˜

| Loss | ìˆ˜ì‹ | ë¯¸ë¶„ | ë…¸ì´ì¦ˆ ë¶„í¬ |
|------|------|------|------------|
| **MSE** | $\frac{1}{n}\sum(y_i - \hat{y}_i)^2$ | $2(y - \hat{y})$ | Gaussian: $p(\epsilon) \propto \exp(-\epsilon^2/2\sigma^2)$ |
| **MAE** | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | $\text{sign}(y - \hat{y})$ | Laplace: $p(\epsilon) \propto \exp(-\|\epsilon\|/b)$ |
| **Huber** | Smooth combination | Smooth | Robust mixture |

### 2.2 ë…¸ì´ì¦ˆ ë¶„í¬ ë¹„êµ

**Gaussian Distribution (MSE)**:
```
p(Îµ) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-ÎµÂ²/(2ÏƒÂ²))
```
- Tailì´ ë¹ ë¥´ê²Œ ê°ì†Œ (exponential decay of ÎµÂ²)
- Outlierì— ë¯¼ê° (ì œê³± penalty)

**Laplace Distribution (MAE)**:
```
p(Îµ) = (1/2b) exp(-|Îµ|/b)
```
- Tailì´ Gaussianë³´ë‹¤ ëŠë¦¬ê²Œ ê°ì†Œ (exponential decay of |Îµ|)
- Outlierì— robust (ì„ í˜• penalty)

<figure>
  <img src="/post/ML/laplace_vs_gaussian.png" alt="Laplace vs Gaussian noise distribution ë¹„êµ">
  <figcaption>Laplace ë¶„í¬(ì í•©: MAE)ì™€ Gaussian ë¶„í¬(ì í•©: MSE) í˜•íƒœ ë¹„êµ</figcaption>
</figure>

### 2.3 ì¶”ì •ëŸ‰ ë¹„êµ

| Loss | ì¶”ì •ëŸ‰ | ë…¸ì´ì¦ˆ ê°€ì • | Robustness | ì‚¬ìš© ì´ìœ  |
|------|--------|------------|------------|-----------|
| **MSE** | Conditional Mean | Gaussian | ë‚®ìŒ (Outlierì— ì•½í•¨) | ì•ˆì •ì , ì´ë¡  ê¸°ë°˜, ë¯¸ë¶„ ê°€ëŠ¥ |
| **MAE** | Conditional Median | Laplace | ë†’ìŒ | Outlierì— robust |
| **Quantile** | Conditional Quantile | Asymmetric Laplace | ì¡°ì ˆ ê°€ëŠ¥ | ë¦¬ìŠ¤í¬/ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§ |
| **Huber** | Hybrid | Robust mixture | ì¤‘ê°„ | MSEì™€ MAEì˜ ì¥ì  ê²°í•© |

---

## 3. Loss ì„ íƒì˜ ì›ì¹™

> **âš ï¸ ì£¼ì˜**  
> Loss ì„ íƒì€ **ì‹¤í—˜ì´ ì•„ë‹ˆë¼ ê°€ì„¤ ê¸°ë°˜**ì´ì–´ì•¼ í•©ë‹ˆë‹¤.

### 3.1 ì˜¬ë°”ë¥¸ ì ‘ê·¼

**"ì™œ MAEë¥¼ ì“°ë‚˜ìš”?"**
- âœ… Outlierê°€ ë§ì€ í™˜ê²½ì´ë¼ëŠ” **ë°ì´í„° ê¸°ë°˜ ê°€ì„¤**
- âœ… Laplace ë¶„í¬ë¥¼ ê°€ì •í•  ìˆ˜ ìˆëŠ” **ì´ë¡ ì  ê·¼ê±°**
- âœ… ì¤‘ì•™ê°’ì´ í‰ê· ë³´ë‹¤ ì í•©í•œ **ë„ë©”ì¸ íŠ¹ì„±**

**"ì™œ Quantile lossë¥¼ ì“°ë‚˜ìš”?"**
- âœ… ë¶ˆí™•ì‹¤ì„±/ë¶„í¬ tailì„ ëª¨ë¸ë§í•˜ë ¤ëŠ” **ëª…í™•í•œ ëª©ì **
- âœ… Upper/lower bound estimationì´ í•„ìš”í•œ **ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­**
- âœ… Risk-sensitive decision making

**"ì™œ Huber lossë¥¼ ì“°ë‚˜ìš”?"**
- âœ… ëŒ€ë¶€ë¶„ì€ Gaussianì´ì§€ë§Œ **ê°€ë” outlier**ê°€ ì¡´ì¬
- âœ… MSEì˜ íš¨ìœ¨ì„±ê³¼ MAEì˜ robustness **ë‘˜ ë‹¤** í•„ìš”
- âœ… ì‹¤ì œ ë°ì´í„° ë¶„í¬ê°€ **heavy-tailed Gaussian**

### 3.2 ì˜ëª»ëœ ì ‘ê·¼

âŒ "ì—¬ëŸ¬ lossë¥¼ ì‹¤í—˜í•´ë´¤ëŠ”ë° ì´ê²Œ 0.01% ë” ì¢‹ì•„ìš”"
- í†µê³„ì  ìœ ì˜ì„±ì´ ì—†ì„ ìˆ˜ ìˆìŒ
- ë‚´ì¼ì˜ ë°ì´í„°ì—ì„œëŠ” ì—­ì „ë  ìˆ˜ ìˆìŒ
- ê³¼ì í•©ì˜ ìœ„í—˜

> **ğŸš¨ ì¤‘ìš”**  
> Regressionì—ì„œ ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ ì•Šì„ ë•Œ **ê·¸ì € ì˜ë˜ê¸°ë¥¼ ë°”ë¼ë©´ì„œ ë‹¤ì–‘í•œ lossë¥¼ ì‹¤í—˜í•´ë³´ëŠ” ê²ƒì€ ì§€ì–‘**í•´ì•¼ í•©ë‹ˆë‹¤.  
>  
> íšŒì‚¬ì—ì„œì˜ AIëŠ” ì˜¤ëŠ˜ì˜ ë°ì´í„°ì—ë§Œ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ë‚´ì¼ê³¼ Nì¼ í›„ì˜ ë°ì´í„°ì—ì„œë„ ì˜ ë™ì‘**í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, marginalí•œ í–¥ìƒë³´ë‹¤ëŠ” **ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ë°©ë²•ë¡ **ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ê¸°ë³¸ì…ë‹ˆë‹¤.

ì˜¬ë°”ë¥¸ ìˆœì„œ:
```
ê°€ì„¤ ìˆ˜ë¦½ â†’ ìˆ˜í•™ì  ë°°ê²½ í™•ì¸ â†’ ëª¨ë¸ë§ ëª©ì  ì •ì˜ â†’ Loss ì„ íƒ â†’ ì‹¤í—˜ ê²€ì¦
```

---

## 4. ì‹¤ì „ í™œìš©

### 4.1 MAE LossëŠ” ì–¸ì œ?

> ê·¸ë ‡ë‹¤ë©´ MAE lossëŠ” ì–¸ì œ í•©ë¦¬ì ì¼ê¹Œìš”? MSEëŠ” ì¡°ê±´ë¶€ í‰ê· ì„, MAEëŠ” ì¡°ê±´ë¶€ ì¤‘ì•™ê°’ì„ ì¶”ì •í•©ë‹ˆë‹¤. 
> ì¤‘ì•™ê°’ì´ í‰ê· ë³´ë‹¤ ì í•©í•œ ìƒí™©, ì˜ˆë¥¼ ë“¤ì–´ heavy-tailed noiseë‚˜ outlierê°€ ë§ì€ ê²½ìš°ë¼ë©´ MAEê°€ ìì—°ìŠ¤ëŸ¬ìš´ ì„ íƒì´ ë©ë‹ˆë‹¤. 
> ì´ ê°œë…ì„ ì´í•´í•˜ë©´ MAE lossë¥¼ ì¼ë°˜í™”í•´ quantile regressionê¹Œì§€ë„ í™•ì¥í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

MSEëŠ” **ì¡°ê±´ë¶€ í‰ê· **, MAEëŠ” **ì¡°ê±´ë¶€ ì¤‘ì•™ê°’**ì„ ì¶”ì •í•©ë‹ˆë‹¤.

**ì í•©í•œ ê²½ìš°**:
- Heavy-tailed noiseê°€ ì˜ˆìƒë˜ëŠ” ê²½ìš°
- Outlierê°€ ë§ì€ ë°ì´í„° (ì˜ˆ: ê¸ˆìœµ, ì„¼ì„œ ë°ì´í„°)
- ì¤‘ì•™ê°’ì´ í‰ê· ë³´ë‹¤ ì˜ë¯¸ ìˆëŠ” ë„ë©”ì¸
- Robustnessê°€ ì¤‘ìš”í•œ ê²½ìš°

**ì˜ˆì‹œ**:
```python
# Outlierì— ë¯¼ê°í•˜ì§€ ì•Šì€ ëª¨ë¸ì´ í•„ìš”í•  ë•Œ
loss = nn.L1Loss()  # MAE
```

### 4.2 Quantile Regression

MAE lossë¥¼ ì¼ë°˜í™”í•˜ë©´ **quantile regression**ê¹Œì§€ í™•ì¥ ê°€ëŠ¥í•©ë‹ˆë‹¤:

```python
def quantile_loss(y_true, y_pred, quantile):
    error = y_true - y_pred
    return torch.mean(torch.max(quantile * error, (quantile - 1) * error))
```

**Quantile loss**:
```
L_Ï„(y, Å·) = Î£áµ¢ Ï_Ï„(yáµ¢ - Å·áµ¢)

where Ï_Ï„(u) = u(Ï„ - ğŸ™{u < 0})
            = Ï„Â·u      if u â‰¥ 0
              (Ï„-1)Â·u  if u < 0
```

- Ï„ = 0.5: MAEì™€ ë™ì¼ (ì¤‘ì•™ê°’)
- Ï„ = 0.9: 90th percentile ì¶”ì •
- Ï„ = 0.1: 10th percentile ì¶”ì •

### 4.3 ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§

> ê·¸ëŸ¼ ì´ê±¸ ì•Œë©´ ì–´ë–¤ ë¶€ë¶„ì—ì„œ í˜„ì—…ì— ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆì„ê¹Œìš”? ìš°ë¦¬ëŠ” ì¢…ì¢… deep networksì—ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•´ì•¼í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. 
> ì´ ë•Œ quantile regressionì„ í•œê°€ì§€ ë°©ë²•ìœ¼ë¡œ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
> íŠ¹íˆ, UCB(upper confidence bound)ê°€ í•„ìš”í•œ ìƒí™©ì—ì„œ ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•˜ê³  baselineìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 
> ì´ëŸ° ë‚´ìš©ë“¤ì— ëŒ€í•œ ì´í•´ê°€ ì–•ìœ¼ë©´, ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ ê°€ì¥ ìœ ëª…í•œ monte carlo dropout ë°©ë²•ì„ ì¼ë‹¨ ì‹œë„í•˜ê²Œ ë˜ê³ , í˜„ì‹¤ì ìœ¼ë¡œ overconfident ë¬¸ì œ ë“± ì œëŒ€ë¡œ ë¶ˆí™•ì‹¤ì„±ì´ ëª¨ë¸ë§ì´ ì•ˆë˜ëŠ” í˜„ìƒì„ ê²½í—˜í•˜ë©´ì„œ ì‹œí–‰ì°©ì˜¤ê°€ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.

Deep networksì—ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•´ì•¼ í•  ë•Œ:

**ë°©ë²• 1: Quantile Regression (ê°„ë‹¨, baseline)**
```python
# Upper Confidence Bound (UCB) ì¶”ì •
model_upper = train_with_quantile_loss(quantile=0.9)
model_lower = train_with_quantile_loss(quantile=0.1)

# Confidence interval
confidence_interval = model_upper(x) - model_lower(x)
```

**ì¥ì **:
- êµ¬í˜„ ê°„ë‹¨, ë¹ ë¥¸ baseline
- UCBê°€ í•„ìš”í•œ ìƒí™©ì—ì„œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥
- í•´ì„ì´ ëª…í™•

**ë°©ë²• 2: Monte Carlo Dropout (ë³µì¡)**
- ì´ë¡ ì  ì´í•´ ì—†ì´ ì‹œë„í•˜ë©´ overconfident ë¬¸ì œ ë°œìƒ
- ë¶ˆí™•ì‹¤ì„±ì´ ì œëŒ€ë¡œ ëª¨ë¸ë§ ì•ˆ ë  ìˆ˜ ìˆìŒ
- ì‹œí–‰ì°©ì˜¤ ì¦ê°€

> **ğŸ’¡ íŒ**  
> ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§ì´ í•„ìš”í•˜ë‹¤ë©´, **quantile regressionì„ ë¨¼ì € baseline**ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”. ê°„ë‹¨í•˜ê³  í•´ì„ ê°€ëŠ¥í•˜ë©°, UCB/LCBë¥¼ ì§ì ‘ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 5. í•µì‹¬ ì •ë¦¬

### MLE ê´€ì ì—ì„œ ì´í•´

```
Gaussian noise ê°€ì •
    â†“
Maximum Likelihood Estimation
    â†“
MSE Loss ë„ì¶œ
```

### Loss ì„ íƒ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. âœ… **ë°ì´í„° íŠ¹ì„± íŒŒì•…**: ë…¸ì´ì¦ˆ ë¶„í¬ëŠ”? Outlierê°€ ë§ì€ê°€?
2. âœ… **ë„ë©”ì¸ ì§€ì‹ í™œìš©**: í‰ê·  vs ì¤‘ì•™ê°’ ì¤‘ ë¬´ì—‡ì´ ì˜ë¯¸ ìˆëŠ”ê°€?
3. âœ… **ëª¨ë¸ë§ ëª©ì  ì •ì˜**: Point estimate? Uncertainty? Quantile?
4. âœ… **ì´ë¡ ì  ê·¼ê±° í™•ë¦½**: ì™œ ì´ lossê°€ ì í•©í•œê°€?
5. âœ… **ì‹¤í—˜ ê²€ì¦**: ê°€ì„¤ì´ ë§ëŠ”ì§€ í™•ì¸

### ê¸°ì–µí•  ì 

> **ğŸ“Œ ìš”ì•½**  
> - **MSE**: ì´ë¡ ì  ê¸°ë°˜ì´ íƒ„íƒ„, ì•ˆì •ì , ê¸°ë³¸ ì„ íƒ  
> - **MAE**: Outlierì— robust, ì¤‘ì•™ê°’ ì¶”ì •, ê·¼ê±° í•„ìš”  
> - **Quantile**: ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§, ëª…í™•í•œ ëª©ì  í•„ìš”  
> - **ì‹¤í—˜ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±**: ê°€ì„¤ â†’ ì´ë¡  â†’ ì‹¤í—˜

Loss function ì„ íƒì€ ë‹¨ìˆœí•œ ì‹¤í—˜ì´ ì•„ë‹ˆë¼ **ë°ì´í„°ì— ëŒ€í•œ ê°€ì •ê³¼ ëª¨ë¸ë§ ëª©ì ì— ëŒ€í•œ ì´í•´**ì—ì„œ ì‹œì‘ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.



