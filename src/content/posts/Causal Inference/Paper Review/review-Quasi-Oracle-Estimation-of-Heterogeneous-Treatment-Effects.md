---
title: "[Paper Review] Quasi-Oracle Estimation of Heterogeneous Treatment Effects"
date: "2025-07-17"
excerpt: "R-leaner ë°©ë²• ì†Œê°œì™€ ì´ê²Œ ê°€ì§€ëŠ” quasi-oracle propertyì— ëŒ€í•´ ì„¤ëª…"
category: "Causal Inference"
tags: ["Paper Review"]
---

[paper link](https://arxiv.org/pdf/1712.04912)


# ë…¼ë¬¸ ë¦¬ë·°

# Abstract

> Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation.

ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì •í•˜ëŠ” ê²ƒì€ ë§ì€ ë¶„ì•¼ì—ì„œ í†µê³„ì  ë¬¸ì œì˜ í•µì‹¬ì…ë‹ˆë‹¤.

ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì•Œê³  ìˆìœ¼ë©´ ê°œì¸í™”ëœ ì•½ì²˜ë°©ì´ë‚˜ êµìœ¡ì •ì±… ê²°ì •, ìì› ë¶„ë°° ë“± ë§ì€ ë¬¸ì œì—ì„œ ì¢‹ì€ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
í•˜ì§€ë§Œ ì´ë¥¼ ì¶”ì •í•˜ëŠ” ì¼ì€ ê½¤ë‚˜ ë³µì¡í•œ ì¼ì´ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” ì´ë¥¼ ìœ ì—°í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

ì´ ë…¼ë¬¸ì—ì„œëŠ” marginal effectsì™€ treatment propensityë¼ëŠ” ë‘ ê°€ì§€ nuisance componentë¥¼ ì¶”ì •í•˜ì—¬ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•©ë‹ˆë‹¤.

> we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. 

ì´ëŸ¬í•œ ë°©ë²•ì€ quasi-oracle propertyë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•©ë‹ˆë‹¤.

<small> *quasi-oracle property : marginal effectsì™€ treatment propensityì˜ ì¶”ì •ì´ ì •í™•í•˜ì§€ ì•Šë”ë¼ë„, ë§ˆì¹˜ ì´ ë‘ nuisance componentë¥¼ ë¯¸ë¦¬ ì•Œê³  ìˆëŠ” ê²ƒê³¼ ê°™ì€ ì˜¤ì°¨ ê²½ê³„ë¥¼ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ì„±ì§ˆ</small>


# 1. Introduction

**ê¸°ì¡´ ì—°êµ¬ì˜ í•œê³„ì **:
1. ë°©ë²•ë¡ ì  ë¶ˆì¼ì¹˜: ê´€ì°° ì—°êµ¬ì—ì„œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì— ì–´ë–»ê²Œ ì ìš©í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•œ í¬ê´„ì ì¸ ë‹µì´ ì•„ì§ í™•ë¦½ë˜ì§€ ì•ŠìŒ
2. ê°œë°œ ê³¼ì •ì˜ ë³µì¡ì„±: ì¸ê³¼ê´€ê³„ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ê°œë°œí•˜ëŠ” ê³¼ì •ì´ ë…¸ë™ì§‘ì•½ì ì´ë©°, ì „ë¬¸ ì—°êµ¬ìë“¤ì˜ ì°¸ì—¬ê°€ í•„ìˆ˜ì 
3. ì´ë¡ ì  ê·¼ê±° ë¶€ì¡±: ëŒ€ë¶€ë¶„ì˜ ë°©ë²•ë“¤ì´ ìˆ˜ì¹˜ì  ì‹¤í—˜ìœ¼ë¡œë§Œ ê²€ì¦ë˜ê³ , í˜•ì‹ì ì¸ ìˆ˜ë ´ ë³´ì¥ì´ë‚˜ ì˜¤ì°¨ ê²½

**ì´ ë…¼ë¬¸ì˜ ìƒˆë¡œìš´ ì ‘ê·¼ë²•**:
1. ìë™í™”ëœ í”„ë ˆì„ì›Œí¬: ì„ì˜ì˜ ì†ì‹¤ ìµœì†Œí™” ì ˆì°¨ë¥¼ í†µí•´ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë¥¼ ì™„ì „ ìë™ìœ¼ë¡œ ëª…ì„¸í•  ìˆ˜ ìˆëŠ” í”„ë ˆì„ì›Œí¬ ì œê³µ
2. Oracle ìˆ˜ì¤€ì˜ ì„±ëŠ¥: ë°ì´í„° ìƒì„± ë¶„í¬ì— ëŒ€í•œ ëª¨ë“  ì •ë³´ë¥¼ ì•Œê³  ìˆëŠ” oracle ë°©ë²•ê³¼ ë¹„êµ ê°€ëŠ¥í•œ ì˜¤ì°¨ ê²½ê³„ ë‹¬ì„±
3. ì´ë¡ ì  ê¸°ë°˜: ì´ì¤‘ ê°•ê±´ ì¶”ì •(double robust estimation), oracle ë¶€ë“±ì‹, êµì°¨ ê²€ì¦ì„ ê²°í•©í•˜ì—¬ ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ë„êµ¬ë¡œ ì›ë¦¬ì  í†µê³„ ì¶”ì •ì„ ìœ„í•œ ì†ì‹¤ í•¨ìˆ˜ ê°œë°œ

ì´ ë…¼ë¬¸ì€ ê¸°ì¡´ì˜ ì‹¤ìš©ì  í•œê³„ì™€ ì´ë¡ ì  ë¶€ì¡±í•¨ì„ ëª¨ë‘ í•´ê²°í•˜ëŠ” ìƒˆë¡œìš´ ì ‘ê·¼ë²•ì„ ì œì‹œí•˜ê³  ìˆìŠµë‹ˆë‹¤.

# 2. A Loss Function for Treatment Effect Estimation

> We formalize our problem in terms of the potential outcomes framework (Neyman, 1923; Rubin, 1974).

potential outcomes frameworkë¥¼ ì‚¬ìš©í•´ ë¬¸ì œë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

**ë°ì´í„° êµ¬ì¡°**:
- **ê´€ì°° ë°ì´í„°**: $(X_i, Y_i, W_i)$ for $i = 1, ..., n$
  - $X_i \in \mathcal{X}$: ê°œì¸ë³„ íŠ¹ì„± (features)
  - $Y_i \in \mathbb{R}$: ê´€ì°°ëœ ê²°ê³¼ (observed outcome)
  - $W_i \in \{0, 1\}$: ì²˜ì¹˜ í• ë‹¹ (treatment assignment)

**ì ì¬ ê²°ê³¼ (Potential Outcomes)**:
- **ì ì¬ ê²°ê³¼**: $\{Y_i(0), Y_i(1)\}$
  - $Y_i(0)$: ì²˜ì¹˜ë¥¼ ë°›ì§€ ì•Šì•˜ì„ ë•Œì˜ ê²°ê³¼
  - $Y_i(1)$: ì²˜ì¹˜ë¥¼ ë°›ì•˜ì„ ë•Œì˜ ê²°ê³¼
- **ê´€ì°°ëœ ê²°ê³¼ì™€ì˜ ê´€ê³„**: $Y_i = Y_i(W_i)$

**ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜íš¨ê³¼ (CATE)**:
- **ëª©í‘œ í•¨ìˆ˜**: $\tau^*(x) = \mathbb{E}[Y(1) - Y(0) \mid X = x]$
  - íŠ¹ì„± $X = x$ì¸ ê°œì¸ë“¤ì˜ í‰ê·  ì²˜ì¹˜íš¨ê³¼

**ì‹ë³„ ì¡°ê±´**:
- **Unconfoundedness**: ì²˜ì¹˜ í• ë‹¹ì´ ê´€ì°°ë˜ì§€ ì•Šì€ confounding variableì— ì˜í•´ ì˜í–¥ì„ ë°›ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê°€ì •

ì´ ìˆ˜ì‹ë“¤ì€ ì¸ê³¼ì¶”ë¡ ì˜ í‘œì¤€ì ì¸ ì ì¬ê²°ê³¼ í”„ë ˆì„ì›Œí¬ë¥¼ ë”°ë¥´ë©°, ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•œ ê¸°ë³¸ êµ¬ì¡°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.

> In order to identify $\tau^*(x)$, we assume unconfoundedness, i.e., the treatment assignment is randomized once we control for the features $X_i$ (Rosenbaum and Rubin, 1983).

CATEë¥¼ ì¶”ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ë¬´ì‘ìœ„ í• ë‹¹ì²˜ëŸ¼ unconfoundedness ê°€ì •ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤.
ì´ë¥¼ ìœ„í•´ì„œ Xë¥¼ í†µì œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

**Assumption 1** : The treatment assignment $W_i$ is unconfounded, $\{Y_i(0), Y_i(1)\} \perp \!\!\! \perp W_i \mid X_i$

- **Treatment Propensity** : $e^*(x) = \Pr(W = 1 \mid X = x)$
- **Conditional Response Surfaces** : $\mu^{*(w)}(x) = \mathbb{E}[Y(w) \mid X = x]$ for $w \in \{0, 1\}$
- **Error Term** : $\varepsilon_i(w) := Y_i(w) - \{\mu^{*(0)}(X_i) + w\tau^*(X_i)\}$
- **ì„±ì§ˆ** : unconfoundedness í•˜ì—ì„œ $\mathbb{E}[\varepsilon_i(W_i) \mid X_i, W_i] = 0$
- **Conditional Mean Outcome** : $m^*(x) = \mathbb{E}[Y \mid X = x] = \mu^{*(0)}(x) + e^*(x)\tau^*(x)$


$$
Y_i - m^*(X_i) = \{W_i - e^*(X_i)\} \tau^*(X_i) + \varepsilon_i \tag{1}
$$

(1)ì—ì„œ **propensity score $e^*(X_i)$ë¥¼ í†µí•´ Xë¥¼ í†µì œ**í•©ë‹ˆë‹¤.

- **$W_i - e^*(X_i)$**: ì‹¤ì œ ì²˜ì¹˜ í• ë‹¹ì—ì„œ ì˜ˆì¸¡ëœ ì²˜ì¹˜ í™•ë¥ ì„ ëº€ ê°’
- ì´ëŠ” Xë¥¼ í†µì œí•œ í›„ì˜ "ì²˜ì¹˜ í• ë‹¹ì˜ í¸ì°¨"ë¥¼ ë‚˜íƒ€ëƒ„
- ë§ˆì¹˜ Xê°€ ê°™ì€ ê·¸ë£¹ ë‚´ì—ì„œ ë¬´ì‘ìœ„ í• ë‹¹ëœ ê²ƒì²˜ëŸ¼ ë§Œë“¦

ë”°ë¼ì„œ ì´ ë“±ì‹ì€ **propensity scoreë¥¼ í†µí•œ X í†µì œ**ë¥¼ êµ¬í˜„í•œ ê²ƒì…ë‹ˆë‹¤.

> The goal of this paper is to study how we can use the Robinsonâ€™s transfomation (1) for flexible treatment effect estimation that builds on modern machine learning approaches such as boosting or deep learning. 

ì´ ë…¼ë¬¸ì˜ ëª©í‘œëŠ” ë¶€ìŠ¤íŒ…ì´ë‚˜ ë”¥ëŸ¬ë‹ê³¼ ê°™ì€ í˜„ëŒ€ì ì¸ ê¸°ê³„í•™ìŠµ ë°©ë²•ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìœ ì—°í•œ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•´ Robinson's transfomation (1)ì„ ì–´ë–»ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ ì—°êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

> Our main result is that we can use this representation to construct a loss function that captures heterogeneous treatment effects, and that we can then accurately estimate treatment effectsâ€”both in terms of empirical performance and asymptotic guaranteesâ€”by finding regularized minimizers of this loss function.

ì´ ë…¼ë¬¸ì˜ ì£¼ìš” ê²°ê³¼ëŠ” ì´ í‘œí˜„ì„ ì‚¬ìš©í•˜ì—¬ ê°œì¸í™”ëœ ì²˜ì¹˜íš¨ê³¼ë¥¼ í¬ì°©í•˜ëŠ” ì†ì‹¤ í•¨ìˆ˜ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ì†ì‹¤ í•¨ìˆ˜ì˜ ì •ê·œí™”ëœ ìµœì†Œí™” í•´ë¥¼ ì°¾ì•„ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì •í™•í•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

$$
\tau^*(\cdot) = \arg\min_{\tau} \mathbb{E}\left[\{Y_i - m^*(X_i)\} - \{W_i - e^*(X_i)\} \tau(X_i)\right]^2 \tag{2}
$$


$$
\tilde{\tau}(\cdot) = \arg\min_{\tau} \left\{ \frac{1}{n} \sum_{i=1}^n \left( \{ Y_i - m^*(X_i) \} - \{ W_i - e^*(X_i) \} \tau(X_i) \right)^2 + \Lambda_n[\tau(\cdot)] \right\} \tag{3}
$$

where the term $\Lambda_n[\tau(\cdot)]$ is interpreted as a regularizer on the complexity of the $\tau(\cdot)$ function

> This regularization could be explicit as in penalized regression, or implicit, e.g., as provided by a carefully designed deep neural network.

ì •ê·œí™”í•­ì€ ëª¨ë¸ì˜ ì„¤ê³„ì— ë§ì¶° ì ìš©í•œë‹¤ê³  í•©ë‹ˆë‹¤. 

> The difficulty, however, is that in practice we never know the weighted main effect function $m^*(x)$ and usually donâ€™t know the treatment propensities $e^*(x)$ either, and so the estimator (3) is not feasible.

í•˜ì§€ë§Œ ì–´ë ¤ìš´ ì ì€ ë‹¹ì—°í•˜ê²Œë„ $m^*(x)$ì™€ $e^*(x)$ë¥¼ ì•Œ ìˆ˜ ì—†ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.

> Given these preliminaries, we here study the following class of two-step estimators using cross-fitting (Chernozhukov et al., 2018; Schick, 1986) motivated by the above oracle procedure:

**Cross-fittingì„ ì´ìš©í•œ 2ë‹¨ê³„ ì¶”ì • ë°©ë²•**:

**Step 1**: ë°ì´í„° ë¶„í•  ë° Nuisance Component ì¶”ì •
- ë°ì´í„°ë¥¼ Qê°œ(ë³´í†µ 5 ë˜ëŠ” 10)ì˜ ê· ë“±í•œ í¬ê¸°ì˜ foldë¡œ ë¶„í• 
- $q(\cdot)$: $i = 1, \ldots, n$ ìƒ˜í”Œ ì¸ë±ìŠ¤ë¥¼ Qê°œì˜ foldì— ë§¤í•‘í•˜ëŠ” í•¨ìˆ˜
- Cross-fittingì„ í†µí•´ $\hat{m}$ê³¼ $\hat{e}$ë¥¼ ìµœì  ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ìœ„í•´ ì¡°ì •ëœ ë°©ë²•ìœ¼ë¡œ ì¶”ì •

**Step 2**: Plug-in ì¶”ì •
- (3)ì˜ plug-in ë²„ì „ì„ í†µí•´ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •
- $\hat{e}^{(-q(i))}(X_i)$ ë“±ì€ ië²ˆì§¸ í›ˆë ¨ ì˜ˆì œê°€ ì†í•œ foldë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ë§Œë“  ì˜ˆì¸¡ê°’

$$
\hat{\tau}(\cdot) = \arg\min_{\tau} \left\{ \hat{L}_n[\tau(\cdot)] + \Lambda_n[\tau(\cdot)] \right\} \tag{4}
$$

$$
\hat{L}_n[\tau(\cdot)] = \frac{1}{n} \sum_{i=1}^n \left[ \{Y_i - \hat{m}^{(-q(i))}(X_i)\} - \{W_i - \hat{e}^{(-q(i))}(X_i)\} \tau(X_i) \right]^2 \tag{4a}
$$


> In other words, the first step learns an approximation for the oracle objective, and the second step optimizes it. We refer to this approach as the R-learner in recognition of the work of Robinson (1988) and to emphasize the role of residualization. We will also refer to the squared loss $L_b^n[\tau(\cdot)]$ as the R-loss.

1. 1ë‹¨ê³„: ì˜¤ë¼í´ ëª©ì í•¨ìˆ˜(ì´ë¡ ì ìœ¼ë¡œ ìµœì ì„ì„ ë³´ì¥í•˜ëŠ” í•¨ìˆ˜)ì˜ ê·¼ì‚¬ê°’ì„ í•™ìŠµ
2. 2ë‹¨ê³„: ê·¸ ê·¼ì‚¬ ëª©ì í•¨ìˆ˜ë¥¼ ì‹¤ì œë¡œ ìµœì í™”
3. ì´ ì „ì²´ ê³¼ì •ì„ R-learnerë¼ ë¶€ë¥´ê³ , ì†ì‹¤í•¨ìˆ˜ë¥¼ R-lossë¼ ë¶€ë¦„


**ë…¼ë¬¸ì˜ ì£¼ìš” ê¸°ì—¬**:
1. ë‹¤ì–‘í•œ ë°©ë²• ì ìš© ë° ì„±ëŠ¥
- R-learnerë¥¼ í˜ë„í‹° íšŒê·€, ì»¤ë„ ë¦¿ì§€ íšŒê·€, ë¶€ìŠ¤íŒ… ë“± ë‹¤ì–‘í•œ ë°©ë²•ì— ì ìš©í•˜ì—¬ ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„.
2. ì´ë¡ ì  ë³´ì¥
- ì»¤ë„ íšŒê·€ì˜ ê²½ìš°, ì‹¤ì œ ì¶”ì •ê¸°(plug-in estimator)ì˜ ì˜¤ì°¨ ê²½ê³„ê°€ ì˜¤ë¼í´(ì´ë¡ ì  ìµœì ) ë°©ë²•ê³¼ ê±°ì˜ ì¼ì¹˜í•¨ì„ ì¦ëª….
- íŠ¹íˆ, nuisance component($m^*(x)$, $e^*(x)$)ì˜ ì¶”ì • ì˜¤ì°¨ê°€ ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ì¤„ì–´ë“¤ë©´, ìµœì¢… ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ì˜ ìˆ˜ë ´ ì†ë„ëŠ” ì˜¤ì§ $\tau^*(x)$ì˜ ë³µì¡ë„ì—ë§Œ ì˜ì¡´í•¨.
3. ì‹¤ìš©ì  ì¥ì 
- R-learnerëŠ” ì²˜ì¹˜í™•ë¥ ê³¼ ê²°ê³¼ ì˜ˆì¸¡ì˜ ìƒê´€ê´€ê³„ë¥¼ ì†ì‹¤í•¨ìˆ˜ êµ¬ì¡°ë¡œ ë¶„ë¦¬í•˜ì—¬, ë‘ ì‘ì—…(ìƒê´€ê´€ê³„ ì œê±°, ì²˜ì¹˜íš¨ê³¼ ì¶”ì •)ì„ ëª…í™•íˆ ë¶„ë¦¬í•¨.
- ì´ë¡œ ì¸í•´ ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë„êµ¬(ì˜ˆ: glmnet, XGBoost, TensorFlow ë“±)ë¥¼ ì†ì‰½ê²Œ í™œìš©í•  ìˆ˜ ìˆê³ , ì†ì‹¤í•¨ìˆ˜(R-loss)ë§Œ ì˜ ìµœì†Œí™”í•˜ë©´ ë¨.
- ë³µì¡í•œ êµì°¨ê²€ì¦ ì—†ì´ë„ ì†ì‹¤í•¨ìˆ˜ ê¸°ë°˜ì˜ ê°„ë‹¨í•œ íŠœë‹ì´ ê°€ëŠ¥í•¨.

# 3. Related Work


## **1. Regularization bias**:

> However, the fact that both $\hat{\beta}^{(0)}$ and $\hat{\beta}^{(1)}$ are regularized towards 0 separately may inadvertently regularize the treatment effect estimate $\hat{\beta}^{(1)} - \hat{\beta}^{(0)}$ away from 0, even when $\tau^*(x) = 0$ everywhere

- CATE(ì¡°ê±´ë¶€ í‰ê·  ì²˜ì¹˜íš¨ê³¼)ëŠ” $\tau^*(x) = \mu^{*(1)}(x) - \mu^{*(0)}(x)$ë¡œ ì“¸ ìˆ˜ ìˆìŒ.
- í”íˆ $\mu^{*(1)}(x)$ì™€ $\mu^{*(0)}(x)$ë¥¼ ê°ê° ë”°ë¡œ ì¶”ì •í•œ ë’¤, ê·¸ ì°¨ì´ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ êµ¬í•˜ëŠ”ë°,
- ì´ë•Œ ë‘ í•¨ìˆ˜ë¥¼ ë³„ë„ë¡œ ì •ê·œí™”(regularization)í•˜ë©´ regularization biasê°€ ë°œìƒí•  ìˆ˜ ìˆìŒ.
- ì˜ˆë¥¼ ë“¤ì–´, ë¼ì˜(lasso) íšŒê·€ë¥¼ ê°ê°ì˜ ì§‘ë‹¨(ì²˜ì¹˜/ë¹„ì²˜ì¹˜)ì— ë”°ë¡œ ì ìš©í•˜ë©´,
- ë‘ ì¶”ì •ì¹˜ ëª¨ë‘ 0ì— ê°€ê¹Œì›Œì§€ë„ë¡ ì •ê·œí™”ë˜ì–´ ì‹¤ì œë¡œëŠ” ì²˜ì¹˜íš¨ê³¼ê°€ ì—†ì„ ë•Œë„ $\hat{\tau}(x) = \hat{\mu}^{(1)}(x) - \hat{\mu}^{(0)}(x)$ê°€ 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆìŒ.
- íŠ¹íˆ, ì²˜ì¹˜êµ°ê³¼ ëŒ€ì¡°êµ°ì˜ ìƒ˜í”Œ ìˆ˜ê°€ ë‹¤ë¥¼ ë•Œ ì´ í˜„ìƒì´ ë” ì‹¬í•´ì§.

$$
\begin{align}
\hat{\beta}^{(w)} = \arg\min_{\beta^{(w)}} \left\{ \sum_{i:W_i=w} \left( Y_i - X_i^\top \beta^{(w)} \right)^2 + \lambda^{(w)} \|\beta^{(w)}\|_1 \right\} \tag{5}
\end{align}
$$


ê°„ë‹¨í•˜ê²Œ ëŒ€ì¡°êµ°ê³¼ í†µì œêµ°ì„ ë”°ë¡œ í•™ìŠµì‹œí‚¤ê³ , ê·¸ ê²°ê³¼ë¥¼ ë¹„êµí•´ì„œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì–»ìœ¼ë©´ ë˜ëŠ”ê±° ì•„ë‹ˆì•¼? ë¼ê³  ìƒê°í•  ìˆ˜ ìˆì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ regularization biasë¼ê³  í•˜ëŠ”, ê³¼í•˜ê²Œ ì •ê·œí™”ë˜ì–´ ì²˜ì¹˜íš¨ê³¼ê°€ 0ì¼ ë•Œë„ 0ì—ì„œ ë©€ì–´ì§ˆ ìˆ˜ ìˆëŠ” ë¬¸ì œê°€ ë°œìƒí•©ë‹ˆë‹¤.

## **2. Regularization Biasë¥¼ í”¼í•˜ëŠ” ìµœê·¼ ë°©ë²•ë“¤**:
- ìµœê·¼ ì—°êµ¬ë“¤ì€ regularization bias ë¬¸ì œë¥¼ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ì–‘í•œ êµ¬ì¡°ì (machine learning êµ¬ì¡° ìì²´ì˜) ê°œì„  ë°©ë²•ì„ ì œì•ˆí•¨.
- ì˜ˆë¥¼ ë“¤ì–´, Imai & Ratkovic (2013)ì€ ì‹ (5)ì²˜ëŸ¼ ì²˜ì¹˜êµ°/ëŒ€ì¡°êµ°ì„ ë”°ë¡œ í•™ìŠµí•˜ëŠ” ëŒ€ì‹ , ì•„ë˜ì™€ ê°™ì´ í•˜ë‚˜ì˜ ë¼ì˜(lasso) íšŒê·€ë¡œ ë™ì‹œì— í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨:

$$
\begin{align}
\hat{b}, \hat{\delta} = \arg\min_{b, \delta} \left\{ \sum_{i=1}^n \left( Y_i - X_i^\top b + (W_i - 0.5) X_i^\top \delta \right)^2 + \lambda_b \|b\|_1 + \lambda_\delta \|\delta\|_1 \right\} \tag{6}
\end{align}
$$

- ì—¬ê¸°ì„œ ìµœì¢… ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì€ $\hat{\tau}(x) = x^\top \hat{\delta}$ë¡œ ê³„ì‚°í•¨.
- ì´ ë°©ë²•ì€ $\delta$ì—ë§Œ í¬ì†Œì„±(sparsity)ì„ ê°•ì œí•˜ì—¬, ì²˜ì¹˜íš¨ê³¼ì˜ êµ¬ì¡°ì  íŠ¹ì„±ì„ ë” ì˜ ë°˜ì˜í•  ìˆ˜ ìˆìŒ.
- ì´ì™¸ì—ë„ ì‹ ê²½ë§ ë“± ë‹¤ì–‘í•œ ê¸°ê³„í•™ìŠµ ë°©ë²•ì—ì„œ ì²˜ì¹˜íš¨ê³¼ ì´ì§ˆì„±(heterogeneity)ì„ ì˜ ì¶”ì •í•˜ë„ë¡ êµ¬ì¡°ë¥¼ ì„¤ê³„í•˜ëŠ” ì—°êµ¬ë“¤ì´ ì§„í–‰ë˜ê³  ìˆìŒ(ì˜ˆ: Shalit et al., 2017).

## **3. Loss Function(ì†ì‹¤í•¨ìˆ˜) ë³€ê²½ì„ í†µí•œ ì ‘ê·¼**:

> Here, instead of trying to modify the algorithms underlying different machine learning tools to improve their performance as treatment effect estimators, we focus on modifying the loss function used to training generic machine learning methods.

- ìµœê·¼ ì—°êµ¬ë“¤ì€ ëª¨ë¸ êµ¬ì¡° ìì²´ë¥¼ ë°”ê¾¸ëŠ” ëŒ€ì‹ , ì†ì‹¤í•¨ìˆ˜(R-loss)ë¥¼ ë°”ê¿”ì„œ ì²˜ì¹˜íš¨ê³¼ ì¶”ì • ì„±ëŠ¥ì„ ë†’ì´ëŠ” ë°©ë²•ì— ì£¼ëª©í•˜ê³  ìˆìŒ.
- ì´ ì ‘ê·¼ì€ van der Laanê³¼ Dudoit(2003) ë“±ì—ì„œ ì‹œì‘ëœ ì—°êµ¬ íë¦„ì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë©°, ì´ë“¤ì€ doubly robust objective(ì´ì¤‘ ê°•ê±´ ëª©ì í•¨ìˆ˜)ì— ëŒ€í•œ êµì°¨ê²€ì¦ì„ í†µí•´ ìµœì ì˜ í†µê³„ì  ê·œì¹™ì„ ì„ íƒí•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•¨.
- Luedtke & van der Laan(2016) ë“±ì€ ì´ëŸ¬í•œ ëª©ì í•¨ìˆ˜ë¥¼ ì´ìš©í•´ ê°œë³„í™” ì²˜ì¹˜ê·œì¹™(individualized treatment rules)ì´ë‚˜
ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼(heterogeneous treatment effects)ë¥¼ í•™ìŠµí•˜ëŠ” ë‹¤ì–‘í•œ ìœ íš¨í•œ ëª©ì í•¨ìˆ˜(oracle loss ë“±)ì˜ ì„±ì§ˆì„ ë¶„ì„í•¨.
- ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬ëŠ”,
  - R-lossë¥¼ í™œìš©í•´ ë²”ìš© ê¸°ê³„í•™ìŠµ(generic machine learning)ìœ¼ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ ì œì‹œí•˜ê³ ,
  - ì»¤ë„ íë²„íŠ¸ ê³µê°„ì—ì„œì˜ ì •ê·œí™” íšŒê·€ ë“± ë„ë¦¬ ì“°ì´ëŠ” ë¹„ëª¨ìˆ˜ì  ë°©ë²•ì— ëŒ€í•´ ê°•í•œ ì´ë¡ ì  ì˜¤ì°¨ ê²½ê³„(ìˆ˜ë ´ë¥  ë³´ì¥)ë¥¼ ì œê³µí•œ ê²ƒì„.

## **4. meta-learning ë°©ë²•ë“¤**:

[meta learner ê´€ë ¨ ì„¤ëª…ìë£Œ](https://matheusfacure.github.io/python-causality-handbook/21-Meta-Learners.html)


**X-learner (KÃ¼nzel et al., 2019)**:
- ë¨¼ì € $\hat{\mu}^{(w)}(x)$ë¥¼ ë¹„ëª¨ìˆ˜ íšŒê·€ ë°©ë²•ìœ¼ë¡œ ì¶”ì •
- ì²˜ì¹˜ ê´€ì°°ê°’ì— ëŒ€í•´ pseudo-effects $D_i = Y_i - \hat{\mu}^{(-i)}{(0)}(X_i)$ë¥¼ ì •ì˜í•˜ê³ , ì´ë¥¼ ì´ìš©í•´ $\hat{\tau}^{(1)}(X_i)$ë¥¼ ë¹„ëª¨ìˆ˜ íšŒê·€ë¡œ ì¶”ì •
- ëŒ€ì¡°êµ°ì— ëŒ€í•´ì„œë„ ìœ ì‚¬í•˜ê²Œ $\hat{\tau}^{(0)}(X_i)$ë¥¼ êµ¬í•˜ê³ , ë‘ ì¶”ì •ê¸°ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ê²°í•©:
$$
\hat{\tau}(x) = \{1 - \hat{e}(x)\} \hat{\tau}^{(1)}(x) + \hat{e}(x) \hat{\tau}^{(0)}(x) \tag{7}
$$

**U-learner (KÃ¼nzel et al., 2019)**:
- $U_i = \frac{Y_i - m^(X_i)}{W_i - e^(X_i)}$ì— ëŒ€í•´ $\mathbb{E}[U_i \mid X_i = x] = \tau(x)$ì„ì„ ì´ìš©
- $U_i$ë¥¼ $X_i$ì— ëŒ€í•´ ë²”ìš© ê¸°ê³„í•™ìŠµ ë°©ë²•ìœ¼ë¡œ íšŒê·€

**Propensity Score ê°€ì¤‘ ë°©ë²•ë“¤**:
  - Athey & Imbens (2016), Tian et al. (2014) ë“±ì´ ì œì•ˆ
  - ê²°ê³¼ë‚˜ ê³µë³€ëŸ‰ì„ propensity scoreë¡œ ê°€ì¤‘í•˜ì—¬ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •
  - ì˜ˆ: $Y_i\{W_i - e^(X_i)\}/\{e^(X_i)(1-e^(X_i))\}$ë¥¼ $X_i$ì— ëŒ€í•´ íšŒê·€

**ë³¸ ë…¼ë¬¸ì˜ ê¸°ì—¬**:
- R-learner ë°©ë²•ì„ ì œì•ˆí•˜ì—¬ ë‹¤ì–‘í•œ ì„¤ì •ì—ì„œ baselineë³´ë‹¤ ì˜ë¯¸ìˆëŠ” ê°œì„ ì„ ì œê³µ
- Quasi-oracle ì˜¤ì°¨ ê²½ê³„ë¥¼ ì œê³µí•˜ì—¬ $\hat{\tau}$ì˜ ì˜¤ì°¨ê°€ $\hat{e}$ë‚˜ $\hat{m}$ì˜ ì˜¤ì°¨ë³´ë‹¤ ë¹ ë¥´ê²Œ ê°ì†Œí•  ìˆ˜ ìˆìŒì„ ì´ë¡ ì ìœ¼ë¡œ ë³´ì¥



## **5. ê´€ë ¨ ì—°êµ¬ì™€ ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì **:

- **ê°€ì¥ ìœ ì‚¬í•œ ê¸°ì¡´ ì—°êµ¬**:
  - Zhao, Small, and Ertefaie (2017):
    - Robinson ë³€í™˜ê³¼ ë¼ì˜(lasso)ë¥¼ ê²°í•©í•´ ê³ ì°¨ì› ì„ í˜•ëª¨í˜•ì—ì„œ íš¨ê³¼ ìˆ˜ì •(effect modification)ì— ëŒ€í•œ ìœ íš¨í•œ ì‚¬í›„ì„ íƒ ì¶”ë¡ (post-selection inference)ì„ ì œê³µ
    - í•˜ì§€ë§Œ ì¼ë°˜ì ì¸ ê¸°ê³„í•™ìŠµ ë§¥ë½ì—ì„œ ì†ì‹¤í•¨ìˆ˜ë¡œ Robinson ë³€í™˜ì„ í™œìš©í•œ ê²ƒì€ ë³¸ ë…¼ë¬¸ì´ ì²˜ìŒ
- **ì´ë¡ ì  ê¸°ë°˜**:
  - ë³¸ ë…¼ë¬¸ì˜ ì´ë¡ ì  ê²°ê³¼ëŠ” Robinson(1988) ë“±ì—ì„œ ë°œì „ëœ ì¤€ëª¨ìˆ˜ì  íš¨ìœ¨ì„±(semiparametric efficiency)ê³¼ ì§êµ ëª¨ë©˜íŠ¸(orthogonal moments) ì´ë¡ ì— ê¸°ë°˜
  - ì•Œê³ ë¦¬ì¦˜ì ìœ¼ë¡œëŠ” Targeted Maximum Likelihood Estimation(TMIE)ì™€ ìœ ì‚¬:
    1. nuisance componentë¥¼ ë¹„ëª¨ìˆ˜ì ìœ¼ë¡œ ì¶”ì •
    2. ì´ë¥¼ í™œìš©í•´ likelihood(ë˜ëŠ” ì†ì‹¤í•¨ìˆ˜)ë¥¼ ìµœì í™”
  - Cross-fitting(í™€ë“œì•„ì›ƒ ì˜ˆì¸¡)ì€ ìµœê·¼ ì¤€ëª¨ìˆ˜ì  ì¶”ì •ì—ì„œ ë„ë¦¬ ì“°ì´ëŠ” ë°©ë²•
- **ë³¸ ë…¼ë¬¸ì˜ ì°¨ë³„ì **:
  - ê¸°ì¡´ ì—°êµ¬ë“¤ì€ ì£¼ë¡œ ë‹¨ì¼(ë˜ëŠ” ì €ì°¨ì›) íŒŒë¼ë¯¸í„° ì¶”ì •ì— ì´ˆì 
  - ë³¸ ë…¼ë¬¸ì€ ë³µì¡í•œ í•¨ìˆ˜ì  ê°ì²´(ì¦‰, $\tau^(\cdot)$ ì „ì²´ í•¨ìˆ˜)ë¥¼ ì¶”ì •í•˜ëŠ” ë° ì´ˆì  ìµœì  ì²˜ì¹˜ í• ë‹¹ ê·œì¹™(optimal treatment allocation rule) ì¶”ì •ê³¼ë„ ê´€ë ¨ ìˆì§€ë§Œ, ëª©ì í•¨ìˆ˜(ì†ì‹¤)ê°€ ë‹¤ë¦„
- **ì¶”ê°€ ë…¼ì˜**:
  - ë³¸ ë…¼ë¬¸ì€ ëª¨ì§‘ë‹¨ì—ì„œ ë¬´ì‘ìœ„ ì¶”ì¶œëœ ìƒ˜í”Œì„ ê°€ì •
  - ì—„ê²©í•œ ë¬´ì‘ìœ„í™” ì¶”ë¡ (randomization inference) í•˜ì—ì„œì˜ ë¹„ëª¨ìˆ˜ì  ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì— ëŒ€í•œ ì¶”ê°€ ì—°êµ¬ë„ í¥ë¯¸ë¡œìš´ ì£¼ì œì„


# 4. The R-Learner in Action

## 4.1 Application to a Voting Study

íˆ¬í‘œì— ê´€í•œ ì—°êµ¬ì— ì´ë¥¼ ì ìš©í•´ë³¸ ì‚¬ë¡€ë¥¼ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ì—°êµ¬ ë°°ê²½
- **ì›ë³¸ ì—°êµ¬**: Arceneaux, Gerber, and Green (2006)
- **ì—°êµ¬ ì£¼ì œ**: ìœ ë£Œ íˆ¬í‘œ ë…ë ¤ ì „í™”ê°€ íˆ¬í‘œìœ¨ì— ë¯¸ì¹˜ëŠ” íš¨ê³¼
- **ì‹¤ì œ ê²°ê³¼**: íˆ¬í‘œ ë…ë ¤ ì „í™”ì˜ íš¨ê³¼ê°€ ê±°ì˜ ì—†ìŒ (1% ë¯¸ë§Œ)

### ì‹¤í—˜ ì„¤ê³„

#### **ë°ì´í„° êµ¬ì„±**
- **ì „ì²´ ìƒ˜í”Œ**: 1,895,468ê°œ ê´€ì¸¡ì¹˜
- **ì²˜ì¹˜ ê·¸ë£¹**:59,264ê°œ (ì „í™” ë°›ì€ ì‚¬ëŒ)
- **ë¶„ì„ ìƒ˜í”Œ**: 148160ê°œ (ëª¨ë“  ì²˜ì¹˜ ê·¸ë£¹ + ëœë¤ ëŒ€ì¡° ê·¸ë£¹)
- **ì²˜ì¹˜ ë¹„ìœ¨**: 2/5 (40

#### **ë°ì´í„° ë¶„í• **
- **í›ˆë ¨ ì„¸íŠ¸**: 1000ê°œ
- **í…ŒìŠ¤íŠ¸ ì„¸íŠ¸**: 25,0ê°œ  
- **í™€ë“œì•„ì›ƒ ì„¸íŠ¸**: ë‚˜ë¨¸ì§€

#### **ì¸ê³µì  ì²˜ì¹˜íš¨ê³¼ ì¶”ê°€**
ì‹¤ì œë¡œëŠ” ì²˜ì¹˜íš¨ê³¼ê°€ ê±°ì˜ ì—†ìœ¼ë¯€ë¡œ, ì¸ê³µì ìœ¼ë¡œ ì²˜ì¹˜íš¨ê³¼ë¥¼ ì¶”ê°€:

$$
\tau^*(X_i) = -\frac{\text{VOTE00}_i}{2 + \frac{100}{\text{AGE}_i}}
$$

- **VOTE00_i**: 200íˆ¬í‘œ ì—¬ë¶€
- **AGE_i**: ë‚˜ì´
- **ëª©ì **: ì´ì§ˆì  ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ ìœ„í•œ ì˜ë¯¸ìˆëŠ” ì‘ì—… ìƒì„±

### R-learner ì ìš© ê³¼ì •

#### **Step 1: Nuisance Components ì¶”ì •**
- **ë¶€ìŠ¤íŒ…ê³¼ ë¼ì˜** ë‘ ë°©ë²•ìœ¼ë¡œ $\hat{e}(\cdot)$ì™€ $\hat{m}(\cdot)$ ì¶”ì •
- **êµì°¨ê²€ì¦**ìœ¼ë¡œ ìµœì  ëª¨ë¸ ì„ íƒ
- **ê²°ê³¼**: ë¶€ìŠ¤íŒ…ì´ ë‘ nuisance components ëª¨ë‘ì—ì„œ ë” ë‚˜ì€ ì„±ëŠ¥

#### **Step2 R-loss ìµœì í™”**
- **ë¼ì˜**: R-loss = 00.1816 (í›ˆë ¨), 0.1781 (í™€ë“œì•„ì›ƒ)
- **ë¶€ìŠ¤íŒ…**: R-loss = 00.1818 (í›ˆë ¨), 0.1783í™€ë“œì•„ì›ƒ)
- **ìµœì¢… ì„ íƒ**: ë¼ì˜ ê¸°ë°˜ $\hat{\tau}(\cdot)$

### ì„±ëŠ¥ ë¹„êµ

#### **Oracle Test Set MSE (Mean Squared Error)**
| ë°©ë²• | MSE |
|------|-----|
| **R-learner (ë¼ì˜)** | 00.47 Ã—10â»Â³ |
| **R-learner (ë¶€ìŠ¤íŒ…)** | 1.2310|
| **Single Lasso** | 00.61 Ã—10â»Â³ |
| **BART** | 40.05 Ã—10â»Â³ |

#### **ì„±ëŠ¥ ë¶„ì„**
1. **R-learner (ë¼ì˜)ê°€ ìµœê³  ì„±ëŠ¥**2. **ë¼ì˜ vs ë¶€ìŠ¤íŒ…**: 
   - ë¼ì˜: í¸í–¥ ìˆì§€ë§Œ ë¶„ì‚° ë‚®ìŒ
   - ë¶€ìŠ¤íŒ…: ë…¸ì´ì¦ˆ ë§ì§€ë§Œ í¸í–¥ ì ìŒ
   - **í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„**ì—ì„œ ë¼ì˜ê°€ ìœ ë¦¬

### í•µì‹¬ ë°œê²¬

#### **1. í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼ë²•ì˜ ìš°ìˆ˜ì„±**
- **Nuisance components**: ë¹„ëª¨ìˆ˜ì  ë°©ë²• (ë¶€ìŠ¤íŒ…) ì‚¬ìš©
- **ì²˜ì¹˜íš¨ê³¼ ì¶”ì •**: ê°„ë‹¨í•œ ë°©ë²• (ë¼ì˜) ì‚¬ìš©
- **ì´ìœ **: ê° ë‹¨ê³„ì— ìµœì í™”ëœ ë°©ë²• ì„ íƒ

#### **2. ê¸°ì¡´ ë°©ë²•ë“¤ì˜ í•œê³„**
- **Single Lasso**: ëª¨ë“  ê³³ì—ì„œ ì„ í˜• ëª¨ë¸ë§ â†’ ëª¨ë¸ ì˜¤ì§€ì • ê°€ëŠ¥ì„±
- **BART**: ëª¨ë“  ê³³ì—ì„œ ë¹„ëª¨ìˆ˜ì  ëª¨ë¸ë§ â†’ ë¶ˆì•ˆì •í•œ $\tau(\cdot)$ ì¶”ì •

#### **3. R-learnerì˜ ì¥ì **
- **ìœ ì—°ì„±**: ê° ë‹¨ê³„ì— ì í•©í•œ ë°©ë²• ì„ íƒ ê°€ëŠ¥
- **ì•ˆì •ì„±**: êµì°¨ê²€ì¦ì„ í†µí•œ ì•ˆì •ì ì¸ ëª¨ë¸ ì„ íƒ
- **ì„±ëŠ¥**: ê¸°ì¡´ ë°©ë²•ë“¤ë³´ë‹¤ ìš°ìˆ˜í•œ ì¶”ì • ì •í™•ë„

### ì‹¤ìš©ì  êµí›ˆ
1. **ë‹¨ê³„ë³„ ìµœì í™”**: ê° ë‹¨ê³„ì— ê°€ì¥ ì í•©í•œ ë°©ë²• ì„ íƒ
2. **êµì°¨ê²€ì¦ì˜ ì¤‘ìš”ì„±**: ê³¼ì í•© ë°©ì§€ì™€ ëª¨ë¸ ì„ íƒ
3. **í¸í–¥-ë¶„ì‚° íŠ¸ë ˆì´ë“œì˜¤í”„**: ë°ì´í„° í¬ê¸°ì™€ ë³µì¡ì„± ê³ ë ¤
4. **ì‹¤ì œ ë°ì´í„° ì ìš©**: ì´ë¡ ì  ë°©ë²•ì˜ ì‹¤ì œ ì„±ëŠ¥ ê²€ì¦

## 4.2 Model Averaging with the R-Learner

ì—¬ëŸ¬ ëª¨ë¸ì„ í•©ì¹˜ëŠ” stacking ë°©ë²•ì„ ì ìš©í•œ ì‹œë„ë¥¼ ì´ì•¼ê¸°í•˜ê³  ìˆìŠµë‹ˆë‹¤.

### ë°°ê²½ê³¼ ë™ê¸°

ì•ì„  ì„¹ì…˜ì—ì„œëŠ” R-learnerì˜ ê° ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•  ì¶”ì • ì „ëµì„ ì„¸ì‹¬í•˜ê²Œ ê³ ë ¤í–ˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œëŠ” ë¶„ì„ì˜ ì‹œì‘ì ìœ¼ë¡œ ê¸°ì„±í’ˆ(off-the-shelf) ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì„ í˜¸í•˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. 

ì´ ì„¹ì…˜ì—ì„œëŠ” **stacking**ì˜ ë³€í˜•ì„ í†µí•´ í•©ì˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ì„ êµ¬ì¶•í•˜ëŠ” R-learning ì ‘ê·¼ë²•ì„ ë…¼ì˜í•©ë‹ˆë‹¤.

### Stacking ê¸°ë°˜ í•©ì˜ ì¶”ì •

#### **ê¸°ë³¸ ì„¤ì •**
- $k = 1, \ldots, K$ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸° $\hat{\tau}_k$ ë³´ìœ 
- í›ˆë ¨ ì„¸íŠ¸ì—ì„œ out-of-fold ì¶”ì •ì¹˜ $\hat{\tau}_k^{(-i)}(X_i)$ ì ‘ê·¼ ê°€ëŠ¥
- ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” out-of-fold ì¶”ì •ì¹˜ $\hat{e}^{(-i)}(X_i)$ì™€ $\hat{m}^{(-i)}(X_i)$ ë³´ìœ 

#### **í•©ì˜ ì¶”ì • ë°©ë²•**

R-lossì— ë”°ë¼ $\hat{\tau}_k(\cdot)$ì˜ ìµœì  ì–‘ì˜ ì„ í˜• ê²°í•©ì„ ì·¨í•˜ì—¬ í•©ì˜ ì¶”ì • $\hat{\tau}(\cdot)$ë¥¼ êµ¬ì¶•:

$$
\hat{\tau}(x) = \hat{c} + \sum_{k=1}^K \alpha_k \hat{\tau}_k(x) \tag{8a}
$$

$$
(\hat{b}, \hat{c}, \hat{\alpha}) = \arg\min_{b, c, \alpha} \left\{ \sum_{i=1}^n \left[ \left(Y_i - \hat{m}^{(-i)}(X_i)\right) - b - \left(c + \sum_{k=1}^K \alpha_k \hat{\tau}_k^{(-i)}(X_i)\right) \left(W_i - \hat{e}^{(-i)}(X_i)\right) \right]^2 : \alpha \geq 0 \right\} \tag{8b}
$$

#### **ìˆ˜ì‹ì˜ êµ¬ì„± ìš”ì†Œ**
- **$\hat{c}$**: ìƒìˆ˜ ì²˜ì¹˜íš¨ê³¼ í•­ (ìœ ì—°ì„±ì„ ìœ„í•´ ììœ ë¡­ê²Œ ì¡°ì •)
- **$\hat{b}$**: ì ˆí¸ ($\hat{m}$ì˜ ì ì¬ì  í¸í–¥ í¡ìˆ˜)
- **$\alpha_k$**: ê° ì¶”ì •ê¸°ì˜ ê°€ì¤‘ì¹˜ (ë¹„ìŒ ì œì•½)

### ì‹¤í—˜ ì„¤ì •

#### **ë°ì´í„° ìƒì„± ê³¼ì •**

ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„° ìƒì„± ë¶„í¬ì—ì„œ ì‹¤í—˜ì„ ìˆ˜í–‰í–ˆìŠµë‹ˆë‹¤:

$$
X_i \sim N(0, I_{d \times d}), \quad W_i \sim \mathrm{Bernoulli}(0.5) \tag{9a}
$$

$$
Y_i \mid X_i, W_i \sim N\left(\frac{3}{1 + e^{X_{i3} - X_{i2}}} + (W_i - 0.5) \tau^*(X_i), \sigma^2\right) \tag{9b}
$$

**ì‹¤í—˜ ì¡°ê±´:**
- ìƒ˜í”Œ í¬ê¸°: $n = 10,000$ (ë¬´ì‘ìœ„ ì—°êµ¬ ì„¤ê³„)
- ì°¨ì›: $d = 10$
- ë‹¤ì–‘í•œ $\tau^*(\cdot)$ì™€ $\sigma$ ì„ íƒ

#### **ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜**

ë‘ ê°€ì§€ ìœ í˜•ì˜ ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ë¥¼ ê³ ë ¤:

1. **ì—°ì† í•¨ìˆ˜**: $\tau^*(X_i) = \frac{1}{1 + e^{X_{i1} - X_{i2}}}$

2. **ë¶ˆì—°ì† í•¨ìˆ˜**: $\tau^*(X_i) = \frac{\mathbf{1}\{X_{i1} > 0\}}{1 + e^{-X_{i2}}}$

### ë¹„êµ ë°©ë²•

**ì¶”ì • ë°©ë²•ë“¤:**
- **BART** (Bayesian Additive Regression Trees)
- **Causal Forests** (ì¸ê³¼ê´€ê³„ ìˆ²)
- **Stacking**: ì‹ (8)ì„ ì´ìš©í•œ ë‘ ë°©ë²•ì˜ ê²°í•©

**ì‹¤í—˜ ì¡°ê±´:**
- ë°ì´í„°ê°€ ë¬´ì‘ìœ„í™”ë˜ì—ˆë‹¤ëŠ” ê²ƒì„ ì‹¤í—˜ìê°€ ì•Œê³  ìˆë‹¤ê³  ê°€ì •
- í•„ìš”í•œ ê³³ì—ì„œëŠ” $\hat{e}(x) = 0.5$ ì‚¬ìš©
- Stackingìš© $\hat{m}(\cdot)$ëŠ” random forestë¡œ ì¶”ì •

### ì‹¤í—˜ ê²°ê³¼ (Figure 2)

#### **ì—°ì† ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ì˜ ê²½ìš°**
- **BART**: Causal Forestsë³´ë‹¤ ì•½ê°„ ìš°ìˆ˜í•œ ì„±ëŠ¥
- **Stacking**: ë…¸ì´ì¦ˆ ìˆ˜ì¤€ $\sigma$ê°€ ë§¤ìš° í´ ë•Œê¹Œì§€ ê°œë³„ ë°©ë²•ë³´ë‹¤ ìš°ìˆ˜
- **ê³ ë…¸ì´ì¦ˆ í™˜ê²½**: ëª¨ë“  ë°©ë²•ì´ ìƒìˆ˜ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ì™€ ë¹„ìŠ·í•œ ìˆ˜ì¤€

#### **ë¶ˆì—°ì† ì²˜ì¹˜íš¨ê³¼ í•¨ìˆ˜ì˜ ê²½ìš°**
- **Causal Forests**: ë‚®ì€ ë…¸ì´ì¦ˆ ìˆ˜ì¤€ì—ì„œ íŠ¹íˆ ìœ ë¦¬
- **Stacking**: ë” ì •í™•í•œ ê¸°ë³¸ í•™ìŠµìì˜ ì„±ëŠ¥ì„ ìë™ìœ¼ë¡œ ë§¤ì¹­

### í•µì‹¬ ë°œê²¬

#### **1. Stackingì˜ ìë™ ì ì‘ì„±**
- ê° ì„¤ì •ì—ì„œ ë” ë‚˜ì€ ê¸°ë³¸ í•™ìŠµìë¥¼ ìë™ìœ¼ë¡œ ì‹ë³„
- ìˆ˜ë™ ì„ íƒ ì—†ì´ë„ ìµœì  ì„±ëŠ¥ì— ê·¼ì ‘

#### **2. ë°©ë²•ë¡ ì  ìœ ì—°ì„±**
- ê¸°ì¡´ ì²˜ì¹˜íš¨ê³¼ ì¶”ì •ê¸°ë“¤ì„ ê·¸ëŒ€ë¡œ í™œìš©
- ë³µì¡í•œ ì•Œê³ ë¦¬ì¦˜ ìˆ˜ì • ì—†ì´ ì„±ëŠ¥ ê°œì„ 

#### **3. ì‹¤ìš©ì  ì¥ì **
- **ìë™í™”**: ë³µì¡í•œ ëª¨ë¸ ì„ íƒ ê³¼ì • ë‹¨ìˆœí™”
- **ê²¬ê³ ì„±**: ë‹¤ì–‘í•œ ë°ì´í„° íŠ¹ì„±ì— ëŒ€í•œ ì ì‘ë ¥
- **íš¨ìœ¨ì„±**: ê¸°ì¡´ ë„êµ¬ë“¤ì˜ ê°•ì ì„ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©


# 5. A Quasi-Oracle Error Bound

ì´ ì„¹ì…˜ì—ì„œëŠ” R-learnerì˜ í•µì‹¬ ì´ë¡ ì  ê¸°ì—¬ì¸ **quasi-oracle property**ì— ëŒ€í•´ ì„¤ëª…í•©ë‹ˆë‹¤.

## 5.1 Oracleê³¼ Quasi-Oracleì˜ ê°œë…

### **Oracle Estimator**
ì´ìƒì ì¸ oracleì€ ë‹¤ìŒì„ ë¯¸ë¦¬ ì•Œê³  ìˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤:
- **True marginal effect**: $m^*(x) = \mathbb{E}[Y \mid X = x]$
- **True propensity score**: $e^*(x) = \Pr(W = 1 \mid X = x)$

Oracleì´ ì´ ì •ë³´ë¥¼ ì•Œê³  ìˆë‹¤ë©´, ë‹¤ìŒ ì†ì‹¤í•¨ìˆ˜ë¥¼ ì§ì ‘ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

$$
\mathcal{L}_n^{\text{oracle}}[\tau(\cdot)] = \frac{1}{n} \sum_{i=1}^n \left[ \{Y_i - m^*(X_i)\} - \{W_i - e^*(X_i)\} \tau(X_i) \right]^2
$$

### **R-learner (Quasi-Oracle)**
ì‹¤ì œë¡œëŠ” $m^*(x)$ì™€ $e^*(x)$ë¥¼ ëª¨ë¥´ë¯€ë¡œ, ì¶”ì •ê°’ $\hat{m}(\cdot)$ê³¼ $\hat{e}(\cdot)$ì„ ì‚¬ìš©:

$$
\hat{\mathcal{L}}_n[\tau(\cdot)] = \frac{1}{n} \sum_{i=1}^n \left[ \{Y_i - \hat{m}^{(-q(i))}(X_i)\} - \{W_i - \hat{e}^{(-q(i))}(X_i)\} \tau(X_i) \right]^2
$$

## 5.2 Main Theoretical Result

ë…¼ë¬¸ì˜ í•µì‹¬ ì´ë¡ ì  ê²°ê³¼ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

### **Theorem (Informal)**
ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ, R-learner $\hat{\tau}$ëŠ” ë‹¤ìŒ ì„±ì§ˆì„ ë§Œì¡±í•©ë‹ˆë‹¤:

$$
\mathbb{E}\left[\|\hat{\tau} - \tau^*\|^2\right] \lesssim \text{Oracle Rate} + \text{Nuisance Rate}
$$

ì—¬ê¸°ì„œ:
- **Oracle Rate**: oracleì´ ë‹¬ì„±í•  ìˆ˜ ìˆëŠ” ìµœì  ìˆ˜ë ´ë¥ 
- **Nuisance Rate**: nuisance components ì¶”ì • ì˜¤ì°¨ì˜ ê³±

### **Quasi-Oracle Property**
ë§Œì•½ nuisance componentsì˜ ì¶”ì •ì´ ì¶©ë¶„íˆ ë¹ ë¥´ê²Œ ìˆ˜ë ´í•œë‹¤ë©´:

$$
\text{Nuisance Rate} = \|\hat{m} - m^*\| \cdot \|\hat{e} - e^*\| \to 0
$$

ì´ ê²½ìš° R-learnerëŠ” **oracle rate**ë¥¼ ë‹¬ì„±í•©ë‹ˆë‹¤:

$$
\mathbb{E}\left[\|\hat{\tau} - \tau^*\|^2\right] \lesssim \text{Oracle Rate}
$$

## 5.3 Why Does This Work?

### **1. Cross-fittingì˜ ì—­í• **
- **Overfitting ë°©ì§€**: ê°™ì€ ë°ì´í„°ë¡œ ì¶”ì •ê³¼ ì˜ˆì¸¡ì„ í•˜ì§€ ì•ŠìŒ
- **Bias ê°ì†Œ**: $\mathbb{E}[\hat{m}^{(-i)}(X_i) - m^*(X_i)] \approx 0$

### **2. Doubly Robust Structure**
R-lossëŠ” doubly robust êµ¬ì¡°ë¥¼ ê°€ì§‘ë‹ˆë‹¤:

$$
\mathbb{E}\left[\left\{Y_i - m^*(X_i)\right\} - \left\{W_i - e^*(X_i)\right\} \tau^*(X_i) \mid X_i\right] = 0
$$

ì´ëŠ” ë‹¤ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤:
- $m^*(x)$ ë˜ëŠ” $e^*(x)$ ì¤‘ í•˜ë‚˜ë§Œ ì •í™•í•´ë„ í¸í–¥ì´ ì—†ìŒ
- ë‘ ì¶”ì •ì˜ ì˜¤ì°¨ê°€ ê³±ì˜ í˜•íƒœë¡œ ë‚˜íƒ€ë‚¨

### **3. Product Form of Error**
ìµœì¢… ì˜¤ì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶„í•´ë©ë‹ˆë‹¤:

$$
\text{Total Error} \approx \text{Oracle Error} + \|\hat{m} - m^*\| \times \|\hat{e} - e^*\|
$$

ë”°ë¼ì„œ ë‘ nuisance componentsê°€ ëª¨ë‘ $o(1)$ ì†ë„ë¡œ ìˆ˜ë ´í•˜ë©´, ê³±ì€ ë” ë¹ ë¥´ê²Œ ìˆ˜ë ´í•©ë‹ˆë‹¤.

## 5.4 Practical Implications

### **1. Flexible Method Choice**
- ê° ë‹¨ê³„ì—ì„œ **ì„œë¡œ ë‹¤ë¥¸ ê¸°ê³„í•™ìŠµ ë°©ë²•** ì‚¬ìš© ê°€ëŠ¥
- $\hat{m}$ì—ëŠ” random forests, $\hat{e}$ì—ëŠ” logistic regression ë“±

### **2. Robustness**
- í•œ nuisance componentê°€ ë¶€ì •í™•í•´ë„ ë‹¤ë¥¸ ê²ƒì´ ë³´ì™„
- ì™„ë²½í•œ ì¶”ì •ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ

### **3. Rate Optimality**
- ì ì ˆí•œ ì¡°ê±´ í•˜ì—ì„œ **minimax optimal** ë‹¬ì„±
- ì´ë¡ ì ìœ¼ë¡œ ìµœì„ ì˜ ì„±ëŠ¥ ë³´ì¥

# ë…¼ë¬¸ì—ì„œ ê¶ê¸ˆí•œ ì ë“¤ ì •ë¦¬

## ê·¸ë˜ì„œ quasi-oracle propertyëŠ” ì–´ë–»ê²Œ ë³´ì¥ë˜ëŠ”ê±´ë°?

### **í•µì‹¬ ë©”ì»¤ë‹ˆì¦˜**

#### **1. Robinson's Transformationì˜ ë§ˆë²•**
Robinson's transformation (1988)ì´ í•µì‹¬ì…ë‹ˆë‹¤:

$$
Y_i - m^*(X_i) = \{W_i - e^*(X_i)\} \tau^*(X_i) + \varepsilon_i
$$

ì´ ë³€í™˜ì˜ íŠ¹ë³„í•œ ì„±ì§ˆ:
- **Orthogonality**: $\mathbb{E}[\varepsilon_i \mid X_i, W_i] = 0$
- **Causal Isolation**: ì²˜ì¹˜íš¨ê³¼ë§Œ ë¶„ë¦¬ë¨

#### **2. Cross-fittingì˜ í•µì‹¬ ì—­í• **
```
Step 1: ë°ì´í„°ë¥¼ Kê°œ foldë¡œ ë¶„í• 
Step 2: kë²ˆì§¸ foldë¥¼ ì œì™¸í•˜ê³  mÌ‚, Ãª ì¶”ì •  
Step 3: kë²ˆì§¸ foldì—ì„œ Ï„Ì‚ ì¶”ì •
Step 4: ëª¨ë“  foldì— ëŒ€í•´ ë°˜ë³µ
```

**ì™œ ì´ê²Œ ì¤‘ìš”í•œê°€?**
- **Independence**: ì¶”ì •ì— ì‚¬ìš©ëœ ë°ì´í„°ì™€ ì˜ˆì¸¡ ë°ì´í„°ê°€ ë…ë¦½
- **Bias Reduction**: $\mathbb{E}[\hat{m}^{(-i)}(X_i) \mid X_i] \approx m^*(X_i)$

#### **3. Doubly Robustì˜ ìˆ˜í•™ì  êµ¬ì¡°**

í•µì‹¬ì€ ë‹¤ìŒ decompositionì…ë‹ˆë‹¤:

$$
\hat{\mathcal{L}}_n - \mathcal{L}_n^{\text{oracle}} = \text{Bias Term} + \text{Variance Term}
$$

**Bias Term**:
$$
\frac{2}{n} \sum_{i=1}^n \left[\{Y_i - m^*(X_i)\} - \{W_i - e^*(X_i)\}\tau^*(X_i)\right] \times \left[\{\hat{m}^{(-i)} - m^*\}(X_i) - \{\hat{e}^{(-i)} - e^*\}(X_i)\tau^*(X_i)\right]
$$

Cross-fittingì— ì˜í•´ ì´ termì˜ ê¸°ëŒ“ê°’ì´ 0ì— ê°€ê¹Œì›Œì§‘ë‹ˆë‹¤.

**Variance Term**:
$$
\left\|\{\hat{m} - m^*\} - \{\hat{e} - e^*\}\tau^*\right\|^2
$$

ì´ëŠ” $\|\hat{m} - m^*\| \times \|\hat{e} - e^*\|$ì˜ orderì…ë‹ˆë‹¤.

#### **4. ìˆ˜ë ´ë¥ ì˜ ê³±ì…ˆ êµ¬ì¡°**

ë§Œì•½:
- $\|\hat{m} - m^*\| = O_p(r_m)$
- $\|\hat{e} - e^*\| = O_p(r_e)$

ê·¸ëŸ¬ë©´:
$$
\|\hat{\tau} - \tau^*\| = O_p(\max\{r_{\text{oracle}}, r_m \times r_e\})
$$

**ì˜ˆì‹œ**: 
- $r_m = r_e = n^{-1/4}$ (ëŠë¦° ìˆ˜ë ´)
- $r_m \times r_e = n^{-1/2}$ (ë¹ ë¥¸ ìˆ˜ë ´!)
- Oracle rate $r_{\text{oracle}} = n^{-1/3}$ì´ë©´
- ìµœì¢… rate = $\max\{n^{-1/3}, n^{-1/2}\} = n^{-1/3}$ âœ¨

### **ì‹¤ì œ ë™ì‘ ì˜ˆì‹œ**

#### **ì‹œë‚˜ë¦¬ì˜¤ 1: ì™„ë²½í•œ Oracle**
```python
# Oracle knows true m*(x) and e*(x)
oracle_loss = R_loss(Y, W, X, tau, m_true, e_true)
oracle_rate = n^(-1/3)  # optimal rate for Ï„* complexity
```

#### **ì‹œë‚˜ë¦¬ì˜¤ 2: R-learner with Imperfect Estimates**
```python
# Step 1: Estimate nuisances with cross-fitting
m_hat = estimate_m_with_crossfit(Y, X)  # rate: n^(-1/4)
e_hat = estimate_e_with_crossfit(W, X)  # rate: n^(-1/4)

# Step 2: Optimize R-loss
tau_hat = optimize_R_loss(Y, W, X, m_hat, e_hat)

# Achieved rate: max(n^(-1/3), n^(-1/4) Ã— n^(-1/4)) = n^(-1/3)
# Same as oracle! ğŸ‰
```

### **ì™œ ì´ê²Œ "Quasi-Oracle"ì¸ê°€?**

1. **Oracleê³¼ ê°™ì€ ìˆ˜ë ´ë¥ **: $n^{-1/3}$ ë‹¬ì„±
2. **í•˜ì§€ë§Œ Oracleì´ ì•„ë‹˜**: $m^*, e^*$ë¥¼ ì‹¤ì œë¡œëŠ” ëª¨ë¦„
3. **Automatic**: ì¶”ê°€ì ì¸ íŠœë‹ ì—†ì´ ìë™ìœ¼ë¡œ ë‹¬ì„±
4. **Robust**: nuisance ì¶”ì •ì´ ì™„ë²½í•˜ì§€ ì•Šì•„ë„ ì‘ë™

ì´ê²ƒì´ ë°”ë¡œ **"quasi-oracle property"**ì˜ í•µì‹¬ì…ë‹ˆë‹¤! ğŸ¯

