---
title: "HyperConnect ML Engineer"
date: "2025-08-25"
excerpt: "하이퍼 커넥트 이력서 컨셉과 방향 적어보기"
category: "Career"
tags: ["이력서"]
---

- [하이퍼 커넥트 채용공고](/posts/Self_Development/Career/Job%20Transition/25-08-04-하이퍼커넥트)
- [지원한 이력서](/posts/Self_Development/Career/Resumes/pdf_files/resume_hyperconnect)

---
탈락 ㅠ -> Fit하지 않다는 피드백. 빈말일 수도 있지만, 실제로 내 이력서에 관련 업무 경험이 없긴 하다.
이 부분이 가장 어려운 부분... 사이드프로젝트를 꾸준히 잘 정리했다면 결과가 달라질 수도 있지 않을까?


```
안녕하세요 김민규님,
하이퍼커넥트 Talent Acquisition Team입니다.
 
하이퍼커넥트에 관심 가지고 지원해 주셔서 진심으로 감사합니다.
서류 지원 과정 중 저희가 의도치 않게 불편을 드린 점은 없었는지 마음이 쓰입니다.
 
김민규님의 뛰어난 역량에도 불구하고, 안타깝게도 이번 서류 심사 과정에서 Machine Learning Engineer (HYPERCONNECT AI - Contents Understanding) 포지션으로 귀하를 모실 수 없게 되었습니다. 
이는 역량의 부족이 아닌 포지션과의 Fit이 맞지 않아서임을 양해해 주시길 부탁드립니다. 
늘 건승하시기 바라며 더 좋은 기회로 인사 드릴 수 있길 희망합니다.
 
하이퍼커넥트도 김민규님이 보여주신 관심에 보답할 수 있도록 더욱 더 성장할 수 있도록 하겠습니다. 
 
감사합니다.
하이퍼커넥트 Talent Acquisition Team

Confidential / 대외비

CONFIDENTIALITY NOTICE: This e-mail communication and any attachments contains confidential and privileged information for the sole use of the designated recipients named above. It is prohibited to disclose, disseminate, distribute or copy this e-mail and any attachments beyond the scope which is mutually agreed with the sender of this e-mail. If you are not the intended recipient, please notify the sender by return e-mail and delete and/or destroy all copies of this communication and any attachments immediately.

본 이메일 및 첨부파일은 지정된 수신인만을 위한 것이며, 회사의 비밀정보를 포함합니다. 본 정보를 발신인과 합의된 범위 외에 사용, 공개, 배포하는 것은 엄격히 금지됩니다. 귀하가 만일 지정된 수신인이 아닌 경우, 발신인에게 그 사실을 통지해 주시고 원본 이메일과 첨부파일 및 모든 사본을 즉시 삭제하여 주시기 바랍니다.
```


---


채용공고 스케치

거의 2주동안 포트폴리오를 완성하지 못하고 있는데, 다른 이유가 아니라 그냥 원하는 기업에 비해 내가 가진게 너무 볼품없어 보여서 그런 것 같다.
회피는 그만! 뭐가 부족한지 똑바로 확인하고 이 부분들을 채워나가보자.

JD에서 확인한 필수요구사항들.

- AI/ML에 대한 기본 지식과 적어도 한 개 이상의 특정 도메인에 대한 깊이 있는 지식을 갖추고, 관련 프로젝트 경험이 있으신 분
- Exploratory Data Analysis(EDA)를 통해, 데이터의 통계적 특성과 패턴을 발견하고 이를 ML 모델에 반영하실 수 있는 분
- 공개된 벤치마크 데이터 셋을 이용해 모델의 테스트 성능을 올리기 위해 여러 가지 모델링을 해본 경험이 있으신 분
- 구현체가 공개되지 않은 논문을 읽고, 빠르고 정확하게 구현할 수 있는 구현 역량을 갖추신 분
- Tensorflow, PyTorch, JAX 등 오픈소스 프레임워크 및 전반적인 파이썬 개발에 능숙하신 분
- ML 모델 학습 및 서비스 배포에 필요한 엔지니어링 역량을 갖추신 분
- AI 기술의 서비스화에 관심이 많으신 분
- 학위/국적 무관, 영어로 기초적인 의사소통이 가능하고, 한국어로 원활한 의사소통이 가능한 분

앞으로 내 행동 방향.
1. 필수요구사항과 선호경험에 맞는 경험들 최대한 살려서 이력서 제출하기
2. 부족한 부분들은 일할 때 어떻게 채워나갈 수 있을지 고민해보기


3개의 프로젝트를 리드한 경험 적어보자. + CV 다뤘던 경험


# 이력서 스케치 (영문 PDF 제출)

Led 3 projects with development involvement at a startup, demonstrating rapid growth and leadership capabilities.

Projects:

## Lenze size recommendation

1. Achievement (How + Result one line)
Quantified prediction uncertainty using causal inference to evaluate prediction reliability for unobserved lens sizes and ensured model stability through integrated data pipeline and monitoring system

2. Role/Solution Process (3-4 lines)
- Prediction Evaluation Problem Solving: 
Resolved limitations in evaluating predictions for unobserved treatments by leveraging causal inference's positivity assumption and propensity scores, expressing prediction uncertainty through intervals using CQR and partial identification
- Data Consistency Assurance: 
Addressed data inconsistency between training and inference environments by building integrated data pipeline and feature store, standardizing data preprocessing processes and ensuring consistency across environments
- Monitoring System Development: 
Developed automated monitoring pipeline using Airflow for real-time data drift detection and performance tracking, enabling early detection of silent failures and ensuring service stability
- Domain-Specific Modeling: 
Enhanced model interpretability through clinically meaningful feature engineering and monotonicity constraints aligned with physician intuition, and provided model explanations using Shapley values to increase medical staff trust



- 2025.03 - Present (6 months)

Project Overview:

Developed a service that predicts surgical outcomes for vision correction, enabling surgeons to select optimal lens sizes based on data-driven predictions rather than solely on experience and intuition.

Problem-Solving Process:

1. Model Validation Challenge

- Problem
Faced challenges in evaluating prediction accuracy when the model generated four size options but only one could be validated with real customer data.

- Solution
Leveraged causal inference (positivity assumption) to exploit distributional overlap across treatments (lens sizes), facilitating indirect performance evaluation.
Accounted for non-random treatment assignment (lens sizes) by quantifying overlap in the data and separating overlapping from non-overlapping regions for analysis.
Validated this approach not only through domain expertise but also by statistically examining the distribution across treatments.

Used prediction intervals where treatment distributions overlapped and applied partial identification to quantify prediction ranges in non-overlapping regions.
Motivated by the goal of increasing user trust, expressed predictive uncertainty explicitly through intervals.

- Result
Indirectly evaluated unobserved predictions and visualized both guaranteed and non-guaranteed ranges to effectively communicate predictive uncertainty.


2. Data quality and consistency challenges between training and inference environments

- Problem
In the absence of a unified data pipeline, training and inference each pulled data directly from the data lake, leading to inconsistencies in data processing between environments.

- Solution
Developed a robust data pipeline to cleanse both OCR-collected and external data sources, incorporating data validation and schema enforcement to ensure data quality. 
In addition, built a feature store that unified offline (training) and online (serving) features, securing data consistency across environments and improving the reliability of model deployment.

- Result
Secured data quality through the new pipeline and established consistency across training and inference via the feature store, increasing confidence in model outputs and stability in deployment.



3. Modeling Approach

- Problem

The goal of model development was to deliver results that achieve user satisfaction, thereby fostering trust.
This required not only improving predictive accuracy within the dataset, but also ensuring robust performance in inference settings. 
In addition, the model needed to produce outputs aligned with user intuition—particularly important as the primary users were physicians, necessitating medically interpretable predictions.

- Solution

To improve model performance, I placed strong emphasis on thoroughly examining the data. 
I first analyzed feature distributions and filtered out implausible values that deviated from clinical standards, ensuring a reliable dataset. 

Drawing on physician intuition, I engineered clinically meaningful features—for example, capturing the relationship between age and lens thickness—which not only improved predictive accuracy but also aligned the model’s behavior with medical reasoning. 

To further strengthen reliability, I incorporated medical domain knowledge into the model by applying monotonicity constraints where clinically appropriate. 

Finally, I used interpretability tools such as Shapley values to explain prediction outcomes, enabling physicians to better understand and trust the model’s decisions.

- Result

Through detailed data analysis, I improved data quality and developed new features that enhanced model performance while producing results consistent with medical intuition. 
In addition, the model was not treated as a black box; I provided explanations for its outputs, ensuring that the reasoning behind predictions was transparent and understandable.

4. Lack of monitoring for silent failures and model performance

- Problem

Monitoring was required to ensure model performance in inference settings. 
It was important to track not only how well the model performed during real-world usage, but also to detect potential issues such as data drift. 
Without such monitoring, the service could appear to function normally while causing user discomfort through silent failures, ultimately reducing user trust and engagement.

- Solution
Developed an automated monitoring pipeline with Airflow to detect data drift using statistical measures (e.g., Jensen–Shannon distance) and to continuously compare training vs. inference data distributions. 
Implemented performance tracking by monitoring prediction–outcome gaps in real time, enabling early detection of silent failures and ensuring reliable model performance in production.

- Result
Enhanced service stability and maintained user trust by proactively identifying data drift and performance degradation before they impacted end users.


## OCR Pipeline

Achievement (How + Result):
- Achieved error rate below 1% and 99%+ OCR accuracy by implementing object-oriented design and unit testing framework
Role (3-4 lines):
- Refactored procedural code into object-oriented architecture to clearly separate functional responsibilities and improve debugging efficiency
- Built unit testing and type validation system using pytest and mypy to prevent runtime errors proactively
- Improved OCR accuracy from 70% to 99%+ by applying domain-specific models tailored to fixed regions per device type and optimizing image preprocessing/postprocessing logic
- Established clinical validity verification and anomaly detection monitoring system to prevent silent failures and ensure stable service operations



- 2024.07 - 2024.10 (4 months)

Project Overview:

Developed a robust OCR pipeline to extract test results from medical images and store them in a structured database. 
Since the pipeline served as a critical component for collecting data used in production services, it was designed with a strong focus on real-time processing, stability, and high accuracy.

Problem-Solving Process:

1. Ensuring Stable Service Operations

- Problem
Inherited a legacy project with a large backlog of error logs. The codebase was written in a purely procedural manner, making it extremely difficult to trace the root causes of errors. The lack of clear structure and responsibility boundaries hindered both debugging efficiency and long-term maintainability.

- Solution
Refactored the system into an object-oriented architecture, applying core principles such as single-responsibility per function and well-defined ownership for each object. This restructuring enabled clearer separation of concerns and improved readability. 

Additionally, integrated unit testing with pytest and static type checking with mypy to proactively detect potential runtime errors. Together, these practices established a robust foundation for reliable and maintainable service operations.

- Result
Achieved an error rate below 1%, while significantly improving debuggability. 
Even when issues occurred, the modular design and testing framework enabled rapid root-cause identification and resolution, ensuring stable service delivery and reducing operational overhead.


2. Improving OCR Accuracy

- Problem
The existing deployed OCR model had been developed without a proper test set and was only validated manually by its original developer. 
To rigorously evaluate performance, I constructed a dedicated test dataset covering ~100 samples for each of the six different diagnostic devices. 

Benchmarking revealed that the deployed model achieved less than 70% accuracy, which was far below the reliability required for production use.

In addition, silent failures in production (e.g., plausible but clinically invalid outputs) were not being detected, creating risks for downstream systems and users.

- Solution
Leveraged domain-specific characteristics to boost performance. Although the image types varied across devices, the regions of interest to extract were fixed per device type. 
This allowed the use of rule-based localization to reliably identify the relevant regions, effectively simplifying the text detection step. 

For text recognition, integrated TrOCR, an open-source model specialized in OCR tasks. Model evaluation with the curated dataset showed accuracy around 95%, which was further improved to over 99% by applying tailored image preprocessing and position-specific postprocessing strategies.

To address silent failures, designed a monitoring layer that applied clinical plausibility checks and probabilistic anomaly detection. The system raised alerts whenever extracted values were outside valid ranges or statistically improbable given the device type, enabling proactive detection of hidden errors.

- Result
Achieved over 99% OCR accuracy in a mission-critical pipeline while also ensuring reliability through monitoring. The monitoring layer consistently surfaced out-of-range or anomalous values, reducing undetected OCR errors and enabling faster triage when issues occurred. 
These improvements ensured stable service operations, minimized error propagation to downstream systems, and reinforced trust in the automated data pipeline.


## Chatbot

Achievement (How + Result)
- Automated customer service operations through RAG pipeline implementation (classification model integration, search accuracy improvement) and monitoring system development

Role (3-4 lines)
- Led end-to-end process from problem definition to RAG pipeline design and implementation for automating repetitive customer service tasks
- Improved user satisfaction by implementing classification model at pipeline frontend and developing context-aware question rephrasing and keyword extraction features, reducing unsatisfied feedback by over 50%
- Built Qdrant-based VectorDB system enabling users to select document versions and perform real-time updates
- Established monitoring system using RAGAS framework integrated with Airflow for silent failure detection and performance metric tracking to minimize maintenance costs







- 2024.11 - 2025.03 (4 months)

Project Overview:

Developed an AI chatbot service to automate repetitive customer inquiries, reducing the burden on human agents and improving overall customer service efficiency. The system processed an average of 100-200 daily inquiries, significantly enhancing agent productivity.

Problem-Solving Process:

1. Improving User Satisfaction through Classification

- Problem
User feedback revealed that the most common source of dissatisfaction was the LLM generating ambiguous responses to questions it couldn't clearly answer. This was particularly problematic when customers asked about services that required manual intervention (e.g., appointment scheduling with personal information), yet the system continued to engage in unnecessary back-and-forth conversations.

- Solution
Implemented a classification model at the front-end of the RAG pipeline to categorize incoming questions and route them appropriately. Used LLM-based classification prompts to identify question types and ensure proper handling of different inquiry categories.

- Result
Reduced "unsatisfied" feedback by over 50% by preventing inappropriate responses and unnecessary conversations.

2. Context-Aware Document Retrieval

- Problem
The system needed to maintain conversation context to provide relevant responses. For example, when a user asked "What's the price of LASIK?" followed by "What about LASEK?", the system should understand the second question refers to LASEK pricing.

- Solution
Implemented question rephrasing using Redis to store short-term conversation history. The system retrieves conversation context and rephrases questions to match the conversation flow, enabling more accurate document retrieval.

- Result
Improved response relevance by maintaining conversation context and reducing the need for users to repeat information.

3. Enhanced Search Performance through Query Decomposition and Keyword Extraction

- Problem
Single questions often contained multiple semantic components (e.g., "Tell me the prices of both LASIK and LASEK"), requiring retrieval of documents for multiple sub-questions. Additionally, semantic search limitations and embedding model constraints made it difficult to find documents containing proper nouns like doctor names.

- Solution
Implemented query decomposition to break complex questions into multiple sub-queries, each targeting specific information needs. Added keyword extraction to complement semantic search, enabling filtering based on extracted entities like proper nouns.

- Result
Improved search accuracy for complex queries and enhanced retrieval of documents containing specific entities that embedding models might miss.

4. Document Retrieval Optimization

- Problem
The core challenge was creating high-quality embedding vectors for semantic search to retrieve the most relevant FAQ documents.

- Solution
Built a test dataset from FAQ data using LLM-generated questions to evaluate different embedding models. Selected the best-performing model through systematic comparison. Implemented Qdrant as the vector database with real-time updates when documents are modified, minimizing maintenance costs. Used multi-tenancy features to clearly separate document versions.

- Result
Achieved optimal document retrieval performance with a maintainable and scalable vector database solution.

5. Response Generation

- Problem
Generated responses needed to be both accurate (based only on retrieved documents) and user-friendly, maintaining a conversational tone that customers would find approachable and easy to understand.

- Solution
Applied prompt engineering to constrain the LLM to generate responses based solely on retrieved document content while maintaining a friendly and accessible tone. This dual focus ensured both accuracy and positive user experience.

- Result
Delivered responses that were both factually accurate and user-friendly, improving overall customer satisfaction.

6. Monitoring and Maintenance

- Problem
Silent failures in the chatbot system could go undetected, potentially causing user discomfort and reducing trust in the service. Continuous monitoring was needed to ensure reliable performance and minimize maintenance costs.

- Solution
Implemented an automated monitoring system using the RAGAS framework to evaluate response quality. The system scores responses based on their grounding in retrieved documents and tracks performance metrics. Used Airflow to process chat logs from spreadsheets, generating daily and weekly performance reports and alerting high-scoring conversations via Slack.

- Result
Proactively detected silent failures and performance issues, ensuring reliable service operation and maintaining user trust through continuous quality monitoring.




# Cover letter

I am writing to express my strong interest in the Machine Learning Engineer role at HYPERCONNECT.
Although I am still a junior developer, my career goal has always been clear: to grow into a developer who not only builds technology but also creates meaningful impact for users and the business.

During my time at a startup, I had the opportunity to work on real services, which taught me how technology can directly influence user experience and business outcomes. I consistently strived to go beyond solving technical problems in isolation, and instead focused on how each solution could improve the service and deliver value to both users and the company.

I am particularly inspired by HYPERCONNECT’s work, which I have followed through technical blogs and research publications. I was impressed by how your team demonstrates not only strong technical expertise but also a commitment to driving tangible business results. This aligns closely with my own aspiration to become a developer who bridges technical innovation with real-world impact.

At HYPERCONNECT, I would be eager to contribute my hands-on startup experience, adaptability, and strong motivation to grow. I believe my background equips me to quickly integrate into your team, learn from the challenges ahead, and make meaningful contributions to your mission.

Thank you for considering my application. I look forward to the opportunity to discuss how my skills and passion can contribute to HYPERCONNECT’s continued success.


