---
title: "Non-crossing nonlinear regression quantiles by monotone composite quantile regression neural network, with application to rainfall extremes"
date: "2025-10-21"
excerpt: "í•œì¤„ ìš”ì•½ : tauë¥¼ monotone covariateë¡œ ì‚¬ìš©í•´ composite, monotonity ëª¨ë‘ ë§Œì¡±í•˜ëŠ” ëª¨ë¸ êµ¬ì¡° (MCQRNN) ì œì•ˆí•œ ë…¼ë¬¸"
category: "Machine Learning"
tags: ["Quantile Regression", "paper review"]
---

- [paper link](https://link.springer.com/article/10.1007/s00477-018-1573-6)

ë…¼ë¬¸ ë¹ ë¥´ê²Œ íŒŒì•…í•˜ëŠ” ì—°ìŠµë„ ê°™ì´


# ê°œë… ìš”ì•½, ìŠ¤ì¼€ì¹˜


---

# 1ë‹¨ê³„. êµ¬ì¡°ë¶€í„° ì¡ëŠ” â€˜ìŠ¤ì¼ˆë ˆí†¤ ë¦¬ë”©â€™ (5~10ë¶„)
> ğŸ“ëª©í‘œ: ì´ ë…¼ë¬¸ì´ ë‚˜ë‘ ê´€ë ¨ ìˆëŠ”ì§€ íŒë‹¨

---

## Abstract
- ë¬¸ì œ ì •ì˜ + í•µì‹¬ ê¸°ì—¬ + ë¹„êµ ëŒ€ìƒì´ ë‚˜ì˜¤ëŠ” ë¶€ë¶„.
- ì—¬ê¸°ì„œ â€œì´ ë…¼ë¬¸ì´ í•´ê²°í•˜ë ¤ëŠ” Pain Pointâ€ë¥¼ í•œ ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´.

ë…¼ë¬¸ì—ì„œëŠ” quantile crossing problemì„ í•´ê²°í•˜ê¸° ìœ„í•œ ìƒˆë¡œìš´ ëª¨ë¸ì„ ì œì‹œí•˜ê³  ìˆë‹¤.

> These estimates are prone to â€œquantile crossingâ€, where regression predictions for different quantile probabilities do not increase as probability increases.

> As a remedy, this study introduces a novel nonlinear quantile regression model, the monotone composite quantile regression neural network (MCQRNN)
1. simultaneously estimates multiple non-crossing, nonlinear conditional quantile functions
2. allows for optional monotonicity, positivity/non-negativity, and generalized additive model constraints
3. can be adapted to estimate standard least-squares regression and non-crossing expectile regression functions

MCQRNNì„ ì œì‹œ. ì´ì— ëŒ€í•œ ì„¤ëª….

>  In comparison to standard QRNN models, the ability of the MCQRNN model to incorporate these constraints, in addition to non-crossing, leads to more robust and realistic estimates of extreme rainfall.

QRNN ëª¨ë¸ ëŒ€ë¹„ ì‹¤ì œ ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•œ ë‹¤ì–‘í•œ ì¡°ê±´ë“¤ë„ í¬í•¨ì‹œì¼œì„œ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆë‹¤.

---

## Figure
- ëª¨ë¸ êµ¬ì¡°ë‚˜ ì „ì²´ íŒŒì´í”„ë¼ì¸ì„ ë³´ì—¬ì£¼ëŠ” ë„ì‹ì´ ëŒ€ë¶€ë¶„ í•µì‹¬ ìš”ì•½ì„.
- â€œì…ë ¥â€“ì²˜ë¦¬â€“ì¶œë ¥â€“í‰ê°€â€ íë¦„ì„ ë¨¸ë¦¿ì†ì— ê·¸ë ¤ë‘¬.

<figure>
  <img src="./images/figure1.png" alt="MCQRNN" />
  <figcaption>Fig. 1</figcaption>
</figure>
- ê·¸ë¦¼ ì„¤ëª… ì •ë¦¬:
  - (a), (c): ì¼ë°˜ QRNN(Quantile Regression Neural Network) ê²°ê³¼  
  - (b), (d): MCQRNN(Monotone Composite QRNN) ê²°ê³¼  
  - ê° ê·¸ë˜í”„ì˜ ê²€ì€ ì ì€ synthetic data (Eq. 15: a, b / Eq. 16: c, d)ë¥¼ ì˜ë¯¸  
  - ë¬´ì§€ê°œìƒ‰ ê³¡ì„ ë“¤ì€ ì•„ë˜ì—ì„œ ìœ„ë¡œ ê°ê° ë¶„ìœ„ìˆ˜ s = 0.1, 0.2, ..., 0.9 (ì´ 9ê°œ)ë¥¼ ë‚˜íƒ€ëƒ„  
  - íšŒìƒ‰ ì‹¤ì„ : ì‹¤ì œ(ground truth) ì¡°ê±´ë¶€ ë¶„ìœ„ìˆ˜ í•¨ìˆ˜  
- ì£¼ìš” í¬ì¸íŠ¸:  
  - MCQRNNì€ quantile crossing ì—†ì´, ë” ë¶€ë“œëŸ½ê³  í˜„ì‹¤ì ì¸ ë¶„ìœ„ìˆ˜ ê³¡ì„ ì„ ì‚°ì¶œí•¨

MCQRNNì´ ì¡°ê¸ˆ ë” ì•ˆì •ì ì´ë‹¤. 

<figure>
  <img src="./images/figure2.png" alt="MCQRNN" />
  <figcaption>Fig. 2</figcaption>
</figure>

- Fig. 1b, dì™€ ìœ ì‚¬í•œ ì‹¤í—˜ ê²°ê³¼ì´ì§€ë§Œ,  
  - (a) : MCQRNNì— positivity constraint(ì¶œë ¥ ì–‘ìˆ˜ ì œì•½)ë¥¼ ì¶”ê°€í•¨  
  - (b) : positivity + monotonicity constraint(ì¶œë ¥ ì–‘ìˆ˜ ë° ë‹¨ì¡°ì„± ì œì•½) ëª¨ë‘ ì¶”ê°€  
- (c), (d) : Fig. 1b, dì—ì„œ 0.1, 0.5, 0.9-quantileì— ëŒ€í•´ parametric bootstrap(500íšŒ)ë¡œ ì¶”ì •í•œ 95% ì‹ ë¢°êµ¬ê°„(Confidence Interval) ê²°ê³¼ í‘œì‹œ  

---

## Conclusion
- ì–´ë–¤ ì ì„ â€˜ìƒˆë¡­ê²Œ í–ˆë‹¤â€™ + â€˜í–¥í›„ ê³¼ì œâ€™ë¡œ ì •ë¦¬í•´ë‘ëŠ” ë¶€ë¶„.
- Abstractê³¼ ê²°ë¡ ì„ ë¹„êµí•˜ë©´ â€œì§„ì§œ ì–»ì€ ê±´ ë­”ê°€â€ë¥¼ íŒŒì•… ê°€ëŠ¥.

> MCQRNN is the first neural network-based quantile regression model that guarantees non-crossing of regression quantiles.

> Given its close relationship to composite QR models, MCQRNN is first evaluated using the Monte Carlo simulation experiments adopted by Xu et al. (2017) to demonstrate the CQRNN model

ê¸°ì¡´ ëª¨ë¸ë³´ë‹¤ robust í•˜ë‹¤.

- MCQRNNì€ ê¸°ì¡´ MLP, QRNN, CQRNN ëª¨ë¸ ëŒ€ë¹„ íŠ¹íˆ ì˜¤ì°¨ ë¶„í¬ê°€ ë¹„ì •ê·œ(non-normal)ì¼ ë•Œ ë”ìš± ê²¬ê³ í•˜ë‹¤.
- ìºë‚˜ë‹¤ ê°•ìš°ëŸ‰(IDF ê³¡ì„ ) ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì‹¤ì œ ì„±ëŠ¥ì„ í‰ê°€í•¨.
- ë‹¤ì–‘í•œ í­í’ ì§€ì† ì‹œê°„ê³¼ ì¬í˜„ê¸°ê°„ì— ëŒ€í•´ ì •ë³´ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê³µìœ í•˜ì—¬ ê³¼ì í•©ì— ê°•í•˜ë‹¤(cross-validation result).
- ë‹¨ì¡° ì œì•½(monotonicity constraint) ì ìš©ì´ ê°€ëŠ¥í•˜ì—¬, ê°•ìš° ê°•ë„ê°€ ë°œìƒ ë¹ˆë„ ë° ì§€ì† ì‹œê°„ì´ ì‘ì•„ì§ˆìˆ˜ë¡ ìì—°ìŠ¤ëŸ½ê²Œ ì¦ê°€í•˜ëŠ” ì‹¤ì œ íŠ¹ì„±ì„ ë°˜ì˜í•  ìˆ˜ ìˆë‹¤.
- ê³„ì¸¡ì†Œê°€ ì—†ëŠ” ì§€ì—­ì—ì„œë„ ê·¹í•œ ê°•ìš°ì˜ í˜„ì‹¤ì ì´ê³  ì‹ ë¢°ì„± ìˆëŠ” ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤.

---

## 1ë¬¸ì¥ ìš”ì•½ ì‹œë„
- ì˜ˆ: â€œì´ ë…¼ë¬¸ì€ ê¸°ì¡´ GBDT ê¸°ë°˜ CTR ì˜ˆì¸¡ì˜ calibration ë¬¸ì œë¥¼ NN ê¸°ë°˜ìœ¼ë¡œ í•´ê²°í•¨.â€
- ì´ í•œ ë¬¸ì¥ì´ ì•ˆ ë‚˜ì˜¤ë©´ â†’ ì•„ì§ ìš”ì•½í•  ìˆ˜ì¤€ìœ¼ë¡œ ì´í•´ê°€ ì•ˆ ëœ ê²ƒ.

QRNNì— ë‹¤ì–‘í•œ ì œì•½ì¡°ê±´ì„ ì¶”ê°€í•´ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¬ ìˆ˜ ìˆê³ , ì´ë¥¼ í†µí•´ ì›í•˜ëŠ” ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.


---

# 2ë‹¨ê³„. â€˜êµ¬ì¡°ì  ë¦¬ë”©â€™ â€” í•µì‹¬ë§Œ ê¹Šê²Œ (20~40ë¶„)
> ğŸ“ëª©í‘œ: í•µì‹¬ ì•„ì´ë””ì–´Â·ëª¨ë¸Â·ì‹¤í—˜ êµ¬ì¡°ë¥¼ ë¹ ë¥´ê²Œ ì¬êµ¬ì„±í•˜ê¸°

---

## Introduction
- â€œì™œ ì´ ë¬¸ì œì¸ê°€?â€ â€” ë¬¸ì œì˜ ì¤‘ìš”ì„±
- â€œê¸°ì¡´ ë°©ë²•ì˜ í•œê³„â€ â€” baseline ì •ë¦¬
- â€œìš°ë¦¬ì˜ ê¸°ì—¬â€ â€” bullet pointë¡œ ì„¸ ì¤„ ì •ë¦¬

to provide estimates of predictive uncertinity in forcast ìœ„í•´ quantile regressionì„ ì§„í–‰í•´ì™”ìŒ.

> However, given finite samples, this flexibility can lead to â€˜â€˜quantile crossingâ€™â€™ where, for some values of the covariates, quantile regression predictions do not increase with the specified quantile probability tau.

>  As Ouali et al. (2016) state, â€˜â€˜crossing quantile regression is a serious modeling problem that may lead to an invalid response distributionâ€™â€™.
  
ê³ ì „ì ìœ¼ë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œê°€ , "quantile crossing problem".

> Three main approaches have been used to solve the quantile crossing problem: post-processing, stepwise estimation, and simultaneous estimation. 

ì´ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ 3ê°€ì§€ ë°©ë²•ì„ ì‹œë„í•´ì™”ìŒ.

1. post-processing : ê°•ì œë¡œ ìˆœì„œ ì •ë ¬ ì‹œí‚¤ëŠ” ê²ƒ
2. stepwise estimation : ì´ì „ì— ì¶”ì •ëœ ë¶„í¬ì„ ì„ ë„˜ì§€ ì•Šë„ë¡ ì œì•½ ì¡°ê±´ì„ ê±¸ì–´ê°€ë©° ìˆœì°¨ì ìœ¼ë¡œ íšŒê·€ì„ ì„ í•™ìŠµí•¨. (ì´ì „ ë¶„í¬ì„ ì„ ê¸°ë°˜ìœ¼ë¡œ ë‹¨ê³„ë¥¼ ë‚˜ëˆ  êµì°¨ë¥¼ ë°©ì§€í•˜ë©° í•™ìŠµ)
3. simultaneous estimation : ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ íšŒê·€ì‹ì„ ë™ì‹œì— ì¶”ì •í•˜ë©´ì„œ, íŒŒë¼ë¯¸í„° ìµœì í™”ì— ì¶”ê°€ ì œì•½ì¡°ê±´ì„ ë„£ì–´ ë¶„ìœ„ìˆ˜ ê°„ êµì°¨ë¥¼ ë°©ì§€í•¨ (Takeuchi et al. 2006; Bondell et al. 2010; Liu and Wu 2011; Bang et al. 2016).  
í•œì¤„ ìš”ì•½: ëª¨ë“  ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ í•œ ë²ˆì— í•™ìŠµ + êµì°¨ ë°©ì§€ ì œì•½ ì¶”ê°€

> Unlike sequential estimation, simultaneous estimation is attractive because it does not depend on the order in which quantiles are estimated. Furthermore, fitting for multiple values of tau simultaneously allows one to â€˜â€˜borrow strengthâ€™â€™ across regression quantiles and improve overall model performance (Bang et al. 2016).

ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ë¥¼ í•œë²ˆì— í•™ìŠµí•˜ëŠ” ê±´ ì œì•½ì¡°ê±´ì„ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ê²ƒ ì™¸ì—ë„ ì„±ëŠ¥ í–¥ìƒì— ë„ì›€ì„ ì¤Œ.

> For a flexible nonlinear model like a neural network, imposing extra constraints, for example as informed by process knowledge, can be useful for narrowing the overall search space of potential nonlinearities.

íŠ¹íˆ non-linear í•  ë•Œ ê°€ì¥ ë„ì›€ì„ ì¤Œ. êµ‰ì¥íˆ ë§ì€ ê³µê°„ì„ íƒìƒ‰í•˜ëŠ” non-linear + constraintëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ë‚¼ ê²ƒì´ë¼ ê¸°ëŒ€ ë¨.

- Muggeo et al. (2013): ì„±ì¥ ê³¡ì„ ì´ ë‚˜ì´ì— ë”°ë¼ ë‹¨ì¡° ì¦ê°€í•´ì•¼ í•œë‹¤ëŠ” ì ì„ ë“¤ì–´, non-crossing ì œì•½ì— ë”í•´ monotonicity(ë‹¨ì¡°ì„±) ì œì•½ ì¡°ê±´ì„ ì¶”ê°€í•¨.
- Roth et al. (2015): ë¹„ì„ í˜• ë‹¨ì¡° ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ í™œìš©í•´ ê°•ìš° ê·¹ê°’(rainfall extremes)ì—ì„œ ê°ì†Œ ë˜ëŠ” ì¦ê°€í•˜ëŠ” ì¶”ì„¸(ë‹¨ì¡° ì¶”ì„¸)ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì„¤ëª…í•¨.
- Takeuchi et al. (2006): ì»¤ë„ ê¸°ë°˜ ë¹„ëª¨ìˆ˜(nonparametric) ë°©ì‹ì˜ ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ ì œì•ˆí–ˆê³ , ì´ ëª¨ë¸ì€ SVMê³¼ ìœ ì‚¬í•œ êµ¬ì¡°ì— non-crossing, monotonicity(ë‹¨ì¡°ì„±) ì œì•½ì„ ëª¨ë‘ ì ìš©í•¨. ì¶”ê°€ì ìœ¼ë¡œ ì–‘ìˆ˜ì„±(positivity), ê°€ì‚°ì„±(additivity) ë“± ë‹¤ì–‘í•œ ì œì•½ì¡°ê±´ ì ìš© ë°©ë²•ë„ ì œì‹œí•¨.

> However, standard implementations of the kernel quantile regression model (e.g., Karatzoglou et al. 2004; Hofmeister 2017) are computationally costly, with complexity that is cubic in the number of samples, and do not explicitly implement the proposed constraints.

ì´ ë…¼ë¬¸ì—ì„œ ë§í•˜ëŠ” í•µì‹¬. ê¸°ì¡´ ëª¨ë¸ë“¤ì€ ë¹„ì‹¸ë‹¤ëŠ” í•œê³„ì ì„ ê°€ì§€ê³  ìˆë‹¤. 

> As an alternative, this study introduces an efficient, flexible nonlinear quantile regression model, the monotone composite quantile regression neural network (MCQRNN), that:
1. simultaneously estimates multiple non-crossing quantile functions
2. allows for optional monotonicity, positivity/nonnegativity, and additivity constraints, as well as fine-grained control on the degree of non-additivity
3. can be modified to estimate standard least-squares regression and non-crossing expectile regression functions

ì´ëŸ¬í•œ ê¸°ëŠ¥ë“¤ì€ ì•„ë˜ì™€ ê°™ì€ ê¸°ì¡´ neural network/íšŒê·€ ëª¨ë¸ ìš”ì†Œë“¤ì„ ê²°í•©í•´ì„œ í•˜ë‚˜ì˜ í†µí•© í”„ë ˆì„ì›Œí¬ë¡œ êµ¬í˜„í•¨:

| ì°¸ê³  ëª¨ë¸                   | í•µì‹¬ ì•„ì´ë””ì–´                         |
|-----------------------------|--------------------------------------|
| **QRNN** (White 1992; Taylor 2000; Cannon 2011)      | í‘œì¤€ quantile regression NN êµ¬ì¡°         |
| **Monotone MLP** (Zhang & Zhang 1999; Lang 2005; Minin et al. 2010) | ë‹¨ì¡°ì„± ì œì•½ ì¶”ê°€                       |
| **Composite QRNN (CQRNN)** (Xu et al. 2017)         | ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ ë™ì‹œ ì¶”ì •                    |
| **Expectile Regression NN** (Jiang et al. 2017)     | expectile íšŒê·€ í™•ì¥                     |
| **Generalized Additive NN** (Potts 1999)            | ê°€ì‚°ì„± ë“± ì¶”ê°€ ì œì•½ ì ìš©                |

MCQRNNì€ ë³¸ ë…¼ë¬¸ ê¸°ì¤€ ìµœì´ˆë¡œ, ì‹ ê²½ë§ ê¸°ë°˜ì—ì„œ **ë¹„êµì°¨(non-crossing)** ë¶„ìœ„ìˆ˜ íšŒê·€ë¥¼ ë³´ì¥í•˜ëŠ” ëª¨ë¸ì„.

MCQRNN ëª¨ë¸ ê°œë°œ íë¦„ê³¼ ë…¼ë¬¸ì˜ êµ¬ì„±ì€ ì•„ë˜ì™€ ê°™ë‹¤.

- **Sect. 2**: MMLP â†’ MQRNN â†’ ìµœì¢… MCQRNN ìˆœì„œë¡œ ëª¨ë¸ êµ¬ì¡°ë¥¼ í™•ì¥í•¨.  
  - ë‹¨ì¡°ì„±(monotonicity), ì–‘ìˆ˜ì„±/ë¹„ìŒìˆ˜ì„±(positivity/non-negativity), ê°€ì‚°ì„±(generalized additive) ì œì•½ì„ ì†ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë¨.
  - ì¡°ê±´ë¶€ tau-ë¶„ìœ„ìˆ˜(quantile) ë¶ˆí™•ì‹¤ì„± ì¶”ì • ë°©ë²•ë„ ì œì‹œ.

- **Sect. 3**: Monte Carlo ì‹œë®¬ë ˆì´ì…˜ì„ í†µí•´ MCQRNNê³¼ ê¸°ì¡´ MLP, QRNN, CQRNN ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ê²€ì¦.
  - ë¹„êµ ì‹¤í—˜ì—ëŠ” Xu et al. (2017)ì˜ 3ê°€ì§€ í•¨ìˆ˜ì™€ ì˜¤ì°¨ ë¶„í¬ ì¡°í•©ì„ ì‚¬ìš©.

- **Sect. 4**: ì‹¤ì œ ìºë‚˜ë‹¤ ê¸°í›„ ë°ì´í„°(ì—°ê°„ ìµœëŒ€ ê°•ìˆ˜ëŸ‰)ë¡œ MCQRNN ì ìš© ì‚¬ë¡€ ì œì‹œ.
  - ê´€ì¸¡ì†Œ ë¯¸ì„¤ì¹˜ ì§€ì—­ì˜ IDF ê³¡ì„ (Intensity-Duration-Frequency curve) ì˜ˆì¸¡ ë¬¸ì œì— í™œìš©.
  - IDF ê³¡ì„ ì€ ê·¹í•œ ê°•ìˆ˜(Intensity)ì˜ ë¹ˆë„(Occurrence frequency) ë° ì§€ì†ì‹œê°„(Duration)ê³¼ì˜ ê´€ê³„ë¥¼ ìš”ì•½í•˜ë©°, ê·¹í•œ ê°•ìˆ˜ëŸ‰ì€ ë¹„ìŒìˆ˜ì´ê³ , ë°œìƒí™•ë¥ ì´ ë‚®ê±°ë‚˜(ì¦‰, ì¬í˜„ì£¼ê¸°ê°€ ê¸¸ê±°ë‚˜) ì§€ì†ì‹œê°„ì´ ì§§ì„ìˆ˜ë¡ ê°•ë„ê°€ ë†’ì•„ì§€ëŠ” â€˜ë‹¨ì¡° ì¦ê°€â€™ ì„±ì§ˆì„ ë‚˜íƒ€ëƒ„.
  - ë”°ë¼ì„œ ë‹¨ì¡°ì„±, ë¹„ìŒìˆ˜ì„± ì œì•½ì´ íŠ¹íˆ ì¤‘ìš”.
  - MCQRNN ê¸°ë°˜ IDF ê³¡ì„ ì€ ê° ë¦¬í„´í”¼ë¦¬ì–´ë“œ/ì§€ì†ì‹œê°„ë³„ QRNNì„ ë³„ë„ë¡œ ì í•©ì‹œí‚¤ëŠ” ê¸°ì¡´ ë°©ì‹(Ouali & Cannon, 2018)ê³¼ ë¹„êµ ë¶„ì„.

- **Sect. 5**: ê²°ë¡  ë° í–¥í›„ ì—°êµ¬ ë°©í–¥ ì œì‹œ.


---

## Method
- ë„ì‹(fig)ì„ ë”°ë¼ê°€ë©´ì„œ ìˆ˜ì‹ì€ ê±´ë„ˆë›°ë˜,
  - ê° blockì´ í•˜ëŠ” ì—­í• ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ìš”ì•½: (ì˜ˆ: Encoder â†’ user/item embedding, Decoder â†’ CTR prediction).
  - â€œê¸°ì¡´ ëŒ€ë¹„ ë°”ë€ ì â€ì„ ë§ˆí‚¹í•´. (attention ì¶”ê°€, loss ë³€ê²½ ë“±)

### Modeling framework

#### Monotone multi-layer perceptron (MMLP)

MMLPëŠ” ì¼ë¶€ ì…ë ¥ ë³€ìˆ˜ì— ëŒ€í•´ ë‹¨ì¡°ì„±(monotonicity)ì„ ë³´ì¥í•˜ëŠ” neural networkë‹¤. ì…ë ¥ ë³€ìˆ˜ë¥¼ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆˆë‹¤:
- $M$: ë‹¨ì¡°ì„±ì„ ì›í•˜ëŠ” ë³€ìˆ˜ë“¤ (monotone variables)
- $I$: ë‹¨ì¡°ì„± ì œì•½ì´ ì—†ëŠ” ë³€ìˆ˜ë“¤ (ignore variables)

**Hidden layer:**

$$
h_j(t) = f\left( \sum_{m \in M} x_m(t) \exp(W_{mj}^{(h)}) + \sum_{i \in I} x_i(t) W_{ij}^{(h)} + b_j^{(h)} \right )
$$

ì—¬ê¸°ì„œ:
- $h_j(t)$: $j$ë²ˆì§¸ hidden unitì˜ ì¶œë ¥
- $f$: ë¹„ì„ í˜• activation function (ì˜ˆ: sigmoid, tanh)
- $\exp(W_{mj}^{(h)})$: ë‹¨ì¡° ë³€ìˆ˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ëŠ” exponentialì„ ì·¨í•´ í•­ìƒ ì–‘ìˆ˜ë¥¼ ë³´ì¥
- $W_{ij}^{(h)}$: ì¼ë°˜ ë³€ìˆ˜ì— ëŒ€í•œ ê°€ì¤‘ì¹˜ (ì œì•½ ì—†ìŒ)
- $b_j^{(h)}$: bias term

**Output layer:**

$$
g(t) = \sum_{j=1}^{J} h_j(t) \exp(W_j^{(o)}) + b^{(o)}
$$

ì—¬ê¸°ì„œ:
- $g(t)$: ìµœì¢… ì¶œë ¥
- $\exp(W_j^{(o)})$: hidden layerì—ì„œ outputìœ¼ë¡œ ê°€ëŠ” ê°€ì¤‘ì¹˜ë„ exponentialì„ ì·¨í•´ ì–‘ìˆ˜ ë³´ì¥
- $b^{(o)}$: output bias
- $J$: hidden unitì˜ ê°œìˆ˜

í•µì‹¬ì€ ë‹¨ì¡°ì„±ì„ ì›í•˜ëŠ” ê²½ë¡œì˜ ëª¨ë“  ê°€ì¤‘ì¹˜ì— $\exp(\cdot)$ë¥¼ ì ìš©í•˜ì—¬ ì–‘ìˆ˜ë¡œ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë‹¨ì¡° ë³€ìˆ˜ $x_m$ì´ ì¦ê°€í•  ë•Œ ì¶œë ¥ $g(t)$ë„ ë°˜ë“œì‹œ ì¦ê°€í•˜ê²Œ ëœë‹¤ (activation function $f$ê°€ ë¹„ê°ì†Œ í•¨ìˆ˜ì¼ ë•Œ).

#### Monotone quantile regression neural network (MQRNN)

MQRNNì€ MMLPì˜ êµ¬ì¡°ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ë˜, loss functionì„ quantile regressionìš© pinball lossë¡œ ë³€ê²½í•œ ëª¨ë¸ì´ë‹¤.

**Pinball loss (Check loss):**

$$
\rho_\tau(u) = u(\tau - \mathbb{I}_{u < 0})
$$

ì—¬ê¸°ì„œ:
- $u = y - g(t)$: ì˜ˆì¸¡ ì˜¤ì°¨ (ì‹¤ì œê°’ - ì˜ˆì¸¡ê°’)
- $\tau \in (0, 1)$: ëª©í‘œ ë¶„ìœ„ìˆ˜ (ì˜ˆ: 0.5ëŠ” ì¤‘ì•™ê°’)
- $\mathbb{I}_{u < 0}$: indicator function (u < 0ì´ë©´ 1, ì•„ë‹ˆë©´ 0)

Pinball lossëŠ” ë¹„ëŒ€ì¹­ì (asymmetric)ì¸ ì†ì‹¤ í•¨ìˆ˜ë¡œ:
- $u \geq 0$ (ê³¼ì†Œì¶”ì •)ì¼ ë•Œ: $\tau \cdot u$
- $u < 0$ (ê³¼ëŒ€ì¶”ì •)ì¼ ë•Œ: $(\tau - 1) \cdot u = -(1-\tau) \cdot u$

ì˜ˆë¥¼ ë“¤ì–´ $\tau = 0.9$ì¸ ê²½ìš°:
- ê³¼ì†Œì¶”ì • ì‹œ í˜ë„í‹°: $0.9 \times u$ (í° í˜ë„í‹°)
- ê³¼ëŒ€ì¶”ì • ì‹œ í˜ë„í‹°: $0.1 \times |u|$ (ì‘ì€ í˜ë„í‹°)

ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì´ $\tau$-ë¶„ìœ„ìˆ˜ë¥¼ í•™ìŠµí•˜ê²Œ ëœë‹¤.

**Training objective:**

$$
\min_{W, b} \sum_{t=1}^{T} \rho_\tau(y(t) - g(t))
$$

MQRNNì€ MMLPì˜ ë‹¨ì¡°ì„± ì œì•½ì„ ìœ ì§€í•˜ë©´ì„œ íŠ¹ì • ë¶„ìœ„ìˆ˜ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ì—¬ëŸ¬ ë¶„ìœ„ìˆ˜ë¥¼ ë™ì‹œì— ì¶”ì •í•˜ì§€ëŠ” ëª»í•˜ë©°, ê° $\tau$ë§ˆë‹¤ ë³„ë„ë¡œ í•™ìŠµí•´ì•¼ í•œë‹¤. ì´ ê²½ìš° quantile crossingì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.

**Huber-norm approximation:**

Pinball lossì˜ ë¬¸ì œì ì€ ì›ì ($u = 0$)ì—ì„œ ë¯¸ë¶„ ë¶ˆê°€ëŠ¥(non-differentiable)í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. Gradient-based optimizationì„ ìœ„í•´ì„œëŠ” smooth approximationì´ í•„ìš”í•˜ë‹¤.

Chen (2007)ê³¼ Cannon (2011)ì„ ë”°ë¼, Huber-norm ë²„ì „ìœ¼ë¡œ pinball lossë¥¼ ê·¼ì‚¬í•œë‹¤:

$$
\rho_\tau^{(A)}(e) = 
\begin{cases}
\tau \cdot u(e) & \text{if } e \geq 0 \\
(\tau - 1) \cdot u(e) & \text{if } e < 0
\end{cases}
$$

ì—¬ê¸°ì„œ Huber functionì€:

$$
u(e) = 
\begin{cases}
\frac{e^2}{2\alpha} & \text{if } 0 \leq |e| \leq \alpha \\
|e| - \frac{\alpha}{2} & \text{if } |e| > \alpha
\end{cases}
$$

Huber functionì˜ íŠ¹ì§•:
- $|e| \leq \alpha$ êµ¬ê°„: squared error ($e^2/2\alpha$) ì‚¬ìš© â†’ ì›ì ì—ì„œ ë¯¸ë¶„ ê°€ëŠ¥
- $|e| > \alpha$ êµ¬ê°„: absolute error ($|e| - \alpha/2$) ì‚¬ìš© â†’ quantile regressionì˜ íŠ¹ì„± ìœ ì§€
- $\alpha \to 0$ì¼ ë•Œ: ì •í™•í•œ pinball lossë¡œ ìˆ˜ë ´
- $\alpha$ëŠ” hyperparameterë¡œ, smoothnessì™€ ì •í™•ë„ ì‚¬ì´ì˜ trade-offë¥¼ ì¡°ì ˆ

ì´ approximationì„ ì‚¬ìš©í•˜ë©´ gradient descentë¡œ ì•ˆì •ì ìœ¼ë¡œ í•™ìŠµí•  ìˆ˜ ìˆë‹¤.

<figure>
  <img src="./images/pinball_vs_hubered.png" alt="Pinball loss vs Huber-approximated pinball loss ê·¸ë˜í”„">
  <figcaption>
    Pinball lossì™€ Huber-norm approximationì˜ ë¹„êµ. Huber-normì€ $\alpha$ êµ¬ê°„ì—ì„œ smoothí•˜ê²Œ ì—°ê²°ë˜ì–´ gradient descentì— ì í•©í•˜ë‹¤. $\alpha \to 0$ì¼ ë•Œì—ëŠ” ë‘ í•¨ìˆ˜ê°€ ì¼ì¹˜í•œë‹¤.
  </figcaption>
</figure>

ì‹¤ì œë¡œ ê·¸ë ¤ë´¤ì„ ë•Œ ìœ„ì™€ ê°™ì€ í˜•íƒœê°€ ë‚˜ì˜´. errorê°€ ì‘ì€ ë¶€ë¶„ì—ì„œ ê³¡ì„ ì˜ í˜•íƒœë¥¼ ë³´ì„.


ë˜í•œ, ê³¼ë„í•œ ë¹„ì„ í˜• ëª¨ë¸ë§ì„ ë°©ì§€í•˜ê¸° ìœ„í•´ **regularization term**(ì •ê·œí™” í•­)ì„ ì—ëŸ¬ í•¨ìˆ˜ì— ì¶”ê°€í•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í†µí•´ íŒŒë¼ë¯¸í„° í¬ê¸°ë¥¼ ì œí•œí•  ìˆ˜ ìˆë‹¤.

ìµœì¢…ì ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì—ëŸ¬ í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤:

$$
E_s^{(A)} = E_s^{(A)} + \kappa^{(h)} \frac{1}{VJ} \sum_{i=1}^V \sum_{j=1}^J \left(W_{ij}^{(h)}\right)^2
+ \kappa \frac{1}{J} \sum_{j=1}^J w_j^2
\tag{10}
$$

- $W^{(h)}_{ij}$: ì€ë‹‰ì¸µ weight
- $w_j$: output layer weight
- $\kappa^{(h)} \geq 0$, $\kappa \geq 0$: ê°ê° $W^{(h)}$, $w$ì— ì ìš©ë˜ëŠ” ì •ê·œí™” ê³„ìˆ˜(hyperparameter)
- $V$, $J$: ê°ê° ì€ë‹‰ì¸µ ì…ë ¥, ë…¸ë“œ ê°œìˆ˜

ì´ë•Œ, **ì •ê·œí™” ê³„ìˆ˜**($\kappa^{(h)}$, $\kappa$)ì™€ ë…¸ë“œ ê°œìˆ˜($J$) ë“±ì€ ì¼ë°˜ì ìœ¼ë¡œ cross-validation ë˜ëŠ” Akaike information criterion(QAIC) ë“± ì •ë³´ ê¸°ì¤€ì„ ìµœì†Œí™”í•˜ë„ë¡ ì„ íƒí•œë‹¤.

QAICëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ëœë‹¤:

$$
\text{QAIC} = 2 \log(\hat{L}) + E_s^{(A)} + 2p
\tag{11}
$$

- $\hat{L}$: ëª¨ë¸ì˜ likelihood
- $p$: ëª¨ë¸ì˜ íš¨ê³¼ì ì¸ íŒŒë¼ë¯¸í„° ê°œìˆ˜ ì¶”ì •ì¹˜

ì¦‰, ì •ê·œí™” ê³„ìˆ˜ ë° ëª¨ë¸ ë³µì¡ë„ëŠ” out-of-sample ì„±ëŠ¥(ì¼ë°˜í™” ì˜¤ì°¨)ì„ ê¸°ì¤€ìœ¼ë¡œ íŠœë‹í•œë‹¤.


#### Monotone composite quantile regression neural network (MCQRNN)

composite QRNN í˜•íƒœ.
ë™ì‹œì— ì—¬ëŸ¬ quantileì„ í•™ìŠµí•˜ë©´ì„œ non-crossing quantile conditionì„ ì¶”ê°€í•˜ëŠ” ê²ƒ.

> CQRNN shares the same goal as the linear composite quantile regression (CQR) model (Zou and Yuan 2008), namely to borrow strength across multiple regression quantiles to improve the estimate of the true, unknown relationship between covariates and the response.

> This is especially valuable in situations where the error follows a heavy-tailed distribution.

ê³µí†µëœ ê°€ì¤‘ì¹˜ë¥¼ ì‚¬ìš©í•˜ë©´ì„œ ê³µí†µëœ ì§€ì‹ì„ í•™ìŠµí•˜ê²Œ ë˜ê³ , ì´ë¥¼ í†µí•´ ë” ê°•ê±´í•œ ëª¨ë¸ì„ ë§Œë“¤ ìˆ˜ ìˆìŒ.

> Hence, the models are not explicitly trying to describe the full conditional response distribution, but rather a single tau-independent function that best describes the true covariate-response relationship.

MTL (multi-task learning)ê³¼ ê°™ì€ ì»¨ì…‰.

ì´ë•Œ quantile regression error í•¨ìˆ˜ëŠ” $K$ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ quantile(ë³´í†µ $[0,1]$ êµ¬ê°„ì— ê· ë“± ê°„ê²©ìœ¼ë¡œ ë°°ì¹˜)ì— ëŒ€í•´ í•©ì‚°í•˜ì—¬ ì‚¬ìš©í•œë‹¤.

$$
E^{(A)}_C = \frac{1}{KN} \sum_{k=1}^K \sum_{t=1}^N q^{(A)}_{s_k} \left(y^{(t)} - \hat{y}^{(t)}_{s_k}\right)
\tag{12}
$$

ì—¬ê¸°ì„œ $s_k = \frac{k}{K+1}$ $(k=1,2,...,K)$ë¡œ ì˜ˆë¥¼ ë“¤ ìˆ˜ ìˆë‹¤. íŒ¨ë„í‹° í•­(ì •ê·œí™” í•­, Eq. 10 ì°¸ì¡°)ì€ ë™ì¼í•˜ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤.

- MCQRNN ëª¨ë¸ì€ ì•„ë˜ ë‘ ìš”ì†Œë¥¼ ê²°í•©:
  - MQRNN ë„¤íŠ¸ì›Œí¬ êµ¬ì¡°(Eq. 5)
  - ë³µí•©(composite) quantile regression ì—ëŸ¬ í•¨ìˆ˜(Eq. 12)
  â†’ ì—¬ëŸ¬ quantileì— ëŒ€í•´ non-crossing íšŒê·€ quantileì„ ë™ì‹œì— ì¶”ì •

> tauë¥¼ í”¼ì²˜ë¡œ ì¶”ê°€í•´ì„œ ì‚¬ìš© -> monotone covariate
> X(tau) = [tau, x1, x2, ..., XI]

- ë°ì´í„° ë³€í™˜ ê³¼ì •:
  - ì…ë ¥: N x #I í¬ê¸°ì˜ covariates í–‰ë ¬ X, ê¸¸ì´ Nì˜ response ë²¡í„° y
  - ëª©í‘œ: $\tau_1$, $\tau_2$, ..., $\tau_K$ ì— ëŒ€í•œ non-crossing quantile í•¨ìˆ˜ ì¶”ì •

  - ì•„ë˜ì™€ ê°™ì´ "stacked" ë°ì´í„° ì…‹ êµ¬ì„±:
    1. ê° quantile ì§€ì ($\tau_1$ ~ $\tau_K$)ì„ Në²ˆì”© ë°˜ë³µí•˜ì—¬ S = K x N ê¸¸ì´ì˜ monotone covariate ë²¡í„° xâ‚˜^(S)ë¥¼ ë§Œë“¦
    2. Xë¥¼ Kë²ˆ ë³µì œí•œ ë‹¤ìŒ xâ‚˜^(S)ì™€ í•©ì³ì„œ S x (1 + #I) í¬ê¸°ì˜ covariate matrix X^(S)ë¥¼ ë§Œë“¦
    3. yë¥¼ Kë²ˆ ë°˜ë³µí•˜ì—¬ ê¸¸ì´ Sì˜ y^(S)ë¥¼ ë§Œë“¦

  - ìœ„ 1~3ì„ í†µí•´ stacked dataset êµ¬ì„±

> By treating the $\tau$ values as a monotone covariate, predictions $\hat y_{\tau}^{S}$ from Eq. 5 for fixed values of the non-monotone covariates are guaranteed to increase with s. 
> Non-crossing is imposed by construction.

ì œì•½ì¡°ê±´ì„ ì¶”ê°€ë¡œ ì£¼ê¸°ë³´ë‹¤ëŠ” ëª¨ë¸ êµ¬ì¡° ìì²´ê°€ crossing í•˜ì§€ ëª»í•˜ë„ë¡ ë§Œë“œëŠ” ê²ƒ.

- composite quantile regression error function (stacked ë°ì´í„°ì…‹ ê¸°ì¤€):  
  - s(s) = xâ‚^(S)(s)ë¡œ ì •ì˜  
  - ì—ëŸ¬ í•¨ìˆ˜:  
    $$
    E^{(A,S)}_{Cs} = \sum_{s=1}^S w_s(s) \, q^{(A)}_{s(s)}(y^{(S)}_{s(s)} - \hat{y}^{(S)}_{s(s)})
    $$
    - ì—¬ê¸°ì„œ $w_s(s)$ëŠ” quantileë³„ lossì— ëŒ€í•œ ê°€ì¤‘ì¹˜ (Jiang et al. 2012; Sun et al. 2013)
    - $w_s(s)=1/S$ ë¡œ ì¼ì •í•˜ê²Œ ì£¼ë©´ standard composite quantile regression lossì™€ ê°™ì•„ì§

- Eq. 14 (ìœ„ ìˆ˜ì‹) minimizationì„ í†µí•´ MCQRNN ëª¨ë¸ì„ í•™ìŠµ  
- (ì°¸ê³ ) non-crossing expectile regressionë„ $a \ne 0$ì¸ $q^{(A)}_s$ ì ìš©ìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆìŒ  
- ëª¨ë¸ í•™ìŠµ ì´í›„:  
  - monotone covariate(tau)ì— ì›í•˜ëŠ” tauê°’ì„ ë„£ìœ¼ë©´, ì„ì˜ì˜ $\tau_1 \leq \tau \leq \tau_K$ì— ëŒ€í•´ conditional quantile ì˜ˆì¸¡ ê°€ëŠ¥

- ì‹¤í—˜ ì˜ˆì‹œ(Fig.1):  
  - MCQRNN ëª¨ë¸ íŒŒë¼ë¯¸í„°: J=4, $k^{(h)}=0.00001$, $k=0$, $K=9$, $s=0.1,0.2,\ldots,0.9$  
  - Bondell et al. (2010)ì˜ ë‘ í•¨ìˆ˜ì— ëŒ€í•´ synthetic ë°ì´í„°(500ê°œ ìƒ˜í”Œ) ì‚¬ìš©  
    - $y_1 = 0.5 + 2x + \sin(2\pi x - 0.5) + e$  
    - $y_2 = 3x + [0.5 + 2x + \sin(2\pi x - 0.5)]e$  
    - $x$ ~ $U(0,1)$, $e$ ~ $N(0,1)$
  - ëª¨ë“  sì— ëŒ€í•´ ê°€ì¤‘ì¹˜ ë™ì¼í•˜ê²Œ ì ìš© ($w_s(s)$ëŠ” ìƒìˆ˜)
  - ë¹„êµ: ê° s-quantile ë³„ë¡œ ë…ë¦½ì ìœ¼ë¡œ QRNN í•™ìŠµ(J=4, $k^{(h)}=0.00001$)
    - QRNN: quantile curveë“¤ì´ training data ê²½ê³„ì—ì„œ crossing ë°œìƒ
    - MCQRNN: quantile crossing ì—†ì´, true conditional quantileì— ë” ê·¼ì ‘í•˜ê²Œ ì—¬ëŸ¬ ê°œì˜ non-crossing quantile í•¨ìˆ˜ ì¶”ì • ê°€ëŠ¥
      - QRNNì—ì„œë„ weight penalty(Cannon 2011)ë¡œ crossingì„ ì™„í™”ì‹œí‚¤ëŠ” ê²ƒì€ ê°€ëŠ¥í•˜ì§€ë§Œ, ì™„ì „íˆ ë³´ì¥í•  ìˆ˜ ì—†ìŒ
      - MCQRNNì€ êµ¬ì¡° ìì²´ë¡œ crossing ë°©ì§€


#### Aditional constraints and uncertainty estimates

> As mentioned above, constraints in addition to non-crossing of quantile functions may be useful for some MCQRNN modelling tasks.

> A form of the parametric bootstrap can be used to estimate uncertainty in the conditional tau-quantile functions.

parametric bootstrapì„ í†µí•´ quantile regressionì˜ ë¶ˆí™•ì‹¤ì„±ì„ ì¶”ì •.

- ë¶ˆí™•ì‹¤ì„± ì¶”ì •ì„ ìœ„í•´ parametric bootstrap ë°©ë²• ì‚¬ìš©
  - MCQRNNì€ Kê°œì˜ s(quantile) ê°’ì— ëŒ€í•´ ëª…ì‹œì ìœ¼ë¡œ í•™ìŠµí•˜ì§€ë§Œ, monotone covariateë¡œ tau(s)ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ì„ì˜ì˜ êµ¬ê°„ $s_1 \leq s \leq s_K$ì— ëŒ€í•´ ë³´ê°„ ê°€ëŠ¥
  - ë¶„í¬ì˜ ê¼¬ë¦¬ ë¶€ë¶„(tail)ì— ëŒ€í•´ parametric formì„ ê°€ì •í•˜ë©´ ë¶„í¬í•¨ìˆ˜, í™•ë¥ ë°€ë„í•¨ìˆ˜, quantile í•¨ìˆ˜ ëª¨ë‘ ìƒì„± ê°€ëŠ¥ (Quinonero Candela et al. 2006; Cannon 2011)
  - parametric bootstrap ì ˆì°¨
    1. ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ì¡°ê±´ë¶€ ë¶„í¬ì—ì„œ ì„ì˜ë¡œ ìƒ˜í”Œ ìƒì„±
    2. MCQRNN ëª¨ë¸ì„ ë‹¤ì‹œ í•™ìŠµ
    3. ì¡°ê±´ë¶€ s-quantileì„ ì¶”ì •
    4. ìœ„ ê³¼ì •ì„ ì—¬ëŸ¬ ë²ˆ ë°˜ë³µ (repeat)
    5. ë°˜ë³µ ê²°ê³¼ë¡œ ì–»ì€ bootstrapped conditional s-quantile ê°’ë“¤ë¡œ ì‹ ë¢°êµ¬ê°„(confidence interval) ì¶”ì •
- positivity, monotonicity ì œì•½ ë° bootstrap ê¸°ë°˜ ì‹ ë¢°êµ¬ê°„ì„ ì ìš©í•œ ì˜ˆì‹œëŠ” Fig. 2(Bondell et al.(2010) í•¨ìˆ˜) ì°¸ê³ 

---

## Experiment
- ì–´ë–¤ ë°ì´í„°ì…‹, ë¹„êµ ëª¨ë¸, ì§€í‘œ, í–¥ìƒ ì •ë„ì¸ê°€
- í‘œ 1ê°œ, ê·¸ë¦¼ 1ê°œë§Œ ì„ íƒí•´ì„œ ìˆ«ì ë©”ëª¨
- ë‚˜ì¤‘ì— â€œì´ ë…¼ë¬¸ì€ ê¸°ì¡´ ëŒ€ë¹„ ~% í–¥ìƒâ€ ì´ëŸ° ì‹ìœ¼ë¡œ ë°”ë¡œ ì¸ìš© ê°€ëŠ¥í•˜ê²Œ

### Monte Carlo simulation

> Given the close relationship between the MCQRNN and CQRNN models, performance is first assessed via Monte Carlo simulation using the experimental setup adopted by Xu et al. (2017) for CQRNN

Xu et al. (2017) CQRNN ì‹¤í—˜ ì¬í˜„í•´ì„œ ëª¨ë¸ í‰ê°€
- ì˜ˆì œ í•¨ìˆ˜ 3ê°œ ì‚¬ìš© (Eq.17â€“19)
  - ì˜ˆì œ1:  $y = \sin(2x_1) + 2e^{-16x_2^2} + 0.5\epsilon$,  $x_1, x_2 \sim U(0,1)$,  $\epsilon \sim N(0,1)$
  - ì˜ˆì œ2:  $y = (1 - x + 2x^2) e^{-0.5x^2} + (1 + 0.2x) 5\epsilon$,  $x \sim U(0,1)$,  $\epsilon \sim N(0,1)$
  - ì˜ˆì œ3:  
    - $y = 40 \exp\left\{-8\left[(x_1-0.5)^2 + (x_2-0.5)^2\right]\right\}$
    - $\quad\,/\, \exp\left\{-8\left[(x_1-0.2)^2 + (x_2-0.7)^2\right]\right\}$
    - $+ \exp\left\{-8\left[(x_1-0.7)^2 + (x_2-0.7)^2\right]\right\} + \epsilon$
    - $x_1, x_2 \sim U(0,1)$, $\epsilon \sim N(0,1)$
- (ë¬¸ì œ) â€” (í•µì‹¬ ì•„ì´ë””ì–´) â€” (ê²°ê³¼)ë¡œ í•œ ì¤„ ìš”ì•½ ì¤€ë¹„

For each example function, random errors (noise) are generated from three different distributions:
- Normal distribution: $e \sim N(0, 0.25)$
- t-distribution with 3 degrees of freedom: $e \sim t(3)$
- Chi-squared distribution with 3 degrees of freedom: $e \sim \chi^2(3)$

> Monte Carlo simulations are performed for the nine resulting datasets.

example functionì—ì„œ ë°˜ë³µì ìœ¼ë¡œ ë°ì´í„° ì¶”ì¶œ -> ëª¨ë¸ í‰ê°€

ì‹¤í—˜ê³¼ì •
- ê° ì˜ˆì œí•¨ìˆ˜(3ê°œ)ì— ëŒ€í•´ 3ì¢…ë¥˜ì˜ ì¡ìŒë¶„í¬(N(0,0.25), t(3), Ï‡Â²(3))ë¥¼ ì ìš©í•´ ì´ 9ê°œ ë°ì´í„°ì…‹ ìƒì„±
- ê° ë°ì´í„°ì…‹ì€ 400 ìƒ˜í”Œ(í›ˆë ¨ 200, í…ŒìŠ¤íŠ¸ 200)ë¡œ êµ¬ì„±, ì‹¤í—˜ 1000íšŒ ë°˜ë³µ
- ë¹„êµëª¨ë¸: QRNN(s=0.5), MLP, CQRNN, CQRNN*(monotonicity ë¯¸ì ìš©), MCQRNN(K=19ê°œ quantile ë™ì‹œì¶”ì •)
- ëª¨ë“  ëª¨ë¸ì˜ ì€ë‹‰ë…¸ë“œìˆ˜: ì˜ˆì œ1ì€ 4ê°œ, ì˜ˆì œ2/3ì€ 5ê°œë¡œ í†µì¼, ì •ê·œí™”/ê°€ì¤‘ì¹˜í˜ë„í‹° ë¯¸ì ìš©
- í…ŒìŠ¤íŠ¸ì…‹ RMSEë¡œ ì„±ëŠ¥ ë¹„êµ, CQRNN*/MCQRNNì€ ì˜ˆì¸¡ quantile í‰ê· ê°’ìœ¼ë¡œ ì ì¶”ì •

ì‹¤í—˜ê²°ê³¼
- eâˆ¼N(0,0.25)ì—ì„œëŠ” MLPê°€ í‰ê· ì ìœ¼ë¡œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥(RMSE) ë³´ì´ë‚˜, ëª¨ë“  ëª¨ë¸ì˜ ì°¨ì´ê°€ 10% ì´ë‚´ë¡œ ë¯¸ë¯¸í•¨
- eâˆ¼t(3), eâˆ¼Ï‡Â²(3) ë“± ë¹„ì •ê·œë¶„í¬ì—ì„œëŠ” CQRNN*/MCQRNNì´ í…ŒìŠ¤íŠ¸ì…‹ì—ì„œ ê°€ì¥ ë‚®ì€ RMSE ê¸°ë¡, íŠ¹íˆ MCQRNNì´ ì˜ˆì œ3ì—ì„œ ê°€ì¥ ê°•ì¸í•¨
- MLPëŠ” ì´ìƒì¹˜ì— ì·¨ì•½(ìµœì•… 5/95í¼ì„¼íƒ€ì¼RMSE ê¸°ì¤€), MCQRNNì€ Stableí•˜ê³  ì „ë°˜ì  ìµœì 
- MCQRNNì˜ non-crossing ì œì•½ì´ ì„±ëŠ¥ì— ì¶”ê°€ì ìœ¼ë¡œ ê¸°ì—¬í•¨(CQRNN* ëŒ€ë¹„ ì˜ˆì œ3ê³¼ ë¹„ì •ê·œë¶„í¬ì—ì„œ ìš°ìœ„)
- ì „ì²´ì ìœ¼ë¡œ MCQRNNì´ ê¸°ì¡´ ì‹ ê²½ë§ ê¸°ë°˜ quantile ì¶”ì •ëª¨ë¸ ëŒ€ë¹„ ìš°ìˆ˜ í˜¹ì€ ë™ë“± ì´ìƒì˜ ì„±ëŠ¥ ë‹¬ì„±


### Rainfall IDF curve

ì´ ë¶€ë¶„ì€ MCQRNN ëª¨ë¸ì„ ì‹¤ì œ ê°•ìš° ë¹ˆë„-ì§€ì†ì‹œê°„-ê°•ë„(IDF, Intensity-Duration-Frequency) ê³¡ì„  ë¬¸ì œì— ì ìš©í•œ ì‚¬ë¡€ ì—°êµ¬ë‹¤.

#### ë°°ê²½: IDF ê³¡ì„ ì´ë€?

IDF(Intensity-Duration-Frequency) curveëŠ” ë¹„ê°€ ì–¼ë§ˆë‚˜ ì„¸ê²Œ(ê°•ë„), ì–¼ë§ˆë‚˜ ì˜¤ë˜(ì§€ì†ì‹œê°„), ì–¼ë§ˆë‚˜ ìì£¼(ë¹ˆë„) ì˜¤ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê³¡ì„ ìœ¼ë¡œ, í™ìˆ˜ ì„¤ê³„(ë°°ìˆ˜, ëŒ, ìˆ˜ë¬¸ ì„¤ê³„)ì— í•„ìˆ˜ì ì¸ ê¸°ì´ˆ ìë£Œë‹¤. ë¹ˆë„(Frequency/Return period)ëŠ” 2ë…„, 5ë…„, 10ë…„, 100ë…„ ë¹ˆë„ë¡œ í‘œí˜„ë˜ë©° ì´ëŠ” ê°ê° $s=0.5, 0.8, 0.9, 0.99$ quantileì— í•´ë‹¹í•œë‹¤. ì§€ì†ì‹œê°„(Duration)ì€ ë³´í†µ 5ë¶„ì—ì„œ 24ì‹œê°„ê¹Œì§€ ë²”ìœ„ë¥¼ ê°€ì§€ë©°, ê°•ë„(Intensity)ëŠ” ì£¼ì–´ì§„ ë¹ˆë„ì™€ ì§€ì†ì‹œê°„ì—ì„œì˜ ê°•ìš° ê°•ë„(mm/hr)ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.

#### ê¸°ì¡´ ECCC(ìºë‚˜ë‹¤ í™˜ê²½ì²­)ì˜ IDF ê³¡ì„  ì‘ì„± ë°©ë²•

ECCCëŠ” ìºë‚˜ë‹¤ ì „ì—­ 565ê°œ ê´€ì¸¡ì†Œì˜ ë°ì´í„°ë¥¼ ì´ìš©í•´ ë‹¤ìŒ ì ˆì°¨ë¡œ ê³µì‹ IDF ê³¡ì„ ì„ ë§Œë“¤ì–´ì™”ë‹¤. ë¨¼ì € ê° ê´€ì¸¡ì†Œì™€ ì§€ì†ì‹œê°„ë³„ë¡œ ì—° ìµœëŒ€ ê°•ìš°ëŸ‰(annual max rainfall)ì„ êµ¬í•˜ê³ , ê·¸ ë°ì´í„°ì— Gumbel ë¶„í¬(ê·¹ê°’ ë¶„í¬)ë¥¼ í”¼íŒ…í•œë‹¤. ê·¸ ë‹¤ìŒ ê° ì¬í˜„ê¸°ê°„(2ë…„, 5ë…„, ..., 100ë…„)ì˜ ê°•ìš°ê°•ë„ë¥¼ ê³„ì‚°í•˜ê³ , log-log ì„ í˜• ë³´ê°„ì‹(log(duration) vs log(intensity))ìœ¼ë¡œ IDF ê³¡ì„ ì„ ê·¸ë¦°ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ê° ì§€ì ë§ˆë‹¤ 30ê°œì˜ íŒŒë¼ë¯¸í„°(ë¶„í¬+ë³´ê°„ì‹)ë¡œ IDF ê³¡ì„ ì„ ë§Œë“ ë‹¤.

ì´ ë°©ë²•ì˜ ë¬¸ì œì ì€ ê´€ì¸¡ì†Œê°€ ì—†ëŠ” ì§€ì—­(ungauged location)ì—ëŠ” IDF ê³¡ì„ ì„ ë§Œë“¤ ìˆ˜ ì—†ë‹¤ëŠ” ì ê³¼, íŒŒë¼ë©”íŠ¸ë¦­ ë¶„í¬(Gumbel ê°€ì •)ê°€ í•­ìƒ ì‹¤ì œ ë¶„í¬ë¥¼ ì˜ ì„¤ëª…í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì´ë‹¤.

#### MCQRNNì„ ì´ìš©í•œ ìƒˆë¡œìš´ ì ‘ê·¼

ë…¼ë¬¸ì€ MCQRNNì„ ì´ìš©í•´ ë¹„ê´€ì¸¡ ì§€ì—­ì—ì„œë„ IDF ê³¡ì„ ì„ ì˜ˆì¸¡í•˜ê³ , monotonic(ë¹„êµì°¨/non-crossing) ì œì•½ì„ ë³´ì¥í•˜ëŠ” ë°©ë²•ì„ ì œì•ˆí•œë‹¤.

**ëª¨ë¸ ì…ë ¥ê³¼ ì¶œë ¥:**

ì…ë ¥(covariates)ìœ¼ë¡œëŠ” ìœ„ë„(lat), ê²½ë„(lon), ê³ ë„(elev), ê²¨ìš¸ ê°•ìˆ˜ëŸ‰(DJF), ì—¬ë¦„ ê°•ìˆ˜ëŸ‰(JJA) ë“± 5ê°œì˜ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ë©°, ë‹¨ì¡°í˜•(monotone) covariatesë¡œëŠ” quantile level(s, ì¦‰ ì¬í˜„ê¸°ê°„)ê³¼ log(Duration, D)ë¥¼ ì‚¬ìš©í•œë‹¤. ì¶œë ¥(response)ì€ ë‹¨ê¸° ê°•ìš° ê°•ë„(rainfall rate)ë‹¤.

ëª¨ë¸ êµ¬ì¡°ëŠ” ì£¼ë³€ 80ê°œ ê´€ì¸¡ì†Œ ë°ì´í„°ë¥¼ ëª¨ì•„ ì§€ì—­ ë°ì´í„° í’€(pool)ì„ í˜•ì„±í•˜ëŠ” regionalization ë°©ì‹ì„ ì‚¬ìš©í•˜ë©°, ê° ê´€ì¸¡ì†Œë¥¼ í•œ ë²ˆì”© "ë¹„ê´€ì¸¡(ungauged)" ìƒíƒœë¡œ ë‘ê³  leave-one-out ê²€ì¦ì„ ìˆ˜í–‰í•œë‹¤.

#### ëª¨ë¸ í‰ê°€ ë°©ë²•

Leave-one-out cross-validation(LOO-CV) ë°©ì‹ì„ ì‚¬ìš©í•˜ì—¬, 565ê°œ ê´€ì¸¡ì†Œ ê°ê°ì„ "ë¹„ê´€ì¸¡ì†Œ"ë¡œ ê°€ì •í•˜ê³  ì£¼ë³€ 80ê°œ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•œ í›„ í•´ë‹¹ ê´€ì¸¡ì†Œì—ì„œ ì˜ˆì¸¡í•˜ì—¬ ì‹¤ì œê°’ê³¼ ë¹„êµí•œë‹¤. ë¹„êµ ëŒ€ìƒì€ ê¸°ì¡´ QRNN(Quantile Regression Neural Network)ê³¼ ì œì•ˆëœ MCQRNNì´ë‹¤.

ëª¨ë¸ ë³µì¡ë„ëŠ” hidden node ìˆ˜(J)ë¡œ ì œì–´í•˜ë©°, QAIC(Akaike Information Criterion ê¸°ë°˜)ìœ¼ë¡œ ìµœì  Jë¥¼ ì„ íƒí•œë‹¤. ê²°ê³¼ì ìœ¼ë¡œ QRNNì€ ìµœì  $J=1$, MCQRNNì€ ìµœì  $J=3$ì„ ë³´ì˜€ë‹¤.

#### ì‹¤í—˜ ê²°ê³¼

**ì„±ëŠ¥ ë¹„êµ:**

ë‘ ëª¨ë¸ì˜ í‰ê·  ì˜¤ì°¨(quantile regression error)ëŠ” 5% ì´ë‚´ë¡œ ë¹„ìŠ·í•˜ì§€ë§Œ, MCQRNNì´ ë‹¨ê¸°(5ë¶„~2ì‹œê°„) ê°•ìš°ì—ì„œëŠ” ì¡°ê¸ˆ ë” ìš°ìˆ˜í•˜ê³  QRNNì€ ì¥ê¸°(6~24ì‹œê°„)ì—ì„œ ì•½ê°„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤. í•˜ì§€ë§Œ MCQRNNì— ê°€ì¤‘ì¹˜($w_s(s) \propto \log(D)$)ë¥¼ ì£¼ë©´ ì¥ê¸°ì—ì„œë„ ê°œì„ ëœë‹¤.

**êµ¬ì¡°ì  ì¥ì :**

MCQRNNì€ non-crossing, monotonic(ë¹ˆë„â†‘ â†’ ê°•ë„â†‘, ì§€ì†ì‹œê°„â†‘ â†’ ê°•ë„â†“)ì„ êµ¬ì¡°ì ìœ¼ë¡œ ë³´ì¥í•˜ì§€ë§Œ, QRNNì€ êµì°¨(crossing)ë‚˜ ë¹„ë‹¨ì¡°ì  íŒ¨í„´ì´ ë‚˜íƒ€ë‚  ìˆ˜ ìˆë‹¤. ì‹¤ì œë¡œ Fig.7ì—ì„œ QRNNì˜ 100ë…„ë¹ˆë„ ê³¡ì„ ì´ ë’¤ì„ì´ëŠ” í˜„ìƒì´ ê´€ì°°ë˜ì—ˆë‹¤.

**ëª¨ë¸ íš¨ìœ¨ì„±:**

| ëª¨ë¸     | êµ¬ì¡°                                       | ì´ íŒŒë¼ë¯¸í„° ìˆ˜     |
| ------ | ---------------------------------------- | ------------ |
| QRNN   | 54ê°œ ë³„ë„ ëª¨ë¸ (6 quantiles Ã— 9 durations) | 432ê°œ         |
| MCQRNN | ëª¨ë“  s, D í†µí•© í•™ìŠµ                           | 28ê°œ (J=3 ê¸°ì¤€) |

MCQRNNì€ ì•½ 15ë°° ì´ìƒ ë‹¨ìˆœí™”ë˜ë©´ì„œë„ ê±°ì˜ ê°™ì€ ì„±ëŠ¥ì„ ë³´ì—¬, QRNNì˜ íŒŒë¼ë¯¸í„° ìƒë‹¹ìˆ˜ê°€ ì¤‘ë³µ(redundant)ì„ì„ ë³´ì—¬ì¤€ë‹¤.

**ê³¼ì í•© ë°©ì§€ íš¨ê³¼:**

QRNNì€ hidden node ìˆ˜(J)ê°€ ì»¤ì§€ë©´ overfittingì´ ì‹¬í™”ë˜ì§€ë§Œ, MCQRNNì€ monotonic constraint ë•ë¶„ì— Jê°€ ì»¤ì ¸ë„ ì„±ëŠ¥ ì €í•˜ê°€ ê±°ì˜ ì—†ë‹¤. ì¦‰, ë¹„êµì°¨ ì œì•½ ìì²´ê°€ regularization ì—­í• ì„ ìˆ˜í–‰í•œë‹¤.

**ì‹¤ì œ ECCC ê³¡ì„ ê³¼ ë¹„êµ:**

$R_s$ = (ECCC ê³¡ì„ ì˜ in-sample ì˜¤ì°¨) / (MCQRNNì˜ ì˜ˆì¸¡ ì˜¤ì°¨)ë¡œ ì •ì˜í•  ë•Œ, $R_s = 1$ì´ë©´ ë¹„ê´€ì¸¡ MCQRNN ì˜ˆì¸¡ì´ ì‹¤ì œ ê³¡ì„ ê³¼ ë™ì¼í•œ ìˆ˜ì¤€ì´ë©°, $R_s \geq 0.9$ì´ë©´ ë§¤ìš° ì–‘í˜¸í•œ ìˆ˜ì¤€ì´ë‹¤. 54ê°œì˜ (duration, quantile) ì¡°í•© ì¤‘ 41ê°œ(ì•½ 76%)ê°€ $R_s > 0.9$ë¥¼ ê¸°ë¡í–ˆìœ¼ë©°, ë‚˜ë¨¸ì§€ ì „ë¶€ $R_s > 0.7$ë¡œ ì „ë°˜ì ìœ¼ë¡œ ë§¤ìš° ë†’ì€ ì¬í˜„ì„±ì„ ë³´ì˜€ë‹¤.

**ê´€ì¸¡ì†Œ ë°€ë„ ì˜í–¥:**

ê´€ì¸¡ì†Œ ê°„ê²©ì´ ì¢ì„ìˆ˜ë¡ ì„±ëŠ¥ì´ ìš°ìˆ˜í•˜ë©°($R_s \approx 1$), ê±°ë¦¬ 500km ì´ìƒ ë–¨ì–´ì§„ ì§€ì—­ë¶€í„° ì˜¤ì°¨ê°€ ì¦ê°€í•œë‹¤. ë”°ë¼ì„œ ë°ì´í„°ê°€ í¬ì†Œí•œ ë¶ë¶€ ì§€ì—­ì—ì„œëŠ” ì¶”ì • ì‹ ë¢°ë„ê°€ ë‚®ë‹¤.

#### ì „ì²´ ìš”ì•½

| êµ¬ë¶„        | ë‚´ìš©                                                                    |
| --------- | --------------------------------------------------------------------- |
| **ëª©ì **    | ë¹„ê´€ì¸¡ ì§€ì—­ì—ì„œ ê°•ìš° IDF ê³¡ì„ ì„ ì¶”ì •í•˜ëŠ” ê°•ê±´í•œ ë°©ë²• ì œì•ˆ                                    |
| **ëª¨ë¸**    | MCQRNN (monotone composite quantile regression neural net)            |
| **ì…ë ¥ ë³€ìˆ˜** | ìœ„ë„, ê²½ë„, ê³ ë„, ê³„ì ˆë³„ ê°•ìˆ˜ëŸ‰, quantile level s, log(duration)                  |
| **ë¹„êµ ëª¨ë¸** | QRNN (separate quantileë³„ ì‹ ê²½ë§)                                         |
| **ê²€ì¦ ë°©ë²•** | Leave-one-out cross-validation (565ê°œ ê´€ì¸¡ì†Œ)                             |
| **ì„±ëŠ¥**    | í‰ê·  ì˜¤ì°¨ Â±5% ì´ë‚´, $R_s > 0.9$ (41/54 ì¡°í•©)                                 |
| **ì¥ì **    | - non-crossing/monotonic ë³´ì¥<br>- íŒŒë¼ë¯¸í„° ìˆ˜ ëŒ€í­ ê°ì†Œ<br>- overfitting ì €í•­ì„± ê°•í•¨ |
| **í•œê³„**    | ê´€ì¸¡ì†Œ ë°€ë„ ë‚®ì€ ì§€ì—­ì—ì„œëŠ” ì •í™•ë„ í•˜ë½                                                |

**í•œ ë¬¸ì¥ ìš”ì•½:**

MCQRNNì€ ìºë‚˜ë‹¤ ì „ì—­ì˜ ê°•ìš° IDF ê³¡ì„ ì„ í•™ìŠµí•˜ë©´ì„œ, ê¸°ì¡´ QRNNë³´ë‹¤ í›¨ì”¬ ë‹¨ìˆœí•˜ê³  ì•ˆì •ì ì¸ êµ¬ì¡°ë¡œ ë¹„êµì°¨Â·ë‹¨ì¡° ì œì•½ì„ ë§Œì¡±í•˜ë©´ì„œë„ ìœ ì‚¬í•œ ì„±ëŠ¥ì„ ë³´ì˜€ê³ , ë¹„ê´€ì¸¡ ì§€ì—­ì—ì„œë„ ë†’ì€ ì •í™•ë„ì˜ IDF ì¶”ì •ì´ ê°€ëŠ¥í•˜ë‹¤ëŠ” ê²ƒì„ Monte Carlo-Cross-validation ì‹¤í—˜ìœ¼ë¡œ ê²€ì¦í–ˆë‹¤.


---


# 3ë‹¨ê³„. â€˜ë…¼ë¦¬ì  ë¦¬ë”©â€™ â€” ì •ë§ ì“¸ ë…¼ë¬¸ë§Œ (1~2ì‹œê°„)
> ğŸ“ëª©í‘œ: ì´ë¡ ì  ê·¼ê±°ì™€ ì¬í˜„ ê°€ëŠ¥ì„±ì„ ì™„ì „íˆ ì´í•´

---

## ìˆ˜ì‹/ê°€ì • ì •ë¦¬
- ì´ Lossê°€ ì‹¤ì œë¡œ convexì¸ì§€, regularizerì˜ ì˜ë¯¸, gradient ê³„ì‚° êµ¬ì¡° ë“± â€˜ì´ë¡ ì  ì •ë‹¹ì„±â€™ í™•ì¸.

---

## Discussion / Ablation / Limitation
- ì—°êµ¬ìë“¤ì´ ì¸ì •í•œ í•œê³„ì™€ future workëŠ” ë„¤ê°€ í›„ì† í”„ë¡œì íŠ¸ ì•„ì´ë””ì–´ë¡œ ì¨ë¨¹ì„ í¬ì¸íŠ¸.

---

## Reference Jump
- ì¸ìš©ëœ í•µì‹¬ 2~3ê°œ ë…¼ë¬¸ì„ ë°”ë¡œ ì²´í¬í•´ â€œê³„ë³´â€ íŒŒì•….
- ì´ê²Œ â€˜ë¦¬ì„œì¹˜ íŠ¸ë¦¬(tree)â€™ë¥¼ í˜•ì„±í•¨.