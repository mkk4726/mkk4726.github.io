---
title: "MSE Lossë¥¼ ë” ê¹Šê²Œ ì´í•´í•´ë³´ê¸°"
date: "2025-11-28"
excerpt: "ì™œ ê¸°ë³¸ì ìœ¼ë¡œ MSE lossë¥¼ ì‚¬ìš©í• ê¹Œ?"
category: "Data Science"
tags: ["ê¸°ë³¸ê¸°", "MSE", "MAE", "huber"]
---

regression modelì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ mse lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì™œ ê·¸ëŸ°ê±¸ê¹Œ?
ë‚˜ ê¸°ë³¸ê¸° ì§„ì§œ ë¶€ì¡±í•˜êµ¬ë‚˜...ã…ã…

ê°œë… í•œë²ˆ ì •ë¦¬í•´ë³´ì.

## ë“¤ì–´ê°€ë©°

Regression ëª¨ë¸ì—ì„œ ê¸°ë³¸ì ìœ¼ë¡œ MSE lossë¥¼ ì‚¬ìš©í•˜ëŠ”ë°, ì™œ ê·¸ëŸ´ê¹Œ?
ë‹¨ìˆœíˆ "ì„±ëŠ¥ì´ ì˜ ë‚˜ì™€ì„œ"ê°€ ì•„ë‹ˆë¼, **ì´ë¡ ì  ë°°ê²½**ì´ ìˆë‹¤.

> [ë§í¬ë“œì¸ ì´ì œí—Œë‹˜ ê¸€]
> error termì„ Gaussianìœ¼ë¡œ ê°€ì •í•˜ê³  maximum likelihood estimation ê´€ì ì—ì„œ í•´ì„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. 
> ê·¸ ê°€ì •ì˜ ë°°ê²½ì—ëŠ” ì¤‘ì‹¬ê·¹í•œì •ë¦¬(central limit theorem)ê°€ ìˆìŠµë‹ˆë‹¤. 
> linear regressionê³¼ ê°™ì€ ìƒê°ì˜ íë¦„ì„ deep networksì—ì„œë„ ì ìš©í•  ìˆ˜ ìˆëŠ” ê²ƒê¹Œì§€ ì´í•´í•˜ë©´, deep networksì—ì„œë„ regression ë¬¸ì œë¥¼ í’€ ë•Œ MSE lossë¥¼ ì“°ëŠ” ê²Œ ìì—°ìŠ¤ëŸ¬ì›Œì§‘ë‹ˆë‹¤. 
> ê·¸ë¦¬ê³ , MSE lossì™¸ì— ë‹¤ë¥¸ lossë¥¼ ì“°ë ¤ë©´ ì¶©ë¶„í•œ rationaleì´ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì´í•´í•˜ê²Œ ë©ë‹ˆë‹¤.

---

## 1. MSE Lossì˜ ì´ë¡ ì  ë°°ê²½

### 1.1 ì¤‘ì‹¬ê·¹í•œì •ë¦¬(Central Limit Theorem)

ì¸¡ì • ì˜¤ì°¨ëŠ” ì¼ë°˜ì ìœ¼ë¡œ **ì—¬ëŸ¬ ì‘ì€ ë…ë¦½ì ì¸ ìš”ì¸ë“¤ì˜ í•©**ìœ¼ë¡œ êµ¬ì„±ë¨:

```
ì˜¤ì°¨ = ì„¼ì„œ ë…¸ì´ì¦ˆ + í™˜ê²½ ë³€í™” + ì¸¡ì • íƒ€ì´ë° + ...
```

ì¤‘ì‹¬ê·¹í•œì •ë¦¬ì— ë”°ë¥´ë©´, 
> ë…ë¦½ì ì¸ í™•ë¥ ë³€ìˆ˜ë“¤ì˜ í•©(í‘œë³¸í‰ê· )ì€ í‘œë³¸ í¬ê¸°ê°€ ì¶©ë¶„íˆ í¬ë©´ **ì •ê·œë¶„í¬(Gaussian distribution)**ì— ê·¼ì‚¬í•©ë‹ˆë‹¤.

ë”°ë¼ì„œ:
- ì˜¤ì°¨í•­ Îµ ~ N(0, ÏƒÂ²)ë¡œ ê°€ì •í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì 
- ì´ëŠ” ë‹¨ìˆœí•œ í¸ì˜ê°€ ì•„ë‹ˆë¼ **í†µê³„í•™ì  ê·¼ê±°**ê°€ ìˆëŠ” ê°€ì •

### 1.2 Maximum Likelihood Estimation (MLE)

íšŒê·€ ëª¨ë¸ì„ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```
y = f(x; Î¸) + Îµ,  Îµ ~ N(0, ÏƒÂ²)
```

ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ í™•ë¥  ëª¨ë¸ë¡œ í•´ì„ë©ë‹ˆë‹¤:

```
p(y | x, Î¸) = N(f(x; Î¸), ÏƒÂ²)
```

**Likelihood function**:

```
L(Î¸) = âˆáµ¢ p(yáµ¢ | xáµ¢, Î¸) = âˆáµ¢ (1/âˆš(2Ï€ÏƒÂ²)) exp(-(yáµ¢ - f(xáµ¢; Î¸))Â²/(2ÏƒÂ²))
```

**Log-likelihood**:

```
log L(Î¸) = Î£áµ¢ [-1/2 log(2Ï€ÏƒÂ²) - (yáµ¢ - f(xáµ¢; Î¸))Â²/(2ÏƒÂ²)]
```

**MLEëŠ” log-likelihoodë¥¼ ìµœëŒ€í™”**í•˜ëŠ”ë°, ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬ë¨:

```
maximize log L(Î¸) âŸº minimize Î£áµ¢ (yáµ¢ - f(xáµ¢; Î¸))Â²
```

> **ğŸ’¡ í•µì‹¬ ê°œë…**  
> **MSE lossëŠ” Gaussian noise ê°€ì • í•˜ì—ì„œ MLEì˜ ê²°ê³¼**. ì¦‰, MSEë¥¼ ìµœì†Œí™”í•˜ëŠ” ê²ƒ = ë°ì´í„°ì˜ likelihoodë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒ.

### 1.3 MSEì™€ ÏƒÂ²ì˜ ê´€ê³„

ëª¨ë¸ì—ì„œ Îµ ~ N(0, ÏƒÂ²)ë¡œ ê°€ì •í–ˆì„ ë•Œ, **MSEëŠ” ì¼ë°˜ì ìœ¼ë¡œ ÏƒÂ²ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ë‹¤**.

**Bias-Variance Decomposition** ê´€ì ì—ì„œ:

$$
E[(y - \hat{y})^2] = \underbrace{(E[\hat{y}] - f(x))^2}_{\text{Bias}^2} + \underbrace{\text{Var}(\hat{y})}_{\text{Variance}} + \underbrace{\sigma^2}_{\text{Irreducible Error}}
$$

ì´ë¥¼ ë‹¤ìŒê³¼ ê°™ì´ ì •ë¦¬í•  ìˆ˜ ìˆë‹¤:

$$
\text{ì´ ì˜¤ì°¨ (MSE)} = \text{ëª¨ë¸ ì˜¤ì°¨} + \text{ë°ì´í„° ì˜¤ì°¨}
$$

$$
\text{MSE} = \underbrace{\text{Bias}^2 + \text{Variance}}_{\text{ëª¨ë¸ ì˜¤ì°¨}} + \underbrace{\sigma^2}_{\text{ë°ì´í„° ì˜¤ì°¨}}
$$

ì—¬ê¸°ì„œ:
- **ëª¨ë¸ ì˜¤ì°¨ (Reducible Error)**: ëª¨ë¸ ìì²´ì˜ í•œê³„ë¡œ ì¸í•œ ì˜¤ì°¨
  - **BiasÂ²**: ëª¨ë¸ì˜ ì˜ˆì¸¡ í‰ê· ê³¼ ì‹¤ì œ í•¨ìˆ˜ f(x) ê°„ì˜ ì°¨ì´ (ëª¨ë¸ì´ ë°ì´í„° ìƒì„± ê³¼ì •ì„ ì–¼ë§ˆë‚˜ ì˜ ê·¼ì‚¬í•˜ëŠ”ì§€)
  - **Variance**: ëª¨ë¸ ì˜ˆì¸¡ê°’ì˜ ë¶„ì‚° (í›ˆë ¨ ë°ì´í„°ì˜ ì‘ì€ ë³€í™”ì— ì–¼ë§ˆë‚˜ ë¯¼ê°í•œì§€)
- **ë°ì´í„° ì˜¤ì°¨ (Irreducible Error)**: ÏƒÂ²ë¡œ í‘œí˜„ë˜ëŠ” ë°ì´í„° ìì²´ì˜ ë…¸ì´ì¦ˆë¡œ ì¸í•œ ì˜¤ì°¨. ëª¨ë¸ì´ ì•„ë¬´ë¦¬ ì™„ë²½í•´ë„ ì¤„ì¼ ìˆ˜ ì—†ëŠ” ì˜¤ì°¨

### ë¶ˆí™•ì‹¤ì„± ê´€ì ì—ì„œì˜ í•´ì„

Bias-Variance Decompositionì˜ ê° êµ¬ì„±ìš”ì†Œë¥¼ ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§ ê´€ì ì—ì„œ í•´ì„í•  ìˆ˜ ìˆë‹¤:

| í†µê³„ì  êµ¬ì„±ìš”ì†Œ               | ì˜ë¯¸                     | ë¶ˆí™•ì‹¤ì„±(ëª¨ë¸ë§) ê´€ì                          |
| ---------------------- | ---------------------- | ------------------------------------ |
| BiasÂ²                  | ëª¨ë¸ì˜ êµ¬ì¡°ì  í•œê³„ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤ì°¨ | **Epistemic Uncertainty** (ì§€ì‹ ë¶€ì¡±)    |
| Variance               | í›ˆë ¨ ë°ì´í„° ë³€í™”ì— ëŒ€í•œ ë¯¼ê°ë„      | **Epistemic Uncertainty** (ë°ì´í„° ë¶€ì¡±)   |
| Irreducible Error (ÏƒÂ²) | ë°ì´í„° ìƒì„± ê³¼ì • ìì²´ì˜ ë…¸ì´ì¦ˆ      | **Aleatoric Uncertainty** (ë³¸ì§ˆì  ë¬´ì‘ìœ„ì„±) |

**í•µì‹¬ í¬ì¸íŠ¸**:
- ëª¨ë¸ì´ ì™„ë²½í•˜ë©´ (Bias = 0, Variance = 0): **MSE = ÏƒÂ²**
- ì‹¤ì œë¡œëŠ” ëª¨ë¸ì´ ì™„ë²½í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ: **MSE â‰¥ ÏƒÂ²**
- ë”°ë¼ì„œ ÏƒÂ²ëŠ” MSEì˜ **í•˜í•œì„ (lower bound)**ì…ë‹ˆë‹¤

ì‹¤ì œ íšŒê·€ ë¬¸ì œì—ì„œ:
- ëª¨ë¸ì´ ë°ì´í„° ìƒì„± ê³¼ì •ì„ ì™„ë²½íˆ í•™ìŠµí•˜ë©´ â†’ MSE â‰ˆ ÏƒÂ²
- ëª¨ë¸ì´ ë¶€ì¡±í•˜ê±°ë‚˜ ê³¼ì í•©ë˜ë©´ â†’ MSE > ÏƒÂ²

### 1.4 MSEëŠ” ì¡°ê±´ë¶€ í‰ê· ì„ ì¶”ì •

MSE lossì˜ ë˜ ë‹¤ë¥¸ ì¤‘ìš”í•œ íŠ¹ì„±:

```
argmin E[(y - Å·)Â²] = E[y | x]  (ì¡°ê±´ë¶€ í‰ê· )
```

ì¦‰, **MSE lossë¥¼ ìµœì†Œí™”í•˜ë©´ ì¡°ê±´ë¶€ í‰ê· (conditional mean)**ì„ ì¶”ì •í•˜ê²Œ ë¨.

ì´ ë¶€ë¶„ì€ ì§ê´€ì ìœ¼ë¡œë„ ì´í•´í•´ë³¼ ìˆ˜ ìˆìŒ.
y = 1, 3, 8, 10, 100 ë“±ì´ ìˆì„ ë•Œ mseë¥¼ ìµœì†Œí™”í•˜ëŠ” y hatì€ meanì´ ë˜ê¸° ë•Œë¬¸.

---

## 2. Loss Function ë¹„êµ

### 2.1 ìˆ˜í•™ì  ì •ì˜

| Loss | ìˆ˜ì‹ | ë¯¸ë¶„ | ë…¸ì´ì¦ˆ ë¶„í¬ |
|------|------|------|------------|
| **MSE** | $\frac{1}{n}\sum(y_i - \hat{y}_i)^2$ | $2(y - \hat{y})$ | Gaussian: $p(\epsilon) \propto \exp(-\epsilon^2/2\sigma^2)$ |
| **MAE** | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | $\text{sign}(y - \hat{y})$ | Laplace: $p(\epsilon) \propto \exp(-\|\epsilon\|/b)$ |
| **Huber** | $L_\delta = \begin{cases} \frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\ \delta|y - \hat{y}| - \frac{1}{2}\delta^2 & \text{if } |y - \hat{y}| > \delta \end{cases}$ | Smooth | Robust mixture |
| **Quantile** | $L_\tau = \sum_i \rho_\tau(y_i - \hat{y}_i)$ where $\rho_\tau(u) = u(\tau - \mathbf{1}\{u < 0\})$ | $\tau$ or $\tau-1$ | Asymmetric Laplace |

### 2.2 ë…¸ì´ì¦ˆ ë¶„í¬ ë¹„êµ

**Gaussian Distribution (MSE)**:
```
p(Îµ) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-ÎµÂ²/(2ÏƒÂ²))
```
- Tailì´ ë¹ ë¥´ê²Œ ê°ì†Œ (exponential decay of ÎµÂ²)
- Outlierì— ë¯¼ê° (ì œê³± penalty)

**Laplace Distribution (MAE)**:
```
p(Îµ) = (1/2b) exp(-|Îµ|/b)
```
- Tailì´ Gaussianë³´ë‹¤ ëŠë¦¬ê²Œ ê°ì†Œ (exponential decay of |Îµ|)
- Outlierì— robust (ì„ í˜• penalty)

**Huber Lossì˜ ë™ì‘ ì›ë¦¬**:
```
L_Î´(y, Å·) = {
  1/2 Â· (y - Å·)Â²          if |y - Å·| â‰¤ Î´  (ì‘ì€ ì˜¤ì°¨: MSEì²˜ëŸ¼)
  Î´ Â· |y - Å·| - 1/2 Â· Î´Â²  if |y - Å·| > Î´  (í° ì˜¤ì°¨: MAEì²˜ëŸ¼)
}
```
- Î´ëŠ” threshold parameter (ì¼ë°˜ì ìœ¼ë¡œ 1.0 ë˜ëŠ” 1.35)
- ì‘ì€ ì˜¤ì°¨: squared penalty (MSEì˜ íš¨ìœ¨ì„±)
- í° ì˜¤ì°¨: linear penalty (MAEì˜ robustness)
- **ëª©ì **: MSEì™€ MAEì˜ ì¥ì ì„ ê²°í•©

**Quantile Loss(Pinball loss)ì˜ ë™ì‘ ì›ë¦¬**:
```
L_Ï„(y, Å·) = Î£áµ¢ Ï_Ï„(yáµ¢ - Å·áµ¢)

where Ï_Ï„(u) = {
  Ï„ Â· u      if u â‰¥ 0  (ê³¼ì†Œì˜ˆì¸¡ì— ëŒ€í•œ penalty)
  (Ï„-1) Â· u  if u < 0  (ê³¼ëŒ€ì˜ˆì¸¡ì— ëŒ€í•œ penalty)
}
```
- Ï„ëŠ” quantile level (0 < Ï„ < 1)
- Ï„ = 0.5: MAEì™€ ë™ì¼ (ì¤‘ì•™ê°’)
- Ï„ â‰  0.5: ë¹„ëŒ€ì¹­ penalty (ì˜ˆ: Ï„=0.9ëŠ” ê³¼ì†Œì˜ˆì¸¡ì— ë” í° penalty)
- **ëª©ì **: íŠ¹ì • quantile ì¶”ì • ë° ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§

<figure>
  <img src="./images/laplace_vs_gaussian.png" alt="Laplace vs Gaussian noise distribution ë¹„êµ">
  <figcaption>Laplace ë¶„í¬(ì í•©: MAE)ì™€ Gaussian ë¶„í¬(ì í•©: MSE) í˜•íƒœ ë¹„êµ</figcaption>
</figure>

### 2.3 ì¶”ì •ëŸ‰ ë¹„êµ

| Loss | ì¶”ì •ëŸ‰ | ë…¸ì´ì¦ˆ ê°€ì • | Robustness | ì‚¬ìš© ì´ìœ  |
|------|--------|------------|------------|-----------|
| **MSE** | Conditional Mean | Gaussian | ë‚®ìŒ (Outlierì— ì•½í•¨) | ì•ˆì •ì , ì´ë¡  ê¸°ë°˜, ë¯¸ë¶„ ê°€ëŠ¥ |
| **MAE** | Conditional Median | Laplace | ë†’ìŒ | Outlierì— robust |
| **Quantile** | Conditional Quantile | Asymmetric Laplace | ì¡°ì ˆ ê°€ëŠ¥ | ë¦¬ìŠ¤í¬/ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§ |
| **Huber** | Hybrid | Robust mixture | ì¤‘ê°„ | MSEì™€ MAEì˜ ì¥ì  ê²°í•© |

---

## 3. Loss ì„ íƒì˜ ì›ì¹™

> Loss ì„ íƒì€ **ì‹¤í—˜ì´ ì•„ë‹ˆë¼ ê°€ì„¤ ê¸°ë°˜**ì´ì–´ì•¼ í•¨!

### 3.1 ì˜¬ë°”ë¥¸ ì ‘ê·¼

**"ì™œ MAEë¥¼ ì“°ë‚˜ìš”?"**
- âœ… Outlierê°€ ë§ì€ í™˜ê²½ì´ë¼ëŠ” **ë°ì´í„° ê¸°ë°˜ ê°€ì„¤**
- âœ… Laplace ë¶„í¬ë¥¼ ê°€ì •í•  ìˆ˜ ìˆëŠ” **ì´ë¡ ì  ê·¼ê±°**
- âœ… ì¤‘ì•™ê°’ì´ í‰ê· ë³´ë‹¤ ì í•©í•œ **ë„ë©”ì¸ íŠ¹ì„±**

**"ì™œ Quantile lossë¥¼ ì“°ë‚˜ìš”?"**
- âœ… ë¶ˆí™•ì‹¤ì„±/ë¶„í¬ tailì„ ëª¨ë¸ë§í•˜ë ¤ëŠ” **ëª…í™•í•œ ëª©ì **
- âœ… Upper/lower bound estimationì´ í•„ìš”í•œ **ë¹„ì¦ˆë‹ˆìŠ¤ ìš”êµ¬ì‚¬í•­**
- âœ… Risk-sensitive decision making

**"ì™œ Huber lossë¥¼ ì“°ë‚˜ìš”?"**
- âœ… ëŒ€ë¶€ë¶„ì€ Gaussianì´ì§€ë§Œ **ê°€ë” outlier**ê°€ ì¡´ì¬
- âœ… MSEì˜ íš¨ìœ¨ì„±ê³¼ MAEì˜ robustness **ë‘˜ ë‹¤** í•„ìš”
- âœ… ì‹¤ì œ ë°ì´í„° ë¶„í¬ê°€ **heavy-tailed Gaussian**

### 3.2 ì˜ëª»ëœ ì ‘ê·¼

âŒ "ì—¬ëŸ¬ lossë¥¼ ì‹¤í—˜í•´ë´¤ëŠ”ë° ì´ê²Œ 0.01% ë” ì¢‹ì•„ìš”"
- í†µê³„ì  ìœ ì˜ì„±ì´ ì—†ì„ ìˆ˜ ìˆìŒ
- ë‚´ì¼ì˜ ë°ì´í„°ì—ì„œëŠ” ì—­ì „ë  ìˆ˜ ìˆìŒ
- ê³¼ì í•©ì˜ ìœ„í—˜

> [ë§í¬ë“œì¸ ì´ì œí—Œë‹˜ ê¸€]
> Regressionì—ì„œ ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ ì•Šì„ ë•Œ **ê·¸ì € ì˜ë˜ê¸°ë¥¼ ë°”ë¼ë©´ì„œ ë‹¤ì–‘í•œ lossë¥¼ ì‹¤í—˜í•´ë³´ëŠ” ê²ƒì€ ì§€ì–‘**í•´ì•¼ í•©ë‹ˆë‹¤.  
> 
> íšŒì‚¬ì—ì„œì˜ AIëŠ” ì˜¤ëŠ˜ì˜ ë°ì´í„°ì—ë§Œ ì˜ ì‘ë™í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ **ë‚´ì¼ê³¼ Nì¼ í›„ì˜ ë°ì´í„°ì—ì„œë„ ì˜ ë™ì‘**í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, marginalí•œ í–¥ìƒë³´ë‹¤ëŠ” **ì•ˆì •ì ìœ¼ë¡œ ì‘ë™í•˜ëŠ” ë°©ë²•ë¡ **ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ê¸°ë³¸ì…ë‹ˆë‹¤.

ì˜¬ë°”ë¥¸ ìˆœì„œ:
```
ê°€ì„¤ ìˆ˜ë¦½ â†’ ìˆ˜í•™ì  ë°°ê²½ í™•ì¸ â†’ ëª¨ë¸ë§ ëª©ì  ì •ì˜ â†’ Loss ì„ íƒ â†’ ì‹¤í—˜ ê²€ì¦
```

---

## 4. ì‹¤ì „ í™œìš©

### 4.1 MAE LossëŠ” ì–¸ì œ?

> [ë§í¬ë“œì¸ ì´ì œí—Œë‹˜ ê¸€]
> ê·¸ë ‡ë‹¤ë©´ MAE lossëŠ” ì–¸ì œ í•©ë¦¬ì ì¼ê¹Œìš”? MSEëŠ” ì¡°ê±´ë¶€ í‰ê· ì„, MAEëŠ” ì¡°ê±´ë¶€ ì¤‘ì•™ê°’ì„ ì¶”ì •í•©ë‹ˆë‹¤. 
> ì¤‘ì•™ê°’ì´ í‰ê· ë³´ë‹¤ ì í•©í•œ ìƒí™©, ì˜ˆë¥¼ ë“¤ì–´ heavy-tailed noiseë‚˜ outlierê°€ ë§ì€ ê²½ìš°ë¼ë©´ MAEê°€ ìì—°ìŠ¤ëŸ¬ìš´ ì„ íƒì´ ë©ë‹ˆë‹¤. 
> ì´ ê°œë…ì„ ì´í•´í•˜ë©´ MAE lossë¥¼ ì¼ë°˜í™”í•´ quantile regressionê¹Œì§€ë„ í™•ì¥í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

MSEëŠ” **ì¡°ê±´ë¶€ í‰ê· **, MAEëŠ” **ì¡°ê±´ë¶€ ì¤‘ì•™ê°’**ì„ ì¶”ì •í•©ë‹ˆë‹¤.

**ì í•©í•œ ê²½ìš°**:
- Heavy-tailed noiseê°€ ì˜ˆìƒë˜ëŠ” ê²½ìš°
- Outlierê°€ ë§ì€ ë°ì´í„° (ì˜ˆ: ê¸ˆìœµ, ì„¼ì„œ ë°ì´í„°)
- ì¤‘ì•™ê°’ì´ í‰ê· ë³´ë‹¤ ì˜ë¯¸ ìˆëŠ” ë„ë©”ì¸
- Robustnessê°€ ì¤‘ìš”í•œ ê²½ìš°

**ì˜ˆì‹œ**:
```python
# Outlierì— ë¯¼ê°í•˜ì§€ ì•Šì€ ëª¨ë¸ì´ í•„ìš”í•  ë•Œ
loss = nn.L1Loss()  # MAE
```

### 4.2 Quantile Regression

> MAE lossë¥¼ ì¼ë°˜í™”í•˜ë©´ **quantile regression**ê¹Œì§€ í™•ì¥ì´ ê°€ëŠ¥í•˜ë‹¤.

```python
def quantile_loss(y_true, y_pred, quantile):
    error = y_true - y_pred
    return torch.mean(torch.max(quantile * error, (quantile - 1) * error))
```

**Quantile loss**:
```
L_Ï„(y, Å·) = Î£áµ¢ Ï_Ï„(yáµ¢ - Å·áµ¢)

where Ï_Ï„(u) = u(Ï„ - ğŸ™{u < 0})
            = Ï„Â·u      if u â‰¥ 0
              (Ï„-1)Â·u  if u < 0
```

- Ï„ = 0.5: MAEì™€ ë™ì¼ (ì¤‘ì•™ê°’)
- Ï„ = 0.9: 90th percentile ì¶”ì •
- Ï„ = 0.1: 10th percentile ì¶”ì •

### 4.3 ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§

> [ë§í¬ë“œì¸ ì´ì œí—Œë‹˜ ê¸€]
> ê·¸ëŸ¼ ì´ê±¸ ì•Œë©´ ì–´ë–¤ ë¶€ë¶„ì—ì„œ í˜„ì—…ì— ë„ì›€ì„ ë°›ì„ ìˆ˜ ìˆì„ê¹Œìš”? ìš°ë¦¬ëŠ” ì¢…ì¢… deep networksì—ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•´ì•¼í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. 
> ì´ ë•Œ quantile regressionì„ í•œê°€ì§€ ë°©ë²•ìœ¼ë¡œ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
> íŠ¹íˆ, UCB(upper confidence bound)ê°€ í•„ìš”í•œ ìƒí™©ì—ì„œ ì•„ì£¼ ê°„ë‹¨í•˜ê²Œ ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•˜ê³  baselineìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. 
> ì´ëŸ° ë‚´ìš©ë“¤ì— ëŒ€í•œ ì´í•´ê°€ ì–•ìœ¼ë©´, ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•˜ê¸° ìœ„í•´ ê°€ì¥ ìœ ëª…í•œ monte carlo dropout ë°©ë²•ì„ ì¼ë‹¨ ì‹œë„í•˜ê²Œ ë˜ê³ , í˜„ì‹¤ì ìœ¼ë¡œ overconfident ë¬¸ì œ ë“± ì œëŒ€ë¡œ ë¶ˆí™•ì‹¤ì„±ì´ ëª¨ë¸ë§ì´ ì•ˆë˜ëŠ” í˜„ìƒì„ ê²½í—˜í•˜ë©´ì„œ ì‹œí–‰ì°©ì˜¤ê°€ ëŠ˜ì–´ë‚©ë‹ˆë‹¤.

Deep networksì—ì„œ ë¶ˆí™•ì‹¤ì„±ì„ ëª¨ë¸ë§í•´ì•¼ í•  ë•Œ:

**ë°©ë²• 1: Quantile Regression (ê°„ë‹¨, baseline)**
```python
# Upper Confidence Bound (UCB) ì¶”ì •
model_upper = train_with_quantile_loss(quantile=0.9)
model_lower = train_with_quantile_loss(quantile=0.1)

# Confidence interval
confidence_interval = model_upper(x) - model_lower(x)
```

**ì¥ì **:
- êµ¬í˜„ ê°„ë‹¨, ë¹ ë¥¸ baseline
- UCBê°€ í•„ìš”í•œ ìƒí™©ì—ì„œ ì¦‰ì‹œ í™œìš© ê°€ëŠ¥
- í•´ì„ì´ ëª…í™•

**ë°©ë²• 2: Monte Carlo Dropout (ë³µì¡)**
- ì´ë¡ ì  ì´í•´ ì—†ì´ ì‹œë„í•˜ë©´ overconfident ë¬¸ì œ ë°œìƒ
- ë¶ˆí™•ì‹¤ì„±ì´ ì œëŒ€ë¡œ ëª¨ë¸ë§ ì•ˆ ë  ìˆ˜ ìˆìŒ
- ì‹œí–‰ì°©ì˜¤ ì¦ê°€

> **ğŸ’¡ íŒ**  
> ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§ì´ í•„ìš”í•˜ë‹¤ë©´, **quantile regressionì„ ë¨¼ì € baseline**ìœ¼ë¡œ ì‹œë„í•´ë³´ì„¸ìš”. ê°„ë‹¨í•˜ê³  í•´ì„ ê°€ëŠ¥í•˜ë©°, UCB/LCBë¥¼ ì§ì ‘ ì¶”ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## 5. í•µì‹¬ ì •ë¦¬

### MLE ê´€ì ì—ì„œ ì´í•´

```
Gaussian noise ê°€ì •
    â†“
Maximum Likelihood Estimation
    â†“
MSE Loss ë„ì¶œ
```

### Loss ì„ íƒ ì²´í¬ë¦¬ìŠ¤íŠ¸

1. âœ… **ë°ì´í„° íŠ¹ì„± íŒŒì•…**: ë…¸ì´ì¦ˆ ë¶„í¬ëŠ”? Outlierê°€ ë§ì€ê°€?
2. âœ… **ë„ë©”ì¸ ì§€ì‹ í™œìš©**: í‰ê·  vs ì¤‘ì•™ê°’ ì¤‘ ë¬´ì—‡ì´ ì˜ë¯¸ ìˆëŠ”ê°€?
3. âœ… **ëª¨ë¸ë§ ëª©ì  ì •ì˜**: Point estimate? Uncertainty? Quantile?
4. âœ… **ì´ë¡ ì  ê·¼ê±° í™•ë¦½**: ì™œ ì´ lossê°€ ì í•©í•œê°€?
5. âœ… **ì‹¤í—˜ ê²€ì¦**: ê°€ì„¤ì´ ë§ëŠ”ì§€ í™•ì¸

### ê¸°ì–µí•  ì 

> **ğŸ“Œ ìš”ì•½**  
> - **MSE**: ì´ë¡ ì  ê¸°ë°˜ì´ íƒ„íƒ„, ì•ˆì •ì , ê¸°ë³¸ ì„ íƒ  
> - **MAE**: Outlierì— robust, ì¤‘ì•™ê°’ ì¶”ì •, ê·¼ê±° í•„ìš”  
> - **Quantile**: ë¶ˆí™•ì‹¤ì„± ëª¨ë¸ë§, ëª…í™•í•œ ëª©ì  í•„ìš”  
> - **ì‹¤í—˜ë§Œìœ¼ë¡œëŠ” ë¶€ì¡±**: ê°€ì„¤ â†’ ì´ë¡  â†’ ì‹¤í—˜

> Loss function ì„ íƒì€ ë‹¨ìˆœí•œ ì‹¤í—˜ì´ ì•„ë‹ˆë¼ **ë°ì´í„°ì— ëŒ€í•œ ê°€ì •ê³¼ ëª¨ë¸ë§ ëª©ì ì— ëŒ€í•œ ì´í•´**ì—ì„œ ì‹œì‘ë˜ì–´ì•¼ í•¨.



